<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="VcC-PHB4Om9SIR3Roqm7k1N-SHiBtQ6c3LJLVMKgU4U" />










  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="数学," />





  <link rel="alternate" href="/atom.xml" title="吴良超的学习笔记" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="之前曾写过一篇最优化课程总结， 涉及到的内容较多也较细。而在最优化中，凸优化是最为常见而又最为重要的，因为凸优化有一个良好的性质：局部最优是全局最优，这个性质使得我们不需要去证明解是否会收敛到全局最优，或者如何避免局部最优。因此凸优化有广泛应用，在优化问题不是凸的时候，往往也会尝试将其变为凸问题便于求解。本文着重讲凸优化，算是对之前写的文章的一个拓展和补充。
本文主要讲述下面内容，凸优化的概念以及">
<meta property="og:type" content="article">
<meta property="og:title" content="凸优化总结">
<meta property="og:url" content="http://wulc.github.io/2017/05/20/凸优化总结/index.html">
<meta property="og:site_name" content="吴良超的学习笔记">
<meta property="og:description" content="之前曾写过一篇最优化课程总结， 涉及到的内容较多也较细。而在最优化中，凸优化是最为常见而又最为重要的，因为凸优化有一个良好的性质：局部最优是全局最优，这个性质使得我们不需要去证明解是否会收敛到全局最优，或者如何避免局部最优。因此凸优化有广泛应用，在优化问题不是凸的时候，往往也会尝试将其变为凸问题便于求解。本文着重讲凸优化，算是对之前写的文章的一个拓展和补充。
本文主要讲述下面内容，凸优化的概念以及">
<meta property="og:image" content="http://static.zybuluo.com/WuLiangchao/lp79qxqog11jlwzn5h5rz43v/image_1bgff3run1u5f8ivpt6tb19it9.png">
<meta property="og:image" content="http://static.zybuluo.com/WuLiangchao/cm2in2nvs6jvmj74fh3xyfig/image_1bgfffffmlao1ka2l241maffrm.png">
<meta property="og:image" content="http://static.zybuluo.com/WuLiangchao/mhmbxi6ktuh7emo2cyga4g43/image_1bgfgj5e31lhe1ti15q81t5fig613.png">
<meta property="og:image" content="http://static.zybuluo.com/WuLiangchao/ubhj5hvtsoexyinqkslvn1va/image_1bgidgkp7h8p1cg71gji1qfl9i9.png">
<meta property="og:image" content="http://static.zybuluo.com/WuLiangchao/e8mkxz323dnq2elgygyy8bax/image_1bgii8g37ine1569fr97gd12881g.png">
<meta property="og:image" content="http://static.zybuluo.com/WuLiangchao/cz31xv8iwvmnq7e15t8tokk3/image_1bgiiehmc19eb1j631gejubdc191t.png">
<meta property="og:image" content="http://static.zybuluo.com/WuLiangchao/6a4swr0b88hktwgjarqapshg/image_1bgiiioee1oir157n1ij714db14982a.png">
<meta property="og:image" content="http://static.zybuluo.com/WuLiangchao/0zmk660ifcvm1y4vo3m5sy1z/image_1bgiinl7i1ejimgncpdp0v13b62n.png">
<meta property="og:updated_time" content="2018-12-21T11:53:29.199Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="凸优化总结">
<meta name="twitter:description" content="之前曾写过一篇最优化课程总结， 涉及到的内容较多也较细。而在最优化中，凸优化是最为常见而又最为重要的，因为凸优化有一个良好的性质：局部最优是全局最优，这个性质使得我们不需要去证明解是否会收敛到全局最优，或者如何避免局部最优。因此凸优化有广泛应用，在优化问题不是凸的时候，往往也会尝试将其变为凸问题便于求解。本文着重讲凸优化，算是对之前写的文章的一个拓展和补充。
本文主要讲述下面内容，凸优化的概念以及">
<meta name="twitter:image" content="http://static.zybuluo.com/WuLiangchao/lp79qxqog11jlwzn5h5rz43v/image_1bgff3run1u5f8ivpt6tb19it9.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://wulc.github.io/2017/05/20/凸优化总结/"/>





  <title> 凸优化总结 | 吴良超的学习笔记 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  








  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1258114456&web_id=1258114456" language="JavaScript"></script>
  </div>





  
  
    
  

  <div class="container one-collumn sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">吴良超的学习笔记</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <h1 class="site-subtitle" itemprop="description"></h1>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            站内搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://wulc.github.io/2017/05/20/凸优化总结/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="良超">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="http://wulc.me/files/profile.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="吴良超的学习笔记">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="吴良超的学习笔记" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
            
            
              
                凸优化总结
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-20T20:08:55+08:00">
                2017-05-20
              </time>
            

            

            
          </span>


          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-tags"></i>
              </span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/tags/数学/" itemprop="url" rel="index">
                    <span itemprop="name">数学</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>之前曾写过一篇<a href="http://wulc.me/2017/02/01/%E6%9C%80%E4%BC%98%E5%8C%96%E8%AE%A1%E7%AE%97%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/" target="_blank" rel="external">最优化课程总结</a>， 涉及到的内容较多也较细。而在最优化中，凸优化是最为常见而又最为重要的，因为凸优化有一个良好的性质：<strong>局部最优是全局最优</strong>，这个性质使得我们不需要去证明解是否会收敛到全局最优，或者如何避免局部最优。因此凸优化有广泛应用，在优化问题不是凸的时候，往往也会尝试将其变为凸问题便于求解。本文着重讲凸优化，算是对之前写的文章的一个拓展和补充。</p>
<p>本文主要讲述下面内容，凸优化的概念以及凸优化中的三类常见解法：<strong>梯度类方法，对偶方法和ADMM方法</strong>。</p>
<a id="more"></a>
<h2 id="凸集，凸函数与凸优化"><a href="#凸集，凸函数与凸优化" class="headerlink" title="凸集，凸函数与凸优化"></a>凸集，凸函数与凸优化</h2><h3 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h3><p>凸集的定义非常清楚</p>
<blockquote>
<p>对于集合 $K$ ，$\forall x_1,x_2 \in K$,若 $\alpha x_1 + (1-\alpha)x_2 \in K$,其中$\alpha \in [0,1])$,则 $K$ 为凸集</p>
</blockquote>
<p>即集合中任意两点的连线均在凸集中，如在下图中左边的是凸集而右边的不是</p>
<p><img src="http://static.zybuluo.com/WuLiangchao/lp79qxqog11jlwzn5h5rz43v/image_1bgff3run1u5f8ivpt6tb19it9.png" alt="凸集概念"></p>
<p>有时候需要对某个凸集进行放缩转换等操作，对凸集进行以下操作后，得到的集合依然是凸集</p>
<ol>
<li>凸集的重叠（intersection）部分任然为凸集</li>
<li>若 $C$ 为凸集，则 $$aC+b = \lbrace ax+b , x \in C, \forall a, b\rbrace$$ 也为凸集</li>
<li>对于函数 $f(x) = Ax+b$, 若 $C$ 为凸集，则下面得到的转换也为凸集，注意这里的 $A$ 是矩阵$$f(C) = \lbrace f(x):x\in C\rbrace$$<br>而当 $D$ 是一个凸集的时候，下面得到的转换也是凸集$$f^{-1}(D) = \lbrace x: f(x)\in D\rbrace$$这两个转换互为逆反关系</li>
</ol>
<p>常见的凸集有下面这些(下式中 $a, x, b$ 均为向量, $A$ 为矩阵)</p>
<ul>
<li>点（point）、线（line）、面（plane）</li>
<li>norm ball: $\lbrace x: ||x|| \le r\rbrace$</li>
<li>hyperplane: $\lbrace x: a^Tx=b\rbrace$</li>
<li>halfspace: $\lbrace x: a^Tx \le b\rbrace$</li>
<li>affine space: $\lbrace x: Ax = b\rbrace$</li>
<li>polyhedron: $\lbrace x: Ax &lt; b\rbrace$</li>
</ul>
<blockquote>
<p>polyheron 的图像为<br><img src="http://static.zybuluo.com/WuLiangchao/cm2in2nvs6jvmj74fh3xyfig/image_1bgfffffmlao1ka2l241maffrm.png" alt="polhedron"></p>
</blockquote>
<h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p>凸函数的定义如下</p>
<blockquote>
<p>设$f(x)$为定义在n维欧氏空间中某个凸集S上的函数，若对于任何实数$\alpha(0&lt;\alpha&lt;1)$以及S中的任意不同两点 $x$ 和 $y$，均有$$f(\alpha x+ (1-\alpha)y) \le \alpha f(x) + (1-\alpha)f(y)$$则称$f(x)$为定义在凸集 S 上的凸函数</p>
</blockquote>
<p>凸函数的定义也很好理解，任意两点的连线必然在函数的上方，如下是一个典型的凸函数</p>
<p><img src="http://static.zybuluo.com/WuLiangchao/mhmbxi6ktuh7emo2cyga4g43/image_1bgfgj5e31lhe1ti15q81t5fig613.png" alt="凸函数"></p>
<p><strong>严格凸(strictly convex)与強凸(strongly convex)</strong></p>
<ul>
<li>严格凸指的是假如将上面不等式中的 $\le$ 改为 $\lt$， 则称该函数为严格凸函数。</li>
<li>严格凸指的是$\forall m &gt; 0, f - \frac{m}{2}||x||_2^2$ 也是凸的，其含义就是该凸函数的“凸性”比二次函数还要强，即使减去一个二次函数还是凸函数</li>
</ul>
<p>凸函数有几个非常重要的性质，对于一个凸函数 $f$, 其重要性质</p>
<ol>
<li><strong>一阶特性（First-order characterization）</strong>： $$f(y) \ge f(x) + \nabla f(x)(y - x)$$</li>
<li><strong>二阶特性（Second-order characterization）</strong>： $$\nabla^2f(x) \succeq 0$$这里的 $\succeq 0$ 表示 Hessian 矩阵是半正定的。</li>
<li><strong>Jensen不等式（<a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="external">Jensen’s inequality</a>）</strong>：$$f(E(x)) \le E(f(x))$$这里的 $E$ 表示的是期望，这是从凸函数拓展到概率论的一个推论，这里不详细展开。</li>
<li><strong>sublevel sets</strong>，即集合 $\lbrace x:f(x) \le t\rbrace$ 是一个凸集</li>
</ol>
<p>其中，<strong>一阶特性或二阶特性是一个函数为凸函数的充要条件，通常用来证明一个函数是凸函数。</strong></p>
<p>常见的凸函数有下面这些</p>
<ul>
<li>仿射函数( Affine function ): $a^Tx + b$</li>
<li>二次函数( quadratic function),注意这里的 $Q$ 必须为半正定矩阵: $\frac{1}{2}x^TQx + b^Tx+c(Q \succeq 0)$</li>
<li>最小平方误差( Least squares loss ): $||y-Ax||_2^2$ (总是凸的，因为 $A^TA$ 总是半正定的)</li>
<li>示性函数（Indicator function）：$$I_C(X) = \begin{cases} 0&amp;x \in C\\<br>\infty &amp; x \notin C\end{cases}$$</li>
<li>max function: $f(x) = max \lbrace x_1,…x_n \rbrace$</li>
<li>范数（Norm）：范数分为向量范数和矩阵范数，任意范数均为凸的，各种范数的定义如下</li>
</ul>
<p><strong>向量范数</strong></p>
<blockquote>
<p>0范数：$||x||_0 $= 向量中非零元素的个数<br>1范数： $||x||_1 = \sum_{i=1}^n |x_i|$<br>$p$ 范数：$||x||_p = (\sum_{i=1}^nx_i^p)^{1/p}~~(p &gt; 1)$<br>无穷范数: $||x||_{\infty} = max_{i=1,…n} |x_i|$</p>
</blockquote>
<p><strong>矩阵范数</strong></p>
<blockquote>
<p>核(nuclear)范数: $||X||_{tr} = \sum_{i=1}^{r}\sigma_i(X)$ , ($\sigma_i(X)$是矩阵分解后的奇异值,核范数即为矩阵所有奇异值之和)<br>谱（spectral）范数：$||X||_{op} = max_{i=1,…r}\sigma_i(X)$, 即为最大的奇异值</p>
</blockquote>
<h3 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h3><p>对于下面的优化问题</p>
<p>$$<br>\begin{align*}<br>&amp;\min_x\quad f(x)\\<br>&amp;\begin{array}\\<br>s.t.&amp;g_i(x) \le 0,~i=1,\ldots,m\\<br>&amp;h_j(x)=0,~j=1,\ldots,r<br>\end{array}<br>\end{align*}<br>$$</p>
<p>当 $f(x), g_i(x)$ 均为凸函数， 而$h_j(x)$ 为仿射函数（affine function）时，该优化称为凸优化,注意上面的 $\min$ 以及约束条件的符号均要符合规定。</p>
<p>凸优化也可以解释为目标函数 $f(x)$ 为凸函数而起约束围成的可行域为一个凸集。</p>
<p>常见的一些凸优化问题有：<strong>线性规划（linear programs），二次规划（quadratic programs），半正定规划（semidefinite programs）</strong>，且 $LP \in QP \in SDP$, 即后者是包含前者的关系。</p>
<p>线性规划问题一般原型如下($c$为向量，$D,A$为矩阵)</p>
<p>$$<br>\begin{align*}<br>&amp;\min_x\quad c^Tx\\<br>&amp;\begin{array}\\<br>s.t.&amp;Dx \le d\\<br>&amp;Ax=b<br>\end{array}<br>\end{align*}<br>$$</p>
<p>二次规划问题一般原型如下（要求矩阵 $Q$ 半正定）</p>
<p>$$<br>\begin{align*}<br>&amp;\min_x\quad \frac{1}{2}x^TQx+c^Tx\\<br>&amp;\begin{array}\\<br>s.t.&amp;Dx \le d\\<br>&amp;Ax=b<br>\end{array}<br>\end{align*}<br>$$</p>
<p>而半正定规划问题一般原型如下($X$ 在这里表示矩阵)<br>$$<br>\begin{align*}<br>&amp;\min_X\quad CX\\<br>&amp;\begin{array}\\<br>s.t.&amp;A_iX \le b_i, i=1,…m\\<br>&amp;X \succeq 0<br>\end{array}<br>\end{align*}<br>$$</p>
<h2 id="梯度类方法"><a href="#梯度类方法" class="headerlink" title="梯度类方法"></a>梯度类方法</h2><p>梯度类方法是无约束优化中非常常用的方法，其依据的最根本的事实就是梯度的负方向是函数值下降最快的方向。但是常用的 <code>gradient descent</code> 必须要求函数的连续可导，而对于某些连续不可导的问题（如lasso  regression），<code>gradient descent</code> 无能为力，这是需要用到<code>subgradient descent</code>和<code>proximal gradient descent</code>.</p>
<h3 id="gradient-descent"><a href="#gradient-descent" class="headerlink" title="gradient descent"></a>gradient descent</h3><p>梯度下降法的迭代公式为 $$x^{(k)} = x^{(k-1)} - t_k\nabla f(x^{(k-1)} )$$</p>
<p>上式中上标 $(k)$ 表示第 $k$ 次迭代, 而 $t_k$表示步长，$\nabla f(x^{(k-1)} )$表示在点 $x^{(k-1)}$ 的梯度。</p>
<p>这里对于梯度下降主要讨论其步长选择的问题， 最简单直接的方式是固定每次的步长为一个恒定值，但是如果步长过大或过小时，可能会导致结果难以收敛或者收敛速度很慢。因此提出了可变长步长的方法，可变长步长的方法指的是根据每次迭代依照一定的规则改变步长，下面介绍两种：<code>backtracking line search</code> 和 <code>exact line serach</code>。</p>
<p><strong>backtracking line search</strong></p>
<p>backtracking line search 需要先选择两个固定的参数 $\alpha, \beta$, 要求 $0 &lt; \beta &lt; 1, 0&lt;\alpha&lt;1/2$</p>
<p>每次迭代的时候，假如下式成立</p>
<p>$$f(x - t\nabla f(x)) &gt; f(x) - \alpha t||\nabla f(x)||_2^2$$</p>
<p>则改变步长为 $t = \beta t$, 否则步长不变。</p>
<p>这种方法的思想是当步长过大的时候(即跨过了最优点)，减小步长，否则保持步长不变，如下式是一个简单的例子</p>
<p><img src="http://static.zybuluo.com/WuLiangchao/ubhj5hvtsoexyinqkslvn1va/image_1bgidgkp7h8p1cg71gji1qfl9i9.png" alt="backtracking line search"></p>
<p><strong>exact line serach</strong></p>
<p>exact line serach 则是得到先计算出梯度 $\nabla f(x^{(k-1)} )$,然后代入下面的函数中，此时只有步长 $t_k$ 是未知，因此可对 $t_k$ 进行求导并令其为0，求得的 $t_k$ 即为当前的最优的步长，因为这个步长令当前迭代下降的距离最大。</p>
<p>$$f(x^{(k-1)} - t_k\nabla f(x^{(k-1)} ))$$</p>
<p>这种方法也被称为最速下降法。</p>
<h3 id="subgradient-descent"><a href="#subgradient-descent" class="headerlink" title="subgradient descent"></a>subgradient descent</h3><p><strong>subgradient 可以说是 gradient 的升级版，用于解决求导时某些连续不可导的函数梯度不存在的问题</strong>，我们知道，对于可微的凸函数有一阶特性，即</p>
<p>$$f(y) \ge f(x) + \nabla^T f(x)(y - x)$$</p>
<p>加入将上面的 $\nabla^T f(x)$ 换成 $g^T$ 且不等式恒成立，则  $g$ 被称为 subgradient，当函数可微时，$\nabla f(x) = g$ ，但是若函数不可微，subgradient 不一定存在，下面是几个特殊函数的subgradient例子。</p>
<p>对于函数 $f(x) = |x|$,其图像如下</p>
<p><img src="http://static.zybuluo.com/WuLiangchao/e8mkxz323dnq2elgygyy8bax/image_1bgii8g37ine1569fr97gd12881g.png" alt="sub1"></p>
<p>其subgradient为 $$g = \begin{cases}sign(x) &amp;x \neq 0\\<br>[-1,1] &amp; x=0\end{cases}$$</p>
<p>对于函数 $f(x) = ||x||_2$,其图像如下</p>
<p><img src="http://static.zybuluo.com/WuLiangchao/cz31xv8iwvmnq7e15t8tokk3/image_1bgiiehmc19eb1j631gejubdc191t.png" alt="sub2"></p>
<p>其subgradient为 $$g = \begin{cases} x/||x||_2&amp;x \neq 0\\<br>\lbrace z: ||z||_2 \le 1\rbrace &amp; x=0\end{cases}$$</p>
<p>对于函数 $f(x) = ||x||_1$,其图像如下</p>
<p><img src="http://static.zybuluo.com/WuLiangchao/6a4swr0b88hktwgjarqapshg/image_1bgiiioee1oir157n1ij714db14982a.png" alt="sub3"></p>
<p>其subgradient为 $$g = \begin{cases} sign(x_i) &amp;x_i \neq 0\\<br>[-1,1] &amp; x_i=0\end{cases}$$</p>
<p>对于两个相交的函数 $f(x) = \max \lbrace f(x_1), f(x_2)\rbrace$,设其函数图像如下</p>
<p><img src="http://static.zybuluo.com/WuLiangchao/0zmk660ifcvm1y4vo3m5sy1z/image_1bgiinl7i1ejimgncpdp0v13b62n.png" alt="sub4"></p>
<p>则其subgradient为$$g = \begin{cases} \nabla f(x_1) &amp;f(x_1) &gt; f(x_2) \\<br>\nabla f(x_2) &amp;f(x_1) &lt; f(x_2) \\<br>[\min \lbrace \nabla f(x_1),\nabla f(x_2)\rbrace, \max \lbrace \nabla f(x_1), \nabla f(x_2)\rbrace] &amp;f(x_1) = f(x_2)\end{cases} $$</p>
<p>而 subgradient descent 与 gradient descent 的不同地方就是当函数不可微的时候，将 gradient descent 中更新公式中的 gradient 换成 subgradient。下面看一个经典的 <code>lasso regression</code> 问题。</p>
<p>$$\min_{\beta \in \mathbb {R}^n} \frac{1}{2} ||y-\beta||_2^2 + \lambda ||\beta||_1~~(\lambda \ge 0)$$</p>
<p>对目标函数求导并令其为0，其中 $||\beta||_1$ 项不可导，用前面提到的subgradient代替，则有以下等式</p>
<p>$$\begin{cases} y_i - \beta_i = \lambda sign(\beta_i) &amp;\beta_i \neq 0\\<br>|y_i - \beta_i| \le \lambda &amp;\beta_i = 0<br>\end{cases}$$</p>
<p>则解可表示成</p>
<p>$$\beta_i = \begin{cases} y_i - \lambda &amp; y_i &gt; \lambda\\<br>0 &amp;-\lambda \le y_i \le \lambda \\<br>y_i + \lambda &amp; y_i &lt; -\lambda\end{cases}$$</p>
<p>上面实际上是简化了的 <code>lasso regression</code>， 因为更一般的lasso 问题表示如下</p>
<p>$$\min_{\beta \in \mathbb {R}^n} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||\beta||_1~~(\lambda \ge 0)$$</p>
<p>这时如果采用上面的解法，那么会得到</p>
<p>$$\begin{cases} X_i^T(y - X\beta) = \lambda sign(\beta_i) &amp;\beta_i \neq 0\\<br>|X_i^T(y - X\beta)| \le \lambda &amp;\beta_i = 0<br>\end{cases}$$</p>
<p>上式并没有为这个 lasso 问题提供一个明确的解，这个问题可以通过下面要提到的 proximal gradient 进行求解，但是上面的式子一定程度上解释了<code>L1 regularization</code>的导致的参数的稀疏性的特点，从上面的表达式可知，只有当 $X_i^T(y - X\beta) = \lambda sign(\beta_i)$ 时，对应的 $\beta_i$ 才不为0，而其他大多数的情况下 $\beta_i$ 为0.</p>
<h3 id="proximal-gradient-descent"><a href="#proximal-gradient-descent" class="headerlink" title="proximal gradient descent"></a>proximal gradient descent</h3><p>proximal gradient descent 也可以说是 subgradient 的升级版，<strong>proximal 通过对原问题的拆分并利用 proximal mapping，能够解决 subgradient descent 无法解决的问题（如上面的一般化 lasso 问题）。</strong></p>
<p>一般来说，这类方法将目标函数描述成以下形式</p>
<p>$$f(X) = g(x) + h(x)$$</p>
<p>上面的 $g(x)$ 是凸且可微的， 而 $h(x)$ 也是凸的，但是不一定可微。则 proximal gradient descent 的迭代公式为</p>
<p>$$x^{(k)} = x^{(k-1)} - t_kG_{t_k}(x^{(k-1)})\\<br>G_{t}(x) = \frac{x - prox_{th}(x-t\nabla g(x))}{t}\\<br>prox_{th}(x) = argmin_{z\in \mathbb {R}^n} \frac{1}{2t}||x-z||_2^2+h(z)$$</p>
<p>上面 $prox_{th}(x)$ 表示对函数 $h$ proximal mapping。这里仅给出结论，证明过程略，接下来以上面没有解决的一般化的 lasso 问题为例讲述这种方法的应用。</p>
<p>$$\min_{\beta \in \mathbb {R}^n} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||\beta||_1~~(\lambda \ge 0)$$</p>
<p>记 $g(\beta) = \frac{1}{2} ||y-X\beta||_2^2, h(\beta) = \lambda ||\beta||_1$</p>
<p>则有<br>$$prox_{th}(\beta) = argmin_{z \in \mathbb {R}^n} \frac{1}{2t}||\beta - z||_2^2+h(z) $$</p>
<p>这个问题通过前面的 subgradient 方法已经解出来，结果为</p>
<p>$$z_i = \begin{cases} y_i - \lambda t &amp; y_i &gt; \lambda t\\<br>0 &amp;-\lambda t \le y_i \le \lambda t\\<br>y_i + \lambda t &amp; y_i &lt; -\lambda t\end{cases}$$</p>
<p>将上面的解记为 $S_{\lambda t}(y)$, 同时 $\nabla g(\beta) = - X^T(y-X\beta)$</p>
<p>代取上面列出的 proximal gradient descent 列出的迭代公式，则 $\beta$ 的迭代公式如下</p>
<p>$$\beta^{(k)} = S_{\lambda t}(\beta^{(k-1)} + t_kX^T(y-X\beta^{(k-1)}))$$</p>
<p>其中 $t_k$ 为步长。</p>
<p>这种方法之所以比 subgradient 方法更加一般化，是因为 $prox_{th}(x)$ 对于绝大部分的 $h$ 是易求的。</p>
<h2 id="对偶方法与KKT条件"><a href="#对偶方法与KKT条件" class="headerlink" title="对偶方法与KKT条件"></a>对偶方法与KKT条件</h2><p>对偶理论在最优化中非常重要，其中具有代表性的两条定理是弱对偶定理和强对偶定理，<strong>弱对偶定理告诉了我们最优化的目标的上界( max 问题)或下界(min 问题)，而强对偶定理告诉了当 KKT 条件满足的时候，可以通过对偶问题的解推出原问题的解。</strong></p>
<p>弱对偶条件总是成立，而强对偶需要在 <code>Slater&#39;s condition</code> 成立时才成立，该条件描述如下</p>
<p>对于优化问题</p>
<p>$$<br>\begin{align*}<br>&amp;\min_x\quad f(x)\\<br>&amp;\begin{array}\\<br>s.t.&amp;g_i(x) \le 0,~i=1,\ldots,m\\<br>&amp;h_j(x)=0,~j=1,\ldots,r<br>\end{array}<br>\end{align*}<br>$$</p>
<p>假如存在可行解 $x’$ 使得 $g_i(x’) &lt; 0(i=1,…m)$, 即不等式约束严格成立（注意同时等式约束也要成立，否则就不是可行解了），那么称<code>Slater&#39;s condition</code> 成立，同时强对偶也成立。</p>
<h3 id="线性规划对偶"><a href="#线性规划对偶" class="headerlink" title="线性规划对偶"></a>线性规划对偶</h3><p>对于形如下面的线性规划问题</p>
<p>$$<br>\begin{align*}<br>&amp;\min_x\quad c^Tx\\<br>&amp;\begin{array}\\<br>s.t.&amp;Ax = b\\<br>&amp;Gx \le h<br>\end{array}<br>\end{align*}<br>$$</p>
<p>其对偶问题为</p>
<p>$$<br>\begin{align*}<br>&amp;\min_{u,v}\quad -b^Tu - h^Tv\\<br>&amp;\begin{array}\\<br>s.t.&amp;-A^Tu - G^Tv = c\\<br>&amp; v \ge 0<br>\end{array}<br>\end{align*}<br>$$</p>
<p>对于 LP 问题，其对偶的特殊性在于只要存在原问题存在可行解，那么 <code>Slater&#39;s condition</code> 一定成立，因此强对偶也成立.</p>
<p>LP 对偶问题的一个经典的例子是最大流(max flow)问题和最小割(min cut)问题.</p>
<h3 id="拉格朗日对偶"><a href="#拉格朗日对偶" class="headerlink" title="拉格朗日对偶"></a>拉格朗日对偶</h3><p>对于更一般的非 LP 问题的对偶问题，需要用到拉格朗日对偶理论得到，并称该问题为拉格朗日对偶问题。</p>
<p>这个方法可以说是对求解等式约束的<a href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0" target="_blank" rel="external">拉格朗日乘子法</a>的一个推广。</p>
<p>对于下面的优化问题</p>
<p>$$<br>\begin{align*}<br>&amp;\min_x\quad f(x)\\<br>&amp;\begin{array}\\<br>s.t.&amp;g_i(x) \le 0,~i=1,\ldots,m\\<br>&amp;h_j(x)=0,~j=1,\ldots,r<br>\end{array}<br>\end{align*}<br>$$</p>
<p>其増广拉格朗日函数为</p>
<p>$$L(x,u,v) = f(x) + \sum_{i=1}^m u_ig_i(x) + \sum_{j=1}^r v_jh_j(x)~~(u_i \ge 0)$$</p>
<p>则原问题的拉格朗日对偶函数为</p>
<p>$$ g(u,v) = \min_x L(x,u,v)$$</p>
<p>且原问题的对偶问题为</p>
<p>$$<br>\begin{align*}<br>&amp;\max_{u,v}\quad g(u,v)\\<br>&amp;\begin{array}\\<br>s.t.&amp;u_i \ge 0,~i=1,\ldots,m<br>\end{array}<br>\end{align*}<br>$$</p>
<p>上面得到对偶问题的简单推导流程如下：</p>
<p>首先原问题的目标函数<strong>加上约束</strong>可以表示为 </p>
<p>$$f(x) = \max_{u,v} L(x,u,v)$$ </p>
<p>原因是在增广拉格朗日函数中，假如 $x$ 违反了约束条件(即 $ g(x) &gt; 0 $ )，那么 $f(x)$ 会趋向无穷大；而不违反约束条件时，$u$ 必须取0才能使得目标最小。</p>
<p>进一步，原问题可以表示为 $$\min_x \max_{u,v} L(x,u,v) $$</p>
<p>而由于下式恒成立（弱对偶定理）</p>
<p>$$\min_x \max_{u,v} L(x,u,v) \ge \max_{u,v} \min_xL(x,u,v) $$</p>
<p>因此将 $\min_xL(x,u,v)$ 作为对偶函数， $\max_{u,v} \min_xL(x,u,v) $ 作为对偶问题。</p>
<p>假设现在找到了对偶问题，并且对偶问题比原问题要容易求解得多，也求出了对偶问题的解，那么该转化去求原问题的解，这里就要用到下面的KKT条件。</p>
<h3 id="KKT-条件"><a href="#KKT-条件" class="headerlink" title="KKT 条件"></a>KKT 条件</h3><p>KKT 条件是非线性规划领域中最重要的理论成果之一，是确定某点是最优点的一阶必要条件，只要是最优点就一定满足这个条件，但是一般来说不是充分条件，因此满足这个点的不一定是最优点。但<strong>对于凸优化而言，KKT条件是最优点的充要条件</strong>。</p>
<p>同样地</p>
<p>对于下面的优化问题</p>
<p>$$<br>\begin{align*}<br>&amp;\min_x\quad f(x)\\<br>&amp;\begin{array}\\<br>s.t.&amp;g_i(x) \le 0,~i=1,\ldots,m\\<br>&amp;h_j(x)=0,~j=1,\ldots,r<br>\end{array}<br>\end{align*}<br>$$</p>
<p>其増广拉格朗日函数为</p>
<p>$$L(x,u,v) = f(x) + \sum_{i=1}^m u_ig_i(x) + \sum_{j=1}^r v_jh_j(x)~~(u_i \ge 0)$$</p>
<p>则 KKT 条件为</p>
<p>$$\begin{cases} \nabla_x L(x,u,v) = 0\\<br>u_ig(x) = 0\\<br>u_i \ge 0\\<br>g_i(x) \le 0, h_j(x)=0<br>\end{cases}$$</p>
<p>因此其实只要能够求解出上面的联立方程组，得到的解就是最优解（对于凸优化而言，非凸的问题一般用KKT来验证最优解）。</p>
<p>但是上面的方程组往往很难求解，一些特殊情况下解是有限的，可以分类讨论；但是更一般的情况下可能的解是无限的，因此无法求解。这里要结合上面的拉格朗日对偶问题得到的解进行求解。</p>
<p>求解之前，首先要知道下面的定理</p>
<blockquote>
<p>假如一个问题满足强对偶，那么 $x’,u’,v’$ 是原问题和对偶问题的最优解 $\longleftrightarrow$ $x’,u’,v’$ 满足KKT条件。</p>
</blockquote>
<p>因此通过对偶问题求得 $u’,v’$ 后，带入上面的 KKT 条件即可求出 $x’$ 。</p>
<p>svm 是利用拉格朗日对偶和KKT条件进行求解的经典问题，这里不详细展开，有兴趣的可以参考 <a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="external">Andrew Ng 公开课</a>中关于 svm 的那章或<a href="http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982684.html" target="_blank" rel="external">这篇文章</a>。</p>
<h2 id="ADMM"><a href="#ADMM" class="headerlink" title="ADMM"></a>ADMM</h2><p>ADMM(Alternating Direction Method of Multipliers) 是解决带约束的凸优化问题的一种迭代解法，当初提出这个算法最主要的目的是为了在分布式环境(Hadoop, MPI 等)中迭代求解这个问题，关于这方面的资料可参考<a href="http://stanford.edu/~boyd/admm.html" target="_blank" rel="external">这里</a> </p>
<p>ADMM 将要解决的问题描述成以下形式</p>
<p>$$<br>\begin{align*}<br>&amp;\min_x\quad f_1(x_1) + f_2(x_2)\\<br>&amp;\begin{array}\\<br>s.t.&amp; A_1x_1+A_2x_2 = b<br>\end{array}<br>\end{align*}<br>$$</p>
<p>这里省略证明过程，直接给出 ADMM 的迭代公式</p>
<p>$$\begin{align*}<br>&amp;x_1^{(k)} = argmin_{x_1} f_1(x_1) + \frac{\rho}{2}||A_1x_1+A_2x_2^{(k-1)} - b + w^{(k-1)}||_2^2\\<br>&amp;x_2^{(k)} = argmin_{x_2} f_2(x_2) + \frac{\rho}{2}||A_1x_1^{(k)} + A_2x_2 - b + w^{(k-1)}||_2^2\\<br>&amp;w^{(k)} = w^{(k-1)} + A_1x_1^{(k)} + A_2x_2^{(k)} - b<br>\end{align*}<br>$$</p>
<p>上式中的 $\rho$ 是事先选择的参数，可以选择定值，选择定值的时候会遇到跟梯度下降固定步长的类似问题，因此也可以根据每次迭代情况改变 $\rho$ 的值，具体可参考<a href="http://stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf" target="_blank" rel="external">这篇文献</a>。</p>
<p>实际中，<strong>难点在于把一个问题变为 ADMM 求解的形式</strong>，即上面列出的优化问题的形式。下面给出一个例子说明这个问题，这个例子是一个更一般的 lasso regression 问题，称为 <code>fused lasso regression</code>.</p>
<p>$$\min_{\beta \in \mathbb {R}^n} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||D\beta||_1~~(\lambda \ge 0)$$</p>
<p>一般的 lasso regression 问题中 $D = I$.下面用 ADMM 的形式表示上面的问题</p>
<p>$$\min_{\beta \in \mathbb {R}^n，\alpha \in \mathbb {R}^n} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||\alpha||_1~~(\lambda \ge 0)\\<br>s.t. D\beta - \alpha = 0$$</p>
<p>将这个问题映射到上面的优化问题的模式有</p>
<p>$$\frac{1}{2} ||y-X\beta||_2^2\rightarrow f_1(x1) \\<br>\lambda ||\alpha||_1 \rightarrow f_2(x_2)\\<br>D\beta - \alpha = 0 \rightarrow A_1x_1+A_2x_2 = b$$</p>
<p>则根据上面的迭代公式计算（对问题变量求导且令结果为0）可得到下面的迭代公式</p>
<p>$$\begin{align*}<br>&amp;\beta^{(k)} = (X^TX+\rho D^TD)^{-1}(X^Ty+\rho D^T(\alpha^{(k-1)}-w^{(k-1)})\\<br>&amp;\alpha^{(k)} = S_{\lambda/\rho}(D\beta^{(k)}+w^{(k-1)})\\<br>&amp;w^{(k)} = w^{(k-1)} + D\beta^{(k)} - \alpha^{(k)}<br>\end{align*}<br>$$</p>
<p>而对于无约束的优化也可以通过 ADMM 求解，例如对于下面的问题</p>
<p>$$\min_x \sum_{i=1}^B f_i(x)$$</p>
<p>将其表示为ADMM的形式如下</p>
<p>$$\min_{x_1,..x_B,x} \sum_{i=1}^B f_i(x_i)\\<br>s.t. x_i = x,~~i=1,..B$$</p>
<p>则ADMM的迭代规则如下</p>
<p>$$\begin{align*}<br>&amp;x_i^{(k)} = argmin_{x_i} f_i(x_i)+\frac{\rho}{2}||x_i-x^{(k-1)}+w_i^{(k-1)}||~(i=1,..B)\\<br>&amp;x^{(k)} = \frac{1}{B} \sum_{i=1}^B (x_i^{(k)} + w_i^{(k-1)})\\<br>&amp;w_i^{(k)} = w_i^{(k-1)} + x_i^{(k)} - x^{(k)}~(i=1,..B)<br>\end{align*}<br>$$</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/数学/" rel="tag">数学</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/15/ImageNet Classification with Deep Convolutional Neural Networks 阅读笔记/" rel="next" title="《ImageNet Classification with Deep Convolutional Neural Networks》阅读笔记">
                <i class="fa fa-chevron-left"></i> 《ImageNet Classification with Deep Convolutional Neural Networks》阅读笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/05/25/机器学习中模型优化不得不思考的几个问题/" rel="prev" title="机器学习中模型优化不得不思考的几个问题">
                机器学习中模型优化不得不思考的几个问题 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://wulc.me/files/profile.jpg"
               alt="良超" />
          <p class="site-author-name" itemprop="name">良超</p>
          <p class="site-description motion-element" itemprop="description">算法工程师首先得是个工程师</p>
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">200</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">39</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/WuLC" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github-alt"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.linkedin.com/in/wuliangchao/" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#凸集，凸函数与凸优化"><span class="nav-number">1.</span> <span class="nav-text">凸集，凸函数与凸优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#凸集"><span class="nav-number">1.1.</span> <span class="nav-text">凸集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#凸函数"><span class="nav-number">1.2.</span> <span class="nav-text">凸函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#凸优化"><span class="nav-number">1.3.</span> <span class="nav-text">凸优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度类方法"><span class="nav-number">2.</span> <span class="nav-text">梯度类方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-descent"><span class="nav-number">2.1.</span> <span class="nav-text">gradient descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#subgradient-descent"><span class="nav-number">2.2.</span> <span class="nav-text">subgradient descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#proximal-gradient-descent"><span class="nav-number">2.3.</span> <span class="nav-text">proximal gradient descent</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对偶方法与KKT条件"><span class="nav-number">3.</span> <span class="nav-text">对偶方法与KKT条件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性规划对偶"><span class="nav-number">3.1.</span> <span class="nav-text">线性规划对偶</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#拉格朗日对偶"><span class="nav-number">3.2.</span> <span class="nav-text">拉格朗日对偶</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KKT-条件"><span class="nav-number">3.3.</span> <span class="nav-text">KKT 条件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ADMM"><span class="nav-number">4.</span> <span class="nav-text">ADMM</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <center>
<div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2019</span>
  &nbsp;&nbsp;|&nbsp;&nbsp;Powered by <a href="http://hexo.io">Hexo</a> and <a href="http://theme-next.iissnan.com/">NexT</a></span>
  </br>
  <span>Documentation Licensed Under <a href="https://creativecommons.org/licenses/by/4.0/deed.en">CC BY 4.0</a></span>
</div>
</center>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  
<script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>


  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  


</body>
</html>
