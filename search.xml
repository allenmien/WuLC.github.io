<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Effective Go 摘记]]></title>
      <url>%2F2019%2F02%2F18%2FEffective%20Go%20%E6%91%98%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[本文是 Effective Go 中的一些摘记，主要涉及 golang 中的语法、技巧、风格等。为了尽可能保持原文意思，会通过英文记录相关的知识点。 Formatting The gofmt program (also available as go fmt, which operates at the package level rather than source file level) reads a Go program and emits the source in a standard style of indentation and vertical alignment, retaining and if necessary reformatting comments. Commentary Every package should have a package comment, a block comment preceding the package clause. For multi-file packages, the package comment only needs to be present in one file, and any one will do. The package comment should introduce the package and provide information relevant to the package as a whole, for example 1234567/*Package regexp implements a simple library for regular expressions.The syntax of the regular expressions accepted is: .......*/package regexp Doc comments work best as complete sentences, which allow a wide variety of automated presentations. The first sentence should be a one-sentence summary that starts with the name being declared. 123// Compile parses a regular expression and returns, if successful,// a Regexp that can be used to match against text.func Compile(str string) (*Regexp, error) &#123; Grouping variables can indicate relationships between items, such as the fact that a set of variables is protected by a mutex. 123456var ( countLock sync.Mutex inputCount uint32 outputCount uint32 errorCount uint32) Names By convention, packages are given lower case, single-word names, no need for underscores or mixedCaps Another convention is that the package name is the base name of its source directory; the package in “src/encoding/base64” is imported as “encoding/base64” but has name base64, not encoding_base64 and not encodingBase64 By convention, one-method interfaces are named by the method name plus an -er suffix or similar modification to construct an agent noun: Reader, Writer, Formatter, CloseNotifier the convention in Go is to use MixedCaps or mixedCaps rather than underscores to write multiword names Control structures if and switch accept an optional initialization statement like that of for 1234if err := file.Chmod(0664); err != nil &#123; log.Print(err) return err&#125; In a := declaration a variable v may appear even if it has already been declared, providing that there is at least one other variable in the declaration that is being declared anew, otherwise an error no new variables on left side of := will occur 1234567891011// legal for errf, err := os.Open(name)d, err := f.Stat()// not legal for aa := 1a := 2// not legal for a and ba, b := 1, 1a, b := 2, 2 For strings, the range breaks out individual Unicode code points by parsing the UTF-8. Erroneous encodings consume one byte and produce the replacement rune U+FFFD, rune is Go terminology for a single Unicode code point, similar to char in other languages 123456789for pos, char := range "日本\x80語" &#123; // \x80 is an illegal UTF-8 encoding fmt.Printf("character %#U starts at byte position %d\n", char, pos)&#125;/* outputcharacter U+65E5 '日' starts at byte position 0character U+672C '本' starts at byte position 3character U+FFFD '�' starts at byte position 6character U+8A9E '語' starts at byte position 7*/ if the switchhas no expression it switches on true. It’s therefore possible—and idiomatic—to write an if-else-if-else chain as a switch. 1234567891011func unhex(c byte) byte &#123; switch &#123; case '0' &lt;= c &amp;&amp; c &lt;= '9': return c - '0' case 'a' &lt;= c &amp;&amp; c &lt;= 'f': return c - 'a' + 10 case 'A' &lt;= c &amp;&amp; c &lt;= 'F': return c - 'A' + 10 &#125; return 0&#125; Functions Deferred functions are executed in LIFO order(imagine it like a stack), so the following code will cause 4 3 2 1 0 to be printed when the function returns. 123for i := 0; i &lt; 5; i++ &#123; defer fmt.Printf("%d ", i)&#125; The arguments to deferred functions are evaluated when the defer executes, not when the function executes 1234567891011121314151617181920212223func trace(s string) string &#123; fmt.Println("entering:", s) return s&#125;func un(s string) &#123; fmt.Println("leaving:", s)&#125;func a() &#123; defer un(trace("a")) fmt.Println("in a")&#125;func b() &#123; defer un(trace("b")) fmt.Println("in b") a()&#125;func main() &#123; b()&#125; the output of the above code is123456entering: bin bentering: ain aleaving: aleaving: b Datanew v.s make Go has two allocation primitives, the built-in functions new and make New does not initialize the memory, new(T) allocates zeroed storage for a new item of type T and returns its address, that is a pointer to a newly allocated zero value of type T, it’s helpful to arrange when designing your data structures that the zero value of each type can be used without further initialization Sometimes the zero value isn’t good enough and an initializing constructor is necessary, as in this example derived from package os 1234567891011func NewFile(fd int, name string) *File &#123; if fd &lt; 0 &#123; return nil &#125; f := new(File) f.fd = fd f.name = name f.dirinfo = nil f.nepipe = 0 return f&#125; We can simplify it using a composite literal, which is an expression that creates a new instance each time it is evaluated(File{fd, name, nil, 0} in the following code). If a composite literal contains no fields at all, it creates a zero value for the type. The expressions new(File) and &amp;File{} are equivalent. 1234567func NewFile(fd int, name string) *File &#123; if fd &lt; 0 &#123; return nil &#125; f := File&#123;fd, name, nil, 0&#125; return &amp;f&#125; Note that unlike in C, it’s perfectly OK to return the address of a local variable, the storage associated with the variable survives after the function returns make(T, args) serves a purpose different from new(T), it creates slices, maps, and channels only, and it returns an initialized (not zeroed) value of type T (not *T) The reason for the distinction is that these three types(slices, maps, and channels) represent, under the covers, references to data structures that must be initialized before use array v.s slice There are major differences between the ways arrays work in Go and C. In Go, Arrays are values. Assigning one array to another copies all the elements. In particular, if you pass an array to a function, it will receive a copy of the array, not a pointer to it. The size of an array is part of its type. The types [10]int and [20]int are distinct The value property can be useful but also expensive; if you want C-like behavior and efficiency, you can pass a pointer to the array A slice does not store any data, it just describes a section of an underlying array, so if you assign one slice to another, both refer to the same array 12345678910111213141516names := [4]string&#123; "John", "Paul", "George", "Ringo",&#125;a := names[0:2]b := names[1:3]b[0] = "XXX"fmt.Println(a, b)fmt.Println(names)// output// [John XXX] [XXX George]// [John XXX George Ringo] If a function takes a slice argument, modification of elements of the slice will be visible to the caller, but append elements won’t, if you want to append elements to slice in function, pass the address instead slices are variable-length, for a two-dimensional slice, it is possible to have each inner slice be a different length 12345text := LinesOfText&#123; []byte("Now is the time"), []byte("for all good gophers"), []byte("to bring some fun to the party."),&#125; map For a map in golang like map[KeyType]ValueType, KeyType may be any type that is comparable ,such as integers, floating point and complex numbers, strings, pointers, interfaces (as long as the dynamic type supports equality).Slices cannot be used as map keys, because equality is not defined on them, and ValueType may be any type at all, including another map! 12hits := make(map[string]map[string]int)n := hits["/doc/"]["au"] Like slices, maps hold references to an underlying data structure. If you pass a map to a function that changes the contents of the map, the changes will be visible in the caller. An attempt to fetch a map value with a key that is not present in the map will return the zero value for the type of the entries in the map.The zero value is: 0 for numeric types, false for the boolean type “” (the empty string) for strings. If you need to judge whether a key in map, you can do this 123if val, ok := dict["foo"]; ok &#123; //do something here&#125; Methods Methods can be defined for any named type (except a pointer or an interface); the receiver does not have to be a struct. 12345type ByteSlice []bytefunc (slice ByteSlice) Append(data []byte) []byte &#123; // Body exactly the same as the Append function defined above.&#125; The rule about pointers vs. values for receivers is that value methods can be invoked on pointers and values, but pointer methods can only be invoked on pointers. Interfaces and other typesInterfaces An interface is defined as a set of method signatures, and a type implements an interface by implementing its methods. A type can implement multiple interfaces 123456789101112type Sequence []int// Methods required by sort.Interface.func (s Sequence) Len() int &#123; return len(s)&#125;func (s Sequence) Less(i, j int) bool &#123; return s[i] &lt; s[j]&#125;func (s Sequence) Swap(i, j int) &#123; s[i], s[j] = s[j], s[i]&#125; You can define your own interface and a value of interface type can hold any value that implements those methods. 12345678910111213141516171819type Abser interface &#123; Abs() float64&#125;type MyFloat float64func (f MyFloat) Abs() float64 &#123; if f &lt; 0 &#123; return float64(-f) &#125; return float64(f)&#125;func main() &#123; var a Abser f := MyFloat(-math.Sqrt2) a = f // a MyFloat implements Abser fmt.Println(a.Abs())&#125; ConcurrencyShare by communicating Concurrent programming in many environments is made difficult by the subtleties required to implement correct access to shared variables. Go encourages a different approach in which shared values are passed around on channels and, in fact, never actively shared by separate threads of execution，only one goroutine has access to the value at any given time. For example，Reference counts may be best done by putting a mutex around an integer variable. But as a high-level approach, using channels to control access makes it easier to write clear, correct programs. Goroutines A goroutine has a simple model: it is a function executing concurrently with other goroutines in the same address space. Prefix a function or method call with the go keyword to run the call in a new goroutine. When the call completes, the goroutine exits silently，don’t wait for it. Channels Like maps, channels are allocated with make, and the resulting value acts as a reference to an underlying data structure There are lots of nice idioms using channels. For example, if we launched a sort in the background and do sth else while waiting for the goroutine to finish. A channel allows us to do so 12345678c := make(chan int) // Allocate a channel.// Start the sort in a goroutine; when it completes, signal on the channel.go func() &#123; list.Sort() c &lt;- 1 // Send a signal; value does not matter.&#125;()doSomethingForAWhile()&lt;-c // Wait for sort to finish; discard sent value. The above code works becase receivers always block until there is data to receive. As for the sender, if the channel is unbuffered, the sender blocks until the receiver has received the value. If the channel has a buffer, the sender blocks only until the value has been copied to the buffer; if the buffer is full, this means waiting until some receiver has retrieved a value. A buffered channel can be used like a semaphore, for instance to limit throughput. In the following example, incoming requests are passed to handle, which sends a value into the channel, processes the request, and then receives a value from the channel to ready the “semaphore” for the next consumer. The capacity of the channel buffer limits the number of simultaneous calls to process. 1234567891011121314var sem = make(chan int, MaxOutstanding)func handle(r *Request) &#123; sem &lt;- 1 // Wait for active queue to drain. process(r) // May take a long time. &lt;-sem // Done; enable next request to run.&#125;func Serve(queue chan *Request) &#123; for &#123; req := &lt;-queue go handle(req) // Don't wait for handle to finish. &#125;&#125; The above design has a problem: Serve creates a new goroutine for every incoming request, even though only MaxOutstanding of them can run at any moment. As a result, the program can consume unlimited resources if the requests come in too fast. We can address that deficiency by changing Serve to gate the creation of the goroutines. 123456789func Serve(queue chan *Request) &#123; for req := range queue &#123; sem &lt;- 1 go func() &#123; process(req) // Buggy; see explanation below. &lt;-sem &#125;() &#125;&#125; The bug in the above code is that in a Go for loop, the loop variable is reused for each iteration, so the req variable is shared across all goroutines. But we need to make sure that req is unique for each goroutine. Here’s one way to do that, passing the value of req as an argument to the closure in the goroutine: 123456789func Serve(queue chan *Request) &#123; for req := range queue &#123; sem &lt;- 1 go func(req *Request) &#123; process(req) &lt;-sem &#125;(req) &#125;&#125; Another solution is just to create a new variable with the same name, like the following code, req := req may seem odd, but it’s legal and idiomatic in Go to do this. You get a fresh version of the variable with the same name 12345678910func Serve(queue chan *Request) &#123; for req := range queue &#123; req := req // Create new instance of req for the goroutine. sem &lt;- 1 go func() &#123; process(req) &lt;-sem &#125;() &#125;&#125; Another approach that manages resources well is to start a fixed number of handle goroutines all reading from the request channel. The number of goroutines limits the number of simultaneous calls to process. 12345678910111213func handle(queue chan *Request) &#123; for r := range queue &#123; process(r) &#125;&#125;func Serve(clientRequests chan *Request, quit chan bool) &#123; // Start handlers for i := 0; i &lt; MaxOutstanding; i++ &#123; go handle(clientRequests) &#125; &lt;-quit // Wait to be told to exit.&#125; Channels of channels In the example in the previous section, handle was an idealized handler for a request but we didn’t define the type it was handling. If that type includes a channel on which to reply, each client can provide its own path for the answer. 12345type Request struct &#123; args []int f func([]int) int resultChan chan int&#125; The client provides a function and its arguments, as well as a channel inside the request object on which to receive the answer. 12345678910func sum(a []int) (s int) &#123; for _, v := range a &#123; s += v &#125; return&#125;request := &amp;Request&#123;[]int&#123;3, 4, 5&#125;, sum, make(chan int)&#125;// Send requestclientRequests &lt;- request On the server side, the handler function is the only thing that changes. 12345func handle(queue chan *Request) &#123; for req := range queue &#123; req.resultChan &lt;- req.f(req.args) &#125;&#125; Some Syntax int(math.Pow(float64(x), float64(count))) Atoi (string to int) and Itoa (int to string). 12i, err := strconv.Atoi("-42")s := strconv.Itoa(-42) concate string s1 and s2 123buffer := bytes.NewBufferString(s1)buffer.WriteString(s2)buffer.String() append function 12345678// append multiple elementsx := []int&#123;1,2,3&#125;x = append(x, 4, 5, 6)// append a slice x := []int&#123;1,2,3&#125;y := []int&#123;4,5,6&#125;x = append(x, y...)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[文本分类中的一些经验和 tricks]]></title>
      <url>%2F2019%2F02%2F06%2F%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%8F%E9%AA%8C%E5%92%8C%20tricks%2F</url>
      <content type="text"><![CDATA[最近在总结之前做的文本分类实验的一些经验和 tricks，同时也参考了网上的一些相关资料(见文末)，其中有些 tricks 没尝试过，先在这里记下，或者日后能用上。 这里的经验和 tricks 大概可分为两部分：预处理部分和模型训练部分，下面分别介绍 预处理 文本更正，主要是将文本标准化，包括繁体转简体，全角转半角，拼音纠错等 文本泛化，如一个手机号码，因为有几千万的手机号码，不可能为每个手机号码设一个特征，所以最好将手机号码转化为同一个特征；另外表情符号、人名、地址、网址、命名实体等也要考虑这种泛化，泛化的程度这个视具体的任务，比如说地址可以以国家为粒度，也可以以省份为粒度 规范文本为统一长度时，取所有长度的均值或者中位数，但是别取最大值；截断时根据具体任务考虑从前面阶段或从后面截断 构建数据集的 vocabulary 时，需要考虑以下几个方面 取前N个高频词或者过滤掉出现次数小于某个阈值的词 根据具体任务确定是否需要去掉 stop words 假如采用了预训练的词向量，要尽可能让 vocabulary 中的词语能找到对应的词向量(这个问题也涉及到预训练的词向量和分词器的选择) 词向量的选择，当数据集较小时，直接使用预训练好的词向量（如google、facebook开源的），且不用微调；当训练集比较大的时候，可随机初始化进行训练，也可以对预训练的词向量进行微调（微调收敛得更快，但是结果差异不大） 分词时考虑以下几个方面 是否需要分词，使用 char-level 的方法时不需要分词，但是在很多场景下 word-level 的效果都要比 char-level 的要好 分词时可以只保留长度大于1的词(会去除很多停止词)，对结果精度没什么影响，但是可以有效降低特征维度 采用预训练的词向量时，要保证分词后的大部分词语能够出现在预训练的词向量表中，否则这个词语的 embedding 就相当于是随机初始化，预训练的词向量没有提供任何信息；具体方法可参考这里 数据增强 常见的方法有：drop(随机删掉文本)、shuffle(随机改变文本顺序)、replace(用近义词进行替换) 数据增强对 word-level 的方法有一定的提升，但是对于 char-level 的方法效果不一定好，甚至会起到反作用 模型训练 规则有时能解决大部分的问题，不一定要用到模型，使用时要权衡模型带来的收益和复杂性 传统的机器学习方法根据其特征工程的不同可分为三大类 词袋模型：将出现的词记为1，否则记为 0，问题是维度高且稀疏性严重 向量空间模型：根据文档频率、互信息、信息增益、χ²统计量等进行了特征(词语)的选择，同时通过 tfidf 值为每个词赋权重；一定程度上缓解了上面提到的词袋模型维度高且稀疏性严重的问题 主题模型：pLSA/LDA/HDP 等主题模型将文本表示低维实数向量，类似于深度学习中的 embedding，但是比 embedding 有更好的解释性 fasttext 简单、速度快，是一个非常不错的 baseline；随着问题复杂性增加可依次尝试 CNN -&gt; RNN -&gt; BERT 对于深度学习模型，把模型变得更深更宽更复杂往往能够提升效果；但是当模型复杂到一定程度的时候，提升的效果微乎其微甚至没提升 rnn 类模型用双向一般会比单向要好 使用 dropout(经验值为 0.5) 基本都能提升效果，使用的地方包括：embedding 层后、FC层后 训练震荡问题：增加随机采样因素尽可能使得数据分布 iid，默认 shuffle 机制能使得训练结果更稳定。如果训练模型仍然很震荡，可以考虑调整学习率 或 mini_batch_size 采用预训练的 embedding 并进行 finetune 时，在最开始 embedding 的学习率设为 0，训练到效果较好时才开始 finetune embedding 层 学习率的设置考虑以下几个方面 经验值一般为 1、0.1、0.01、0.001, 一般从1开始尝试。很少见学习率大于10的 学习率一般要随着训练进行衰减，衰减系数一般是0.5；衰减时机可以是验证集准确率不再上升，或固定训练 N 个 epoch 后 比起自定义的衰减方法，更便捷的方法是使用自适应梯度的办法，例如 adam,adadelta,rmsprop 等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率 对RNN来说，如果要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。 超参数的设置经验可参考 A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification 模型融合时，差异性越大，融合效果越好，具体可参这里 参考 在文本分类任务中，有哪些论文中很少提及却对性能有重要影响的tricks？ 知乎看山杯夺冠记 用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践 深度学习网络调参技巧]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MLE 与 MAP 简介]]></title>
      <url>%2F2019%2F01%2F25%2FMLE%20%E4%B8%8E%20MAP%20%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[最近看到一篇关于 MLE(Maximum Likelihood Estimation) 和 MAP（Maximum A Posteriori) 的文章，写的很好，非常值得一看，文章链接为 聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计，本文几乎不加修改地转载了文章，侵删。 概述有时候和别人聊天，对方会说自己有很多机器学习经验，深入一聊发现，对方竟然对MLE和MAP一知半解，至少在我看来，这位同学的机器学习基础并不扎实。难道在这个深度学习盛行的年代，不少同学都只注重调参数？ 现代机器学习的终极问题都会转化为解目标函数的优化问题，MLE 和 MAP 是生成这个函数的很基本的思想，因此我们对二者的认知是非常重要的。这次就和大家认真聊一聊 MLE 和 MAP 这两种 estimator。 两大学派的争论抽象一点来讲，频率学派(Frequentist)和贝叶斯学派(Bayesian)对世界的认知有本质不同：频率学派认为世界是确定的，有一个本体，这个本体的真值是不变的，我们的目标就是要找到这个真值或真值所在的范围；而贝叶斯学派认为世界是不确定的，人们对世界先有一个预判，而后通过观测数据对这个预判做调整，我们的目标是要找到最优的描述这个世界的概率分布。 在对事物建模时，用 $\theta$ 表示模型的参数，解决问题的本质就是求 $\theta$ 。那么频率学派和贝叶斯学派的区别在于： 频率学派：存在唯一真值 $\theta$ 。举一个简单直观的例子–抛硬币，我们用 $P(head)$ 来表示硬币的 bias。抛一枚硬币100次，有20次正面朝上，要估计抛硬币正面朝上的 bias $P(head)=\theta$ 。在频率学派来看，$\theta$ = 20 / 100 = 0.2，很直观。当数据量趋于无穷时，这种方法能给出精准的估计；然而缺乏数据时则可能产生严重的偏差。例如，对于一枚均匀硬币，即 $\theta$ = 0.5，抛掷5次，出现5次正面 (这种情况出现的概率是 1/2^5=3.125%)，频率学派会直接估计这枚硬币 $\theta$ = 1，出现严重错误。 贝叶斯学派： $\theta$ 是一个随机变量，符合一定的概率分布。在贝叶斯学派里有两大输入和一大输出，输入是先验 (prior) 和似然 (likelihood)，输出是后验 (posterior)。先验，即 $P(\theta)$ ，指的是在没有观测到任何数据时对 \theta 的预先判断，例如给我一个硬币，一种可行的先验是认为这个硬币有很大的概率是均匀的，有较小的概率是是不均匀的；似然，即 $P(X|\theta)$ ，是假设 $\theta$ 已知后我们观察到的数据应该是什么样子的；后验，即 $P(\theta|X)$ ，是最终的参数分布。贝叶斯估计的基础是贝叶斯公式，如下： $$P(\theta|X)=\frac{P(X|\theta) \times P(\theta)}{P(X)}$$ 同样是抛硬币的例子，对一枚均匀硬币抛5次得到5次正面，如果先验认为大概率下这个硬币是均匀的 (例如最大值取在0.5处的Beta分布)，那么 $P(head)$ ，即 $P(\theta|X)$ ，是一个distribution，最大值会介于0.5~1之间，而不是武断的 $\theta$ = 1。 这里有两点值得注意的地方： (1) 随着数据量的增加，参数分布会越来越向数据靠拢，先验的影响力会越来越小(2) 如果先验是 uniform distribution，则贝叶斯方法等价于频率方法。因为直观上来讲，先验是uniform distribution本质上表示对事物没有任何预判 MLE - 最大似然估计Maximum Likelihood Estimation, MLE是频率学派常用的估计方法！ 假设数据 $x_1, x_2, …, x_n$ 是 i.i.d.的一组抽样，$X = (x_1, x_2, …, x_n)$ 。其中i.i.d.表示Independent and identical distribution，独立同分布。那么MLE对 $\theta$ 的估计方法可以如下推导： $$\begin{align*}\hat{\theta}_\text{MLE} &amp;= \arg \max P(X; \theta) \\&amp;= \arg \max P(x_1; \theta) P(x_2; \theta) \cdot\cdot\cdot\cdot P(x_n;\theta) \\&amp; = \arg \max\log \prod_{i=1}^{n} P(x_i; \theta) \\&amp;= \arg \max \sum_{i=1}^{n} \log P(x_i; \theta) \\&amp;= \arg \min - \sum_{i=1}^{n} \log P(x_i; \theta)\end{align*}$$ 最后这一行所优化的函数被称为 Negative Log Likelihood (NLL)，这个概念和上面的推导是非常重要的！ 我们经常在不经意间使用MLE，例如 上文中关于频率学派求硬币概率的例子，其方法其实本质是由优化NLL得出。给定一些数据，求对应的高斯分布时，我们经常会算这些数据点的均值和方差然后带入到高斯分布的公式，其理论依据是优化NLL深度学习做分类任务时所用的cross entropy loss，其本质也是MLE MAP - 最大后验估计Maximum A Posteriori, MAP是贝叶斯学派常用的估计方法！ 同样的，假设数据 $x_1, x_2, …, x_n$ 是i.i.d.的一组抽样，$X = (x_1, x_2, …, x_n)$ 。那么MAP对 $\theta$ 的估计方法可以如下推导： $$\begin{align*}\hat{\theta}_\text{MAP} &amp;= \arg \max P(\theta | X) \\&amp;= \arg \min -\log P(\theta | X) \\&amp; = \arg \min -\log P(X|\theta) - \log P(\theta) + \log P(X) \\&amp;= \arg \min -\log P(X|\theta ) - \log P(\theta)\end{align*}$$ 其中，第二行到第三行使用了贝叶斯定理，第三行到第四行 $P(X)$ 可以丢掉因为与 $\theta$ 无关。注意 $-\log P(X|\theta )$ 其实就是NLL，所以MLE和MAP在优化时的不同就是在于先验项 $- \log P(\theta)$ 。好的，那现在我们来研究一下这个先验项，假定先验是一个高斯分布，即 $$P(\theta) = \text{constant} \times e^{-\frac{\theta^2}{2\sigma^2}}$$ 那么， $-\log P(\theta) = \text{constant} + \frac{\theta^2}{2\sigma^2}$ 。至此，一件神奇的事情发生了： 在MAP中使用一个高斯分布的先验等价于在MLE中采用L2的regularizaton！ 参考: Bayesian Methods MLE, MAP, Bayes classification]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统笔记(3)-GFS]]></title>
      <url>%2F2019%2F01%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0(3)-GFS%2F</url>
      <content type="text"><![CDATA[本系列文章是学习课程 6.824: Distributed Systems 时的一些学习笔记，整个课程的相关材料已整理至 DistributedSystemInGo。本文是 LEC3 的内容，介绍了分布式文件系统 GFS，GFS 为 MapReduce 提供了存储，同样是出自 Google，同样是年代久远，但是其中的一些设计思想同样值得我们参考。 设计理念GFS 的设计理念也可以理解为其适用场景 整个分布式系统是由普通的商用机器构成的，因此故障会比较频繁 系统存储着数百万级的大文件，每个文件的大小基本都大于 100MB；小文件也存在，但是不是优化的目标 文件系统读操作主要有两种：large streaming read 和 small random read，且前者占主导; 区别在于 large streaming read 是顺序访问，读取量大；small random read 则刚好相反 文件系统的写操作主要是对文件进行追加(append)操作, 追加的数据是 large、 sequencial 的；且写入后文件就很少会被修改；同样的 small random write 也支持，但是不是优化的目标 系统需要支持多个 client 同时写一个文件 相比低延迟，更看重高吞吐量 系统结构GFS 总体的系统结构如下图所示 整个系统包括一个 master 和若干个 chunkservers，master 存储着文件系统的 metadata，chunkservers 则存储着真正的文件内容，client 会先从 master 获取文件存储在哪一个的 chunkserver，然后从这个 chunkserver 直接读取。该过程有以下几点需要注意: metadatamaster 存储的 metadata 主要包括文件系统的 namespace、文件与chunk的映射、chunk 的位置等。 文件系统的 namespace 在GFS中存储为B+树。树上的每个叶子节点代表普通文件，而中间节点则代表目录文件。根节点是文件系统的根目录。 master 启动时会将所有元数据加载至内存中，优点是元数据操作速度很快，但是由于采用的是 single master，master 的内存会限制了文件系统的可扩展性，由于每个 64MB 的 chunk 会占据 64B 的metadata，则 64GB 内存的服务器最多可支持的 chunk 的数量约为 64GB/64B = 10亿。但由于GFS应用场景是大文件，所以这个问题并不是严峻 chunk每个大文都件被划分为若干个固定大小的 chunk，每个 chunk 在创建的时候都会被 master 赋予一个唯一的 ID，称为 chunk handle chunk 的大小理论上可为任意值，GFS 中为 64MB)，大的 chunk size 有以下优点与不足 减少了 client 与 master 的通信次数，从而减少了 master 和 network 的负载；对于 sequential read，chunk size 越大，数据在同一个 chunk 上的概率就越大，因此避免了读取多个 chunk 减少 master 存储的 metadata 的大小，因为 chunk size 越大，chunk 的数量就越小 可能会浪费空间，如一个 65MB 的文件会被分成两个 chunk，但是第二个 chunk 只有 1MB 的数据却占了 64MB 的大小 可能会导致 load imbalance，如一个小文件只有一个 chunk，因此存储这些 chunk 的 chunkserver 会成为 hot spot；但是在文章提到的应用中，hot spot 并不是一个大问题，因为文章内的应用应用场景是 read large multi-chunk files sequentially 此外，在每个 chunk 都会有另外的两个副本，分别存储在三台机器上，其作用有两个：high availability 和 load balancing；在这个机制中，副本位置的选取是一个比较关键的问题，一个好的副本位置定义算法满足下面特性： (1) 保证足够的可靠性，例如，不能将所有副本存放在同一个磁盘或者物理机器上；(2) 保证写入高效性，多副本位置尽量靠近，降低写入延迟，提高读写性能 论文中创建chunk时副本位置的选择方法如下： （1）选择存储空间利用率最低的节点和磁盘（2）选择最近一段时间内新建 chunk 数量较少的节点和磁盘；（3）将多个副本分散在不同的机架上 1和3比较容易理解，2是为了保证一个节点/磁盘不会被频繁新建chunk（新建完接下来就是数据写入了），否则很容易沦为 hot spot，导致磁盘IO和网络带宽被占满，影响效率。 operation log由于 metadata 存储着 GFS 的核心信息，因此 GFS 还设置了日志记录 metadata 的变更信息，这个日志就是 operation log operation log 中一个关键信息是时间信息，包括 chunk 的版本、创建时间等，从而能够处理 concurrent opration client 请求的 operation 首先会被 master 接受，然后 master 会先写日志，在修改内存中的 metadata，这样即使出现断电等异常，也不会丢失更新，因为可以在重启时通过 operation log 重新构造内存的 metadata 如果 operation log 记录着 GFS 自使用以来的所有 operation，那么 log 会非常大且重启时构建耗时会非常长，GFS采用的机制是当 log 达到一定大小时，将当前内存的 metadata 持久化到硬盘上，称为 checkpoint；则 operation log 只需要存储创建 checkpoint 的时刻后的所有 operation，恢复时根据 latest checkpoint 恢复最新状态，且重新执行一遍 opration log 里面的操作即可 single mastersingle master 的设计显然存在着单点故障的问题，但是论文表明这么做两个理由 (1)这样大大减化了设计和实现 (2)实际数据直接在 client 和 chunkserver 间交流，所以 single master 不会成为 bottleneck master 与 chunkserver 通过周期性的 HeartBeat 通信，用于动态搜集 chunkserver 的状态：如 chunkserver 上有哪些 chunk，从而 master 能够及时更新而无需长期存储这些信息 读写操作下面主要讨论 GFS 中的读写操作 读读操作比较简单，上面的系统架构图也显示了这一过程，其步骤如下 (1) client 告诉 master 需要读取的具体文件及其位置(chunk index)(2) master 返回这部分文件所在的 chunservers 以及 chunk 的 version(3) client 缓存这些信息(信息有一个过期时间，client 会等到这个信息过期后才会再次向 master 请求, 从而缓解了频繁读写时，向 master 请求次数过多从而导致 master 负载过大)(4) client 请求最近的 chunkserver，然后检查 chunkserver 上 chunk 的 version 是否与从 master 获取的相同；如果相同则读取数据，否则重新向 master 请求这些信息 写写操作主要分为两种：write 和 append，write 是修改数据，append 则是在文件末尾添加数据 首先是 write 的过程，为了让同一个 chunk 多个副本数据保持一致， master 将存储 chunk 的其中一个 chunkserver 作为 primary，其他是 secondary，primary 用于确定数据写入这个 chunk 的顺序，secondary 则复制 primary 的写入顺序即可，下图是一个写操作的流程 各个步骤的具体操作如下 (1) client 询问 master 要写入的 chunk 所对应的 primary 和 secondary 位置（如果这时没有 primary，master就会指定一个(2) master 返回相关信息(chunk locatioin，chunk version 等)，client会把信息存入cache，以后就不再重复询问 master 以节省开销，直到该 primary 的身份失效(3) 客户端把数据发给所有 replicas (包括 primary 和 secondary)，replicas 们会把数据暂存在 LRU buffer cache 中，但是此时还并没有真的写入磁盘(4) 当所有 replica 都确认收到数据后，client 发写入指令给 primary；primary 会给这个指令定一个序列号(当同时收到多个 client 的请求时，在 primary 这里确定顺序)，primary 依序列号修改本地的数据(5) primary 把写入指令和序列号发给 secondary，secondary都依同样序列号修改自己的数据(6) 当 primary 收到 secondary 的回复时，返回成功信息给客户端(7) 如果有 secondary 失败了，primary 会返回失败信息给客户端。此时数据就是不一致的。客户端会发起重试。 另外一个写操作是 record append，其过程与上面相似，但是在这里 client 不会指定 offset，而是只提供数据，GFS 会把数据 append 进去后再返回 offset 给 client。 record append 还在 primary 添加了以下逻辑： 每次 append 时会针对待写文件的最后一个 chunk; 如果发现该 chunk 剩余空间不足以写入，则把当前 chunk 用空白补齐（padding)，然后把数据写入新的chunk 数据的写入是at-least-once，如果写失败了（即只在部分secondary上写成功），则会在新的末尾重新写一次，这样就会导致上次已经写成功的 replica 上数据出现两次，而上次写失败的replica上会有一段空白。返回给客户端的是成功写入的offset位置 客户端程序需要能正确处理这些情况：对于不正确的数据，可以在每个记录开头加一个magic number，或者加一个checksum之类；对于重复的数据，需要客户端判重，比如在记录里加一个unique id。 其他策略 chunk version 上面提到读写过程中，master 均会告诉 client 对应的 chunk 目前的 version，从而保证 clinet 读取的是最新的数据 chunk version 会在 master 为这个 chunk 选择新的 primary 时增加，并通知包含这个 chunk 的所有 chunkservers 更新这个 version 如果 client 在某个 chunkserver 读到的 chunk 的 version 与从 master 读取的不同，说明这个 chunkserver 的数据不是最新的 snapshot snapshot是对系统当前状态进行的一次拍照。用户可以在任意时刻回滚到快照的状态 采用 COW(copy-on-wirte) 机制实现 snapshot，即如果被 snapshot 的文件有更新操作时，就将文件的要被更新的chunk复制一份，然后对复制的chunk进行更新，而原来的chunk作为快照数据被保留，以后要恢复到该快照时，直接将该chunk读出即可 当GFS的Master节点收到Snapshot请求时，其处理逻辑如下 回收 snapshot 请求覆盖的文件的 chunks 上的租约(即撤销 primary)，这样的话接下来客户端要对文件修改时，就必须向 master 申请，而此时 master 就可以对 chunk 进行复制 master 在日志中记录本次 snapshot 操作，然后在内存中执行 snapshot 动作，具体是将被 snapshot 的文件或目录的元数据复制一份，被复制出的文件与原始文件指向相同的 chunk； 假如客户端申请更新被 snapshot 的文件内容，那么找到需要更新的 chunk，向其多个副本发送拷贝命令，在其本地创建出 chunk 的副本 ，之所以本地创建是因为可以避免跨节点之间的数据拷贝，节省网络带宽； 客户端收到 master 的响应后，表示该 chunk 已经 COW 结束，接下来客户端的更新流程与正常的没有区别。 checksum checksum 解决的是数据完整性问题，即磁盘损坏从而导致数据损坏的问题 每个 chunk 会被划分为大小为 64KB 的block，每个 block 有一个 32 位的 checksum client 从某个 chunkserver 读取数时，chunkserver 首先会验证要读取的数据的 checksum，如果 checksum 不符合已知记录(写入时的记录)，会返回错误，从而让 client 去读取其他 chunkserver 上的 chunk 一致性由于 GFS 中的 metadata 只在 master 可写，因此通过加锁保证修改的 atomicity；而对于修改 metadata 的 concurrent operation，operation log 中定义了这些 operation 的顺序， metadata 的一致性能够得到保证。 因此这里的一致性着重讨论的是文件的一致性，GFS 针对文件定义了以下的一致性状态 上图中的四种状态含义如下 defined：从客户端角度来看，客户端完全了解已写入集群的数据的 offset，例如，客户端串行写入且成功（serial success），此时的状态是defined consistent：客户端角度来看，chunk 多副本的数据完全一致，但不一定 defined(defined 包含了 consistent) inconsistent：多副本数据不一致 undefined：数据未定义 下面分别以几个案例介绍上面的状态，这部分内容主要摘自这里 serial write 当 client 串行更新时时，客户端自己知道写入文件范围以及写入数据内容，且本次写入在数据服务器的多副本上均执行成功。因此，本次写结果对于客户端来说就是明确的，且多副本上数据一致，故而结果是 defined。如下图： concurrent write 多个 client 同时写入时, 由于多个客户端由于写入范围可能交叉而形成交织写。这时候，由于单个客户端无法决定写入顺序（只有 primary 才能决定谁先写谁后写），因此，即使写入成功，客户端仍无法确定在并发写入时交叉部分最终写入结果，但是因为写入成功，所以多副本数据必然一致， 如下图所示 图中红色部分代表并发追加的部分，这部分数据由于无法确定谁先谁后执行，因此结果不确定；但由于跟新成功，因此，副本间数据是一致的，这就是consistent but undefined。需要注意的是，consistent but undefined 只会出现在原始的write操作被划分为几个子操作时，原文解析如下 If a write by the application is large or straddles a chunk boundary, GFS client code breaks it down into multiple write operations. They all follow the control flow described above but may be interleaved with and overwritten by concurrent operations from other clients. Therefore, the shared file region may end up containing fragments from different clients, although the replicas will be identical because the individual operations are completed successfully in the same order on all replicas. This leaves the file region in consistent but undefined state as noted in Section 2.7. 而无论是 serial write 还是并行 concurrent write，一旦失败，多个 chunk 副本上的数据可能都不一致了，因此便是 inconsistent，必然也是undefined。 append 上面提到，client 的 append 操作无需指定offset，由 chunk 的 primary 根据当前文件大小决定写入offset，在写入成功后将该offset返回给客户端。因此，客户端能够根据offset 确切知道写入结果，无论是串行写入还是并发写入，其行为是defined。如下所示 假设上面的append经历了一次重试，那可能实际chunk的布局如下 由于第一次写失败（错误可能发生在任意一个副本），导致了多副本之间从50至80的数据可能不一致。但接下来重试成功，从80至110之间的数据一致，因此，其状态是 interspersed with inconsistent。 小结本文主要介绍了 GFS 的适用场景、系统架构、读写过程以及 一致性的保证，GFS 可以说是为 MapReduce 量身定做的文件系统: 大文件、文件写入后基本不修改、更着重吞吐量等，也有人说认为 GFS 也就没有 MapReduce 了，其实与其说 MapReduce 多么牛，不如说是GFS牛，因为 MapReduce 模型早就是数据库领域几十年前玩剩下的了, 但是只有 Google 做出了那种廉价高效的分布式系，主要是因为 Google 的下层的支持系统也就是 GFS 做得好。 参考 The Google File System Google File System Paper Reading: GFS]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统笔记(2)-RPC and threads]]></title>
      <url>%2F2019%2F01%2F16%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0(2)-RPC%20and%20threads%2F</url>
      <content type="text"><![CDATA[本系列文章是学习课程 6.824: Distributed Systems 时的一些学习笔记，整个课程的相关材料已整理至 DistributedSystemInGo。本文是 LEC2 的内容，主要介绍了 RPC 的概念并通过 RPC 实现了一个简单的 c/s 架构的 kv 数据库；同时介绍了多线程编程并通过两种方式实现了一个多线程爬虫。 这门课程采用的语言是 go，原因是 go 对 concurrency、RPC 和 gc 等有较好的支持，且上手较快，可以把问题集中在分布式系统而不是由于对语言不熟悉而带来的 bug。因此，上面提到的两个 demo 也是采用 go 实现。 RPCRPC 基本概念RPC(Remote Procedure Call) 的概念很好理解，类比函数调用，只是两个函数不在一个内存空间，不能直接调用，需要通过网络进行远程调用。RPC 的调用过程如下，图片摘自 Remote Procedure Calls 需要明确的一点是 RPC 只是一个概念，因此广义上任意实现远程调用的方法都可称为 RPC（如 http），而区别于各个 RPC 的实现(RPC 框架)在于其实现的协议的不同，而最基本的协议包含编码协议和传输协议。 编码协议表明了该如何将要传递的参数等信息打包好；常见的有基于文本编码的 xml、 json，也有二进制编码的 protobuf、binpack，也可自定义协议。 而传输协议则表明如何将打包好的数据传输到远端；如著名的 gRPC 使用的 http2 协议，也有如 dubbo 一类的自定义报文的tcp协议(精简了传输内容)等。 类似于计算机网络中的各种协议一样，这些协议是比较繁琐的且通用的，因此产生了很多 RPC 框架来完成这些协议层面的东西，而除了上面提到的最基本的编码协议和传输协议，成熟的 rpc 框架还会实现额外的策略, 如服务注册发现、错误重试、服务升级的灰度策略，服务调用的负载均衡等。上面提到的 gRPC 和 dubbo 就是两个比较有名的 RPC 框架，通过 RPC 框架，在编码时能够像本地调用一样使用 RPC。 对于一个 RPC 框架，实现中最关注以下三点(1) Call ID映射：即告诉远程服务器要调用的是哪个函数或应用(2) 序列化和反序列化：即上面的编码协议(3) 网络传输：即上面的传输协议更详细的解析可参考这个回答，谁能用通俗的语言解释一下什么是 RPC 框架？ - 洪春涛的回答， RPC in gogo 提供了自己的 rpc 库，这里通过这个库来实现一个简单的 c/s 架构的 kv 数据库 server 端的核心代码如下，KV 这个 struct 提供了 Put 和 Get 这两个存取方法，且存取时通过 sysc.Mutex 进行加锁来保证一致性。通过 go 内置的 rpc 框架启动一个 server 且将 KV 注册在 1234 端口，网络传输采用的是 tcp 协议；每当 server 与一个 client 建立一个连接时，server 会启动一个线程(goroutine) 去处理这个连接对应的请求 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152type KV struct &#123; mu sync.Mutex keyvalue map[string]string&#125;func (kv *KV) Get(args *GetArgs, reply *GetReply) error &#123; kv.mu.Lock() defer kv.mu.Unlock() reply.Err = "OK" val, ok := kv.keyvalue[args.Key] if ok &#123; reply.Err = OK reply.Value = val &#125; else &#123; reply.Err = ErrNoKey reply.Value = "" &#125; return nil&#125;func (kv *KV) Put(args *PutArgs, reply *PutReply) error &#123; kv.mu.Lock() defer kv.mu.Unlock() kv.keyvalue[args.Key] = args.Value reply.Err = OK return nil&#125;func server() &#123; kv := new(KV) kv.keyvalue = map[string]string&#123;&#125; rpcs := rpc.NewServer() rpcs.Register(kv) l, e := net.Listen("tcp", "ServerIP:1234") if e != nil &#123; log.Fatal("listen error:", e) &#125; go func() &#123; for &#123; conn, err := l.Accept() if err == nil &#123; go rpcs.ServeConn(conn) &#125; else &#123; break &#125; &#125; l.Close() fmt.Printf("Server done\n") &#125;()&#125; client 端的程序如下，client 首先通过 Dial 函数 与 server 建立连接，Get 和 Put 则分别调用了 server 端对应的 Get 函数和 Put 函数（在 rpc.Call 中声明), 可以看到，进行 RPC 就如同本地调用一样123456789101112131415161718192021222324252627282930func Dial() *rpc.Client &#123; client, err := rpc.Dial("tcp", "ServerIP:1234") if err != nil &#123; log.Fatal("dialing:", err) &#125; return client&#125;func Get(key string) string &#123; client := Dial() args := &amp;GetArgs&#123;"subject"&#125; reply := GetReply&#123;"", ""&#125; err := client.Call("KV.Get", args, &amp;reply) if err != nil &#123; log.Fatal("error:", err) &#125; client.Close() return reply.Value&#125;func Put(key string, val string) &#123; client := Dial() args := &amp;PutArgs&#123;"subject", "6.824"&#125; reply := PutReply&#123;""&#125; err := client.Call("KV.Put", args, &amp;reply) if err != nil &#123;log.Fatal("error:", err) &#125; client.Close()&#125; 完整的代码可参考这里 failure in RPC上面只是一个简单的 RPC 例子，没有考虑到这个过程中可能出现的异常情况。而最常见的异常个情况就是 client 发出 request 后收到不 server 的 response，引起这种问题的原因有很多：网络断了、server 宕机了等。针对这个问题有什么解决方法呢？ 最直观也是最简单的方法是让 client 等待一段时间收不到回复后的重新发送 request，且设置重复发送的次数的上限，如果超过这个上限，则先调用的应用程序返回 error 异常信息。 但是，当 client 重复发送请求时，server 有可能已经收到了 client 的前一个请求，只是网络的延迟使得 client 还没收到 response，那这时候 server 就会收到重复的 request。如果 client 发出的是读请求，那么问题不大；但是如果是写操作，server 端就需要处理这些重复的写请求从而使得最终只有一个被执行。 这里可针对 server 端采取 at most once 的策略，即同一个 request 最多只能在 server 端被执行一次，如果收到了重复的 request，那么就将之前的结果返回。这样需要解决的问题就是为每个 request 生成一个 unique id，生成 unique id 也有很多方法，比如说可以利用 client 的 ID 及其 request 编号的组合等方法。 go 的 RPC 库采用的就是 at most once 的策略，库已经在传输层进行了过滤重复 request 的操作，因此在代码中无需体现这一操作。 threads多线程是 concurrency 的重要手段，在 golang 中的 thread 也被称为 goroutine，一般多线程都能够充分利用 CPU 的多个核(Python 的 Cpython 解析器除外，GIL 的限制) 进行多线程编程时有以下几点值得注意： （1）同一个进程内的线程是共享地址空间的，因此在对共享数据进行写操作时需要加锁（2）当任务间有依赖性，一项任务拆分给多个线程去完成时，线程间往往需要先共完成任务 A 才能开始任务 B（3）要确定线程并行的粒度，比如说多线程爬虫，每个线程是负责一个站点？还是站点下的一个目录？一般粗粒度的实现会很简单，但是并行性不高；而细粒度的并行化程度会更高，但是会更容易出现死锁等问题。 下面会通过 golang 实现一个多线程爬虫，分别采用了两种方式，第一种是通过经典的队列方法（channel)，这种防范没有加锁；第二种则通过加锁(mutex)和设定任务完成的 threshold(waitgroup)；在这个例子中两种方式的并行化粒度均是网页 channel队列是很常见的多线程编程采用的方式，将需要执行的任务送入队列，然后线程从队列中取出任务执行，并将新的任务入列(在这个例子中就是当前网页所含有的其他网页的链接)，这里还需要额外检查网页是否已经被抓取过原因有两个 （1）网页间的指向有可能形成闭环，不判断会导致死循环（2）效率问题，不希望执行重复工作 因此，golang 通过 channel 完成的多线程爬虫如下所示, master 从队列头读出一个网页并判断其是否已经被执行，如果没有执行，就启动一个 goroutine 来执行 dofetch 任务，dofetch 会将其当前网页所指向的其他网页入列 12345678910111213141516171819202122232425262728293031323334353637func dofetch(url1 string, ch chan []string, fetcher Fetcher) &#123; // body is content of url1, urls are those to which url1 refer body, urls, err := fetcher.Fetch(url1) if err != nil &#123; fmt.Println(err) ch &lt;- []string&#123;&#125; &#125; else &#123; fmt.Printf("found: %s %q\n", url1, body) ch &lt;- urls &#125;&#125;func master(ch chan []string, fetcher Fetcher) &#123; n := 1 fetched := make(map[string]bool) for urls := range ch &#123; for _, u := range urls &#123; if _, ok := fetched[u]; ok == false &#123; fetched[u] = true n += 1 go dofetch(u, ch, fetcher) &#125; &#125; n -= 1 if n == 0 &#123; break // or close(ch) &#125; &#125;&#125;func CrawlConcurrentChannel(url string, fetcher Fetcher) &#123; ch := make(chan []string) go func() &#123; ch &lt;- []string&#123;url&#125; &#125;() master(ch, fetcher)&#125; 那终止的条件是什么呢？队列为空, 但是在 golang 中没有显示判断 channel 为空的方法，且通过 for 遍历 channel 时，只有关闭了 channel 后循环才能正常退出，否则会出现 deadlock 的错误，但是显然无法随意关闭 channel，因为每一个 goroutine 都不知道是否还有其他的 goroutine 要写入这个 channel；上面的解决方法是通过 n 来记录当前队列的长度，如果 n == 0 就关闭 channel 或退出 mutex 与 waitgroup上面只是在 master 中判断某个 url 是否已经被访问过了，那么每个独立的 goroutine 能否自行判断某个 url 是否别访问过了呢？答案是肯定的，只是需要对存储 url 是否被访问的 hashmap 进行加锁，在 golang 中可通过 sys.Mutex 对某个数据进行加锁和解锁操作 同样，我们需要设定终止条件。该如何衡量所有任务都完成，这里我们可以想像一颗多叉树的结构，每个节点表示一个网页，而每个节点的子节点是其网页中指向的其他网页，那么一个节点被判为完成当且仅当其所有的子节点都完成，这就涉及到了上面提到的任务间的依赖性问题，实际上就是要等待 goroutines 共同完成当前节点的所有子节点，这要用到 sys.Waitgroup 实现的具体代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344type fetchState struct &#123; mu sync.Mutex fetched map[string]bool&#125;func (f *fetchState) CheckAndMark(url string) bool &#123; defer f.mu.Unlock() f.mu.Lock() if f.fetched[url] &#123; return true &#125; f.fetched[url] = true return false&#125;func mkFetchState() *fetchState &#123; f := &amp;fetchState&#123;&#125; f.fetched = make(map[string]bool) return f&#125;func CrawlConcurrentMutex(url string, fetcher Fetcher, f *fetchState) &#123; if f.CheckAndMark(url) &#123; return &#125; body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) var done sync.WaitGroup for _, u := range urls &#123; done.Add(1) go func(u string) &#123; defer done.Done() CrawlConcurrentMutex(u, fetcher, f) &#125;(u) // Without the u argument there is a race &#125; done.Wait() return&#125; 上面可以说是 sync.WaitGroup 的经典用法，在为每个线程分配任务时通过 done.Add(1) 增加未完成任务， 在线程完成任务时通过 done.Done() 表示当前子任务已完成, 通过 done.Wait() 阻塞直到所有的子任务都完成。 小结这一课介绍了 RPC 和多线程编程的基本概念，并分别用 go 语言实现了一个简单的例子，主要是为后面的几个实验做准备。 RPC 是个广义的概念，RPC 需要解决最基本的通信协议和编码协议；除此之外，一些高级的 RPC 框架还帮我们处理了、服务注册发现、错误重试等细节，让远程调用如同本地调用一样。 关于多线程编程，给出的爬虫例子实现了两种形式的多线程编程，一种是 Mutex + WaitGroup 的方式，一种则是 Channel 的方式；需要注意的是，这两种方式不是非此即彼，而是可以混用的，可以参考 MutexOrChannel 的介绍，比如说通过 Mutex 来让每个单独的线程判断某个 url 是否被访问过，通过 Channel 来将未完成的队列入列，通过 WaitGroup 来分类下载资源（其实这也涉及到了并行的粒度的划分，即并行地下载某一类的资源）。回想起来，很久之前写的一个爬取几个输入法的词库的程序 ThesaurusSpider 就是这么做的，只是通过 python 实现而已，有兴趣可参考。 参考： 既然有 HTTP 请求，为什么还要用 RPC 调用？ 谁能用通俗的语言解释一下什么是 RPC 框架？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式系统笔记(1)-MapReduce]]></title>
      <url>%2F2019%2F01%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0(1)-MapReduce%2F</url>
      <content type="text"><![CDATA[一直都想系统性地学习一下分布式系统的一些理论，所以打算开个坑学习一下 MIT 的课程 6.824: Distributed Systems 。本文主要是 LEC 1 中的内容，简单介绍了分布式系统的几个核心问题，以及经典的分布式计算框架-MapReduce, 虽然这是耳熟能详的一个框架（或者说是编程范式）了，但是其设计思想至今还是非常值得参考的。 分布式系统概述简单来说，分布式的目的就是通过多台机器进行协作来提供一台机器所无法提供的运算能力和存储能力。除了通过增加机器来拓展运算能力和存储能力的伸缩性，分布式会额外带来机器隔离后的安全性、多份数据副本的错误容忍性等。 性能(performance)，一致性(consistency), 容错(fault tolerance) 是分布式系统中比较关注的问题。 如何让总体的运算能力随着机器数量的增长而线性增长？这是 performance 所关心的，各台机器的负载差别大吗(load balance)? 网络能承受得住随着机器数量增加而增加的通信吗(共享资源的瓶颈)?所有的代码都能够被并行化吗？ 如何让多台机器上的同一份数据副本被多个进程读写后仍然保持一致？这是 consistency 所关心的，这个过程需要考虑的问题就太多了，因此也产生了很多一致性协议(Paxos, Raft等)专门处理这个问题。比如说当 client 或 server 在写数据的不同阶段时宕机该怎么办？网络的抖动导致了 server的假死(脑裂)怎么办？性能(performance)与一致性(consistency)总是相悖的，也就是说强一致性往往会导致比较慢的系统，高性能的系统通常会以牺牲强一致性为代价，需要根据具体场景进行 trade-off。 如何让不可避免的宕机不影响总体的服务？这是 fault tolerance 所关心的，在一台机器宕掉后，其执行过的 tasks 该怎么办？其他与这台机器发生过的 communication 的机器的依赖性该怎么解决？ 可以说任意的分布式系统都会涉及到这三个问题，只是会各有侧重；目前常见的分布式系统从功能上可分为分布式计算框架(MPI, MapReduce, Spark等)、分布式文件系统(GFS, HDFS等)、分布式调度系统(Mesos, Yarn等)。 MapReduce很久之前写过一篇关于 MapReduce 使用的文章：分布式机器学习(4)-Implement Your MapReduce，这里则主要是根据 MapReduce 论文着重讲述 MapReduce 的一些原理。 下图是从 MapReduce 的论文 MapReduce: Simplified Data Processing on Large Clusters 中摘取的 通常 MapReduce 被人所了解的是上图中 (3)(4)(5)(6) 的过程，也就是输入文件被分成 M 个小文件，每个小文件分别在 Map phase 被 Map 函数处理后输出一系列的（key,valu) 对，然后对 key 进行哈希取模找到除了这个 key 的 reducer worker，reduce worker 在 Reduce phase 会对通过 reduce 函数处理相同的key，两个函数的输入输出如下所示 12Map: (k1,v1) → list(k2,v2)Reduce: (k2,list(v2)) → list(v2) 这个过程理解起来很简单，框架使用起来也不麻烦，这是因为 MapReduce 框架隐藏了上图中通过 master 追踪各个 task 是否顺利完成、以及如何进行 fault tolerance，而这也是论文最值得关注的点之一。 Execution Overview上面讲的 MapReduce 过程可以说是一个编程范式，下面主要根据原始论文讲述整体的执行流程，各个步骤的编号跟上图保持一致 (1) User Program 中的 MapReduce 库会将输入分成 M 个小文件（通常是 16MB 到 64MB），然后 fork 出多个子进程(2) 子进程中有一个作为 master，其他作为 worker。假设有 M 个 map task 和 R 个 reduce task（M 和 R 都远大于机器数量），master 会分配挑选处于 idle 的 worker 分配 task(3) 被分配了 map task 的 worker 读入对应的输入文件，从输入文件中解析出 key/value pairs，并将每个 pair 输入用户自定义的 Map 函数，Map 函数输出的 intermediate key/value pairs 被缓存在内存中(4) 缓存在内存中的 intermediate key/value pairs 会被周期性地写入 map worker 本地的磁盘，根据 key 的不同分别写入到 R 个本地文件中，然后这些文件在本地的路径会传输给 master，从而 master 可以告知 reducer worker 到哪里读取数据(5) 直到所有的 Map 过程完成，Reduce 过程才能开始；当 reduce worker 被 master 告知要读取的文件的位置时，会通过 RPC 从 map worker 的磁盘读取这些数据；reduce worker 读取完所有的 intermediate key/value pairs 后会针对 key 进行排序**，从而让所有相同的 key 汇聚在一块(6) reduce worker 遍历排序后的 pairs，并将每个独立的 key 及其对应的 value 集合传输给 Reduce 函数，Reduce 函数则会将输出添加至最终的文件中。 在完成一个 MapReduce 计算过程后会产生 R 个文件，但是一般不需要对这 R 个文件进行合并，因为这些文件可能会被作为下一个 MapReduce 计算的输入，或者被另外的分布式引用处理，而分布式应用往往能够处理这样被分隔后的小文件。另外，这里的 MapReduce 配合了 GFS 的使用，所以从磁盘读写直接使用的是 GFS 文件系统。 Master 与 Fault Tolerancemaster 会记录每个 map task 和 reduce task 的状态(idle, in-progress 或者 completed), 对于那些不是 idle 的task，master 还会记录执行这个 task 的机器。 从上面描述的 MapReduce 流程可知，master 是 map worker 和 reduce worker 的桥梁，每个 map task 完成后都会将其产生的 M 个文件的路径和大小传输给 master，master 则会将这些信息 push 给那些有处于 in-progress task 的 reduce worker。 Fault Tolerance 可分为两大类：worker 的 Fault Tolerance 和 master 的 Fault Tolerance；而 worker 又可分为 map worker 和 reduce worker 两种，下面分别介绍针对这三种角色的 Fault Tolerance 的策略。 首先，master 会周期性地 ping 各个 worker，并根据是否收到回复来判断 worker 是否发生了宕机。 当一个 map worker 被 master 判为宕机后，这个 worker 所有的 task(包括 in-progress 和 completed 的)都会被重置为 idle 状态，从而让这些 task 能够被重新分配给其他的 map worker 来重新执行。而发生这种重新执行的情况时，所有的 reduce worker 都会被告知重新执行的这个 task 的是哪个 map worker，从而让 reduce worker 从新的 map worker 那里读取数据（针对那些还没从已经宕机的 map worker 读取数据的 reduce worker，原来那些已经读了数据的 reduce worker 不需要） 当一个 reduce worker 被 master 判为宕机后，这个 worker 那些未完成的 task(也就是处于 in-progress 状态的)会被置为 idle 状态，而已完成的 task 不会。这是因为 reduce worker 将输出写入 GFS，worker 宕机后数据仍然可以被读取，而 map worker 则是将输出结果写入到本地的磁盘，宕机后数据无法被读取 这里有两个问题值得思考： map worker 宕机后，reduce worker 从新的 map worker 读取的结果与从原来宕机的 map worker 读取的结果是否一致？ 如果 reduce worker 在数据写入一半的时候宕机了，已经写入的数据怎么办？ 第一个问题的答案是肯定的，因为 map 函数不记录状态，也就是对固定的输入有固定的输出，此外，reduce task 会在所有的 map task 完成后才开始执行，因此也保证了 reduce worker 总能读到 map worker 完整的输出文件。 第二个问题与 GFS 提供的 atomic rename 特性有关，reduce worker 会先将结果写入到临时文件中，直到所有的结果都完成后才将临时文件重命名为最终文件并写入 GFS 中；这个特性也让多个 reduce worker 重复执行一项任务时最终只产生一个文件。 针对 master 宕机的情况，论文的做法是令 master 周期性地往磁盘写入 checkpoint，重启 master 后从上次的 checkpoint 重新执行。 Load Balance，Backup Task 与 Localityload balance 指的是某个 worker 执行 task 时间过长，导致其他已完成 task 的 worker 都在等待这个 worker 完成（因为任务之间有依赖性），这一执行时间过长的 worker 也被称为 文章针对这一问题的做法是把每个 task 分得尽量小，即 M(map task 的数量) 和 R(reduce task 的数量) 的值要远远大于机器数量。这样就不会出现某个 task 执行过的时间过长，不仅解决了上面的问题，还加速了 fault tolerance 后的 recovery。 除了 task 过大，出现 straggler 的原因也可能是机器本身的硬件问题，哪怕 task 已经分得很小了。文章解决这一问题的做法是当 straggler 出现的时候，master 会把 straggler 在做的 task 分给另外一个 worker 做，谁先做完就汇报给 master，而 master 也只会接收其中一个的完成的消息，这在文中称为 Backup Task。 在任意分布式系统中，当 worker 数量增多，网络通信的负载都会变大。文章利用了 MapReduce 和 GFS 架设在同一组机器上的特性，从而让 Map 过程从 GFS 读取文件时尽可能读取处于本地磁盘的的 copy（GFS 为每份数据创建了三份 copies），在本地磁盘找不到时才读取其他 worker 磁盘的数据，这样就大大减少了网络的开销，而文中又称这一特性为 Locality。 小结回到文章开头提到的分布式系统中三个比较关注的问题，MapReduce 这篇论文中主要是关注其中的 performance 和 fault tolerance。 为了提升 performance，通过把 task 分得更小来获得更好的 load balance，通过 backup task 来降低 straggler 对整个系统的影响，通过 locality 来减少网络的负载。 针对 fault tolerance，则通过为 task 设定状态，失败的 worker 的 task 被重置为 idle 状态，从而找到新的 worker 重新执行这些 task。 每种框架都有其适用场景，而对于 MapReduce，首先就是任务要能够表达成一个或多个 MapReduce 过程，文中提到的一些任务包括: Distributed Grep、Distributed Sort、Count of URL Access Frequency、ReverseWeb-Link Graph、Inverted Index等；其次数据量要足够大才能显示出 MapReduce 的效率(其实对于任意分布式系统基本都是这样, 否则整个系统调度的开销比计算的开销还要大)；还有就是涉及到多次 shuffle (也即是多个 MapReduce 过程)时，由于要与磁盘多次交互，因此虽然能够实现，但是效率很低，这时候就要考虑其他更灵活的框架了。不要局限于一定要把算法表达成 MapReduce 过程，而是可以考虑更加灵活的框架，如 spark 等。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[EE 问题概述]]></title>
      <url>%2F2019%2F01%2F05%2FEE(Exploitation%20Exploration)%20%E9%97%AE%E9%A2%98%E6%A6%82%E8%BF%B0%2F</url>
      <content type="text"><![CDATA[EE(Exploitation &amp; Exploration) 问题在计算广告/推荐系统中非常常见，甚至在更广义的范围上，任意决策问题都会牵涉到 EE 问题。简单来说，这个问题就是要解决的是在决策时到底是根据已有经验选择最优的策略(Exploitation)，还是去探索一些新的策略来提升未来的收益(Exploration)。本文主要介绍解决这个问题的三种比较常见的方法：随机方法，UCB 方法，Thompson sampling 方法，侧重于方法的具体流程和基本思想。 MAB 建模EE 问题一般会通过 MAB(Multi-Armed Bandit) 进行建模, 如下所示，所有 arm 就是每次决策中可作出的选择，拉下某个 arm 表示作出了相应的选择。 MAB 符号化表述如下 MAB 可表示为一个二元组 &lt;$A, R$&gt; $A$ 表示为一系列可能的动作, $R(r|a)$ 则表示给定动作下的奖赏的分布， 每一时刻根据给定策略从 $A$ 选择动作 $a_t$, 同时环境根据分布 $R(r|a)$ 生成奖赏 $r_t$ 目标是最大化奖赏之和 $\sum_{t=1}^T r_t$ 上面的第 3 步的策略就是下面要介绍的 EE 问题的解决方法，除去随机方法，UCB 方法和 Thompson sampling 方法的思想均是通过定义每个 arm 的收益的期望，然后选择收益期望最大的 arm。UCB 是频率学派的思想，认为每个 arm 的收益期望是固定的，通过试验记录得到其历史收益状况，然后加上一个 bound 构成了收益期望；Thompson sampling 则是贝叶斯学派的思想，认为 arm 的收益期望服从一个特定的概率分布，通过试验记录更新分布的参数，然后从每个 arm 的分布中产生收益期望。 而根据一个 arm 的历史试验记录判断其优劣又有两种方法，因而也衍生了两类 bandit 问题：Bernoulli Bandit 和 Contextual Bandit。在 Bernoulli Bandit 中，认为每个 arm 的优劣（即当前试验是否产生收益）是服从伯努利分布的，而分布的参数可以通过历史收益状况求解；而在 Contextual Bandit中，没有直接定义出一个概率分布来描述每个 arm 的优劣, 而是假设了 arm 的优劣和描述 arm 的特征组成的向量 $x$ 存在一个线性关系：$x^T \theta$，参数 $\theta$ 可通过历史样本求解和更新。 UCB 方法和 Thompson sampling 方法均可解决这两类问题，UCB 解决 Bernoulli Bandit 的方法有 UCB1，UCB2 等，解决 Contextual Bandit 的方法有 LinUCB 等；而 Thomson Sampling 解决 Bernoulli Bandit 时采用了 Bernoulli 分布和 Beta 分布，解决 Contextual Bandit 时采用了两个正态分布。后面会详细介绍这些方法。 随机方法$\epsilon$-greedy$\epsilon$-greedy 是一种最简单的随机方法，原理很简单：每次决策时，以 1 - $\epsilon$ 的概率选择最优的策略，以 $\epsilon$ 的概率随机选择任意一个策略; 并且在每次做出决策获取到真实的 reward 后更新每个决策的收益情况（用于选择最优策略）。伪代码实现可参考 Multi-Armed Bandit: epsilon-greedy $\epsilon$-greedy 存在着以下几个比较显著的问题 (1) $\epsilon$ 是个超参数，设置过大会导致决策随机性过大，设置过小则会导致探索性不足(2) $\epsilon$-greedy 策略运行一段时间后，对各个 arm 的收益情况有所了解，但没有利用这些信息，仍然不做任何区分地随机 exploration（会选择到明显较差的item）(3) $\epsilon$-greedy 策略运行一段时间后，但仍然花费固定的精力去 exploration，浪费了本应该更多进行 exploitation 机会 针对第 2 个问题，可以在 $\epsilon$-greedy 策略运行一段时间后，选择出收益最高的前 $n$ 个 arm，然后 exploration 时从这 $n$ 个 arm 中随机选择。 针对第 3 个问题，可以设置进行 exploration 的概率 $\epsilon$ 随着策略进行的次数而逐渐下降，比如说可以取如下的对数形式, $m$ 表示目前进行了 $m$ 次的决策 $$\epsilon = \frac{1}{1 + \log(m+1)}$$ Softmax通过 Softmax 进行的 Exploration 也称为 Boltzmann Exploration，这个方法通过一个温度参数来控制 exploration 和 exploitation 的比例，假设各个 arm 的历史收益为 $\mu_0$, $\mu_1$, ……, $\mu_n$, 温度参数记为 $T$，则选择某个 arm 时参考的指标为 $$p_i = \frac{e^{\mu_i/T}}{\sum_{j=0}^{n} e^{\mu_j/T}}(i=0, 1,…., n)$$ 当温度参数 $T=1$, 上面的方法就是纯粹的 exploitation；而当 $T \to \infty$ 时，上面的方法就是纯粹的 exploration，因此，可以控制 $T$ 的范围来控制 exploration 和 exploitation 的比例。某些文献也会将 $1/T$ 称为学习率。一个很直观的想法就是让 $T$ 随着策略运行次数的增加而下降，这样便可让策略从偏向 exploration 转为偏向 exploitation。 但是，这篇 paper Boltzmann Exploration Done Right 证明了单调的学习率（即$1/T$）会导致收敛到局部最优，并提出了一种针对不同的 arm 采用不同的学习率的方法，但是形式已经不是上面的 softmax 形式了。文章涉及的证明和公式符号较多，这里不再展开阐述，感兴趣读者可自行参考。 UCB 方法假如能够对每个 arm 都进行足够多次的试验，根据大数定律，次数越多，这些试验结果统计得到的收益便会约接近各个 arm 真实的收益。然而在实际中，只能对各个 arm 进行有限次的试验，因此这会导致根据统计得到的收益跟真实的收益存在一个误差，UCB 的核心就在于如果预估这个误差(也就是 UCB 中的 B(bound))，然后将 arm 统计的收益加上其通过 UCB 方法计算出来的 bound 进行排序，选择最高的那个。 UCB1UCB1 方法的理论基础是 Hoeffding’s inequality，该不等式的定义如下 假设 $X_1, X_2…X_n$ 是同一个分布产生的 $n$ 个独立变量，其均值为 $\overline{X} = \frac{1}{n}\sum_{i=1}^n X_i$, 则如下公式成立$$p(|E[X] - \overline{X}| \le \delta) \ge 1 - 2e^{-2n\delta^2}$$ 更直观地说，该不等式表明了 $n$ 个独立同分布的变量的均值与该变量的真实期望的误差小于某个预设的阈值 $u$ 会以概率 $1 - e^{-2nu^2}$ 恒成立。 回到我们的问题，可以将 $X_1, X_2…X_n$ 看做某个 arm 在 $n$ 次试验中获得的收益，则通过上面的式子可以设定一个 $\delta$ 使得公式成立, 然后用$\overline{X} + \delta$ 来近似真实的收益 $E(X)$；理论上也可用 $\overline{X} - \delta$，但是 UCB 方法会用上界，这也是 UCB 中 U(upper) 的含义。那么现在的问题便是 $\delta$ 该选多大了？ UCB1 方法中将 $\delta$ 设为如下公式，公式中的 $N$ 表示目前所有 arm 试验的总次数，$n$ 表示某个 arm 的实验次数 $$ \delta = \sqrt{\frac{2\ln{N}}{n}}$$ 直观地看上面定义的 $\delta$, 分子的 $N$ 对所有的 arm 是相同的，分母的 $n$ 则表示某个 arm 目前为止试验的次数，如果这个值越小，那么 $\delta$ 便越大，相当于 exploration；而当各个 arm 的 $n$ 相同时，实际上就是在比较各个 arm 的历史收益情况了。 UCB1 方法的流程如下，该图摘自 Optimism in the Face of Uncertainty: the UCB1 Algorithm 可以看到 UCB1 的 bound 完全是由 Hoeffding’s inequality 推导出来的，而除了 Hoeffding’s inequality，其他的一些 inequality 也能够推导出相应的 bound，Reinforcement Learning: Exploration vs Exploitation 中就提到了一些其他的 inequality Bernstein’s inequality Empirical Bernstein’s inequality Chernoff inequality Azuma’s inequality ……. UCB2从名字上基本就可以猜出 UCB2 是 UCB1 的改进，改进的地方是降低了 UCB1 的 regret 的上界，regret 指的是每次能获得的最大的收益与实际获得的收益的差距，这部分涉及到较多的数学证明，这里略去这部分，详细可参考 Finite-time Analysis of the Multiarmed Bandit Problem。UCB2 算法的流程如下，图片同样摘自 Finite-time Analysis of the Multiarmed Bandit Problem 从流程图上可知，UCB2 与 UCB1 相似，也是为每个 arm 计算一个 bound，然后根据 arm 的历史收益和 bound 选出 arm ，只是对这个 arm 不止试验一次，而是试验 $\tau(r_j+1) - \tau(r_j)$ 次。上面的 $a_{n, r_j}$ 和 $\tau(r)$ 定义如下，由于 $\tau(r)$ 要为整数，因此取了上界 $$a_{n,r} = \sqrt{\frac{(1+\alpha)\ln(ne/\tau(r))}{2\tau(r)}}$$ $$\tau(r) = \lceil (1+\alpha)^r\rceil$$ 上面式子中的 $\alpha$ 是个超参数，根据上面给出的论文中的实验结果(如下图所示)，这个值不能取得太大，论文建议值是 0.0001 LinUCB上面的 UCB1 和 UCB2 算法都是解决 Bernoulli Bandit 问题的，也就是假设每个 arm 的优劣是服从伯努利分布，而根据历史记录计算出的 $\overline {x}_j$（获得收益的试验次数和总试验次数的比值）其实就是伯努利分布的参数。 这样基于统计的方法很简单，但是问题也比较显著，因为 arm 的收益会跟多个因素有关（比如说某个 arm 在早上选择时没有收益，但是晚上就有了），利用这些信息可以预估得更准确；而基于统计的方法则忽略了这一点。 区别于 Bernoulli Bandit，这类利用了上下文信息的问题就是上面提到的 Contextual Bandit 问题，而 LinUCB 就是要解决这个问题的。 LinUCB 中没有直接定义出一个概率分布来描述每个 arm 的历史收益状况, 而是假设了 arm 的优劣和描述 arm 的特征的向量 $x$ 存在一个线性关系： $x^T \theta$ 实际上这是一个经典的 Linear Regression(收益不在局限于 0 和 1) 问题，$x$ 是 arm 的特征组成的向量(需要根据具体的问题选择特征), $\theta$ 则是模型的参数，每一次的试验就是一条样本，label 为具体的收益。 通过历史样本可以求解出 $\theta$, 则在每次选择选择 arm 时，LinUCB 会用 $x^T \theta$ 来替换掉 UCB1或UCB2 中的 $\overline {x}_j$，但是这还不是 LinUCB 的全部，作为 UCB 类方法，LinUCB 中还是有个 bound 的，因为毕竟从历史记录只能对 arm 进行有限次的试验，预估出来的收益情况与真实的还是存在差距的。 UCB1 中推到出 bound 的 Hoeffding’s inequality 不能直接应用到 LinUCB 中，而关于 linear regression 的 bound 最早是在这篇论文 Exploring compact reinforcement-learning representations with linear regression 中提出的，这里不详细展开具体的证明过程了。提出 LinUCB 的论文 A Contextual-Bandit Approach to Personalized News Article Recommendation 直接采用了这一结论，其表达形式，如下所示 $$p(|x_j^T\theta_j - E(r|x_j)| \le \alpha \sqrt{x_j^T(D_j^TD_j+I)^{-1}x_j}) \ge 1- \delta$$ 即对于某个 arm $j$, 计算出来的 $x_j^T\theta_j$ 与实际的期望相差小于 $ \alpha \sqrt{x_j^T(D_j^TD_j+I)^{-1}x_j}$ 的概率要大于 $1- \delta$, 其中 $D_j$ 是 arm $j$ 前面每次被观察到的特征组成的矩阵，比如说 arm $j$ 前面被观察了 $m$ 次，且特征组合成的向量的维度为 $d$, 则 $D_j$ 的大小为 $m$ X $d$, $I$ 为单位向量，而 $\alpha = 1 + \sqrt{\ln(2/\delta)/2}$ ,因此，只要根据概率选定 $\delta$，则各个arm 的 bound 便可通过 $\alpha \sqrt{x_j^T(D_j^TD_j+I)^{-1}x_j}$ 求出。 因此 LinUCB 算法流程如下，算法同时包含了选择 arm 的方式和更新 linear regress 模型。 上面的每个 arm 的 linear model 的参数都是独立的，论文针对这对点设计了这些 model 共享的一些参数，即将原来某个 arm $j$ 计算出来的 $x_j^T\theta_j$ 换成了 $x_j^T\theta_j + z_j^T \beta$, $\beta$ 是各个模型共享的的参数，$z_j^T$ 则是这些参数对应的特征。对应这种情况，也有了论文的第二种算法 Thompson sampling 方法前面的 UCB 方法采用的都是频率学派的思想，即认为评判某个 arm 的优劣的指标是个定值，如果有无限次的试验，便可准确地计算出这个值，但是由于现实中只能进行有限次的试验，因此预估出来的值是有偏差的，需要通过另外计算一个 bound 来衡量这个误差。 而下面要介绍的 Thompson sampling 方法采用的则是贝叶斯学派的思想，即认为评判某个 arm 的优劣的指标不再是个定值，而是服从着某种假定的分布(先验)，通过观察到的历史记录去更新这个分布的参数(似然)，得到了新的分布参数(后验), 然后不断重复这个过程。当需要进行比较时，从分布中随机产生一个样本即可。 Bernoulli Bandit前面提到，Bernoulli Bandit 假设某个 arm 的优劣服从伯努利分布，即每次是否获得收益的服从参数为 $\theta$ 的伯努利分布 $p(reward | \theta) \sim Bernoulli(\theta) $ UCB 方法中的 UCB1 和 UCB2 都是通过简单的历史统计得到 $\overline {x}_j$ 来表示 $\theta$ 的，但是贝叶斯学派则认为 $\theta$ 服从着一个特定的分布，根据贝叶斯公式有 $$p(\theta|reward) = \frac{p(reward|\theta) p{(\theta)}}{p(reward)} \propto p(reward|\theta) p{(\theta)} ＝Bernoulli(\theta) p(\theta)$$ $p(\theta|reward)$ 表示根据观察到的实验收益情况更新的后验概率，且由于似然 $p(reward|\theta)$ 为伯努利分布，为了保持共轭便于计算；先验分布 $p(\theta)$ 选择为了 Beta 分布，即 $Beta(\alpha, \beta)$，而两个分布相乘 $Bernoulli(\theta)*Beta(\alpha, \beta)$ 会得到一个新的 Beta 分布, 简单来说，就是 当 $Bernoulli(\theta)$ 的结果为1，则会得到 $Beta(\alpha + 1, \beta)$ 当 $Bernoulli(\theta)$ 的结果为0，则会得到 $Beta(\alpha, \beta + 1)$ 因此，Bernoulli Bandit 中的 Thompson Sampling 步骤如下, 图片摘自 A Tutorial on Thompson Sampling Contextual Bandit在 Bernoulli Bandit 中，我们假设 arm 是否获得收益是服从伯努利分布的，即 $p(reward | \theta) \sim Bernoulli(\theta)$ 而在 Contextual Bandit 中，我们假设获得的收益和特征向量存在一个线性关系, 即$reward = x^T \theta $，因此无法像前面一样直接通过似然 $p(reward | \theta) \sim Bernoulli(\theta)$ 来更新 $\theta$ 但是根据前面解决 Bernoulli Bandit 的思路，只要定义 $p(reward|\theta)$ 和 $p(\theta)$ 为合适的共轭分布，那么就可以使用Thompson Sampling来解决Contextual Bandit, 因为根据贝叶斯公式有如下的公式 $$p(\theta|reward) \propto p(reward|\theta) p{(\theta)}$$ 理论上，只要 $p(reward|\theta)$ 和 $p(\theta)$ 为共轭即可，但是考虑到假设的分布的合理性，参考这篇论文 Thompson Sampling for Contextual Bandits with Linear Payoffs，分别对这两个分布都采用正态分布的形式, 论文给出了较多的数学证明，这里略去证明，直接给出最终结论 对于 $p(reward|\theta)$ ，由于估计 reward 为 $x^T\theta$ ，因此假设真实的 reward 服从以 $x^T\theta$ 为中心、 $v^2$ 为标准差的正态分布，即 $p(reward|\theta) \sim \mathcal{N}(x^T\theta, v^2)$, $v$ 的具体含义后面会给出 为了运算上的便利性， $p(\theta)$ 一般会选择与 $p(reward|\theta)$ 共轭的形式; 由于 $p(reward|\theta)$ 是正态分布，那么 $p(\theta)$ 也应该是正态分布（正态分布的共轭还是正态分布） 为了便于给出后验概率的形式，论文首先定义了如下的等式 $$B(t) = I_d + X^TX$$ $$\mu(t) = B(t)^{-1}(\sum_{\tau = 1}^{t-1} x_{\tau}^Tr_{\tau})$$ 上面的 $t$ 表示某个 arm 第 $t$ 次的试验，$d$ 表示特征的维度，$X$ 的含义与 LinUCB 中介绍的一致，即是这个 arm 前 $t-1$ 次的特征组成的矩阵，在这个例子中其维度大小为 ($t-1$) X $d$, $r_{\tau}$ 则表示前面 $t-1$ 次试验中第 $\tau$ 次获得的 reward。 则 $p(\theta)$ 在 $t$ 时刻服从正态分布 $\mathcal{N}(\mu(t), v^2B(t)^{-1})$, 而 $p(\theta)$ 与 $p(reward|\theta)$ 相乘计算出来的后验概率 $p(\theta|reward) \sim \mathcal{N}(\mu(t+1), v^2B(t+1)^{-1})$ 前面提到的 $v$ 的定义为 $v = R\sqrt{9d\ln(\frac{t}{\delta})}$，该式子中的 $t$ 和 $d$ 的含义与上面的相同，而 $R$ 和 $\delta$ 是需要自定义的两个常数，均是用在两个不等式中约束 regret bound的，其中 $R \ge 0$ ,$0 \le \delta \lt 1$, 具体的等式可参考原始论文，这里不再展开。 当 $t = 1$ 也就是在最开始一次试验都没有的时候， $p(\theta)$ 会被初始化为 $\mathcal{N}(0, v^2I_d)$, 而每次选择 arm 的策略跟上面的 Bernoulli Bandit 类似: 每个 arm 的 $p(reward|\theta)$ 分布分别产生一个 $\theta$, 然后选择 $x^t\theta$ 最大的那个 arm 进行试验。 小结本文主要介绍了几种针对 EE 问题的策略，除了最简单的随机方法，UCB 和 Thompson Sampling 是这个问题两种比较典型的方法，两个方法的思想均是为每个 arm 计算出一个收益值，然后根据这个值进行 ranking 然后选择值最大的 arm。 UCB 采用的是频率学派的思想，计算的收益值是历史收益加上一个 bound，可以认为历史收益是 Exploitation，而 bound 则是 Exploration；而 Thompson Sampling 方法中每个 arm 的收益值会从一个分布中产生，没有明显的 Exploitation 和 Exploration，但也可以认为从分布中较大概率产生的值是 Exploitation ，而较小概率产生的值是 Exploration。另外，文中重点是介绍这些方法的具体做法，更多关于这些方法的理论基础和数学推导可参考下面这些文献。 参考： 开栏：智能决策系列 推荐系统的EE问题及Bandit算法 Reinforcement Learning: Exploration vs Exploitation Multi-Armed Bandits and the Gittins Index Boltzmann Exploration Done Right Optimism in the Face of Uncertainty: the UCB1 Algorithm Finite-time Analysis of the Multiarmed Bandit Problem Exploring compact reinforcement-learning representations with linear regression A Tutorial on Thompson Sampling A Contextual-Ba ndit Approach to Personalized News Article Recommendation]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(739,901,907)-线性时间寻找数组中各个元素作为最值的最大范围]]></title>
      <url>%2F2018%2F12%2F28%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(739%2C901%2C907)-%E7%BA%BF%E6%80%A7%E6%97%B6%E9%97%B4%E5%AF%BB%E6%89%BE%E6%95%B0%E7%BB%84%E4%B8%AD%E5%90%84%E4%B8%AA%E5%85%83%E7%B4%A0%E4%BD%9C%E4%B8%BA%E6%9C%80%E5%80%BC%E7%9A%84%E6%9C%80%E5%A4%A7%E8%8C%83%E5%9B%B4%2F</url>
      <content type="text"><![CDATA[题目有点拗口，其实就是给定一个数组，要求给出某个元素作为最小值或最小值的那些 continous subarrays 中最长的长度，如对于数组 [1, 2, 5, 6], 元素 5 作为最大值的 continous subarrays 有三个： [5], [2, 5], [1, 2, 5]，长度最长的是 3。遍历的解法找出一个元素要 $O(n)$ 的时间复杂度，找出所有元素则需要 $O(n^2)$ 的时间复杂度，而通过栈能够在 $O(n)$ 的时间复杂度内解决这个问题。 下面这两个题目都直接用到了这种解法 739. Daily Temperatures901. Online Stock Span 以 901. Online Stock Span 为例，如下所示，其实题目就要找出某个元素作为最大值时 subarray 的最大长度，且这个 subarray 是有方向的，即只能从当前元素往左延伸 Write a class StockSpanner which collects daily price quotes for some stock, and returns the span of that stock’s price for the current day. The span of the stock’s price today is defined as the maximum number of consecutive days (starting from today and going backwards) for which the price of the stock was less than or equal to today’s price. For example, if the price of a stock over the next 7 days were [100, 80, 60, 70, 60, 75, 85], then the stock spans would be [1, 1, 1, 2, 1, 4, 6]. 暴力的遍历需要 $O(n^2)$ 的时间复杂度，那通过栈如何在 $O(n)$ 的时间复杂度解决这个问题呢？ 首先创建一个空栈用于存储每个元素的下标，从左到右遍历数组中的元素，对于当前元素，假如栈为空或栈顶元素大于当前元素，则将当前元素入栈，否则一直出栈直到栈为空或栈顶元素大于当前元素，这样做将左边比当前元素小的元素都出栈，最后栈顶元素（如果有）和当前元素之间的距离就是要求的距离，如果栈为空，则当前元素是当前遍历的所有元素中最大的，其 下标+1 便是要求的距离。 由于每个元素最多会被入栈一次和出栈一次，因此其时间复杂度便是 $O(n)$, 其减小时间复杂度的原理其实就是通过出栈减少了元素比较的次数，比如说对于当前元素 e，出栈了 k 个元素，那后面如果有个元素比 e 大，肯定也比出栈的 k 个元素要大，因此无需进行比较，而这 k 个元素已经出栈了，因此减少了这 k 次的比较。 另外，需要注意的是，由于要求的是距离，因此栈存储的是元素的下标，比较元素大小是通过原数组加下标即可。 因此 901 题目的 python 代码如下, 也有 c++版本 和 go版本12345678910111213141516171819class StockSpanner(object): def __init__(self): self.nums = [] self.stack = [] self.idx = 1 def next(self, price): """ :type price: int :rtype: int """ while self.stack and self.nums[self.stack[-1]-1] &lt;= price: self.stack.pop() self.nums.append(price) span = self.idx-self.stack[-1] if self.stack else self.idx self.stack.append(self.idx) self.idx += 1 return span 739.Daily Temperatures 的题目原理是一样的，只是要求的是最小元素往右的最长 subarray，解法同上，只是此时需要从后往前遍历数组了，具体的代码可见: python版本，c++版本，go版本 拓展上面两个题目是比较明确要求出各个元素作为最小值或最大值时的最长 subarray 的长度，但是有些问题不会直接要求这么求解。比如说题目 907. Sum of Subarray Minimums, 题目要求的是求出所有 subarray 中最小元素的和，下面是一个简单的例子 1234Input: [3,1,2,4]Output: 17Explanation: Subarrays are [3], [1], [2], [4], [3,1], [1,2], [2,4], [3,1,2], [1,2,4], [3,1,2,4]. Minimums are 3, 1, 2, 4, 1, 1, 2, 1, 1, 1. Sum is 17. 通过穷举法求解的时间复杂度显然太高了，但是我们可以换一个角度来求解这个问题，就是只要求出某个元素作为最小值的 subarray 个数，那么该元素乘上 subarray 个数便是这个元素对最终的结果的贡献值，比如说比如说对于某个长度为 n 的数组 A, 其各个元素作为最小值的 subarray 个数分别是 f[0], f[1].....f[n-1], 则最终结果为 $$\sum_{i=0}^{n-1} A[i]f[i]$$ 那么现在的问题就是要求出各个元素作为最小值的 subarray 个数，这就要用到了我们前面提到的通过栈求解的方法了，而且要分别往左和往右求出 subarray 的长度。 对于当前元素 A[i], 将当前值作为最小值，分别往左和往右求出的 subarray 长度记为 left 和 right，则包含 A[i] 作为最小值的 subarray 个数为： f[i] = left * right 实现的 python 代码如下，也可参考 c++版本 和 go版本 123456789101112131415161718192021class Solution(object): def sumSubarrayMins(self, A): """ :type A: List[int] :rtype: int """ n = len(A) s, left = [], [] for i in xrange(n): while s and A[s[-1]] &gt;= A[i]: s.pop() left.append(i - s[-1] if s else i + 1) s.append(i) s, right = [], [] for i in xrange(n - 1, -1, -1): while s and A[s[-1]] &gt; A[i]: s.pop() right.append(s[-1] - i if s else n - i) s.append(i) return sum(A[i] * left[i] * right[n-i-1] for i in xrange(n)) % (10**9 + 7) 上面的思路就是分别从左到右和从右到左获取 left 和 right 这两个表示 subarray 数量的数组，需要注意的是，获取 left 数组是用的比较条件是 A[s[-1]] &gt;= A[i], 但是获取 right 数组时用的是 A[s[-1]] &gt; A[i];这里以一个例子简介会比较方便，假如说对于数组 [71,55,82,55], 如果两个比较条件都采用 &gt;=，那么 [55,82,55] 这个 subarray 会被重复计算两次， 如果都采用 &gt;, [55,82,55] 则不会被计算, 因此一定要有一个采用 &gt;=, 而另一个采用 &gt;。 这里其实也引出另外一个非常重要的思想，就是分别求出每个元素对最终结果的贡献，然后累加起来便是最终的结果，在上面的问题即是求出某个元素作为最小值的 subarray 个数，那么该元素乘上 subarray 个数便是这个元素对最终的结果的贡献值。且这一类问题一般都跟 subarray、 subsequence 相关，比如说 828. Unique Letter String 和 891. Sum of Subsequence Widths 都是通过这种思想来解决的 首先看问题 828. Unique Letter String，题目要求出所有的 subarray 中的 unique characters 的数量，遍历的时间复杂度是 $O(n^3)$, 但是采用上面提到的思想，可以分别求出每个元素作为 unique character 时的 subarray 的数量，然后累加起来即可，这样的时间复杂度变为了 $O(n^2)$, AC 的 python 代码如下, 另外也可参考 c++版本 和 go版本 1234567891011121314151617class Solution(object): def uniqueLetterString(self, S): """ :type S: str :rtype: int """ result = 0 MOD = 1000000007 for i in xrange(len(S)): left, right = i - 1, i + 1 while left &gt;=0 and S[left] != S[i]: left -= 1 while right &lt; len(S) and S[right] != S[i]: right += 1 result += ((i - left) * (right - i)) % MOD result %= MOD return result 问题 891. Sum of Subsequence Widths 与前面的不同，前面的都是连续的 subarray， 而这里是可以不连续的 subsequence，题目要求出每个 subsequence 中最大值和最小值的差，然后求和得到最终的结果。同样采用上面的思想，求出某个元素 A[i] 作为最小值的 subsequence 的数量 n1, 作为最大值的 subsequence 的数量 n2, 则 A[i] 对最终结果的贡献是 n2 * A[i] - n1 * A[i]。但是 n1、n2 不能像之前一样分别往左往右延伸获取了，这里有一个很重要但是很容易被忽略的事实：数组的顺序不影响最终的结果。因此可以将数组进行排序，n1 就是当前元素左边元素的一个组合数量了，n2 同理。AC 的 python 代码如下, 在实现中计算 $2^i$ 使用位移操作即 1&lt;&lt;i 而不是直接计算，否则会导致超时 12345678910class Solution(object): def sumSubseqWidths(self, A): """ :type A: List[int] :rtype: int """ MOD = 1000000007 n = len(A) A.sort() return sum((((1 &lt;&lt; i) - (1 &lt;&lt; (n-i-1))) * A[i]) % MOD for i in xrange(n)) % MOD 小结本文主要介绍了两个重要的思想，其中一个是通过栈在线性时间内求解数组中某个元素作为最小值（或最大值）的最长 subarray，代表性的题目有 739. Daily Temperatures、901. Online Stock Span 和 907. Sum of Subarray Minimums；另外一个重要的思想是分别求出每个元素对最终结果的贡献，然后累加起来便是最终的结果，代表性的题目有 828. Unique Letter String、891. Sum of Subsequence Widths 和 907. Sum of Subarray Minimums。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ipdb 使用小记]]></title>
      <url>%2F2018%2F12%2F21%2Fipdb%20%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[最近在魔改 loss function，涉及到很多矩阵运算，而矩阵运算中维度的对齐免不了要多次的调试；沿袭着之前的 print 大法弄了一段时间后，不仅代码凌乱不堪，而且心累：每次 import tensorflow as tf 都要十几秒，然后 print 完之后想进一步看其他变量的信息，又要重新执行一遍。后来找到了 ipdb 这个好用的工具，才发现自己过去调试程序的方法是多么的低效和 naive。 python 提供了一个默认的 debugger：pdb，而 ipdb 则是 pdb 的增强版，提供了补全、语法高亮等功能，类似于 ipython 与 python 默认的交互终端的关系，通过 pip install ipdb 即可安装 ipdb。 使用方式ipdb 的使用方法一般有两种：集成到源代码或通过命令交互。 集成到源代码可以直接在代码指定位置插入断点。如下所示： 1234import ipdbvar1 = 23ipdb.set_trace()... 上面的代码会在执行完 var1 = 23 这条语句之后停止，展开 ipython 环境，之后就可以自由地调试了。 上面的方式虽然简单，但是存在着两个较为比较明显的问题： 插入的断点代码会污染原来的代码空间 每次插入断点都需要修改源码 因此，相比于上面的方式，交互式的命令式调试方法更加方便。启动命令式调试环境的方法也很简单： python -m ipdb code.py 接着就是通过一些常用的命令来进行 debug了，如上面插入断点的样例代码就可以通过以下命令达到同样效果： 1234$python -m ipdb code.pyipdb&gt; b 3Breakpoint 1 at /test.py:3ipdb&gt; c 上面的命令 b 3 表示在第三行设置一个断点，然后通过命令 c 一直执行至断点处，接着就会展开 ipython 环境进行调试了。（b 和 c 分别代表了 break 和 continue，可以用整条命令，也可以只用首字母） 常用命令上面的设置断点和一直执行至断点是比较常见的用法，除此之外，还有其他一些常用命令。 帮助通过命令 h 可以列出所有命令，后面跟上具体的命令如 h command 则可以显示出这条命令的具体作用，非常有用，依靠这条命令能够节省不少 google 的次数。 断点上面提到了断点的一种常见用法，即命令 b line_number 和 c 的组合，b line_number 默认是对当前文件设置断点，也可以在 line_number 前加上其他文件名（比如说要将要引用的其他文件），即 b file_name:line_number；file_name 需要在 sys.path 中，当前目录已经默认存在 sys.path , 也可通过 .. 引用上一层目录的文件。 另外，通过 b 设置的断点在重新运行 debug 程序 (命令 restart 或 run) 后会依然保留，如果要忽略这些断点，有两种做法 通过 disable 关闭这些断点，enable 打开这些断点 通过命令 clear 或 cl 清除这些断点 此外，除了上面那种一直存在的断点，ipdb 中还有一种只生效一次的断点，命令为 tbreak, 使用方法同命令 b。 上面的断点都是直接指定的，pdb 中还有一种条件断点，即只有当某个条件成立时，才设置断点，其使用命令为 condition line_num bool_expression, condition 为关键字，line_num 为设定断点的位置，只有当 expression 为 true 时, 才会设置这个断点。 如果需要列出已经设置的所有断点，可以单独使用命令 b。 逐行执行有两条命令可以进行逐行执行： s（step) 或 n（next), 两个命令的主要区别是：假如当前行调用了某个函数，s 会进入这个函数，n 则不会。因此，如果需要了解函数内部执行的细节，需要 s 命令进入函数内部进行 debug。 进入了函数之后，通过命令 a（argument） 可列出当前的函数的参数，通过 r（return）则可以直接执行至 return 语句。 忽略某段代码使用 j line_number 可以忽略某段代码，下一步直接从 line_number 开始执行。 查看源码通过命令 l 或 ll 可查看源码， ll 是查看整个源码文件， l 可指定需要查看的行数，默认是当前往后 11 行，也可指定具体的范围，如 l 2,5 是查看第 2-5 行的源码。 重启或退出 debugger上面已经提到了重启 debugger 可通过 restart 或 run 命令，需要注意的是，重启 debugger 后断点、debugger 的设置等是会保留的。如果要一个全新的 debugger，可通过命令 q、quit 或 exit 退出 debugger 后进入。 小结以上就是 ipdb 的一些基本用法, 除此之外，更多的用法可参考 pdb 的官方文档，ipdb 的命令跟 pdb 是一样的。另外，gdb 也是一个类似的命令行 debugger，只是一般用来调试 C/C++ 而已，使用的方法类似，甚至很多命令的名称更 pdb 都一样，具体可参考 用GDB调试程序。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 语法杂记--迭代器、生成器、上下文管理器]]></title>
      <url>%2F2018%2F12%2F17%2Fpython%20%E8%AF%AD%E6%B3%95%E6%9D%82%E8%AE%B0--%E7%94%9F%E6%88%90%E5%99%A8%E3%80%81%E8%BF%AD%E4%BB%A3%E5%99%A8%E3%80%81%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8%2F</url>
      <content type="text"><![CDATA[本文主要介绍 python 中几个重要的 “器”（迭代器、生成器、上下文管理器）的原理、实现与使用，还有一个装饰器在前面一篇文章已经进行了介绍，本文主要参考了 Python 之旅 中的相关章节。 迭代器迭代器（iterator）就是用来遍历可迭代对象的（iterable），这两个概念要区分开。 iterable像 list，tuple 等可以通过 for..in.. 进行遍历的对象就是可迭代对象，更严谨的定义则是： 含有 __iter__() 方法或 __getitem__() 方法的对象称之为可迭代对象. 可以使用 Python 内置的 hasattr() 函数来判断一个对象是不是可迭代的： 123456789101112&gt;&gt;&gt; hasattr((), '__iter__')True&gt;&gt;&gt; hasattr([], '__iter__')True&gt;&gt;&gt; hasattr(&#123;&#125;, '__iter__')True&gt;&gt;&gt; hasattr(123, '__iter__')False&gt;&gt;&gt; hasattr('abc', '__iter__')False&gt;&gt;&gt; hasattr('abc', '__getitem__')True 另外，也可使用 isinstance() 进行判断： 123456789101112&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance((), Iterable) # 元组True&gt;&gt;&gt; isinstance([], Iterable) # 列表True&gt;&gt;&gt; isinstance(&#123;&#125;, Iterable) # 字典True&gt;&gt;&gt; isinstance('abc', Iterable) # 字符串True&gt;&gt;&gt; isinstance(100, Iterable) # 数字False iterator迭代器是一个对象，但比较特别，它需要遵循迭代器协议，具体协议如下 迭代器协议（iterator protocol）是指要实现对象的 __iter()__ 和 next() 方法（注意：Python3 要实现 __next__() 方法），其中，__iter()__ 方法返回迭代器对象本身，next() 方法返回容器的下一个元素，在没有后续元素时抛出 StopIteration 异常。 这里需要注意的是，虽然元组、列表和字典等对象是可迭代的，但它们却不是迭代器 首先，可以使用 hasattr() 进行判断：1234567891011121314&gt;&gt;&gt; hasattr((1, 2, 3), '__iter__')True&gt;&gt;&gt; hasattr((1, 2, 3), 'next') # 有 __iter__ 方法但是没有 next 方法，不是迭代器False&gt;&gt;&gt;&gt;&gt;&gt; hasattr([1, 2, 3], '__iter__')True&gt;&gt;&gt; hasattr([1, 2, 3], 'next')False&gt;&gt;&gt;&gt;&gt;&gt; hasattr(&#123;'a': 1, 'b': 2&#125;, '__iter__')True&gt;&gt;&gt; hasattr(&#123;'a': 1, 'b': 2&#125;, 'next')False 同样也可以使用 isinstance() 进行判断： 1234567891011&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance((), Iterator)False&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance('', Iterator)False&gt;&gt;&gt; isinstance(123, Iterator)False 虽然这些可迭代对象不是迭代器，但是可以使用 Python 内置的 iter() 函数获得它们的迭代器对象，如下所示： 12345&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance(iter([1, 2, 3]), Iterator) # 使用 iter() 函数，获得迭代器对象True&gt;&gt;&gt; isinstance(iter('abc'), Iterator)True 事实上，Python 的 for 循环就是先通过内置函数 iter() 获得一个迭代器，然后再不断调用 next() 函数实现的，即： 12for x in [1, 2, 3]: print i 等价于 123456789it = iter([1, 2, 3])while True: try: x = next(it) print x except StopIteration: # 没有后续元素，退出循环 break 下面是一个斐波那契数列迭代器，根据迭代器的定义，我们需要实现 __iter()__ 和 next() 方法（在 Python3 中是 __next__() 方法） 12345678910111213141516171819class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 返回迭代器对象本身 def __iter__(self): return self # 返回容器下一个元素 def next(self): self.a, self.b = self.b, self.a + self.b return self.adef main(): fib = Fib() for i in fib: if i &gt; 10: break print i 因此，关于迭代器和可迭代对象，需要注意下面三点 1. 元组、列表、字典和字符串对象是可迭代的，但不是迭代器，不过我们可以通过 iter() 函数获得一个迭代器对象2. Python 的 for 循环实质上是先通过内置函数 iter() 获得一个迭代器，然后再不断调用 next() 函数实现的3. 定义迭代器需要实现对象的 __iter()__和 next() 方法（Python3 要实现 __next__() 方法），其中，__iter()__ 方法返回迭代器对象本身，next() 方法返回容器的下一个元素，在没有后续元素时抛出 StopIteration 异常。 生成器yield生成器也是迭代器的一种，一个带有关键字 yield 的函数就是一个生成器函数，而当我们使用 yield 时，它帮我们自动创建了 __iter__() 和 next() 方法，而且在没有数据时，也会抛出 StopIteration 异常，非常简洁和高效。如下是一个简单的例子 12345678910111213141516171819&gt;&gt;&gt; def generator_function():... print 'hello 1'... yield 1... print 'hello 2'... yield 2... print 'hello 3'&gt;&gt;&gt;&gt;&gt;&gt; g = generator_function() # 函数没有立即执行，而是返回了一个生成器&gt;&gt;&gt; g.next() # 当使用 next()(或 next(g))的时候开始执行，遇到 yield 暂停并返回hello 11&gt;&gt;&gt; g.next() # 从原来暂停的地方继续执行hello 22&gt;&gt;&gt; g.next() # 从原来暂停的地方继续执行，没有 yield，抛出异常hello 3Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 从上面的例子可知，带有 yield 的函数执行过程如下 1. 调用该函数的时候不会立即执行代码，而是返回了一个生成器对象；2. 当使用 next() (在 for 循环中会自动调用 next()) 作用于返回的生成器对象时，函数开始执行，在遇到 yield 的时候会『暂停』并返回 yield 后的值3. 当再次使用 next() 的时候，函数会从原来『暂停』的地方继续执行，直到遇到 yield 语句，如果没有 yield 语句，则抛出异常** 相比于迭代器，生成器这样的 lazy evaluation 能够节省更多的内存，同时让代码更加简洁，如前面定义的斐波那契数列迭代器，通过生成器实现的代码如下 123456789101112131415161718&gt;&gt;&gt; def fib():... a, b = 0, 1... while True:... a, b = b, a + b... yield a...&gt;&gt;&gt; f = fib()&gt;&gt;&gt; for item in f:... if item &gt; 10:... break... print item...112358 send(), throw(), close()除了上面提到的 yield，生成器还有一些其他的特殊方法：send(), throw() 和 close()，分别用于给生成器发送消息、异常和关闭生成器。 具体用法如下 123456789101112131415161718192021222324252627282930313233343536373839404142In [2]: def generator_function(): ...: # test send() ...: value1 = yield 0 ...: print('value1 is ', value1) ...: ...: # test throw() ...: try: ...: yield 'Normal' ...: except ValueError: ...: yield 'Error' ...: ...: # test close() ...: yield 1 ...: yield 2 ...: yield 3 ...:In [3]: g = generator_function()In [4]: next(g)Out[4]: 0In [5]: g.send(10)value1 is 10Out[5]: 'Normal'In [6]: g.throw(ValueError)Out[6]: 'Error'In [7]: next(g)Out[7]: 1In [8]: g.close()In [9]: next(g)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-9-5f315c5de15b&gt; in &lt;module&gt;()----&gt; 1 next(g)StopIteration: 在上面的代码中，先调用 next() 方法，使函数开始执行，代码执行到 yield 0 的时候暂停，返回了 0；接着，执行 send() 方法，它会恢复生成器的运行，并将发送的值赋给上次中断时 yield 表达式的执行结果，也就是 value1，这时控制台打印出 value1 的值，并继续执行，直到遇到 yield 后暂停，此时返回 ‘Normal’, 因此，简单地说，send() 方法就是 next() 的功能，加上传值给 yield 接着， throw() 方法向生成器函数传递了 ValueError 异常，此时代码进入 except ValueError 语句，遇到 yield ‘Error’，暂停并返回 Error 字符串, 因此，简单的说，throw() 就是 next() 的功能，加上传异常给 yield。 最后使用了 close() 方法来关闭一个生成器。生成器被关闭后，再次调用 next() 方法，不管能否遇到 yield 关键字，都会抛出 StopIteration 异常 上下文管理器__enter__() &amp; __exit__()上下文(context)在计算机中是个很常见的词汇，可以简单将其理解为运行时的环境，如进程上下文指的是进程在执行时 CPU 的所有寄存器中的值、进程的状态以及堆栈上的内容等，当系统需要切换到其他进程时，系统会保留当前进程的上下文，也就是运行时的环境，以便再次执行该进程。 而在 python 中上下文管理器最常见的场景便是 with 语句，with 一般用于对资源进行访问的场景，确保执行过程中出现异常情况时也可以对资源进行回收，比如自动关闭文件等。 类似迭代器协议（Iterator Protocol），上下文管理器（Context manager）也有上下文管理协议（Context Management Protocol）。 上下文管理器协议，是指要实现对象的 __enter__() 和 __exit__() 方法。 上下文管理器也就是支持上下文管理器协议的对象，也就是实现了 __enter__() 和 __exit__() 方法的对象。 如下是一个简单的上下文管理器的例子 1234567891011121314151617from math import sqrt, powclass Point(object): def __init__(self, x, y): print 'initialize x and y' self.x, self.y = x, y def __enter__(self): print "Entering context" return self def __exit__(self, type, value, traceback): print "Exiting context" def get_distance(self): distance = sqrt(pow(self.x, 2) + pow(self.y, 2)) return distance 使用 with 语句调用上下文管理器如下所示 12345678with Point(3, 4) as pt: print 'distance: ', pt.get_distance()# outputinitialize x and y # 调用了 __init__ 方法Entering context # 调用了 __enter__ 方法distance: 5.0 # 调用了 get_distance 方法Exiting context # 调用了 __exit__ 方法 上面的 with 语句执行过程如下： Point(3, 4) 生成了一个上下文管理器； 调用上下文管理器的 __enter__() 方法，并将 __enter__() 方法的返回值赋给 as 字句中的变量 pt; 执行语句体（指 with 语句包裹起来的代码块）内容，输出 distance； 不管执行过程中是否发生异常，都执行上下文管理器的 __exit__() 方法。 一般来说，__exit__() 方法负责执行清理工作，如释放资源，关闭文件等。如果执行过程没有出现异常，或者语句体中执行了语句 break/continue/return，则以 None 作为参数调用 __exit__(None, None, None)；如果执行过程中出现异常，则使用 sys.exc_info 得到的异常信息为参数调用 __exit__(exc_type, exc_value, exc_traceback). 同时出现异常时，如果 __exit__(type, value, traceback) 返回 False 或 None，则会重新抛出异常，让 with 之外的语句逻辑来处理异常；如果返回 True，则忽略异常，不再对异常进行处理。 上面的 with 语句执行过程没有出现异常，下面是出现异常的情形： 1234567891011121314with Point(3, 4) as pt: pt.get_length() # 访问了对象不存在的方法# outputinitialize x and yEntering contextExiting context---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-216-ab4a0e6b6b4a&gt; in &lt;module&gt;() 1 with Point(3, 4) as pt:----&gt; 2 pt.get_length()AttributeError: 'Point' object has no attribute 'get_length' 对前面的 __exit__() 方法修改如下 1234def __exit__(self, type, value, traceback): print "Exception has been handled" print "Exiting context" return True 则执行相同过的代码的结果如下 12345678with Point(3, 4) as pt: pt.get_length() # 访问了对象不存在的方法# outputinitialize x and yEntering contextException has been handledExiting context contextlib除了在类中定义 __enter__ 和 __exit__ 方法来实现上下文管理器，我们还可以通过生成器函数和装饰器来实现上下文管理器，这个装饰器就在 python 提供的 contextlib 模块中。如下是个简单的例子 123456789101112131415from contextlib import contextmanager@contextmanagerdef point(x, y): print 'before yield' yield x * x + y * y print 'after yield'with point(3, 4) as value: print 'value is: %s' % value# outputbefore yieldvalue is: 25after yield 可以看到，yield 产生的值赋给了 as 子句中的 value 变量。 另外，需要强调的是，虽然通过使用 contextmanager 装饰器，可以不必再编写 __enter__ 和 __exit__ 方法，但是获取和清理资源的操作仍需要我们自己编写：获取资源的操作定义在 yield 语句之前，释放资源的操作定义在 yield 语句之后。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 语法杂记--装饰器，类的特殊方法，常量类]]></title>
      <url>%2F2018%2F12%2F15%2Fpython%20%E8%AF%AD%E6%B3%95%E6%9D%82%E8%AE%B0--%E8%A3%85%E9%A5%B0%E5%99%A8%EF%BC%8C%E7%B1%BB%E7%9A%84%E7%89%B9%E6%AE%8A%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B8%B8%E9%87%8F%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[最近在看 python 一些语法知识，虽然 python 代码写了不少，但是对于一些高级语法的了解还不够深入；因此本文主要记录了一些比较生疏的知识点，主要包括了装饰器，类的特殊方法，常量类这三个方面的知识。 装饰器装饰器本质上是一个高阶函数，以被装饰的函数为参数，并返回一个包装后的函数给被装饰函数。 装饰器的一般使用形式如下：123@decoratordef func(): pass 等价于下面的形式：123def func(): passfunc = decorator(func) 装饰器可以定义多个，离函数定义最近的装饰器先被调用，比如：1234@decorator_one@decorator_twodef func(): pass 等价于： 1234def func(): passfunc = decorator_one(decorator_two(func)) 对带参数的函数进行装饰对带参数的函数进行装饰这个需求很常见，简单来说，装饰带参数的函数时，需要将参数传递给装饰器内部需要返回的函数(也叫内嵌包装函数)，也就是说内嵌包装函数的参数跟被装饰函数的参数对应，如下所示 12345678910111213def makeitalic(func): def wrapped(*args, **kwargs): ret = func(*args, **kwargs) return '&lt;i&gt;' + ret + '&lt;/i&gt;' return wrapped@makeitalicdef hello(name): return 'hello %s' % name@makeitalicdef hello2(name1, name2): return 'hello %s, %s' % (name1, name2) 可以看到，装饰器内部需要返回的函数 wrapped 带上了参数 (*args, **kwargs), 目的是为了适应可变参数。使用如下 1234&gt;&gt;&gt; hello('python')'&lt;i&gt;hello python&lt;/i&gt;'&gt;&gt;&gt; hello2('python', 'java')'&lt;i&gt;hello python, java&lt;/i&gt;' 带参数的装饰器上面的例子，我们增强了函数 hello 的功能，给它的返回加上了标签 &lt;i&gt;...&lt;/i&gt;，现在，我们想改用标签 &lt;b&gt;...&lt;/b&gt; 或 &lt;p&gt;...&lt;/p&gt;。是不是要像前面一样，再定义一个类似 makeitalic 的装饰器呢？其实，我们可以可以使用带参数的装饰器，简单来说，就是在原来的装饰器基础上再封装一层函数，将标签作为参数，返回一个装饰器。 12345678def wrap_in_tag(tag): def decorator(func): def wrapped(*args, **kwargs): ret = func(*args, **kwargs) return '&lt;' + tag + '&gt;' + ret + '&lt;/' + tag + '&gt;' return wrapped return decorator 现在，我们可以根据需要生成想要的装饰器了： 1234567makebold = wrap_in_tag(&apos;b&apos;) # 根据 &apos;b&apos; 返回 makebold 生成器`@makebolddef hello(name): return &apos;hello %s&apos; % name&gt;&gt;&gt; hello(&apos;world&apos;)&apos;&lt;b&gt;hello world&lt;/b&gt;&apos; 上面的形式也可以写得更加简洁：123@wrap_in_tag(&apos;b&apos;)def hello(name): return &apos;hello %s&apos; % name 这就是带参数的装饰器，其实就是在装饰器外面多了一层包装，根据不同的参数返回不同的装饰器。 私有成员python 不像 C++ 有 private 之类的关键字，但是可以在属性或方法的名称前面加上两个下划线 __, 来限制用户访问对象的属性或方法。 12345678910111213141516171819In [1]: class Animal(object): ...: def __init__(self, name): ...: self.__name = name ...: def greet(self): ...: print ('Hello, I am %s.' % self.__name) ...:In [2]: a = Animal("dog")In [3]: a.__name---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-5-5d5520ef9fe0&gt; in &lt;module&gt;()----&gt; 1 a.__nameAttributeError: 'Animal' object has no attribute '__name'In [4]: a.greet()Hello, I am dog. 类方法 vs 静态方法python 中的类有两个特殊的方法：类方法和静态方法，两个方法主要有以下特点 两个方法均是属于类而不是属于对象的 两个方法都是通过内置的装饰器定义（@classmethod 和 @staticmethod） 类方法可以访问类属性，静态方法则不能 如下是类方法的一个例子 12345678910class A(object): bar = 1 @classmethod def class_foo(cls): print 'Hello, ', cls print cls.bar&gt;&gt;&gt; A.class_foo() # 直接通过类来调用方法Hello, &lt;class '__main__.A'&gt;1 在上面，我们使用了 classmethod 装饰方法 class_foo，它就变成了一个类方法，class_foo 的参数是 cls，代表类本身，当我们使用 A.class_foo() 时，cls 就会接收 A 作为参数。另外，被 classmethod 装饰的方法由于持有 cls 参数，因此我们可以在方法里面调用类的属性、方法，比如 cls.bar 上面的类方法是可以修改类的属性的，静态方法定义方式类似，但是不会改变类和实例状态；如下所示，静态方法没有 self 和 cls 参数，因此没法改变类的属性，可以把它看成是一个普通的函数，甚至可以把它写到类外面，但是有时候，类就是需要这么一类方法，如果写到外面，一是不利于类的完整性，二是不利于命名空间的整洁性。 12345678910class A(object): @staticmethod def static_foo(): print 'Hello'&gt;&gt;&gt; a = A()&gt;&gt;&gt; a.static_foo()Hello&gt;&gt;&gt; A.static_foo()Hello 那么，这两个方法该在什么时候使用呢？参考 class method vs static method in Python 如下 We generally use class method to create factory methods. Factory methods return class object ( similar to a constructor ) for different use cases.We generally use static methods to create utility functions. 如下是个比较形象的例子 123456789101112131415161718192021222324252627# Python program to demonstrate # use of class method and static method. from datetime import date class Person: def __init__(self, name, age): self.name = name self.age = age # a class method to create a Person object by birth year. @classmethod def fromBirthYear(cls, name, year): return cls(name, date.today().year - year) # a static method to check if a Person is adult or not. @staticmethod def isAdult(age): return age &gt; 18person1 = Person('mayank', 21) person2 = Person.fromBirthYear('mayank', 1996) print person1.age print person2.age # print the result print Person.isAdult(22) 魔法方法以双下划线 __ 包裹起来的方法，比如最常见的 __init__，这些方法被称为魔法方法（magic method）或特殊方法（special method）,这些方法可以给 Python 的类提供特殊功能，方便我们定制一个类。 __new__在 Python 中，当我们创建一个类的实例时，类会先调用 __new__(cls[, ...]) 来创建并返回实例，然后 __init__ 方法再对该实例（self）中的变量进行初始化。 关于 __new__ 和 __init__ 有以下几点需要注意： __new__ 是在 __init__ 之前被调用的 __new__ 是类方法，__init__ 是实例方法 重载 __new__ 方法，需要返回类的实例 一般情况下，我们不需要重载 __new__ 方法。但在某些情况下，我们想控制实例的创建过程，这时可以通过重载 __new__ 方法来实现。 比如说，下面的例子通过了 __new__ 来实现单例模式 12345678class Singleton(object): _instance = None def __new__(cls, *args, **kw): if not cls._instance: cls._instance = super(Singleton, cls).__new__(cls, *args, **kw) return cls._instance class MyClass(Singleton): a = 1 __str__ &amp; __repr__这两个方法主要是在直接打印类时候调用的，通过下面两个例子可以比较直观地看到如何使用 1234567891011121314class Foo(object): def __init__(self, name): self.name = name def __str__(self): return 'Foo object (name: %s)' % self.name&gt;&gt;&gt; print Foo('ethan') # 使用 printFoo object (name: ethan)&gt;&gt;&gt;&gt;&gt;&gt; str(Foo('ethan')) # 使用 str'Foo object (name: ethan)'&gt;&gt;&gt;&gt;&gt;&gt; Foo('ethan') # 直接显示&lt;__main__.Foo at 0x10c37a490&gt; 可以看到，使用 print 和 str 输出的是 __str__ 方法返回的内容，但如果直接显示则不能，因为这个是 __repr__ 方法负责的, 如下： 12345678class Foo(object): def __init__(self, name): self.name = name def __repr__(self): return 'Foo object (name: %s)' % self.name&gt;&gt;&gt; Foo('ethan')'Foo object (name: ethan)' __iter__在某些情况下，我们希望实例对象可被用于 for...in 循环，这时我们需要在类中定义 __iter__ 和 next（在 Python3 中是 __next__）方法，其中，__iter__ 返回一个迭代对象，next 返回容器的下一个元素，在没有后续元素时抛出 StopIteration 异常 如下是一个斐波那契数列的例子： 1234567891011121314151617181920212223class Fib(object): def __init__(self): self.a, self.b = 0, 1 def __iter__(self): # 返回迭代器对象本身 return self def next(self): # 返回容器下一个元素 self.a, self.b = self.b, self.a + self.b return self.a &gt;&gt;&gt; fib = Fib()&gt;&gt;&gt; for i in fib:... if i &gt; 10:... break... print i...112358 __getitem__ &amp; __setitem__ &amp; __delitem__有时，我们希望可以使用 obj[n] 这种方式对实例对象进行取值，比如对斐波那契数列，我们希望可以取出其中的某一项，这时我们需要在类中实现 __getitem__ 方法，比如下面的例子： 12345678910class Fib(object): def __getitem__(self, n): a, b = 1, 1 for x in xrange(n): a, b = b, a + b return a&gt;&gt;&gt; fib = Fib()&gt;&gt;&gt; fib[0], fib[1], fib[2], fib[3], fib[4], fib[5](1, 1, 2, 3, 5, 8) 类似地，__setitem__ 用于设置值，__delitem__ 用于删除值，让我们看下面一个例子： 12345678910111213141516class Point(object): def __init__(self): self.coordinate = &#123;&#125; def __getitem__(self, key): return self.coordinate.get(key) def __setitem__(self, key, value): self.coordinate[key] = value def __delitem__(self, key): del self.coordinate[key] print 'delete %s' % key def __len__(self): return len(self.coordinate) 在上面，我们定义了一个 Point 类，它有一个属性 coordinate（坐标），是一个字典，让我们看看使用： 12345678910&gt;&gt;&gt; p = Point()&gt;&gt;&gt; p[&apos;x&apos;] = 2 # 对应于 p.__setitem__(&apos;x&apos;, 2)&gt;&gt;&gt; p[&apos;y&apos;] = 5 # 对应于 p.__setitem__(&apos;y&apos;, 5)&gt;&gt;&gt; len(p) # 对应于 p.__len__2&gt;&gt;&gt; p[&apos;x&apos;] # 对应于 p.__getitem__(&apos;x&apos;)2&gt;&gt;&gt; del p[&apos;x&apos;] # 对应于 p.__delitem__(&apos;x&apos;)&gt;&gt;&gt; len(p)1 __call__我们一般使用 obj.method() 来调用对象的方法，那能不能直接在实例本身上调用呢？在 Python 中，只要我们在类中定义 __call__ 方法，就可以对实例进行调用，比如下面的例子： 12345class Point(object): def __init__(self, x, y): self.x, self.y = x, y def __call__(self, z): return self.x + self.y + z 使用如下： 12345&gt;&gt;&gt; p = Point(3, 4)&gt;&gt;&gt; callable(p) # 使用 callable 判断对象是否能被调用True&gt;&gt;&gt; p(6) # 传入参数，对实例进行调用，对应 p.__call__(6)13 # 3+4+6 __slots____slots__ 跟前面的方法不太一样，因为这是一个类的属性，当我们创建了一个类的实例后，我们还可以给该实例绑定任意新的属性和方法，如下 1234567891011class Point(object): def __init__(self, x=0, y=0): self.x = x self.y = y&gt;&gt;&gt; p = Point(3, 4)&gt;&gt;&gt; p.z = 5 # 绑定了一个新的属性&gt;&gt;&gt; p.z5&gt;&gt;&gt; p.__dict__&#123;'x': 3, 'y': 4, 'z': 5&#125; 这样其实是违背了 OOP 的封装性的理念，而且会消耗更多的内存，为了禁止这一属性，可以使用 __slots__ 来告诉 Python 只给一个固定集合的属性分配空间，对上面的代码做一点改进，如下： 123456class Point(object): __slots__ = ('x', 'y') # 只允许使用 x 和 y def __init__(self, x=0, y=0): self.x = x self.y = y 我们给 __slots__ 设置了一个元组，来限制类能添加的属性。现在，如果想绑定一个新的属性，就会出错了 常量类在 Python 中使用常量一般来说有以下两种方式： 通过命名风格来提醒使用者该变量代表的意义为常量，如常量名所有字母大写，用下划线连接各个单词，PEP8 给出的编程风格就是这样的 通过自定义的类实现常量功能。这要求符合命名全部为大写和值一旦绑定便不可再修改 这两个条件。下面是一种较为常见的解决办法，将常量放到同一个文件中 123456789101112131415161718192021# FileName：constant.pyclass _const: class ConstError(TypeError): pass class ConstCaseError(ConstError): pass def __setattr__(self, name, value): if name in self.__dict__: raise self.ConstError, "Can't change const value!" if not name.isupper(): raise self.ConstCaseError, 'const "%s" is not all letters are capitalized' %name self.__dict__[name] = valueimport syssys.modules[__name__] = _const()import constantconstant.MAX_COUNT = 10constant.JOBS = 5constant.PROCESSES = 8 简单解释一下，对象的所有属性及属性的值都存储在 __dict__ 中， 上面的 __setattr__ 方法在对象每次创建新常量的时候会判断常量是否已经被定义过，如果已经定义过则 raise error，从而确保了已经创建的常量不可修改。 sys.modules[__name__] = _const() 则确保了当上面的文件被 import 时，其 module 名称(也就是 __name__ 的值，当文件被运行时 __name__ 的值为 __main__, 被 import 时 __name__ 的值则是 module 名称)对应的是一个 _const() 对象，从而可以直接通过其创建常量。因此，使用的方法如下 12import constantprint(constant.MAX_COUNT) 参考： class method vs static method in PythonData modelPython 之旅]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Makefile 语法详解(2)-变量、条件判断与函数]]></title>
      <url>%2F2018%2F12%2F07%2FMakefile%20%E8%AF%AD%E6%B3%95%E8%AF%A6%E8%A7%A3(2)-%E5%8F%98%E9%87%8F%E3%80%81%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD%E4%B8%8E%E5%87%BD%E6%95%B0%2F</url>
      <content type="text"><![CDATA[本文内容是之前的文章 Makefile 简介 的补充，详细介绍了 Makefile 中的变量（包括变量的定义、批量替换、局部变量等）、条件判断和函数（内置函数和自定义函数）。 变量在 Makefile 中的定义的变量，就像是 C/C++ 语言中的宏一样，代表了一个文本字串，在 Makefile 中执行的时候其会自动地展开在所使用的地方。其与C/C++所不同的是，可以在 Makefile 中改变其值。 变量在声明时需要给予初值，而在使用时，需要给在变量名前加上 $ 符号，但最好用小括号 ()或是大括号 {} 把变量给包括起来。如果你要使用真实的 $ 字符，那么你需要用 $$ 来表示。 变量的赋值定义 Makefile 中为变量赋值可用四种操作符：=、:=、?=、+=， 参考 StackOverflow 上的问题 What is the difference between the GNU Makefile variable assignments =, ?=, := and +=?, 这四个符号的主要区别是 = 赋值是 lazy 的，也就是在使用的时候才会递归的获取变量的值（递归指的是可以通过一个变量为另一个变量赋值） := 则是在声明的时候变量的值就确定了 ?= 表示在变量没有值的时候才给其赋值 += 则是在原来的值上 append 一个其他的值（自动添加空格） 其中 = 赋值是的递归获取值比较难理解，简单来说就是右侧中的变量不一定非要是已定义好的值，也可以使用后面定义的值。如下是一个简单地例子 123456foo = $(bar)bar = $(ugh)ugh = Hahaall: echo $(foo) 执行 make all 时输出的值是 Haha, 而是用 := 赋值时就不允许这么赋值，在 := 右边的值必须只能是字符串或者前面定义的变量，也就是在 := 右边的值必须要是目前为止已经确定的值。 12345678# 示例 1x := fooy := $(x) barx := later# 示例 2y := $(x) barx := foo 上面示例1 中的 y 的值为 foo bar，示例2 中的值为 bar。 变量值的替换我们可以替换变量中的共有的部分，其格式是 $(var:a=b) 或是 $(var: %a=%b)，其意思是，把变量 var 中所有以 a 字串结尾的那些值从 a 替换成 b; 如下是一个简单的示例 1234foo := a.o b.o c.obar := $(foo:.o=.c) 或 bar := $(foo:%.o=%.c)# bar 的值是 a.c b.c c.c 把变量值当做变量名Makefile 中如果变量 a 的值是变量 b 的名称，那么可以把变量 a 的值直接当做变量 b 使用，如下是一些简单的例子 1234567891011# 示例一x = yy = zz = ua := $($($(x))) # u# 示例二x = $(y)y = zz = Helloa := $($(x)) # Hello 局部变量前面我们所讲的在 Makefile 中定义的变量都是全局变量, 但是也可以为某个目标设置局部变量，这种变量被称为 Target-specific Variable，因为它的作用范围只在这条规则以及连带规则中，所以其值也只在作用范围内有效。而不会影响规则链以外的全局变量的值，因为局部变量的名称可与全局变量的名称相同。 其一般语法如下，首先在 target 中定义局部变量，则在 target 及其依赖的目标中使用该变量即可12target : local_variable assignmenttarget : use variable 如下是个简单的例子123456789101112prog : CFLAGS = -gprog : prog.o foo.o bar.o $(CC) $(CFLAGS) prog.o foo.o bar.oprog.o : prog.c $(CC) $(CFLAGS) prog.cfoo.o : foo.c $(CC) $(CFLAGS) foo.cbar.o : bar.c $(CC) $(CFLAGS) bar.c 除了 Target-specific Variable，还有 Pattern-specific Variable，即不是在某个特定的 target 中定义局部变量，而是在某个特定的 pattern 中定义局部变量，其语法与上面的类似，如下所示是定义了所有以 .o 结尾的目标中的一个局部变量 1%.o : CFLAGS = -O 条件判断使用条件判断，可以让 make 根据运行时的不同情况选择不同的执行分支，主要有以下几个关键字： ifeq、ifneq、ifdef、ifndef、else 和 endif; 根据名称其实也能基本能猜出各个关键字的作用了, ifeq 和 ifneq 是一对关键字，表示其后面跟随的两个参数是够相等，ifdef 和 ifndef 是一对关键字，表示其后跟随的变量是否已经被定义过。 如下是一个简单的例子，表示目标 foo 可以根据变量 $(CC) 值来选取不同的函数库来编译123456789libs_for_gcc = -lgnunormal_libs =foo: $(objects)ifeq ($(CC),gcc) $(CC) -o foo $(objects) $(libs_for_gcc)else $(CC) -o foo $(objects) $(normal_libs)endif 当 $(CC) 是 gcc 时，目标 foo 的规则是 12foo: $(objects) $(CC) -o foo $(objects) $(libs_for_gcc) 因此，上面的写法可以写成如下更简洁且容易理解的形式 1234567891011libs_for_gcc = -lgnunormal_libs =ifeq ($(CC), gcc) libs=$(libs_for_gcc)else libs=$(normal_libs)endiffoo: $(objects) $(CC) -o foo $(objects) $(libs) 需要注意的一点是在关键字所在的这一行上，多余的空格是被允许的，但是不能以 Tab 键做为开始，否则就被认为是命令 使用 ifeq 可有若干种形式，如下所示的五种形式都是等价的, ifneq 的使用方法相同 12345ifeq (&lt;arg1&gt;, &lt;arg2&gt;) ifeq '&lt;arg1&gt;' '&lt;arg2&gt;' ifeq "&lt;arg1&gt;" "&lt;arg2&gt;" ifeq "&lt;arg1&gt;" '&lt;arg2&gt;' ifeq '&lt;arg1&gt;' "&lt;arg2&gt;" ifdef 和 ifndef 的使用方法也类似，只是其后面只跟着一个变量。 函数GNU make 内置了一些函数，在 Makefile 中使用函数来处理变量，可以让我们的命令或是规则更为的灵活 函数调用很像变量的使用，也是以 $ 来标识的，其语法如下，其中 &lt;function&gt; 就是函数名，&lt;arguments&gt; 为函数的参数，参数间以逗号 , 分隔，而函数名和参数之间以空格分隔 123$(&lt;function&gt; &lt;arguments&gt;)或$&#123;&lt;function&gt; &lt;arguments&gt;&#125; 以下是 GNU make 内置的一些函数 字符串处理函数 subst用法：$(subst &lt;from&gt;,&lt;to&gt;,&lt;text&gt;)功能：把字串 &lt;text&gt; 中的 &lt;from&gt; 字符串替换成 &lt;to&gt;。返回：函数返回被替换过后的字符串。 patsubst用法：$(patsubst &lt;pattern&gt;,&lt;replacement&gt;,&lt;text&gt;)功能：查找 &lt;text&gt; 中的单词是否符合模式 &lt;pattern&gt;，如果匹配的话，则以 &lt;replacement&gt;替换。这里，&lt;pattern&gt; 可以包括通配符 %，表示任意长度的字串。如果 &lt;replacement&gt; 中也包含 %，那么，&lt;replacement&gt;中的这个 % 将是 &lt;pattern&gt; 中的那个 % 所代表的字串。（可以用\来转义）返回：函数返回被替换过后的字符串示例：$(patsubst %.c,%.o,x.c.c bar.c) 返回的结果是 x.c.o bar.o findstring用法：$(findstring &lt;find&gt;,&lt;string&gt;)功能：在字串 &lt;string&gt; 中查找 &lt;find&gt; 字串。返回：如果找到，那么返回 &lt;find&gt;，否则返回空字符串。 sort用法：$(sort &lt;list&gt;)功能：给字符串 &lt;list&gt; 中的单词排序（空格分隔）。返回：返回排序后的字符串。示例：$(sort foo bar lose) 返回 bar foo lose 。 word用法：$(word &lt;n&gt;,&lt;text&gt;)功能：取字符串 &lt;text&gt; 中第 &lt;n&gt; 个单词。（从1开始）返回：返回字符串 &lt;text&gt; 中第 &lt;n&gt; 个单词示例：$(word 2, foo bar baz) 返回值是 bar foreach 函数foreach 函数是用作循环的，其用法如下 $(foreach &lt;var&gt;,&lt;list&gt;,&lt;text&gt;) 该函数的意思是，把参数 &lt;list&gt; 中的单词逐一取出放到参数所指定的变量 &lt;var&gt; 中，然后再执行 &lt; text&gt; 所包含的表达式。每一次 &lt;text&gt; 会返回一个字符串，循环过程中，&lt;text&gt; 的所返回的每个字符串会以空格分隔，最后当整个循环结束时，&lt;text&gt;所返回的每个字符串所组成的整个字符串（以空格分隔）将会是foreach函数的返回值。 如下是一个简单的例子 123names := a b c dfiles := $(foreach n,$(names),$(n).o) 上面的例子中，$(name) 中的单词会被挨个取出，并存到变量 n 中，$(n).o 每次根据 $(n) 计算出一个值，这些值以空格分隔，最后作为 foreach 函数的返回值，所以，$(files) 的值是 a.o b.o c.o d.o 需要注意的是，foreach 中的参数是一个临时的局部变量，foreach函数执行完后，参数的变量将不在作用，其作用域只在foreach函数当中。 call函数call 函数可以用来创建自定义函数。其语法是： $(call &lt;expression&gt;,&lt;parm1&gt;,&lt;parm2&gt;,&lt;parm3&gt;,...) 当 make 执行这个函数时，&lt;expression&gt; 参数中的变量: $(1)，$(2)，$(3)等，会被参数 &lt;parm1&gt;，&lt;parm2&gt;，&lt;parm3&gt;依次取代。而 &lt;expression&gt; 的返回值就是 call 函数的返回值。例如： 12reverse = $(2) $(1) foo = $(call reverse,a,b) 经过上面的表达式得到的 foo 的值是 b a。当然，可以为 reverse 定义更复杂的操作。 shell函数，shell函数把执行操作系统命令后的输出作为函数返回。因此可以用操作系统命令以及字符串处理命令 awk，sed 等等命令来生成一个变量，如： 12contents := $(shell cat foo)files := $(shell echo *.c)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Makefile 语法详解(1)-文件搜索、伪目标与命令执行]]></title>
      <url>%2F2018%2F12%2F06%2FMakefile%20%E8%AF%AD%E6%B3%95%E8%AF%A6%E8%A7%A3(1)-%E6%96%87%E4%BB%B6%E6%90%9C%E7%B4%A2%E3%80%81%E4%BC%AA%E7%9B%AE%E6%A0%87%E4%B8%8E%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%2F</url>
      <content type="text"><![CDATA[本文内容是之前的文章 Makefile 简介 的补充，详细介绍了 Makefile 中的文件搜索（即通过 VPATH 和 vpath 进行源文件的搜索）、伪目标（定义多个生成目标）以及执行多条命令的一些做法。 文件搜索在一些源文件较多的大工程中，通常会把源文件分类并存放在不同的目录中(比如自定义的头文件放在 include 目录，源文件放在 src 目录)，而当 make 需要去找寻文件的依赖关系时，可以在文件前加上路径，但最好的方法是把一个路径告诉 make，让 make 自动去搜索。 Makefile 文件中的特殊变量 VPATH 就是完成这个功能的，如果没有指明这个变量，make只会在当前的目录中去找寻依赖文件和目标文件。如果定义了这个变量，make就会在当前目录找不到的情况下，到所指定的目录中去找寻文件。 1VPATH = src:../headers 上面的的定义指定两个目录，src 和 ../headers，make 会按照这个顺序进行搜索。目录由 :分隔; 然，当然，在此之前会在当前目录查找 另一个设置文件搜索路径的方法是使用 make 的 vpath 关键字（全小写），这不是变量，这是一个 make 的关键字，这和上面提到的那个 VPATH 变量很类似，但是它更为灵活。它可以指定不同的文件在不同的搜索目录中。这是一个很灵活的功能。它的使用方法有三种： 1、vpath &lt;pattern&gt; &lt;directories&gt;在目录 &lt;directories&gt; 中搜索符合模式 &lt;pattern&gt; 的文件 2、vpath &lt;pattern&gt;清除符合模式 &lt;pattern&gt; 的文件的搜索路径 3、vpath清除所有已被设置好了的文件搜索目录。 vpath 使用方法中的 &lt;pattern&gt; 需要包含 % 字符。% 的意思是匹配零或若干字符，如，%.h 表示所有以 .h 结尾的文件。&lt;pattern&gt; 指定了要搜索的文件集，而 &lt;directories&gt; 则指定了 &lt;pattern&gt; 的文件集的搜索的目录。例如： vpath %.h ../headers 表示要 make 在 ../headers 目录下搜索所有以 .h 结尾的文件。（如果某文件在当前目录没有找到的话） 我们可以连续地使用 vpath 语句，以指定不同搜索策略。如果连续的 vpath 语句中出现了相同的 &lt;pattern&gt;，或是被重复了的 &lt;pattern&gt;，那么，make 会按照 vpath 语句的先后顺序来执行搜索。如： 123vpath %.c foovpath %.c blishvpath %.c bar 其表示“.c”结尾的文件，先在 foo 目录，然后是 blish ，最后是 bar 目录。 12vpath %.c foo:barvpath %.c blish 而上面的语句则表示 .c 结尾的文件，先在 foo 目录，然后是 bar 目录，最后才是 blis 目录。 伪目标最早先的一个例子中，我们提到过一个 clean 的目标，这是一个“伪目标”，因为并不生成“clean”这个文件 12clean: rm *.o temp 为了避免伪目标名称和文件重名的这种情况，可以使用一个特殊的标记 .PHONY 来显式地指明一个目标是伪目标, 如下所示 .PHONY : clean 只要有这个声明，不管是否有 clean 文件，要运行 clean 这个目标，只能运行 make clean。于是整个过程可以这样写： 123.PHONY : cleanclean : rm *.o temp 伪目标一般没有依赖的文件,但是也可以为伪目标指定所依赖的文件。伪目标同样可以作为“默认目标”，只要将其放在第一个。一个常用的做法就是，如果你的 Makefile 需要一次生成若干个可执行文件，可以通过伪目标实现，如下所示 1234567891011all : prog1 prog2 prog3.PHONY : allprog1 : prog1.o utils.o cc -o prog1 prog1.o utils.oprog2 : prog2.o cc -o prog2 prog2.oprog3 : prog3.o sort.o utils.o cc -o prog3 prog3.o sort.o utils.o 由于 Makefile 中的第一个目标会被作为其默认目标，上面声明的伪目标 all 会作为默认目标，但由于 all 又是一个伪目标，所以不会有 all 文件产生, 但是会生成其依赖的三个文件 从上面的例子我们可以看出，目标也可以成为依赖。所以，伪目标同样也可成为依赖。看下面的例子：12345678910.PHONY : cleanall cleanobj cleandiffcleanall : cleanobj cleandiff rm programcleanobj : rm *.ocleandiff : rm *.diff make cleanall 将清除所有要被清除的文件。cleanobj 和 cleandiff 这两个伪目标有点像“子程序”的意思。我们可以输入 make cleanall 和 make cleanobj 和 make cleandiff 命令来达到清除不同种类文件的目的。 命令执行执行连续命令执行多条命令时可以分多行写；但是如果要让上一条命令的结果应用在下一条命令时，应该使用分号或 &amp;&amp; 分隔这两条命令。比如第一条命令是cd命令，并且希望第二条命令在 cd 之后的基础上运行，那么就不能把这两条命令写在两行上，而应该把这两条命令写在一行上，用分号分隔。如： 12345678# 示例一：exec: cd /usr/lib/ pwd # 示例二：exec: cd /usr/lib/; pwd 或 cd /usr/lib/ &amp;&amp; pwd 当我们执行 make exec 时，第一个例子中的 cd 没有起到作用，pwd 会打印出当前的 Makefile 目录，而第二个例子中，cd就起作用了，pwd 会打印出 /usr/lib/ 嵌套执行make在一些大的工程中，往往会把不同模块或是不同功能的源文件放在不同的目录中，可以在每个目录中都书写一个该目录的 Makefile，这有利于 Makefile 变得更加地简洁且更容易维护，而不至于把所有的东西全部写在一个 Makefile 中，这个技术对于我们模块编译和分段编译有着非常大的好处。 例如，有一个子目录叫subdir，这个目录下有个 Makefile 文件，来指明了这个目录下文件的编译规则。那么我们总控的Makefile可以这样书写： 12345subsystem: cd subdir &amp;&amp; $(MAKE)或subsystem: $(MAKE) -C subdir $(MAKE) 是自定义的宏变量，不直接使用 make 命令，而是定义 $(MAKE) 这个宏变量的原因是 make 有时需要一些参数，所以定义成一个变量比较利于维护。 如果要传递变量到下级 Makefile 中，那么可以使用这样的声明 export variable_name如果不想让某些变量传递到下级 Makefile 中，那么可以这样声明 unexport variable_name如果你要传递所有的变量，那么，只要一个 export 就行了; 后面什么也不用跟，表示传递所有的变量。 定义命令包如果 Makefile 中出现一些相同命令序列，那么可以为这些相同的命令序列定义成一个变量。定义这种命令序列的语法以 define 开始，以 endef 结束，如： 12345678define run-actionaction 1action 2action 3endeffoo.o : foo.c $(run-action) 这里的 run-action 是这个命令包的名字，在 define 和 endef 中的三行就是命令序列；可以看到，使用这个命令包就好像使用变量一样。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Makefile 简介]]></title>
      <url>%2F2018%2F12%2F05%2FMakefile%20%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[C/C++ 在 linux 下可通过 gcc 进行编译，当文件数量少，文件依赖关系简单时可通过命令进行编译，但是当文件数量庞大且关系复杂时，就要依赖于 make 和 Makefile 管理这些复杂关系了。MakeFile 类似于 shell 脚本，定义了文件的依赖关系，以及编译的先后顺序。本文主要介绍 Makefile 的基本语法，本系列文章主要参考了 跟我一起写Makefile。 基本格式Makefile 书写格式一般如下 123456# 方式一target : prerequisites command# 方式二target : prerequisites; command target 可以是一个 object file(目标文件)，也可以是一个执行文件，还可以是一个标签（label） prerequisites 是要生成那个 target 所需要的文件或是目标。 command 也就是 make 需要执行的命令，如果其不与 targets : prerequisites 在一行，那么，必须以 Tab键开头，如果和 prerequisites 在一行，那么可以用分号做为分隔。 这是一个文件的依赖关系，也就是说，target 这一个或多个的目标文件依赖于 prerequisites 中的文件，其生成规则定义在 command 中。说白一点就是说，prerequisites 中如果有一个以上的文件比 target 文件要新的话，command 所定义的命令就会被执行。这就是 Makefile 的规则。也就是 Makefile 中最核心的内容。 示例如果一个工程有3个头文件，和8个c文件，我们为了完成前面所述的那三个规则，我们的 Makefile 应该是下面这个样子的。 12345678910111213141516171819202122232425# 如果后面这些.o文件比edit可执行文件新,那么才会去执行命令edit : main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o \ insert.o search.o files.o utils.omain.o : main.c defs.h cc -c main.ckbd.o : kbd.c defs.h command.h cc -c kbd.ccommand.o : command.c defs.h command.h cc -c command.cdisplay.o : display.c defs.h buffer.h cc -c display.cinsert.o : insert.c defs.h buffer.h cc -c insert.csearch.o : search.c defs.h buffer.h cc -c search.cfiles.o : files.c defs.h buffer.h command.h cc -c files.cutils.o : utils.c defs.h cc -c utils.cclean : rm edit main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o 我们可以把这个内容保存在名字为 “makefile” 或 “Makefile” 的文件中，然后在该目录下直接输入命令 “make” 就可以生成执行文件edit。如果要删除执行文件和所有的中间目标文件，那么，只要简单地执行一下 make clean 就可以了。 在定义好依赖关系后，后续的那一行定义了如何生成目标文件的操作系统命令，一定要以一个tab键作为开头。记住，make 并不管命令是怎么工作的，他只管执行所定义的命令。make 会比较 targets 文件和 prerequisites 文件的修改日期，如果 prerequisites 文件的日期要比 targets 文件的日期要新，或者target不存在的话，那么，make就会执行后续定义的命令。 输入 make 命令后，发生了如下的动作 make 会在当前目录下找名字叫 “Makefile” 或 “makefile” 的文件。如果找到，它会找文件中的第一个目标文件（target）作为最终的目标文件，在上面的例子中即为 edit 这个文件 如果 edit 文件不存在，或是 edit 所依赖的后面的 .o 文件的文件修改时间要比 edit 这个文件新，那么，他就会执行后面所定义的命令来生成 edit 这个文件。 如果 edit 所依赖的 .o 文件也不存在，那么 make 会在当前文件中找目标为 .o 文件的依赖性，如果找到则再根据那一个规则生成 .o 文件。（这有点像一个堆栈的过程） 这就是整个make的依赖性，make 会一层又一层地去找文件的依赖关系，直到最终编译出第一个目标文件, 在找寻的过程中，如果出现错误，比如最后被依赖的文件找不到，那么 make 就会直接退出并报错。 而像 clean 这种没有被第一个目标文件直接或间接关联，那么它后面所定义的命令将不会被自动执行，但可以通过 make clean 进行显式执行，以此来清除所有的目标文件，以便重编译。 如果这个工程已被编译过，当我们修改了其中一个源文件时，比如 file.c，那么根据我们的依赖性，我们的目标 file.o 会被重编译, 因此 file.o 文件修改时间要比edit要新，所以 edit 也会被重新链接了 使用变量与自动推导上面同一个 .o 文件在多个地方都重复了，因此可通过变量的方式简化 Makefile，Makefile 的变量也就是一个字符串，类似于 C 语言的宏，通过 $(变量名) 来访问变量内容 1234567891011121314151617181920212223objects = main.o kbd.o command.o display.o \ insert.o search.o files.o utils.oedit : $(objects) cc -o edit $(objects)main.o : main.c defs.h cc -c main.ckbd.o : kbd.c defs.h command.h cc -c kbd.ccommand.o : command.c defs.h command.h cc -c command.cdisplay.o : display.c defs.h buffer.h cc -c display.cinsert.o : insert.c defs.h buffer.h cc -c insert.csearch.o : search.c defs.h buffer.h cc -c search.cfiles.o : files.c defs.h buffer.h command.h cc -c files.cutils.o : utils.c defs.h cc -c utils.cclean : rm edit $(objects) make 能够自动推导文件以及文件依赖关系后面的命令，即只要make看到一个 .o 文件，它就会自动的把 .c 文件加在依赖关系中，如果 make 找到一个 whatever.o，那么 whatever.c，就会是 whatever.o 的依赖文件。并且 cc -c whatever.c 也会被推导出来，于是，我们的 Makefile 再也不用写得这么复杂。 12345678910111213141516171819objects = main.o kbd.o command.o display.o \ insert.o search.o files.o utils.o cc = gccedit : $(objects) cc -o edit $(objects)main.o : defs.hkbd.o : defs.h command.hcommand.o : defs.h command.hdisplay.o : defs.h buffer.hinsert.o : defs.h buffer.hsearch.o : defs.h buffer.hfiles.o : defs.h buffer.h command.hutils.o : defs.h.PHONY : cleanclean : -rm edit $(objects) .PHONY 意思表示clean是一个伪目标。而在 rm 命令前面加了一个小减号的意思是当某些文件出现问题时不要管，继续做后面的事。当然，clean 的规则不要放在文件的开头，这就会变成 make 的默认目标。不成文的规矩是clean从来都是放在文件的最后 引用其他 MakeFile在 Makefile 使用 include 关键字可以把别的 Makefile 包含进来，这很像 C/C++ 语言的 #include，被包含的文件会原模原样的放在当前文件的包含位置。include的语法是： 1include &lt;filename&gt;; filename可以是当前操作系统Shell的文件模式（可以包含路径和通配符） 比如有这样几个Makefile：a.mk、b.mk、c.mk，还有一个文件叫 foo.make，以及一个变量 $(bar)，其包含了 e.mk和f.mk，那么，下面两个语句等价： 123include foo.make *.mk $(bar)include foo.make a.mk b.mk c.mk e.mk f.mk make 命令开始时，会找寻 include 所指出的其它 Makefile，并把其内容安置在当前的位置。类似 C/C++的 #include 指令一样。如果文件都没有指定绝对路径或是相对路径的话，make会在当前目录下首先寻找，如果当前目录下没有找到，那么，make还会在下面的几个目录下找： 如果 make 执行时，有 -I 或 --include-dir 参数，那么 make 就会在这个参数所指定的目录下去寻找。 如果目录 &lt;prefix&gt;/include（一般是：/usr/local/bin或/usr/include）存在的话，make也会去找。如果有文件没有找到的话，make会生成一条警告信息，但不会马上出现致命错误。它会继续载入其它的文件，一旦完成makefile的读取， make会再重试这些没有找到，或是不能读取的文件，如果还是不行，make才会出现一条致命信息。 如果想让 make 忽略那些无法读取的文件，可以在include前加一个减号 -, 即 -include &lt;filename&gt;; 表示无论 include 过程中出现什么错误，都不要报错继续执行。和其它版本make兼容的相关命令是sinclude，其作用和这一个是一样的。 小结Makefile里主要包含了五个东西：显式规则、隐晦规则、变量定义、文件指示和注释。 显式规则: 说明了如何生成一个或多个目标文件。这是由 Makefile 的书写者明显指出，要生成的文件，文件的依赖文件，生成的命令。 隐晦规则: make 有自动推导的功能，所以隐晦的规则可以让我们比较简略地书写 Makefile，这是由make所支持的。 变量的定义。在 Makefile 中我们要定义一系列的变量，变量一般都是字符串，这个有点 C 语言中的宏，当 Makefile 被执行时，其中的变量都会被扩展到相应的引用位置上。 文件指示。其包括了三个部分，一个是在一个 Makefile 中引用另一个 Makefile，就像 C语言中的 #include 一样；另一个是指根据某些情况指定 Makefile 中的有效部分，就像C语言中的预编译 #if 一样； 还有就是定义一个多行的命令。有关这一部分的内容，我会在后续的部分中讲述。 注释。Makefile 中只有行注释，和UNIX的Shell脚本一样，其注释是用 # 字符，这个就像C/C++中的 // 一样。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A Tour of Go 摘记]]></title>
      <url>%2F2018%2F11%2F28%2FA%20Tour%20of%20Go%20%20%E6%91%98%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[最近在学习 golang，本文主要是 A Tour of Go 的一些摘记，涵盖了 go 的一些基本语法与数据类型、通过 struct 和 method 实现类的特性、以及 go 中重要的 concurrency 特性。 Basicspackages, variables and functions 程序的入口在 main 这个 package 中, 需要在文件头声明 import 通过 () 引入多个 package 1234import ( "fmt" "math/rand") 定义变量的几种形式（定义常量只能用第一种方法，且 var 改成 const) 1234567// 方式 1var i, j intvar i, j int = 1, 2var i, j int = 1, 2// 方式 2i, j := 1, 2 golang 中的变量类型 123456789101112131415boolstringint int8 int16 int32 int64uint uint8 uint16 uint32 uint64 uintptrbyte // alias for uint8rune // alias for int32 // represents a Unicode code pointfloat32 float64complex64 complex128 函数声明时类型放在最后：参数类型放在参数后，返回类型放在函数名后; 且连续多个参数类型一样时可只为最后一个写类型，如下 1234567func add(x int, y int) int &#123; return x + y&#125;// same as abovefunc add(x , y int) int &#123; return x + y&#125; 函数可返回多个值, 且返回值可以被命名（此时的返回值相当函数里两个命名的变量） 1234567891011// 多个返回值func swap(x, y string) (string, string) &#123; return y, x&#125;// 具名返回值, 返回 x 和 yfunc split(sum int) (x, y int) &#123; x = sum * 4 / 9 y = sum - x return&#125; 函数可作为参数传入其他函数，也可作为返回值, 如下 123456789101112131415161718import ( "fmt" "math")func compute(fn func(float64, float64) float64) float64 &#123; return fn(3, 4)&#125;func main() &#123; hypot := func(x, y float64) float64 &#123; return math.Sqrt(x*x + y*y) &#125; fmt.Println(hypot(5, 12)) fmt.Println(compute(hypot)) fmt.Println(compute(math.Pow))&#125; 闭包（一个函数访问并能更新在其函数域外的变量）一般会将函数作为返回值 1234567891011121314func fibonacci() func() int &#123; a, b := -1, 1 return func() int &#123; a, b = b, a + b return b &#125;&#125;func main() &#123; f := fibonacci() for i := 0; i &lt; 10; i++ &#123; fmt.Println(f()) &#125;&#125; for, if else, switch and defer go 里面没有 while，for 相当于 while 包含 for 的三个元素的圆括号 () 是没有的(不仅是 for 语句，if else 等其他语句也没有)，且花括号 {} 总是必须的(哪怕只有一条语句) 12345678910sum := 0for i := 0; i &lt; 10; i++ &#123; sum += i&#125;// 三个元素的头尾元素可缺省sum := 1for ; sum &lt; 1000; &#123; sum += sum&#125; for 跟 range 结合可以遍历 slice 和 map, 每次返回两个值，对于 map 是 key:value，对于 slice 则是 index:value 1234567891011121314var pow = []int&#123;1, 2, 4, 8, 16, 32, 64, 128&#125;func main() &#123; for i, v := range pow &#123; fmt.Printf("2**%d = %d\n", i, v) &#125; // 不要第一个元素 for _, v := range pow &#123; &#125; // 不要第二个元素 for i := range pow &#123; &#125;&#125; if 语句的判断条件中可带有一个短的声明语句 123if v := math.Pow(3, 5); v &lt; 100 &#123; return v&#125; switch 语句不需要 break（实际上是go自动添加了） 123456789101112131415161718import ( "fmt" "runtime")func main() &#123; fmt.Print("Go runs on ") switch os := runtime.GOOS; os &#123; case "darwin": fmt.Println("OS X.") case "linux": fmt.Println("Linux.") default: // freebsd, openbsd, // plan9, windows... fmt.Printf("%s.", os) &#125;&#125; defer 后面的语句直到其所在的函数返回才执行, 实际上 defer 后面的语句被 push 到了 stack 中，返回时就出栈 12345678910// 输出 hello worldfunc main() &#123; defer fmt.Println("world") fmt.Println("hello")&#125;// 从 9 到 0 输出for i := 0; i &lt; 10; i++ &#123; defer fmt.Println(i)&#125; pointer, struct, slice and map go 中的指针跟 C/C++ 中的类似 123456789101112131415import "fmt"func main() &#123; i, j := 42, 2701 var p1 *int p1 = &amp;i fmt.Println(*p1) // read i through the pointer *p1 = 21 // set i through the pointer fmt.Println(i) // see the new value of i p2 := &amp;j // point to j *p2 = *p2 / 37 // divide j through the pointer fmt.Println(j) // see the new value of j&#125; go 中的结构体也跟 C/C++ 的类似，只是声明方式不一样，多了 type 这个关键字，且类型 struct 放在最后 123456789type Vertex struct &#123; X int Y int&#125;func main() &#123; v := Vertex&#123;1, 2&#125; fmt.Println(v.X)&#125; 指向结构体的指针访问结构体的元素的方式跟 C/C++ 不一样，C/C++要用 -&gt;, go 中可直接使用 . 1234567891011type Vertex struct &#123; X int Y int&#125;func main() &#123; v := Vertex&#123;1, 2&#125; p := &amp;v p.X = 1e9 fmt.Println(v)&#125; array 与 slice 是两个不同的类型，区别在于 array 长度固定，slice 长度可变，声明时一个指定长度，一个不指定长度, 语法均是 []type 1234567891011121314151617181920212223func main() &#123; // array 声明方式一 var a [2]string a[0] = "Hello" a[1] = "World" fmt.Println(a[0], a[1]) fmt.Println(a) // array 声明方式二 primes := [6]int&#123;2, 3, 5, 7, 11, 13&#125; fmt.Println(primes) // slice 声明方式一 s := prime[:4] // slice 声明方式二 var s []int // slice 声明方式三 s := []int&#123;2, 3, 5, 7, 11, 13&#125; // slice 声明方式四 s := make([]int, 5) 需要注意的是，每个 slice 都有一个 underlying array， slice 就是这个 array 的 reference ，当 slice 被其他 array 的部分元素初始化时，修改 slice 就是在修改这个 array；slice 有两个方法：len() 和 cap(), len() 是 slice 的长度，cap() 则是 slice 对应的 underlying array 从 slice 的第一个元素开始计算的长度 1234567891011121314151617181920212223242526272829func printSlice(s []string) &#123; fmt.Printf("len=%d cap=%d %v\n", len(s), cap(s), s)&#125;func main() &#123; names := [4]string&#123; "John", "Paul", "George", "Ringo", &#125; fmt.Println(names) a := names[0:2] b := names[1:3] printSlice(a) printSlice(b) b[0] = "XXX" fmt.Println(a, b) fmt.Println(names)&#125;// 输出如下[John Paul George Ringo]len=2 cap=4 [John Paul]len=2 cap=3 [Paul George][John XXX] [XXX George][John XXX George Ringo] slice 是可变长的，通过 append 函数往 slice 末尾添加元素，如下 123456789101112func main() &#123; var s []int printSlice(s) // The slice grows as needed. s = append(s, 1) printSlice(s) // We can add more than one element at a time. s = append(s, 2, 3, 4) printSlice(s)&#125; map 初始化方式有以下几种, map 声明的中括号里面是 key 的类型，外面是 value的类型，即 map[key]value 12345// method 1m := make(map[int]int)// method 2var m = map[int]int&#123;1:1, 2:2&#125; 删除 map 中某个元素可直接使用 delete (map, key) 测试某个 key 是否在 map 中可通过 map[key] 返回一个两元组实现，即elem, ok := m[key], 如果 ok 为 true，则表示 key 里面 Methods and interfaces go 没有提供类，但是可以通过为 struct 定义 method 来提供类相近的特性；method 就是在普通函数基础上定义一个 receiver 参数（定义在 func 和函数名之间），表明这个方法是属于某个 struct 的 123456789101112type Vertex struct &#123; X, Y float64&#125;func (v1 Vertex) Abs() float64 &#123; return math.Sqrt(v1.X*v1.X + v1.Y*v1.Y)&#125;func main() &#123; v := Vertex&#123;3, 4&#125; fmt.Println(v.Abs())&#125; reveiver 参数也可以是指针类型，这意味着通过 receiver 参数能够直接修改 struct 的值(函数的普通参数也是需要指针才能修改实参的值)， 同时也不用在调用 method 时创建新的内存来存储临时对象，因此指针类型的 receiver 也更加常用 123456789101112131415161718type Vertex struct &#123; X, Y float64&#125;func (v Vertex) Abs() float64 &#123; return math.Sqrt(v.X*v.X + v.Y*v.Y)&#125;func (v *Vertex) Scale(f float64) &#123; v.X = v.X * f v.Y = v.Y * f&#125;func main() &#123; v := Vertex&#123;3, 4&#125; v.Scale(10) // equal to (&amp;v).Scale(10) fmt.Println(v.Abs())&#125; go 中的接口（interface) 概念与 Java中的类似，声明一系列的方法，实现了这些方法就是实现了这个接口(无需显式声明） 123456789101112131415161718type I interface &#123; M()&#125;type T struct &#123; S string&#125;// This method means type T implements the interface I,// but we don't need to explicitly declare that it does so.func (t T) M() &#123; fmt.Println(t.S)&#125;func main() &#123; var i I = T&#123;"hello"&#125; i.M()&#125; 一个非常普遍的 interface 是 Stringer, 这是由 fmt 这个 package 中定义的，一旦实现过了这个接口里面的 String() string 方法, fmt.Println() 时就会调用这个方法 123456789101112131415161718192021type Stringer interface &#123; String() string&#125;type Person struct &#123; Name string Age int&#125;func (p Person) String() string &#123; return fmt.Sprintf("%v (%v years)", p.Name, p.Age)&#125;func main() &#123; a := Person&#123;"Arthur Dent", 42&#125; z := Person&#123;"Zaphod Beeblebrox", 9001&#125; fmt.Println(a, z)&#125;// 输出Arthur Dent (42 years) Zaphod Beeblebrox (9001 years) 类似于上面的接口 fmt.Stringer, error 也是一个常用的接口，实现该接口需要实现 Error 方法 123456789101112131415161718192021222324252627import ( "fmt" "time")type MyError struct &#123; When time.Time What string&#125;func (e *MyError) Error() string &#123; return fmt.Sprintf("at %v, %s", e.When, e.What)&#125;func run() error &#123; return &amp;MyError&#123; time.Now(), "it didn't work", &#125;&#125;func main() &#123; if err := run(); err != nil &#123; fmt.Println(err) &#125;&#125; go 的很多标准库都实现了 io.Reader 这个接口，接口主要定义了这个方法 func (T) Read(b []byte) (n int, err error), 如下是一个简单地用法，每次从 string 中读取 8 个 byte 12345678910111213141516171819import ( "fmt" "io" "strings")func main() &#123; r := strings.NewReader("Hello, Reader!") b := make([]byte, 8) for &#123; n, err := r.Read(b) fmt.Printf("n = %v err = %v b = %v\n", n, err, b) fmt.Printf("b[:n] = %q\n", b[:n]) if err == io.EOF &#123; break &#125; &#125;&#125; Concurrency goroutine 相当于是轻量级的线程，使用方法很简单 go f() 就启动了一个 goroutine 来执行函数 f() channel 类似于一个队列，但是出列时要等到所有入列的操作已完成则可进行，反之亦然，这就为 goroutine 提供了 synchronize 的功能 12345678910111213141516171819202122232425ch := make(chan int) // create a channel of type intch &lt;- v // Send v to channel ch.v := &lt;-ch // Receive from ch, and assign value to v.// simple examplefunc sum(s []int, c chan int) &#123; sum := 0 for _, v := range s &#123; sum += v &#125; c &lt;- sum // send sum to c&#125;func main() &#123; s := []int&#123;7, 2, 8, -9, 4, 0&#125; c := make(chan int) go sum(s[:len(s)/2], c) go sum(s[len(s)/2:], c) x, y := &lt;-c, &lt;-c // receive from c fmt.Println(x, y, x+y)&#125;// output -5 17 12 Buffered channel：可以为 channel 指定长度(如chan := make(chan int, 100))，这样当 channel 满了之后不能再往其中写数据（再写会报错），这种 channel 也被称为 buffered channel；也可以 close 一个 channel， 这样也不能继续入列 通过 range 来遍历 channel 会自动判断 channel 是否已经为空，即 for v := range channel, 需要注意的是，用 for 来遍历一个 channel 时，该 channel 必须要先 close，否则会出现错误 fatal error: all goroutines are asleep - deadlock! select 包含的代码块中有多个 case 语句，当其中的任一条件被满足时才会执行，否则会阻塞（可以添加 default 选项使得 select 不会被阻塞），当有多个被满足时则随机选一个；通常 select 也被用来协调多个 goroutine 的通信, 如下 123456789101112131415161718192021222324func fibonacci(c, quit chan int) &#123; x, y := 0, 1 for &#123; select &#123; case c &lt;- x: x, y = y, x+y case &lt;-quit: fmt.Println("quit") return &#125; &#125;&#125;func main() &#123; c := make(chan int) quit := make(chan int) go func() &#123; for i := 0; i &lt; 10; i++ &#123; fmt.Println(&lt;-c) &#125; quit &lt;- 0 &#125;() fibonacci(c, quit)&#125; channel 是 goroutine 通信的一个有效工具，但是除了通信，多个 goroutine 往往还会存在着同时读写一个变量的情况，这时候就要加锁，os 中也将这样的锁机制成为互斥量（mutex），go 的 sync.Mutex 就是一个能够实现加锁和解锁的互斥量;如下是多个 goroutine 共享一个 Counter 的例子 12345678910111213141516171819202122232425262728293031323334353637import ( "fmt" "sync" "time")// SafeCounter is safe to use concurrently.type SafeCounter struct &#123; v map[string]int mux sync.Mutex&#125;// Inc increments the counter for the given key.func (c *SafeCounter) Inc(key string) &#123; c.mux.Lock() // Lock so only one goroutine at a time can access the map c.v. c.v[key]++ c.mux.Unlock()&#125;// Value returns the current value of the counter for the given key.func (c *SafeCounter) Value(key string) int &#123; c.mux.Lock() // Lock so only one goroutine at a time can access the map c.v. defer c.mux.Unlock() return c.v[key]&#125;func main() &#123; c := SafeCounter&#123;v: make(map[string]int)&#125; for i := 0; i &lt; 1000; i++ &#123; go c.Inc("somekey") &#125; time.Sleep(time.Second) fmt.Println(c.Value("somekey"))&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[C++ 编译初步]]></title>
      <url>%2F2018%2F11%2F24%2FC%2B%2B%20%E7%BC%96%E8%AF%91%E5%88%9D%E6%AD%A5%2F</url>
      <content type="text"><![CDATA[文章为转载，转载自 Compiling Cpp，主要涉及到 C++ 在linux 下通过 g++ 编译的一些基础知识，包括编译单个源文件、多个源文件、创建并使用静态库等。 关于程序的编译和链接一般来说，无论是 C、C++、还是pas，首先要把源文件编译成中间代码文件，在Windows下也就是 .obj 文件，UNIX下是 .o 文件，即 Object File，这个动作叫做编译（compile）。然后再把大量的 Object File合成执行文件，这个动作叫作链接（link）。 编译时，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件应该对应于一个中间目标文件。都如果函数未被声明，编译器会给出一个警告，但可以生成Object File。但在链接程序时，链接器会在所有的 Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error） 链接时，主要是链接函数和全局变量，链接器并不管函数所在的源文件，只管函数的中间目标文件，在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“库文件”（Library File)，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。 单个源文件生成可执行程序下面是一个保存在文件 helloworld.cpp 中一个简单的 C++ 程序的代码：123456/* helloworld.cpp */#include &lt;iostream&gt;int main(int argc,char *argv[]) &#123; std::cout &lt;&lt; "hello, world" &lt;&lt; std::endl; return(0);&#125; 程序使用定义在头文件 iostream 中的 cout，向标准输出写入一个简单的字符串。该代码可用以下命令编译为可执行文件： $ g++ helloworld.cpp 编译器 g++ 通过检查命令行中指定的文件的后缀名可识别其为 C++ 源代码文件。编译器默认的动作：编译源代码文件生成对象文件(object file)，链接对象文件和 libstdc++ 库中的函数得到可执行程序, 然后删除对象文件。 由于命令行中未指定可执行程序的文件名，编译器采用默认的 a.out。程序可以这样来运行： 12$ ./a.outhello, world 更普遍的做法是通过 -o 选项指定可执行程序的文件名。下面的命令将产生名为 helloworld 的可执行文件： $ g++ helloworld.cpp -o helloworld 在命令行中输入程序名可使之运行： 12$ ./helloworldhello, world 程序 g++ 是将 gcc 默认语言设为 C++ 的一个特殊的版本，链接时它自动使用 C++ 标准库而不用 C 标准库。通过遵循源码的命名规范并指定对应库的名字，用 gcc 来编译链接 C++ 程序是可行的，如下例所示： $ gcc helloworld.cpp -l stdc++ -o helloworld 选项 -l (ell) 通过添加前缀 lib 和后缀 .a 将跟随它的名字变换为库的名字 libstdc++.a。而后它在标准库路径中查找该库。gcc 的编译过程和输出文件与 g++ 是完全相同的。 在大多数系统中，GCC 安装时会安装一名为 c++ 的程序。如果被安装，它和 g++ 是等同，如下例所示，用法也一致： $ c++ helloworld.cpp -o helloworld 多个源文件生成可执行程序如果多于一个的源码文件在 g++ 命令中指定，它们都将被编译并被链接成一个单一的可执行文件。下面是一个名为 speak.h 的头文件；它包含一个仅含有一个函数的类的定义： 123456/* speak.h */#include &lt;iostream&gt;class Speak &#123; public: void sayHello(const char *);&#125;; 下面列出的是文件 speak.cpp 的内容：包含 sayHello() 函数的函数体： 12345/* speak.cpp */#include "speak.h"void Speak::sayHello(const char *str) &#123; std::cout &lt;&lt; "Hello " &lt;&lt; str &lt;&lt; "\n";&#125; 文件 hellospeak.cpp 内是一个使用 Speak 类的程序： 12345678/* hellospeak.cpp */#include "speak.h"int main(int argc,char *argv[])&#123; Speak speak; speak.sayHello("world"); return(0);&#125; 下面这条命令将上述两个源码文件编译链接成一个单一的可执行程序： $ g++ hellospeak.cpp speak.cpp -o hellospeak PS：这里说一下为什么在命令中没有提到 speak.h 这个文件，原因是在 speak.cpp 中包含有 #include&quot;speak.h&quot; 这句代码，它的意思是搜索系统头文件目录之前将先在当前目录中搜索文件 speak.h,而 speak.h 正在该目录中，不用再在命令中指定了。 源文件生成对象文件选项 -c 用来告诉编译器编译源代码但不要执行链接，输出结果为对象文件。文件默认名与源码文件名相同，只是将其后缀变为 .o。例如，下面的命令将编译源码文件 hellospeak.cpp 并生成对象文件 hellospeak.o： $ g++ -c hellospeak.cpp 命令 g++ 也能识别 .o 文件并将其作为输入文件传递给链接器。下列命令将编译源码文件为对象文件并将其链接成单一的可执行程序： 123$ g++ -c hellospeak.cpp $ g++ -c speak.cpp $ g++ hellospeak.o speak.o -o hellospeak 选项 -o 不仅仅能用来命名可执行文件。它也用来命名编译器输出的其他文件。例如：除了中间的对象文件有不同的名字外，下列命令生将生成和上面完全相同的可执行文件： 123$ g++ -c hellospeak.cpp -o hspk1.o $ g++ -c speak.cpp -o hspk2.o $ g++ hspk1.o hspk2.o -o hellospeak 编译预处理选项 -E 使 g++ 将源代码用编译预处理器处理后不再执行其他动作。下面的命令预处理源码文件 helloworld.cpp 并将结果显示在标准输出中： $ g++ -E helloworld.cpp 本文前面所列出的 helloworld.cpp 的源代码，仅仅有六行，而且该程序除了显示一行文字外什么都不做，但是，预处理后的版本将超过 1200 行。这主要是因为头文件 iostream 被包含进来，而且它又包含了其他的头文件，除此之外，还有若干个处理输入和输出的类的定义。 预处理过的文件的 GCC 后缀为 .ii，它可以通过 -o 选项来生成，例如： $ gcc -E helloworld.cpp -o helloworld.ii 生成汇编代码选项 -S 指示编译器将程序编译成汇编语言，输出汇编语言代码而後结束。下面的命令将由 C++ 源码文件生成汇编语言文件 helloworld.s： $ g++ -S helloworld.cpp 生成的汇编语言依赖于编译器的目标平台。 创建静态库静态库是编译器生成的一系列对象文件的集合。链接一个程序时用库中的对象文件还是目录中的对象文件都是一样的。库中的成员包括普通函数，类定义，类的对象实例等等。静态库的另一个名字叫归档文件(archive)，管理这种归档文件的工具叫 ar 。 在下面的例子中，我们先创建两个对象模块，然后用其生成静态库。 头文件 say.h 包含函数 sayHello() 的原型和类 Say 的定义： 123456789101112131415/* say.h */#include &lt;iostream&gt;void sayhello(void);class Say &#123; private: char *string; public: Say(char *str)&#123; string = str; &#125; void sayThis(const char *str)&#123; std::cout &lt;&lt; str &lt;&lt; " from a static library\n"; &#125; void sayString(void);&#125;; 下面是文件 say.cpp 是我们要加入到静态库中的两个对象文件之一的源码。它包含 Say 类中 sayString() 函数的定义体；类 Say 的一个实例 librarysay 的声明也包含在内： 1234567/* say.cpp */#include "say.h"void Say::sayString() &#123; std::cout &lt;&lt; string &lt;&lt; "\n";&#125;Say librarysay("Library instance of Say"); 源码文件 sayhello.cpp 是我们要加入到静态库中的第二个对象文件的源码。它包含函数 sayhello() 的定义： 12345/* sayhello.cpp */#include "say.h"void sayhello()&#123; std::cout &lt;&lt; "hello from a static library\n";&#125; 下面的命令序列将源码文件编译成对象文件，命令 ar 将其存进库中 123$ g++ -c sayhello.cpp$ g++ -c say.cpp$ ar -r libsay.a sayhello.o say.o 程序 ar 配合参数 -r 创建一个新库 libsay.a 并将命令行中列出的对象文件插入。采用这种方法，如果库不存在的话，参数 -r 将创建一个新的库，而如果库存在的话，将用新的模块替换原来的模块。 下面是主程序 saymain.cpp，它调用库 libsay.a 中的代码： 1234567891011/* saymain.cpp */#include "say.h"int main(int argc, char *argv[])&#123; extern Say librarysay; // 使用库的对象 Say localsay = Say("Local instance of Say"); //使用库的类定义 sayhello(); // 使用库的普通该函数 librarysay.sayThis("howdy"); librarysay.sayString(); localsay.sayString(); return(0);&#125; 该程序可以下面的命令来编译和链接： $ g++ saymain.cpp libsay.a -o saymain 程序运行时，产生以下输出： 1234hello from a static libraryhowdy from a static libraryLibrary instance of SayLocal instance of Say 小结上面介绍了手动通过命令编译 C++ 源文件，但是面对一些大工程，源码文件数量多且依赖关系复杂时，手动编译不太现实，这时候就要依赖 make 和 Makefile 对程序进行自动编译了，简单来说就是把编译规则写好在 Makefile 里，然后通过 make 进行自动编译，具体细节可参考这个教程 跟我一起写Makefile。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Forecasting High-Dimensional Data》阅读笔记]]></title>
      <url>%2F2018%2F11%2F15%2F%E3%80%8AForecasting%20High-Dimensional%20Data%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[《Forecasting High-Dimensional Data》 是 Yahoo! 一篇关于流量预估的论文。在合约广告中，需要提前预估某个定向下的流量情况，从而进行合理的售卖和分配。但是由于定向的组合非常多（广告主的多样的需求导致的），而工程上不允许为每个可能的定向预估其流量，因此这篇论文提出了先预估一些基本定向的流量，然后通过 correlation model 从基本定向的流量计算出各种定向下的流量情况，具有较强的工程性，也是之前提到的文章 《Budget Pacing for Targeted Online Advertisements at LinkedIn》 中采用的流量预估方法。 问题定义前面简单提到了文章解决的问题的背景，文章将一个最基本的定向（如性别为男）称为一个 attribute，而每个广告主的需要的流量就是若干个 attribute 的 combination，因此 combination 的数量是非常庞大的，文章要求的就是如何有效地预估所有 combination 的流量情况。 一个最直观的思路就是统计出某个 combination 的历史数据，然后训练模型并进行预估。但是这个方法最致命的地方在于统计 combination 的历史数据以及模型训练所需的耗时在实际中是无法容忍的，需要数小时（文章要求需要在数百毫秒内返回某个 combination 的流量预估值）。那可以将这些放到理想来做么？答案也是不可以的，因为我们不知道哪些 combination 在未来会用到，因此所有的 combination 都需要进行预估，而这个数量过于庞大了。 解决思路文章提出的解决思路是先对一小部分有代表性的 combination 进行上面的历史统计和预估操作，这里的有代表性的流量可以从统计的历史数据中得到（如访问量最多的combination 等），也可以手工选择等；然后结合文章提到的 correlation model，可以从这一小部分的 combination 中预估出所有的 combination 的流量情况。 下面具体详细介绍整体的解决思路 系统总览文章提出的方法系统图如下，每个的 Historical Data Point 实际上就是一个历史 query，包含了某些 attribute 的组合，这些数据主要有两个用途 （1）送入到 Historical Data Aggregator 中对某些有代表性的 Selected Attribute Combinations 进行历史统计，并通过模型进行时间序列的预估，这里的时间序列预估使用了 SARIMA 模型（2）用于构建 Correlation Model，并与上面若干 Selected Attribute Combinations 的预估结果共同对在线的 query 返回预估值 可以看到文章的重点在于 correlation model 的构建以及 correlation model 在 online 部分是如何工作的。 Correlation Model上文提到了对一部分有代表性的 combination（也就是上图中的 Selected Attribute Combinations）进行时间序列预估，这里记为 $Q = \lbrace Q_1, Q_2,… Q_m \rbrace$, 称其为 base queries。则对于一个在线的 query $q$, 其预估步骤如下 选择 $Q$ 中某个 base query $Q_k$, 使得 $q \subseteq Q_k$ 计算 $Q_k$ 在未来时间 $T$ 内的流量，记为 $B(Q_k, T)$ 计算 $q$ 在 $Q_k$ 中出现的概率，记为 $R(q|Q_k)$ 返回 $q$ 的预估结果为 $B(Q_k, T) \times R(q|Q_k)$ Correlation Model 所做的事情就是第3步：计算比例。 而在第一步中，必然会存在某个 $Q_k$，使得$q \subseteq Q_k$, 为什么呢？ 假设总共有 n 个 attribute，那么只要将每个 attribute 作为一个 base query（这样 $Q$ 中便有了 n 个 base queries），则 attribute 的任意 combination 肯定会属于 Q 中的某几个 base queries 的。那如果不止一个 $Q_k$，使得$q \subseteq Q_k$ 时该怎么办？这时候就需要选择一个最小的 $Q_k$, 这里的最小指的是不存在某个 $Q_l$ 同时满足 $q \subseteq Q_l$ 且 $Q_l \subseteq Q_k$。举个简单的例子，假如有两个 base queries $Q1 = a_1 , Q_2 = a1 \wedge a_2$, 则对于某个 query $q = a1 \wedge a_2 \wedge a_3$，应该选择 $Q_2$ 作为 $Q_k$。 下面分别讲述文中提到的三种 correlation model Full Independence Model (FIM)FIM 实际上就是个 Naive Bayes，假设了每个 attribute 是相互独立的，也就是 $$R(Gender = male \wedge Age &lt; 30|Q_k) = R(Gender = male|Q_k) \times R(Age &lt; 30|Q_k)$$ 也就是只要为 $Q_k$ 每个单独的 attribute 计算其比例 $R(.| Q_k)$ 即可。具体计算方法如下 令 $P_t$ 是截止到时间 $t$ 时所有的访问量，$|Q_k \wedge P_t|$ 为这些访问量中满足 $|Q_k|$ 的那些访问量，$|a_i \in (Q_k \wedge P_t)|$ 为满足 $|Q_k|$ 的那些访问量中同时满足 attribute $a_i$的，则对于 attribute，其计算公式如下 $$R(a_i|Q_k) = \frac{|p \in (Q_k \wedge P_t)|}{|Q_k \wedge P_t|}$$ Partwise Independence Model (PIM)将所有的 attribute 都认为是相互独立的显然是不合理的，因为有某些 attribute 之间是相互关联的，比如说年龄和收入一般是存在关联的，因为 PIM 实际上就是将某些可能有关联的 attribute 的比例一起计算。即 $$R(Gender = male \wedge Age &lt; 30 \wedge Incom &gt; 10000|Q_k) = R(Gender = male|Q_k) \times R(Age &lt; 30 \wedge Incom &gt; 10000|Q_k)$$, 而比例的计算公式也跟上面的类似 Sampling-based Joint Model(SJM)上面的两种方法均假设了 attribute 之间的的相互独立性，这依然会存在一定的局限性，能够完全避免独立性的假设呢？这篇文章提出的 Sampling-based Joint Model(SJM) 就避免了独立性的假设。 虽然说是 model ，但是方法还是统计，只是为了避免数量太大，首先做了 sampling，选出经过 sample 后的数据并记为 $S$，然后计算 base query $|Q_k|$ 在 $S$ 中的数量 $|Q_k \cap S|$, 这部分会在离线做，然后在线来了一个 query $q$ 后，会计算在 $S$ 中满足 $q$ 的数量并记为 $n$。则比例计算公式就很简单了 $$R(q|Q_k) = \frac{n}{|Q_k \cap S|}$$ 整个过程思路非常简单，没有涉及到 attribute，因此也没有 attribute independent 的假设。但是关键的问题在于如何高效地算出上面的分子和分母的那些计数。论文使用的是bitmap index, 且使用了论文 Optimizing bitmap indices with efficient compression 中提出的关于 bitmap index 的一种改进方法。 实验效果数据实验采用的数据是 Yahoo！部分页面过去一年的历史访问数据，其中一半用于进行 time-series forecasting，另一半用于验证效果（按时间划分），且对于某些 combination 用了过去四年累积的数据进行 forecasting。Correlation Model 则用了过去一周的数据进行训练，且 SJM 采样得到了 20 million 的数据。 评估指标工程上的评估指标有 speed 和 space，就是时间和空间的评估。 效果上的评估指标主要就是 accuracy，采用的是 absolute percent age error (APE), 假设预估值为 $F$, 真实值为 $A$, 则 APE 的定义如下 $$APE = \frac{|F-A|}{A}$$ APE 针对的是单个 query，但是往往希望的是验证一系列 query 的效果，其指标为 Root Mean Square Error (RMSE)，定义如下, $w_q$ 表示每个 query 的权重，这个值被设为 query 在过去两年的合约中出现的次数 $$RMSE = \sqrt{\frac{\sum_{q \in Q}w_qAPE^2(q)}{\sum_{q \in Q}w_q}}$$ 效果对比下面是三个模型的 RMSE（取了对数） 效果对比，横轴的 forecast horizon 表示预估未来多少天的时的效果。可以看到假设 attribute independent 会降低最后的效果。 除此之外，每个模型在进行 online 是返回结果的耗时都要小于 50 毫秒；且 FIM 和 PIM 模型的内存占比大概是 500 MB，因为这两个模型只需要存储比例值和预估的趋势曲线，但是 SJM 的内存大概到了 20GB（20 million的data points），空间主要由 bit-map index 消耗。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Code Complete 阅读笔记-创建高质量的代码(2)]]></title>
      <url>%2F2018%2F10%2F31%2FCode%20Complete%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-%E5%88%9B%E5%BB%BA%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E4%BB%A3%E7%A0%81(2)%2F</url>
      <content type="text"><![CDATA[本文主要是 Code Complete 中创建高质量的代码部分的的两章笔记：第 8 章（防范式编程）、第 9 章（伪代码编码过程），介绍了如何进行防范式编程（defensive programming），即保护程序免遭非法输入数据的破坏，目的其实就是增强程序的鲁棒性；同时介绍了如何通过伪代码编码方法来创建类和子程序。 防御式编程（defensive programming）这里的防御式编程的主要思想是：子程序不应该因为传入错误的数据而被破坏，哪怕是由其他子程序产生的错误数据。下面主要就是讲述一些方法来处理这类问题 断言（assertion）assert 关键字在多门语言中均有出现，如 Python， Java，c++ 等；其目的就是非常肯定某个条件表达式是成立的，否则就是出错了，如确保分母不为 0 等。而应用在防范式编程中，可以用来检查如下条件 输入参数和输出参数的取值处于预期的范围内 子程序开始（或结束）执行时，文件或流是打开（或关闭）的状态 子程序开始（或结束）执行时，文件或流的读写位置处于开头（或结尾）的状态 子程序开始（或结束）执行时，某个容器是空的（满的） 文件或流已用只读、只写或可读可写的方式打开 仅用于输入的变量的值没有被子程序修改 指针非空 传入子程序的数组或其他容器的 size 能容纳设定的数据元素个数 …….. 需要注意的是，断言只是在开发阶段被编译到目标代码中，而在生成产品代码是并不编译进去，以降低系统的性能。 关于使用断言，有如下的建议 用错误处理代码来处理预期会发生的情况，而用断言来处理绝对不应该发生的状况 避免把执行代码放到断言中，因为这样会导致关闭断言时，编译器很可能就把这些代码排除在外,正确的做法是先将执行代码的结果在断言外用变量存起来，如下所示 123456// badassert PerformAction() == True//goodresult = PerformAction() assert result == True 用断言来注解并验证前条件（precondition）和后条件（postcondition）。简单来说，前条件就是在执行函数前需要为函数准备好的条件，后条件则是在函数执行后要完成的任务。 错误处理技术上面提到用断言来处理绝对不应该发生的状况，而用错误处理代码来处理预期会发生的情况，如网络阻塞等。那么该怎么处理那些可能发生的错误呢？本章给出了如下可行的方法 返回中立值。如数值计算结果返回0，字符串可以返回空字符，指针操作可以返回一个空指针等。 返回下一个正确的数据。如在处理数据流的时候，如果发现某一条记录已经损坏，可以继续读下去知道又找到一条正确记录为止，比如说以每秒 100 次的速度读取体温计的数据，那么如果某一次得到的数据有误，只需再等上 1/100 秒然后继续读取即可。 返回与前一次相同的数据。同样是上面的体温计的例子，如果在某次读取中没有获得数据，可以简单地返回前一次的读取结果，这是根据实际的应用情况决定的，在某些变化较大的场景下不能这么使用。 使用最接近的合法值。比如说汽车的速度盘，倒车时无法显示负值的速度，因此简单地显示0，即最接近的合法值。 把警告信息记录到日志文件中，然后继续执行。 返回一个错误码。即只让系统的某些部分处理错误，其他部分不在本地处理错区，而是简单地报告说有错误发生。 调用错误处理子程序或对象。把错误处理都集中在一个全局的错误处理子程序或对象中。 …. 异常异常是把代码中的错误或异常事件传递给调用代码的一种特殊手段。异常的基本结构是：子程序使用 throw 跑出一个异常对象，再被调用链上层其他子程序的 try-catch 语句捕获。 使用异常时有以下建议 只有在真正例外的情况下才抛出异常。也就是说假如子程序局部能够处理这个错误就不要抛出异常；因为异常虽然能够增加程序的鲁棒性，但是会使程序的复杂性增加。调用子程序的代码需要了解呗调用的代码中可能会抛出的异常，因此异常弱化了封装性。 避免在构造函数和析构函数中抛出异常。比如在C++里只有当对象完全构造后才可能调用析构函数，也就是说，如果在构造函数的代码里抛出异常，就不会调用析构函数，从而造成潜在的资源泄露问题。 在合适的抽象层次抛出异常。即抛出的异常应该与子程序接口的抽象层次一致的。如下所示，第一个例子中 GetTaxId() 将更底层的 EOFException 返回给调用方, 这样破坏了封装性。与之相反的是第二个例子，GetTaxId() 里的异常处理代码可能只要把一个 io_disk_not_ready 异常映射为 EmployeeDataNotAvailable 异常就好了。 123456789101112131415// 抛出抽象层次不一致的异常的类class Employee &#123; .... public TaxId GetTacId() throws EOFException &#123; .... &#125;&#125;// 一个在一致的抽象层次上抛出的异常的类class Employee &#123; .... public TaxId GetTacId() throws EmployeeDataNotAvailable &#123; .... &#125;&#125; 把项目中对异常的使用标准化。为了保持异常处理尽可能便于管理，可以用以下几种途径把对异常的使用标准化 某些语言允许抛出的类型多种多样，如C++ 就可以抛出对象、数据以及指针， 因此应该为可以抛出哪些种类的异常建立一个标准；可以考虑只抛出从 std::exception 基类派生出来的对象 考虑创建项目的特定异常类，用作项目所有可能抛出的异常的基类 规定在何种场合先允许代码使用 throw-catch 语句在局部对错误进行处理 规定在何种场合允许代码抛出不在局部进行处理的异常 隔离程序隔栏（barricade）是一种容损策略，与防火墙类似，当火灾发生时，防火墙能阻止火势从建筑物的一个部位向其他部位蔓延。而以防御式编程为目的而进行隔离的一种方法，就是把某些接口选定为 “安全” 区域的边界。对穿越安全区域边界的数据进行合法性校验，并当数据非法时做出对策，如下图所示 同样的可以在类的层次中使用这种方法，类的公用方法可以假设数据是不安全了，需要负责检查数据并进行清理。一旦类的公用 方法接受了数据，那么类的私有方法就可以假定数据都是安全的了。 隔栏的使用使断言和错误处理有了清晰的区分，隔栏外部的程序应使用错误处理技术，在哪里对数据做任何假定都是不安全的；而在隔栏的内部的程序就应该使用断言技术，因为传进来的数据应该已在通过隔栏时被清理过了。 小结防御式编程能够让错误更容易发现和修改，并减少错误对产品代码的破坏，增加程序的鲁棒性，但是过度的使用也会引起问题。如果在每一个能够想到的提防用一种能想到的方法检查从参数传入的数据，那么程序将会变得臃肿而缓慢，而且引入了额外的代码增加了软件的复杂度。因此需要考虑好在那些重要的地方进行防御，然后因地制宜地调整进行防御式编程的优先级。 伪代码编码过程这一章主要关注创建类及其子程序的一种方式：伪代码编码。伪代码编程过程是一种通过书写伪代码而更加高效的创建程序代码的专门方法。 伪代码关于使用伪代码有以下指导原则 用类似英语的语言来精确描述特定操作 避免使用目标编程语言中的语法元素，而应该在一个比代码本身略高的层次上进行设计 在意图层面上编写伪代码，即用伪代码去描述解决问题的方法的意图，而不是写如何在目标语言中实现这个方法 如下是一段违背了上面的指导原则的伪代码，这段代码的意图不明确，而且包含了 C 语言的具体语法以及编码细节（返回1表示null） 123456increment resource number by 1allocate a dlg struct using mallocif malloc() returns NULL then return 1invoke OSrsrc_init to initialize a resource for the operating system*hRsrcPtr = resource numberreturn 0 下面是针对同样功能所写的伪代码，比起上面的就要好很多了 12345678910Keep track of current number of resources in use If another resource is available Allocate a dialog box structure If a dialog box structure could be allocated Note that one more resource is in use Initialize the resource Store the resource number at the location provided by the caller Endif EndifReturn true if a new resource was created; else return false 使用这种风格的伪代码能够带来以下好处 伪代码使得评审更加容易。无须检查源代码就可以评审细节设计 伪代码支持反复迭代精化的思想。从高层设计开始，将其精化为伪代码，然后再把伪代码精化为源代码。这样持续不断的小步精化，可以在推向更低的细节层次的同时，不断检查已形成的设计；及时修复各个层次的错误 伪代码使变更更加容易。这跟在产品最具可塑性的阶段进行变动的原则是相同的 伪代码比其他形式的设计文档更加容易维护。使用其他方法时，设计和代码是分离的，当其中之一变动时，两者就不再一致，而使用伪代码编程时，伪代码中的语句将会变为代码中的注释。 通过伪代码创建子程序通过伪代码创建子程序主要包括以下步骤 设计子程序 编写子程序的代码 检查代码 收尾工作 按照需要重复上述步骤 设计子程序设计子程序可以从以下角度出发 （1）检查先决条件。即检查子程序与整体设计是否匹配，是否是真正必需的，至少是间接需要的（2）定义子程序要解决的问题。应该详细说明如下问题 子程序将要隐藏的信息 传给这个子程序的各项输入 从该子程序得到的输出 调用程序前确保有关的前条件成立 在子程序将控制权交回给调用方之前，确保其后条件的成立 （3）为子程序命名，这一部分在上一篇笔记有提及（4）决定如何测试子程序（5）在标准库中搜寻可用的功能。即如果在标准库中已经有该子程序特定的功能的实现，可以直接使用而不重复造轮子（6）研究算法和数据类型（7）编写伪代码。首先为子程序编写一般性注释，然后为子程序编写高层次的伪代码。如下所示 12345678910111213141516// 一般性注释This routine outputs an error message based on an error codesupplied by the calling routine. The way it outputs the messagedepends on the current processing state, which it retrieveson its own. It returns a value indicating success or failure.// 伪代码set the default status to &quot;fail&quot;look up the message based on the error codeif the error code is validif doing interactive processing, display the error messageinteractively and declare successif doing command line processing, log the error message to thecommand line and declare successif the error code isn&apos;t valid, notify the user that an internal errorhas been detectedreturn status information 编写子程序代码主要过程就是在伪代码的每一句话下填入代码。如下所示 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* This routine outputs an error message based on an error codesupplied by the calling routine. The way it outputs the messagedepends on the current processing state, which it retrieveson its own. It returns a value indicating success or failure.*/Status ReportErrorMessage( ErrorCode errorToReport ) &#123; // set the default status to "fail" Status errorMessageStatus = Status_Failure; // look up the message based on the error code Message errorMessage = LookupErrorMessage( errorToReport ); // if the error code is valid if ( errorMessage.ValidCode() ) &#123; // determine the processing method ProcessingMethod errorProcessingMethod = CurrentProcessingMethod(); // if doing interactive processing, display the error message // interactively and declare success if ( errorProcessingMethod == ProcessingMethod_Interactive ) &#123; DisplayInteractiveMessage( errorMessage.Text() ); errorMessageStatus = Status_Success; &#125; // if doing command line processing, log the error message to the // command line and declare success else if ( errorProcessingMethod == ProcessingMethod_CommandLine ) &#123; CommandLine messageLog; if ( messageLog.Status() == CommandLineStatus_Ok ) &#123; messageLog.AddToMessageQueue( errorMessage.Text() ); messageLog.FlushMessageQueue(); errorMessageStatus = Status_Success; &#125; else &#123; // can't do anything because the routine is already error processing &#125; else &#123; // can't do anything because the routine is already error processing &#125; &#125; // if the error code isn't valid, notify the user that an // internal error has been detected else &#123; DisplayInteractiveMessage( "Internal Error: Invalid error code in ReportErrorMessage()" ); &#125; // return status information return errorMessageStatus;&#125; 检查代码主要包含以下几个步骤 1.人肉运行代码2.编译子程序。把编译器的告警级别调到最高；消除产生错误消息和警告的所有根源3.在调试器中逐行执行代码4.测试代码，编写测试用例来测试代码5.消除程序中的错误 收尾工作收尾工作就是重新审视整个子程序代码来确保子程序的质量合乎标准 检查子程序的接口。确保所有的输入、输出数据都参与了计算，且所有的参数也都用到了 检查整体的设计质量。确认子程序只干一件事；子程序之间的耦合是松散的；子程序采用了防御式编程； 检查子程序中的变量。检查是否存在不准确的变量名称、未被用到的对象、未经声明的变量、未经初始化的对象等 检查子程序的布局。确保正确地使用了空白来明确子程序、表达式及参数列表的逻辑结构 检查子程序的文档，确认有伪代码转化而来的注释仍是准确无误的。 出去冗余的注释 。。。 小结伪代码编码是创建类和子程序的一个有效途径，在编写时需要使用易懂的英语，避免使用特定编程语言中才有的特性，同时要在意图层面上写伪代码，即描述该做什么，而不是怎么去做。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Budget Pacing for Targeted Online Advertisements at LinkedIn》 阅读笔记]]></title>
      <url>%2F2018%2F10%2F25%2F%E3%80%8ABudget%20Pacing%20for%20Targeted%20Online%20Advertisements%20at%20LinkedIn%E3%80%8B%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[《Budget Pacing for Targeted Online Advertisements at LinkedIn》 是 LinkedIn 在 2014 年发表的一篇关于预算控制的论文，里面的预算控制的策略并不复杂，并且具有很强的实践性和工程性。本文主要是根据论文总结了这个方法的基本原理、工程实现以及实验效果。 顾名思义，预算控制（budget control）在广告系统中的作用就是该如何合理花掉广告主的预算。在实际中经常会出现广告主预算消耗过快的问题，这会导致广告主早早退出竞价，不仅会影响广告主体验，也会导致整个广告生态的竞争力下降（因为那些有竞争力的广告主消耗早早就花光了），在二价的机制下，直接影响了平台的收入。论文为了解决这个问题，提出了一个 budget pacing 的算法。 原理算法的主要思想就是令每个 campaign（推广计划）的消耗趋势与其曝光变化趋势基本保持一致，以天为时间单位，campaign 为预算控制单位，首先为每个 campaign 预测出其在当天的曝光情况；然后基于其曝光情况，在当前时间片，假如 已消耗/当天预算 的比例大于 已曝光/预测的总曝光 的比例，则说明预算已经消耗过快，需要减小消耗的速度，反之则要加快消耗的速度。 下面详细讲述这个算法的原理 对于某个 campaign $i$，记其出价为 $b_i$，当天预算为 $d_i$。一天的时间被划分为 $T$ 个时间窗口， $s_{i,t}(0 \le t \lt T)$ 表示截止到第 $t$ 个时间窗口开始时的累积预算，$f_{i,t}$ 与 $s_{i,t}$ 对应，表示截止到第 $t$ 个时间窗口开始时的累积曝光($f_{i,t}$ 是预测出来的，下式的 $f_{i,T}$ 表示预测出来 campaign $i$ 在当天的总曝光)。则在时间窗口 $t$ 开始时，有 $$a_{it} := \frac{f_{i, t}}{f_{i, T}}d_i$$ 根据上面的比例，在每次竞价开始时，为 campaign $i$ 算出其参与这次竞价的概率 $p_{i,t}$，论文称这个概率为 PTR（pass through rate）, 计算方式如下 $$p_{i,t} =\begin{cases} p_{i, t-1}*(1 + r_t)&amp; s_{i, t} \le a_{i, t}\\p_{i, t-1}*(1 - r_t)&amp; s_{i, t} \gt a_{i, t}\end{cases}$$ 上式中的 $r_t(0 &lt; r_t &lt; 1)$ 称作调整速率（adjustment rate）。 对于 campaign $i$ , $a_{it}$ 在当天开始已经确定，因为 $f_{i,t}$ 是预测出来的, 因此控制预算就完全是针对 $s_{i,t}$ 的变化进行。 除了这种调整 campaign 参与竞价概率的控制方式，某些文献也建议通过调整出价的方式进行干预，如下所示是论文 [1] 提出的调价方式 $$b_i^* = b_i \psi(s_{i, t}/d_i)$$ 其中 $\psi(x) = 1 - e^{x-1}$，但是 LinkedIn 这篇论文的作者不建议采用这种方式，原因是对于那些快耗尽预算的campaign，bid 修改的幅度很小，且出价一般存在着保留价（reserve price），因此可调整的幅度很小。论文也对这种方式做了实验，结果显示该方式对提升 campaign 的预算消耗时长无帮助。 整个算法就是这么简单，下面主要说一下具体的实现细节。 实现细节更新 PTR 频率更新 PTR 频率设置成每分钟一次，也就是说时间窗口的大小为 1 min，实验证明这个更新频率使得整个系统更快达到一个稳定状态。 预估曝光量上面预估某个 campaign 的曝光量可以说是整个算法至为关键的地方，论文并没有针对这一点提出自己的方法，而是采用了论文 [2] 里面的方法，这篇论文是 Yahoo 在做保量的合约广告时提出的预估流量的方法。这里不详细展开了，会另外写一篇博客加以阐释。 调整速率的设置调整速率也就是上面的 $r_t$，目的是控制 PTR 变化的快慢，论文将这个值设置为固定的 10%，这不仅实现简单，鲁棒性也很强。另外一种更复杂的设置方法就是将这个值设置成 $s_{i,t}$ 的变化率，也就是 $\partial s_{i,t}/\partial t$, 表示消耗过快的 campaign 其对应的调整速率也应该较大。然后论文还是选择了固定的 10% 的值，原因有两个 1）$s_{i,t}$ 表示的曲线并不光滑（一系列离散的点），尤其是对于 CPC 这种有了点击才会扣费的广告，这时候的 $s_{i,t}$ 波动会比较大，从而使得计算出来的 $\partial s_{i,t}/\partial t$ 会比较 noisy2）PTR 的更新频率比较频繁，因此即使当前 PTR 不在最合适的位置（$\partial s_{i,t}/\partial t$），也能够很快更新到理想位置 设置 PTR 初始值（Slow Start）论文将每个 campaign 的 PTR 初始值设置为 10%，并将这种方式称为 slow start，因为这个初始值较小。设置较小的初始值给予系统以时间来调整每个 campaign 的 PTR，反之若 PTR 一开始就设置得很高，会导致预算很快被花光。 同样，更合理的方式是为每个 campaign 设置一个 PTR，但是论文并没有针对这一点进行深度的探讨。 Fast Finish由于系统存在统计偏差，使得 PTR 的值偏低，这会导致当天预算没法完全花出去，而 fast finish 就是针对这个问题的一种解决方法。具体的做法就是修改上面预测的 $f_{i,t}$ (allocation curve)，令最后两个小时的曝光量为 0，这样会导致 budget pacing 这个算法每天会尝试在 22 小时内花光预算。 工程上的设计下图是 LinkedIn 的广告系统概览图，advertiser action 是指广告主的行为，包括创建 campaign、修改 bid 或 budget 等；ad requests 则是指用户浏览而触发的广告请求。在 ad sever 中的 campaign index 记录着每个 campaign 当前的状态（曝光，消耗等情况），pacing module 会根据预设的更新频率从 database 中获取最新的数据来更新 campaign index。 需要注意的是，pacing module 的更新并不是一次立刻完成，而是采用了较为平缓的方式，上文提到了 pacing 的更新频率为每分钟一次，因此在实际更新是大概每 7s 更新 12% 的campaign；这种更新频率能够让系统的负载较为均匀，也能够较快达到一个稳定状态。 实验的设计与效果由于以上的 pacing 方式是以 campaign 为单位的，因此实验会将所有的 campaign 等分为两部分，分别作为实验组和对照组（当然，也可以采用灰度而不是全量的方式）。 为了避免时间因素（weekly，seasonality）的影响，论文认为设计的实验至少要持续两周，如下图所示是一个 campaign 的实验设置情况，其中 On 表示采用上述的 pacing 算法，Off 表示不采用。 则采用 pacing 的效果是标为 On 的那些天的效果的均值，那么该采用哪些指标来评估效果？ 首先我们要认识到，在线广告是一个广告主、平台和用户的三方博弈过程，因此在考虑任意机制带来的收益或损失时都要同时考虑到这三方的利益；论文也是同时考虑了这三方的利益，设置了以下指标 广告主的利益 Campain life time：预算消耗 95% 所消耗的时间 Unique impressions per spend：单位消耗给广告主带来的unique user数量，计算方式 number of unique user/total spend Number of campaigns：表示当天平台服务的 campaign 的数量 平台的收益 Cost per request：每次请求的平均收益，计算方式 total revenue/number of requests Over delivery: 超扣的金额占预算的比例 用户体验 Unique campaings served: 用户看到的所有广告中有几个 unique campaign，表示用户看到的广告的多样性，论文认为这个值越大越好 论文在 LinkedIn 的两种广告（Direct Ads 和 Sponsored Status Updates）上分别做了这个实验，结果如下，带加号 + 的指标表示 On 在 oFF 的基础上的变化比例；根据下表，在各个指标上均有较高提升 且根据 cost per click 指标，可在一定程度上了解目前系统的竞争力情况，该值越大，表示系统竞争越激烈，论文也根据这个指标对比了采用这个策略前后系统的竞争力，如下图所示，可以看到，采用 pacing 后前期的竞争有所缓和（slow start导致的），而后期的竞争力比原来有所提升，原因是 campaign life time 变长了，因此有竞争力的 campaign 不会早早就退出了竞价环境。 论文参考文献 [1] A. Mehta, A. Saberi, U. Vazirani, and V. Vazirani.Adwords and generalized on-line matching. Journal of the ACM, 54(5):Article no. 22, October 2007[2] D. Agarwal, D. Chen, L.-j. Lin, J. Shanmugasundaram, and E. Vee. Forecasting high-dimensional data. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data, SIGMOD ’10, pages 1003–1012, New York, NY, USA, 2010. ACM.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Code Complete 阅读笔记-创建高质量的代码(1)]]></title>
      <url>%2F2018%2F10%2F18%2FCode%20Complete%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-%E5%88%9B%E5%BB%BA%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E4%BB%A3%E7%A0%81(1)%2F</url>
      <content type="text"><![CDATA[最近在看Code Complete（中文译作代码大全），一本关于代码构建的书。虽然研究生阶段做的东西与算法结合比较紧密，找工作的岗位也叫算法工程师，但是始终觉得算法工程师首先也得是个工程师，而不应该仅仅是调参师，因此一些基本的工程能力还是不可或缺的。本文主要是创建高质量的代码部分的的两章笔记：第 6 章（可以工作的类）、第 7 章（高质量的子程序），主要给出了在构建类和子程序过程中的一些建议。 可以工作的类（Working class）良好的类接口创建高质量的类，第一步也可能是最重要的一步就是创建一个良好的接口，而这又涉及到两部分：抽象和封装 对于抽象，有以下建议 (1) 类的接口应该展示一致的抽象层次, 也就是说如果某个类实现了不止一个 ADT(abstract data type)，那么就应该把这个类重新组织为一个或多个定义更加明确的 ADT，如下所示的 cpp 代码中混合了不同层次抽象的类接口 123456789101112131415class EmployeeCensus: public ListContainer &#123; public: ... // The abstraction of these routines is at the “employee” level. void AddEmployee( Employee employee ); void RemoveEmployee( Employee employee ); //The abstraction of these routines is at the “list” level. Employee NextItemInList(); Employee FirstItem(); Employee LastItem(); ... private: ...&#125;; 这个类展现了两个 ADT： Employee 和 ListContainer，原因是使用容器类或其他类库来实现内部逻辑，但是却没有把使用容器类或其他类库这一事实隐藏起来，如下是修改过有着一直抽象层次的类接口 123456789101112131415class EmployeeCensus &#123; public: ... // The abstraction of all these routines is now at the “employee” level. void AddEmployee( Employee employee ); void RemoveEmployee( Employee employee ); Employee NextEmployee(); Employee FirstEmployee(); Employee LastEmployee(); ... private: // That the class uses the ListContainer library is now hidden. ListContainer m_EmployeeList; ...&#125;; 没有采取 EmployeeCensus 继承 ListContainer 的方式，是因为 EmployeeCensus 与 ListContainer 不是 “is a” 的关系, 因为 EmployeeCensus 中可能还会有排序、统计等 ListContainer 不支持的操作。 (2) 提供成对的服务。大多数操作都有与其相对应、相等以及相反的操作，如上面的第一个和最后一个，添加和删除等。所以在设计一个类的时候，要检查每一个公共子程序，决定是否需要另一个与其互补的操作，但是也不要盲目地创建 (3) 把不相关的信息转移到其他类。如果某个类中一半子程序使用着该类的一半数据，而另一半子程序则使用另一半的数据，这时其实已经把两个类混在一起使用了，需要拆开。 抽象通过提供一个可以让你忽略实现细节的模型来管理复杂度，而封装则强制阻止你看到细节。两个概念比较相似，关于封装，有以下建议 (1) 尽可能限制类和成员的可访问性。当犹豫着子程序的可访问性应设为 public、protected、private 中哪一种时，经验之举是采用最严格且可行的访问级别。 (2) 不要公开暴露成员数据。即不要直接访问成员数据，而是通过 Get 、 Set 子程序进行访问和修改。 (3) 留意过于紧密的耦合关系。耦合指的是两个类之间关联的紧密程度，这种关联通常是约松越好，因此可以有以下一些指导建议 尽可能限制类和成员的可访问性 避免友元类/友元函数 避免在公开接口中暴露成员数据 设计和实现问题包含(has a)与继承(is a)从英文上便可区分两者，包含指的是将某个对象作为类的成员，而继承则是在原来的对象之上进行拓展。 关于包含，需要警惕有超过约 7 个数据成员的类，因为某些研究表明人们在做事情时能记住的离散项目的个数是 7 ± 2，如果一个类包含超过约 7 个数据成员，可考虑将其分解为几个更小的类。 关于继承，其目的是通过定义能为两个或更多的派生类提供共有元素的基类从而写出更精炼的代码，在使用继承时，需要考虑 （1）成员函数是否应对派生类可见？是否应该有默认实现？默认实现能够被覆盖？（2）继承需要遵循 Liskov 替换原则(Liskov Substitution Principle，LSP), 简单来说就是对于基类中定义的所有子程序，用在它的任何一个派生类中时的含义都应该是相同的，而不应该存在着不同派生类在使用同一个基类方法时需要区分其返回的值的单位等细节。（3) 派生类中的成员函数不要与基类中不可覆盖的成员函数重名（4）使用多态来避免类型检查, 对于下面的代码，应该用基类的 shape.Draw() 的方法来替代 shape.DrawCircle() 和 shape.DrawSquare()，从而避免这些类型检查。123456789switch ( shape.type ) &#123; case Shape_Circle: shape.DrawCircle(); break; case Shape_Square: shape.DrawSquare(); break; ...&#125; （5）避免让继承体系过深，建议继承的层次在 2-3 层，派生类的个数在 7±2（6）让所有数据都是 private，如果派生类需要访问基类的属性，应该提供 protected 的 accessor function。 那么，何时使用包含，何时使用继承？ 如果多个类共享数据而非行为，应该创建这些类可以包含的共用对象 如果多个类共享行为而非数据，应该让它们从共同的基类继承而来，并在基类里面定义公用的子程序 如果过多个类既共享数据也共享行为，应该让它们从一个共同过的基类继承，并在基类里面定义共用的数据和子程序 成员函数、数据成员、构造函数关于成员函数和数据成员有以下建议 （1) 让类中的子程序的数量尽可能少（2) 减少类调用的不同子程序的数量（也叫扇入/fan in，因为类用到其他类的数量越高，其出错率也越高（3) 对其他类的子程序的间接调用要尽可能少，比图说 A 对象中创建了 B 对象，应该避免 A 对象直接调用 B 对象中的方法，即 A.B().b_action()（4) 减少类和类之间的互相合作的范围，应尽量让下面这些数字尽可能小，包括实例化的对象的种类、实例化对象上直接调用的不同的子程序的数量、调用由其他对象返回的对象的子程序的数量。 关于构造函数有以下建议 （1）应该尽可能在构造函数中初始化所有的数据成员（2）用 private 构造函数来强制实现单例模式。单例模式指的是一个类只有一个对象，实现的具体方法是把类的所有构造函数都隐藏起来，然后对外提供一个static 的 GetInstance() 子程序，如下所示（Java 示例）： 12345678910111213141516//Java Example of Enforcing a Singleton with a Private Constructorpublic class MaxId &#123; // Private Constructor private MaxId() &#123; ... &#125; ... public static MaxId GetInstance() &#123; return m_instance; &#125; ... // Here is the single instance. private static final MaxId m_instance = new MaxId();...&#125; （3）优先使用 deep copies，除非论证可行，才采用 shallow copies。因为 deep copies 在开发和维护方面都比 shallow copies 简单，shallow copies 需要增加很多代码用于引用计数、确保安全地复制对象、安全地比较对象以及安全地删除对象等。而这些代码是很容易出错的，除非有充分的理由，否则就应该避免它们。 高质量的子程序（High-Quality Routines）创建子程序的必要性不言而喻：能够重用代码、提高可移植性、良好子程序命名甚至能够达到自我注解的作用等等。 内聚性对子程序而言，内聚性指的是子程序中各种操作之间联系的紧密程度。像 Cosine() （余弦函数）这样的函数就是内聚性很强的，因为整个程序只完成了一项功能；而CosinAndTan() (余弦与正切) 这个函数的内聚性就比较弱，因为它完成了多余一项的操作。我们的目标是让每一个子程序只把一件事做好，不再做其他事情，这也是功能上的内聚性，虽然也有一些其他的内聚性，但是功能上的内聚性是最佳的一种内聚性。 命名好的子程序命名非常重要，命名应该遵循以下原则 （1）描述子程序所做的事情，一般采用动宾结构；除了面向对象语言中的类可以忽略宾语，因为对象本身已经包含在调用语句总了，如 document.Print(), orderInfo.Check() 等。（2）避免使用无意义、模糊或表达不清的动词。有些动词的含义非常灵活，可以延伸到涵盖几乎任何含义。像HandleCalculation(), PerformServices(), OutputUser(), ProcessInput(), DealWithOutput() 这些子程序名称根本不能说明子程序是做什么的。这时候要采用更具体的词语，比如说将HandleOutput 改成 FormatAndPrintOutput 就会清晰很多；假如是子程序本身的设计问题而导致了无法采用更具体的词，那么就需要重新组织这个子程序了。（3）给函数命名时要对返回值有所描述。这里的有所描述并不是显式地描述返回值类型，而是通过函数名体现，如 customerID.next(),printer.isReady() 等都较好地体现了返回值（4）准确适用对仗词。命名时遵循对仗词的命名规则有助于保持一致性，从而也提高可读性。下面是一些通用的对仗词。 （5）给常用的操作建立命名规则。如下是作者列举的某个例子，这些方法是某个工程里面获取对象 id 的所有方法，其作用一致，但是到了后来，没人能记住哪个对象应该用哪些子程序了。所以应该一开始就应该统一获取 id 的子程序名称， 1234employee.id.Get()dependent.GetId()supervisor()candidata.id() 参数与返回值关于参数和返回值有以下建议 （1）按照输入-修改-输出的顺序排列参数。而不是按照字母顺序排列，还可以考虑采用某种表示输入、修改、输出的命名规则，如可以给这些参数名字加上 i_, m_, o_ 前缀。（2）如果几个子程序都用了类似的一些参数，应该让这些参数的排列顺序保持一致。这样可以产生一定的记忆效应（3）把状态变量或错误变量放到最后。就是将那些表明发生了错误的变量放到函数的最后，这些参数只是附属于主程序的主要功能，而且是仅用于输出的参数。（4）不要把子程序的参数用于工作变量。如下的代码中 inputVal 就不应该被这么用, 在 C++ 中可以用 const 参数来做这一限制。 123456int Sample( int inputVal ) &#123; inputVal = inputVal * CurrentMultiplier( inputVal ); inputVal = inputVal + CurrentAdder( inputVal ); ... return inputVal;&#125; (5) 关于返回值，要检查所有可能的返回路径。也就是要确保在所有可能的情况下该函数都会返回值。在函数开头用一个默认值来初始化返回值是一个很好的做法，这种方法能够在未正确地设置返回值时提供一张保险网。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Google C++ Style Guide 阅读笔记]]></title>
      <url>%2F2018%2F10%2F02%2FGoogle%20C%2B%2B%20Style%20Guide%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[统一规范的代码风格在团队协作中非常重要，在若干的风格标准中，Google C++ Style 又是较为被认可的，本文是阅读了 Google C++ Style Guide 中第六(命名约定)、七(注释)、八(格式)章后的一些笔记，主要涉及代码的一些基本规范。需要注意的是，各种规范之间并没有绝对的好坏之分，只要团队保持一致即可。 命名约定一般性命名规则 给出描述性的名称(某些为人熟知的词可用缩写如 DNS) 1234567// 使用int num_errors; // Good.int num_completed_connections; // Good.// 不使用int nerr; // Bad - ambiguous abbreviation.int n_comp_conns; // Bad - ambiguous abbreviation. 变量一般是名词(如上）， 函数名一般是指令性的（如 open_file()、 set_num_errors()) 文件命名 文件名要全部小写，可以包含下划线（_） , C++文件以 .cc 结尾，头文件以 .h 结尾 尽量让文件名更加明确， 如 http_server_logs.h 比 logs.h 要好 定义类时文件名一般成对出现，如 foo_bar.h 和 foo_bar.cc，对应类 FooBar 类型命名所有类型命名，包括类、结构体、类型定义（typedef）、枚举，均使用相同约定：每个单词以大写字母开头，不包含下划线，其实就是驼峰法, 如下 12345678// classes and structsclass UrlTable &#123; ...class UrlTableTester &#123; ...struct UrlTableProperties &#123; ...// typedefstypedef hash_map&lt;UrlTableProperties *, string&gt; PropertiesMap;// enumsenum UrlTableErrors &#123; ... 变量命名 变量名一律小写，单词间以下划线相连，类的成员变量以下划线结尾，结构体的数据成员可以和普通变量一样，不用像类那样接下划线，如 12345678910111213string table_name; // OK - uses underscore.string tableName; // Bad - mixed caseclass UrlTableProperties &#123; string name_; int num_entries_; &#125;struct UrlTableProperties &#123; string name; int num_entries;&#125; 对全局变量没有特别要求，少用就好，可以以 g_ 或其他易与局部变量区分的标志为前缀 常量命名 在名称前加 k, 且 k 后接大写字母开头的单词, 如 kDaysInAWeek 函数命名函数这里做了两种分类：普通函数(regular functions)和存取函数(accessors and mutators) 普通函数: 每个单词首字母大写，没有下划线, 如 MyExcitingMethod() 存取函数: 与变量名匹配：如 set_my_exciting_member_variable() 123456789class MyClass &#123; public: ... int num_entries() const &#123; return num_entries_; &#125; void set_num_entries(int num_entries) &#123; num_entries_ = num_entries; &#125; private: int num_entries_;&#125; 枚举命名 枚举值应全部大写，单词间以下划线相连(宏的命名也与其保持一致)： MY_EXCITING_ENUM_VALUE 枚举名称属于类型，因此大小写混合, 即 UrlTableErrors。 12345enum UrlTableErrors &#123; OK = 0, ERROR_OUT_OF_MEMORY, ERROR_MALFORMED_INPUT,&#125;; 小结 总体规则：不要随意缩写，如果说 ChangeLocalValue 写作 ChgLocVal 还有情可原的话，把 ModifyPlayerName 写作 MdfPlyNm 就太过分了，除函数名可适当为动词外，其他命名尽量使用清晰易懂的名词 宏、枚举等使用全部大写+下划线； 变量（含类、结构体成员变量）、文件、命名空间、存取函数等使用全部小写+下划线 ，类成员变量以下划线结尾，全局变量以 g_开头； 普通函数、类型（含类与结构体、枚举类型）、常量等使用大小写混合，不含下划线； 注释注释对保证代码可读性至为重要，当然也要记住，注释的确很重要，但最好的代码本身就是文档（ self-documenting），类型和变量命名意义明确要比通过注释解释模糊的命名好得多。 下面的规则描述了应该注释什么、注释在哪儿。比通过注释解释模糊的命名好得多 文件注释在每一个文件开头加入版权公告，然后是文件内容描述。 1. 版权公告 每一文件包含以下项，依次是：1) 版权（copyright statement） ：如 Copyright 2008 Google Inc.；2) 许可版本（license boilerplate） ：为项目选择合适的许可证版本，如 Apache 2.0、BSD、 LGPL、 GPL；3) 作者（author line） ：标识文件的原始作者 如果你对其他人创建的文件做了重大修改，将你的信息添加到作者信息里，这样当其他人对该文件有疑问时可以知道该联系谁。 2. 文件内容 每一个文件版权许可及作者信息后，都要对文件内容进行注释说明。 1）.h 文件要对所声明的类的功能和用法作简单说明2) .cc 文件包含了更多的实现细节或算法讨论，如果你感觉这些实现细节或算法讨论对于阅读有帮助，可以把 .cc 中的注释放到 .h 中，并在 .cc 中指出文档在 .h 中。 不要单纯在 .h 和 .cc 间复制注释，复制的注释偏离了实际意义。 类注释每个类的定义要附着描述类的功能和用法的注释, 如 123456789// Iterates over the contents of a GargantuanTable. Sample usage:// GargantuanTable_Iterator* iter = table-&gt;NewIterator();// for (iter-&gt;Seek("foo"); !iter-&gt;done(); iter-&gt;Next()) &#123;// process(iter-&gt;key(), iter-&gt;value());// &#125;// delete iter;class GargantuanTableIterator &#123; ...&#125;; 函数注释注释于声明之前，描述函数功能及用法，注释使用描述式（”opens the file”）而非指令式（”open the file”）；注释只是为了描述函数而不是告诉函数做什么。 1234567891011121314// Returns an iterator for this table. It is the client's// responsibility to delete the iterator when it is done with it,// and it must not use the iterator once the GargantuanTable object// on which the iterator was created has been deleted.//// The iterator is initially positioned at the beginning of the table.//// This method is equivalent to:// Iterator* iter = table-&gt;NewIterator();// iter-&gt;Seek("");// return iter;// If you are going to immediately seek to another place in the// returned iterator, it will be faster to use NewIterator()// and avoid the extra seek.Iterator* GetIterator() const; 函数定义处注释的内容: 每个函数定义时要以注释说明函数功能和实现要点，如使用的漂亮代码、实现的简要步骤、如此实现的理由、为什么前半部分要加锁而后半部分不需要。 简要说明函数功能是可以的，重点要放在如何实现上。 变量注释通常变量名本身足以很好说明变量用途，特定情况下，需要额外注释说明 类数据成员每个类数据成员（也叫实例变量或成员变量）应注释说明用途，如果变量可以接受 NULL 或-1等警戒值（ sentinel values） ，须说明之，如： 12345private: // Keeps track of the total number of entries in the table. // Used to ensure we do not go over the limit. -1 means // that we don't yet know how many entries the table has. int num_total_entries_; 全局变量（常量） 和数据成员相似，所有全局变量（常量）也应注释说明含义及用途，如：12// The total number of tests cases that we run through in this regression test.const int kNumTestCases = 6 TODO 注释对那些临时的、短期的解决方案，或已经够好但并不完美的代码使用 TODO 注释。 这样的注释要使用全大写的字符串 TODO，后面括号里加上你的大名、邮件地址等，还可以加上冒号目的是可以根据统一的 TODO 格式进行查找： 12// TODO(kl@gmail.com): Use a "*" here for concatenation operator.// TODO(Zeke) change this to use relations. 如果加上是为了在“将来某一天做某事”，可以加上一个特定的时间（”Fix by November 2005”）或事件（ “Remove this code when all clients can handle XML responses.”）。 格式代码风格和格式确实比较随意，但一个项目中所有人遵循同一风格是非常容易的，作为个人未必同意下述格式规则的每一处，但整个项目服从统一的编程风格是很重要的，这样做才能让所有人在阅读和理解代码时更加容易。 空格还是制表位只使用空格，每次缩进 2 个空格。 使用空格进行缩进，不要在代码中使用 tabs，设定编辑器将 tab 转为空格。 函数声明与定义返回类型和函数名在同一行，合适的话，参数也放在同一行。函数看上去像这样： 1234ReturnType ClassName::FunctionName(Type par_name1, Type par_name2) &#123; DoSomething(); ...&#125; 如果同一行文本较多，容不下所有参数： 123456ReturnType ClassName::ReallyLongFunctionName(Type par_name1, Type par_name2, Type par_name3) &#123; DoSomething(); ...&#125; 甚至连第一个参数都放不下 1234567ReturnType LongClassName::ReallyReallyReallyLongFunctionName( Type par_name1, // 4 space indent Type par_name2, Type par_name3) &#123; DoSomething(); // 2 space indent...&#125; 注意以下几点1) 返回值总是和函数名在同一行；2) 左圆括号（ open parenthesis） 总是和函数名在同一行；3) 函数名和左圆括号间没有空格；4) 圆括号与参数间没有空格；5) 左大括号总在最后一个参数同一行的末尾处；6) 右大括号总是单独位于函数最后一行；7) 右圆括号和左大括号间总是有一个空格；8) 函数声明和实现处的所有形参名称必须保持一致；9) 所有形参应尽可能对齐；10) 缺省缩进为 2 个空格；11) 独立封装的参数保持 4 个空格的缩进。 如果函数为 const 的，关键字 const 应与最后一个参数位于同一行。12345678910// Everything in this function signature fits on a single lineReturnType FunctionName(Type par) const &#123; ...&#125;// This function signature requires multiple lines, but// the const keyword is on the line with the last parameter.ReturnType ReallyLongFunctionName(Type par1, Type par2) const &#123;...&#125; 函数调用 的风格跟定义一样 条件语句有些条件语句写在同一行以增强可读性，只有当语句简单并且没有使用 else 子句时使用： 12if (x == kFoo) return new Foo();if (x == kBar) return new Bar(); 如果语句有 else 分支是不允许的：123// Not allowed - IF statement on one line when there is an ELSE clauseif (x) DoThis();else DoThat(); 通常，单行语句不需要使用大括号，如果你喜欢也无可厚非，也有人要求 if 必须使用大括号：12345if (condition) DoSomething(); // 2 space indent.if (condition) &#123; DoSomething(); // 2 space indent.&#125; 如果语句中哪一分支使用了大括号的话，其他部分也必须使用 1234567891011121314151617181920// Not allowed - curly on IF but not ELSEif (condition) &#123; foo;&#125; else bar; // Not allowed - curly on ELSE but not IFif (condition) foo;else &#123; bar;&#125;// Curly braces around both IF and ELSE required because// one of the clauses used braces.if (condition) &#123; foo;&#125; else &#123; bar;&#125; 循环和开关选择语句switch 语句可以使用大括号分块；空循环体应使用{}或 continue switch 语句中的 case 块可以使用大括号也可以不用，取决于你的喜好，使用时要依下文所述。 如果有不满足 case 枚举条件的值，要总是包含一个 default（如果有输入值没有 case 去处理，编译器将报警）。如果 default 永不会执行，可以简单的使用 assert： 12345678910111213141516171819// switch 语句switch (var) &#123; case 0: &#123; // 2 space indent ... // 4 space indent break; &#125; case 1: &#123; ... break; &#125; default: &#123; assert(false); &#125;&#125;// 空循环体for (int i = 0; i &lt; kSomeNumber; ++i) &#123;&#125; // Good - empty body.while (condition) continue; // Good - continue indicates no logic.while (condition); // Bad - looks like part of do/while loop. 指针和引用表达式句点（.）或箭头（-&gt;）前后不要有空格，指针/地址操作符（*、&amp;）后不要有空格 下面是指针和引用表达式的正确范例： 1234x = *p;p = &amp;x;x = r.y;x = r-&gt;y; 在声明指针变量或参数时，星号与类型或变量名紧挨都可以： 12345678// These are fine, space preceding.char *c;const string &amp;str;// These are fine, space following.char* c; // but remember to do "char* c, *d, *e, ...;"!const string&amp; str;char * c; // Bad - spaces on both sides of *const string &amp; str; // Bad - spaces on both sides of &amp; 布尔表达式如果一个布尔表达式超过标准行宽（80 字符），如果断行要统一一下。下例中，逻辑与（&amp;&amp;）操作符总位于行尾(可也考虑放在行首)： 12345if (this_one_thing &gt; this_other_thing &amp;&amp; a_third_thing == a_fourth_thing &amp;&amp; yet_another &amp; last_one) &#123; ...&#125; 变量及数组初始化选择 = 或 (), 下面的形式都是正确的：1234int x = 3;int x(3);string name("Some Name");string name = "Some Name"; 预处理指令预处理指令不要缩进，从行首开始 即使预处理指令位于缩进代码块中，指令也应从行首开始。 123456789101112131415// Good - directives at beginning of line if (lopsided_score) &#123;#if DISASTER_PENDING // Correct -- Starts at beginning of line DropEverything();#endifBackToNormal(); &#125; // Bad - indented directives if (lopsided_score) &#123; #if DISASTER_PENDING // Wrong! The "#if" should be at beginning of line DropEverything(); #endif // Wrong! Do not indent "#endif" BackToNormal();&#125; 类格式声明属性依次序是 public:、 protected:、 private:, 每次缩进 1 个空格; 除第一个关键词（一般是 public）外，其他关键词前空一行，关键词后不要空行； 每一块中，声明次序一般如下： 1) typedefs 和 enums；2) 常量；3) 构造函数；4) 析构函数；5) 成员函数，含静态成员函数；6) 数据成员，含静态数据成员。 123456789101112131415161718class MyClass : public OtherClass &#123; public: // Note the 1 space indent! MyClass(); // Regular 2 space indent. explicit MyClass(int var); ~MyClass() &#123;&#125; void SomeFunction(); void SomeFunctionThatDoesNothing() &#123; &#125; void set_some_var(int var) &#123; some_var_ = var; &#125; int some_var() const &#123; return some_var_; &#125; private: bool SomeInternalFunction(); int some_var_; int some_other_var_; DISALLOW_COPY_AND_ASSIGN(MyClass); &#125;; 命名空间格式化命名空间不添加额外缩进层次，例如： 123456789101112namespace &#123;void foo() &#123; // Correct. No extra indentation within namespace....&#125; &#125; // namespace namespace &#123; // Wrong. Indented when it should not be. void foo() &#123; ... &#125;&#125; // namespace 水平留白添加冗余的留白会给其他人编辑时造成额外负担，因此，不要加入多余的空格。如果确定一行代码已经修改完毕，将多余的空格去掉；或者在专门清理空格时去掉（确信没有其他人在使用） 一般情况 1234void f(bool b) &#123; // 大括号前需要有空格...int i = 0; // 分号前一般没空格int x[] = &#123;0&#125;; // 初始化数组的分号的空格可选int x[] = &#123; 0 &#125;; // 要使用的话，两边都加上 循环和条件语句 123456if (b) &#123; // Space after the keyword in conditions and loops.&#125; else &#123; // Spaces around else.&#125;while (test) &#123;&#125; // There is usually no space inside parentheses.switch (i) &#123;for (int i = 0; i &lt; 5; ++i) &#123; 操作符 赋值符号和二元操作符一般左右都带上空格，一元操作符则不用, 如下 12345x = 0; // Assignment operators always have spaces around them.v = w*x + y/z; // but it's okay to remove spaces around factors.v = w * (x + z); // Parentheses should have no spaces inside themx = -5; // No spaces separating unary operators and their++x; // arguments. 垂直留白垂直留白越少越好。这不仅仅是规则而是原则问题了：不是非常有必要的话就不要使用空行。尤其是： 1）不要在两个函数定义之间空超过 2 行2）函数体头、尾不要有空行，函数体中也不要随意添加空行。3）基本原则是：同一屏可以显示越多的代码，程序的控制流就越容易理解。当然，过于密集的代码块和过于疏松的代码块同样难看，取决于你的判断，但通常是越少越好。 函数头、尾不要有空行，如： 12345void Function() &#123; // Unnecessary blank lines before and after&#125; 代码块头、尾不要有空行, 如： 12345678while (condition) &#123; // Unnecessary blank line after&#125;if (condition) &#123; // Unnecessary blank line before&#125; 最后，有人根据这些规则总结了一张图，比较直观归纳了上面提到的各种规则风格，图片出自这里]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CTR 预估模型简介--深度学习篇]]></title>
      <url>%2F2018%2F07%2F16%2FCTR%E9%A2%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
      <content type="text"><![CDATA[本文主要介绍 CTR 预估中一些深度学习模型，包括 FNN、Wide&amp;Deep、PNN、DIN、 Deep&amp;Cross等。每个模型会简单介绍其原理、论文出处以及其一些开源实现。 FNN(Factorization-machine supported Neural Network)模型结构FNN 是伦敦大学于 2016 在一篇论文中发表的，模型的结构如下 FNN 假设输入数据的格式是离散的类别特征(表示为 one-hot 编码)，且每个特征属于一个 field，通过 embedding 层将高纬稀疏特征映射成低维稠密特征后，再作为多层感知机(MLP)的输入。 一般来说，embedding 层的参数可以随机初始化，但是在 FNN 中，初始化 embedding 是采用通过 FM 预训练得到的每个特征的隐向量，这样初始化的好处是将预训练的向量作为初始化参数时，能够让模型的参数在初始化的时候就处于较优的位置(训练的目的其实就是为了得到最优的模型参数)，能够加快收敛的过程，至于效果方面，则不一定会优于随机初始化的情况，因为随机初始化经过多轮的迭代也可能会收敛同样的效果。 相关论文提出 FNN 的论文 Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction是张伟楠博士在伦敦大学时发表的，张伟楠博士还有很多与 RTB 相关的论文，具体可参看其主页。 开源实现论文作者在 github 上的 deep-ctr 这个仓库中提供了 FNN 的代码，但是是 Theano 实现的；后来作者又将代码更新为 Tensorflow 框架实现的，详见 product-nets，这个仓库也包含了后面要介绍的 PNN 的实现代码。 Wide&amp;Deep模型结构Wide &amp; Deep 是 Google 在2016年6月中发布的。模型结合了传统的特征工程与深度模型：既有 Wide 的 LR 模型，也有 Deep 的 NN 模型。 其结构如下所示 wide 部分其实就是 LR，deep部分其实就是 FNN，只是 deep 部分中的 embedding 层不用 FM 训练得到的隐向量初始化。根据论文的描述，wide 部分主要负责memorization， deep 部分主要负责 generalization；memorization 主要指的是记住出现过的样本，可以理解为拟合训练数据的能力，generalization 则是泛化能力。 根据论文的实验，wide &amp; deep 比起单纯的 wide 或 deep 都要好，但是根据我后面的实验以及网上的一些文章，wide 部分仍然需要人工设计特征，在特征设计不够好的情况下，wide&amp;deep 整个模型的效果并不如单个的 deep 模型。 Wide&amp;Deep 中还允许输入连续的特征，这点与 FNN 不同，连续特征可以直接作为 Wide 部分或 Deep 部分的输入而无需 embedding 的映射，具体如下图所示。 相关论文Wide&amp;Deep 是 Google 在论文 Wide &amp; Deep Learning for Recommender Systems 中提出的，论文原来是用于 Google Play 的推荐中，但是推荐和CTR实际上是同一类的问题：排序问题，所以也可以迁移到CTR预估的领域。 开源实现由于 Wide&amp;Deep 是 google 提出的，因此在自家的框架 Tensorflow 中提供了 Wide&amp;Deep API，具体的使用方法可参考官方的文档 TensorFlow Wide &amp; Deep Learning Tutorial。 PNN(Product-based Neural Networks)模型结构PNN 是上海交大在2016年发表的，FNN 是在 PNN 的基础上进行了改进，就是增加了特征的二阶交叉项。因此，FNN 和 PNN 的关系，类似于 LR 和 FM 的关系，只是 FNN 和 PNN 均是对原始特征进行了 embedding 映射。PNN 模型的结构如下所示 特征经过 embedding 层映射后，有两种乘积的操作，第一种是跟1做外积，实际上就是将映射后的特征进行拼接, 得到了上图中的 z 向量部分；第二种是与其他特征分别两两进行内积，得到了上图中的 p 向量部分，这个操作其实就相当于进行了特征交叉，只是这种交叉是在 embedding 映射后。再后面的结构其实又是一个多层感知机了。 相关论文PNN 是在上海交大于2016年在这篇论文 Product-based Neural Networks for User Response Prediction 中提出。 开源实现PNN 的作者在 github 上的 product-nets 上开源了其代码，通过 Tensorflow 实现，代码里面也包含了 FNN，DeepFM 等一些其他模型的实现。 DeepFM模型结构DeepFM 是华为诺亚方舟实验室在 2017 提出的用于 CTR 预估的模型，DeepFM 其实就是模仿 Wide&amp;Deep，只是将 Wide 部分替换成了 FM，所以创新性并不算大。其结构如下所示， 相关论文DeepFM 是在这篇论文中提出的 DeepFM: A Factorization-Machine based Neural Network for CTR Prediction 开源实现作者没有公开源码，上面提到的 product-nets 提供了这个模型的实现代码，同时 tensorflow-DeepFM 也提供了一个 tensorflow 实现的版本，star 数是 github 上较高的了。 DIN(Deep Interest Network)模型结构从之前提到的几个模型可知，CTR预估中的深度学习模型的基本思路是将原始的高维稀疏特征映射到一个低维空间中，也即对原始特征做了embedding操作，之后一起通过一个全连接网络学习到特征间的交互信息和最终与CTR之间的非线性关系。这里值得注意的一点是，在对用户历史行为数据进行处理时，每个用户的历史点击个数是不相等的，我们需要把它们编码成一个固定长的向量。以往的做法是，对每次历史点击做相同的embedding操作之后，将它们做一个求和或者求最大值的操作，类似经过了一个pooling层操作。提出 DIN 的论文认为这个操作损失了大量的信息，于是引入了 attention 机制(其实就是一种加权求和)。 DIN 是阿里妈妈在 2017 年提出的，其模型的结构如下所示 Activation Unit 的结构如下所示 DIN模型在对用户的表示计算上引入了attention network (也即图中的Activation Unit) 。DIN把用户特征、用户历史行为特征进行embedding操作，视为对用户兴趣的表示，之后通过attention network，对每个兴趣表示赋予不同的权值。这个权值是由用户的兴趣和待估算的广告进行匹配计算得到的，如此模型结构符合了之前的两个观察——用户兴趣的多样性以及部分对应。attention network 的计算公式如下， $V_u$ 代表用户表示向量， $V_i$ 代表用户兴趣表示向量， $V_a$ 代表广告表示向量，$w_i$ 表示各个用户兴趣表示向量的权重，$g$ 是 Activation Unit 的逻辑，论文中提出了一种如上图的 Activation Unit 所示，当然也可自行设计新的 Activation 方法。 相关论文DIN 是在论文 Deep Interest Network for Click-Through Rate Prediction 中提出的。 开源实现论文作者在 github 上的仓库 DeepInterestNetwork 开源了其代码，通过 Tensorflow 实现。 Deep&amp;Cross模型结构PNN 进行了特征的二阶交叉，目前是为了获得信息量更多的特征，除了二阶，三阶四阶甚至更高阶的特征会更加有区分度；Deep&amp;Cross 就是一个能够进行任意高阶交叉的神经网络。 Deep&amp;Cross 是 StandFord 和 Google 与 2017年 提出的，类似于 Wide&amp;Deep，模型也是由两部分组成，分别是 Deep network 和 Cross network，该模型结构如下所示 $x_i$ 表示可由如下公式确定 从上面两条公式可知，Cross network 中的第 $l+1$ 层的神经元由最原始的输入和第 $l$ 层的神经元共同决定，因此第 $l$ 层相当于对原始特征进行了 $l$ 阶交叉。 相关论文Deep&amp;Cross 是在这篇论文 Deep &amp; Cross Network for Ad Click Predictions 中提出的。 开源实现论文没有公开代码，DeepCTR 中提供了 Deep&amp;Cross 的 tensorflow 实现，可供参考。 总结在CTR预估中，模型适用传统方法还是深度学习方法，其实是一个海量离散特征+简单模型 和 少量连续特征+复杂模型 的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。特征与模型往往是对偶的，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CTR 预估模型简介--非深度学习篇]]></title>
      <url>%2F2018%2F07%2F15%2FCTR%20%E9%A2%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B--%E9%9D%9E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AF%87%2F</url>
      <content type="text"><![CDATA[本文主要介绍 CTR 预估中常用的一些模型，主要是非深度学习模型，包括 LR、GBDT+LR、FM/FFM、MLR。每个模型会简单介绍其原理、论文出处以及其一些开源实现。 LR(Logistic Regerssion)LR + 海量人工特征 是业界流传已久的做法，这个方法由于简单、可解释性强，因此在工业界得到广泛应用，但是这种做法依赖于特征工程的有效性，也就是需要对具体的业务场景有深刻的认识才能提取出好的特征。 原理LR 是一个很简单的线性模型，其输出的值可认为是事件发生($y=1$)的概率，即输出值如下式所示 $$ h(x) = p(y=1|x) = \sigma(w^Tx+b)$$ 其中$w$ 为模型参数，$x$ 为提取的样本特征，两者均为向量，$b$ 是偏置项。$\sigma$ 为 sigmoid 函数，即 $\sigma(x) = 1/(1+e^{-x})$ 有了事件发生的概率，则事件不发生的概率为 $p(y=0|x) = 1-h(x)$,将这两个概率通过如下一条公式表示为 $$p(y|x) = h(x)^y(1-h(x))^{1-y}$$ 有了这个概率值，则给定 $n$ 个样本，便可通过极大似然估计来估算模型参数，即目标函数为 $$\max \prod_{i=1}^np(y_i|x_i)$$ 通常我们还会对概率取 log，同时添加负号将 max 改成min，则可将目标函数改写成如下的形式 $$\min -\sum_{i=1}^ny_i\log h(x_i)+(1-y_i)\log (1-h(x_i))$$ 上面的损失函数也叫作 log loss，实际上多分类的 cross entropy 也同以通过极大似然估计推导出来。 有了损失函数，便可通过优化算法来求出最优的参数，由于这是个无约束的最优化问题，可选用的方法很多，最常用的就是 gradient descent，除此之外，另外还有基于二阶导数的牛顿法系列，适用于分布式中的 ADMM，以及由 Google 在论文 Ad Click Prediction: a View from the Trenches 中提出的 FTRL 算法，目前也是业界普遍采用的方法，该算法具有online learning 和稀疏性的良好特性，online learning 指的是其更新方式与 SGD(stochastic gradient descent) 相似，稀疏性指的是该算法能够解决带非光滑的L1正则项的优化问题。由于这里这篇文章主要讲述各种 CTR 预估模型，因此这里不对优化算法做展开了。 上面提到了 L1 正则项，就是在原来的损失函数基础上加上了 $C\sum_{i=1}^m |w_i|$ 这一项, 表示各个参数的绝对值的和乘上常数 $C$；加上这一项后能够使得最终的求解出来的参数中大部分的 $|w_i|$ 为0，这也是稀疏性的名称来源。稀疏性使得模型的复杂度下降，缓解了过拟合的问题，同时具有有特征筛选的能力。因为 LR 模型可以理解为对各个特征进行加权求和，如果某些特征的权重即 $w_i$ 为0，则可认为这些特征的重要性不高。在CTR预估中输入的是海量人工特征，因此添加 L1 正则化就更有必要了。 由于 L1 正则项不再是处处光滑可导的函数，因此在优化损失函数时。原来的 gradient descent 不能够直接使用，而是要通过 subgradient 的方法或前面提到的 FTRL 算法进行优化。 上面涵盖了 LR 模型的基本原理。而在 CTR 预估中，应用 LR 模型的重点在于特征工程。LR 模型适用于高维稀疏特征。对于 categorical 特征，可以通过 one-hot 编码使其变得高纬且稀疏。而对于 continious 特征，可以先通过区间划分为 categorical 特征再进行 one-hot 编码。同时还需要进行特征的组合/交叉，以获取更有效的特征。 一些问题上面介绍过程中有一些结论我们直接就使用了，下面对于上面提到的某些结论做出一些解释 1. LR 的输出为什么可以被当做是概率值？ 这部分涉及到广义线性模型(GLM，Generalized linear model) 的知识，这里略过复杂的推导，直接给出结论。简单来说，LR 实际上是一个广义线性模型，其假设是二分类中 $(y|x,\theta)$ 服从伯努利分布(二项分布)，即给定输入样本 $x$ 和模型参数 $\theta$, 事件是否发生服从伯努利分布。假设伯努利分布的参数 $\phi$ ，则 $\phi$ 可作为点击率。通过 广义线性模型的推导，能够推出 $\phi$ 的表示形式如下 $$\phi = 1/(1+e^{-\eta})$$ 从上面的式子可知，LR 中的 sigmoid 函数并不是凭空来的，而式子中的 $\eta$ 也被称为连接函数（Link function), 是确定一个 GLM 的重要部分，在 LR 中为简单的线性加权。 另外，如果将输出值与真实值的误差的分布假设为高斯分布，那么从 GLM 可推导出 Linear Regression，关于 GLM 详细的推导可参考这篇文章 广义线性模型（GLM）。 2. 为什么 L1 正则项能够带来稀疏性？ 这里有个很直观的回答，l1 相比于 l2 为什么容易获得稀疏解？，简单来说，就是当不带正则项的损失函数对于某个参数 $w_i$ 的导数的绝对值小于 l1 正则项中的常数 $C$ 时，这个参数 $w_i$ 的最优解就是0。 因为求解某个参数 $w_i$ 使得损失函数取极小值时可分两种情况讨论(下面的 $L$ 为不带正则项的损失函数)1）$w_i&lt;0$ 时, $L+C|w_i|$ 的导数为 $L’- C$2) $w_i&gt;0$时, $L+C|w_i|$ 的导数为 $L’+C$ 当 $w_i 0$, 函数在递增，则 $w_i=0$ 便是使得损失函数最小的最优解，且结合 $L’- C &lt; 0$ 和 $L’+C &gt; 0$，可得 $C &gt; |L’|$。这便是我们上面得到的结论，上面是针对某一个参数，实际上也可以推广到所有参数上。事实上，通过 subgradient descent 求解这个问题时也能够得到相同的结论。 3.连续特征为什么需要离散化？ 参考这个问题：连续特征的离散化：在什么情况下将连续的特征离散化之后可以获得更好的效果？ 离散化后有以下几个好处： 1) 稀疏向量内积乘法运算速度快，计算结果方便存储2) 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；3) 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，可以通过 one-hot 编码为每个变量设置单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；4) 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；5) 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间要取决于具体的场景 4.1 为什么要对 categorical 特征做 One-hot 编码后再输入 LR？ 参考这篇文章 One-Hot编码与哑变量，简单来说，就是LR建模时，要求特征具有线性关系，而实际应用中很少有满足这个假设关系的，因此LR模型效果很难达到应用要求。但是通过对离散特征进行 one-hot 编码，LR 可以为某个特征中所有可能的值设置一个权重，这样就能够更准确的建模，也就能够获得更精准的模型。而 one-hot 编码后特征实际上也是做了一个 min-max 归一化，能够克服不同特征的量纲差异，同时使模型收敛更快。 开源实现由于 LR 模型的广泛性，基本上每个机器学习库或者框架都有相关实现，如 sklearn 提供了单机版的实现，spark 提供了分布式版本的实现，腾讯开源的 Parameter Server Angel 中也提供了 LR+FTRL 的实现，Angel 支持 Spark，目前也还在开发中 。除此之外，Github 上也有很多个人开源的实现，这里不再列举。 LS-PLM(Large Scale Piece-wise Linear Model)LS-PLM(也叫作 MLR, Mixture of Logistics Regression)是阿里妈妈在 2017 年在论文 Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction 中公开的，但是模型早在 2012 年就在阿里妈妈内部使用。这个模型在 LR 基础上进行了拓展，目的是为了解决单个 LR 无法进行非线性分割的问题。 原理LR 是一个线性模型，模型会在数据空间中生成一个线性分割平面，但是对于非线性可分的数据，这一个线性分割面显然无法正确分割这些数据。以下图为例（摘自上面的论文），A）为一组非线性训练数据的正负样本分布；对于该问题，LR会生成 B）中的分割平面，C) 图展示的 LS-PLM 模型 则取得了较好的效果。 在CTR问题中，划分场景分别建模是一种常见的手法。例如，同一产品的PC/APP端，其用户的使用时间和习惯差异可能很大；比如PC可能更多是办公时间在看，而手机则是通勤时间或者临睡前使用更多。假设有hour作为特征，那么“hour=23”对于APP端更加有信息量，而对于PC可能意义不大。因此，区分PC/APP端分别建模可能提升效果。 LS-PLM 也是采用这个思想的，不够这里不是划分场景，而是划分数据，通过将数据划分不同的region、然后每个region分别建立 LR。 这里需要注意的是这里一个样本并不是被唯一分到了一个region，而是按权重分到了不同的region。其思想有点像 LDA(Latent Dirichlet allocation) 中一个单词会按照概率分到多个 topic 上。 论文中的公式如下 $$p(y=1|x) = g ( \sum_{j=1}^m \sigma(\mu_j^T x)\eta(w_j^Tx))$$ 公式中的符号定义如下： 参数定义如下： $m$ : region 的个数(超参数：一般是10~100) $\Theta={\mu_1,\dots,\mu_m, w_1,\dots,w_m }$: 表示模型的参数，需要训练 $g(\cdot)$：为了让模型符合概率定义(概率和为1)的函数 $\sigma(\cdot)$：将样本分到 region 的函数 $\eta(\cdot)$：在 region 中划分样本的函数 前面提出的公式更像个框架，在论文中，只讨论了 $g(x) = x$, $\sigma$ = softmax ，$\eta$ = sigmoid 的情形，而且因此，上面的公式可写成如下的形式 $$p(y=1|x) = \sum_{i=1}^m \frac{e^{\mu_i^Tx}}{\sum_{j=1}^m e^{\mu_j^Tx}}\frac{1}{1+e^{-w_i^Tx}}$$ 这个公式其实已经变成了通过多个 LR 模型进行加权求和的 bagging 模式，只是这里每个模型的权重是学习出来而不是事先确定的。 写出了概率函数, 后面的推导跟前面的 LR 其实是一样的，也是先通过极大似然估计得到 $\max$ 问题，添加负号后转为损失函数求 $\min$ 问题。这里不做详细的推导了。 在 LS-PLM 中也是需要添加正则项的，除了在 LR 中提到的 L1 正则化，论文还提出了 $L_{2,1}$ 正则项，表示如下 $$||\Theta||_{2,1} = \sum_{i=1}^d \sqrt {\sum_{j=1}^m(\mu_{ij}^2+w_{ij}^2)}$$ 上式中的 $d$ 表示特征的维数，其中 $\sqrt {\sum_{j=1}^m(\mu_{ij}^2+w_{ij}^2)}$ 表示对某一维特征的所有参数进行 L2 正则化，而外侧的 $\sum_{i=1}^d$ 表示对所有的 feature 进行 L1 正则化，由于开方后的值必为正，因此这里也不用添加绝对值了。由于结合了 L1 和 L2 正则项，所以论文也将这个叫做$L_{2,1}$ 正则项。 由于损失函数和正则项都是光滑可导的，因此优化方面比带 L1 正则的 LR 更加简单，可选的优化方法也更多。 MLR 适用的场景跟 LR 一样，也是适用于高纬稀疏特征作为输入。 开源实现前面提到的腾讯的 PS Angel 实现了这个算法，具体可参考这里；Angel 是用 Scala 开发的。也有一些个人开源的版本如 alphaPLM，这个版本是用 C++ 写的，如果需要实现可以参考以上资料。 GBDT+LR(Gradient Boost Decision Tree + Logistic Regression)GBDT + LR 是 FaceBook 在这篇论文 Practical Lessons from Predicting Clicks on Ads at Facebook 中提出的，其思想是借助 GBDT 帮我们做部分特征工程，然后将 GBDT 的 输出作为 LR 的输入。 原理我们前面提到的无论 LR 还是 MLR，都避免不了要做大量的特征工程。比如说构思可能的特征，将连续特征离散化，并对离散化的特征进行 One-Hot 编码，最后对特征进行二阶或者三阶的特征组合/交叉，这样做的目的是为了得到非线性的特征。但是特征工程存在几个难题： 连续变量切分点如何选取？ 离散化为多少份合理？ 选择哪些特征交叉？ 多少阶交叉，二阶，三阶或更多？ 而 GBDT + LR 这个模型中，GBDT 担任了特征工程的工作，下面首先介绍一下 GBDT。 GBDT 最早在这篇论文 Greedy Function Approximation：A Gradient Boosting Machine 中提出；GBDT 中主要有两个概念：GB(Gradient Boosting)和DT(Decision Tree)，Gradient Boosting 是集成学习中 boosting 的一种形式，Decision Tree 则是机器学习中的一类模型，这里不对这两者展开，只讲述在 GBDT 中用到的内容。关于决策树的介绍可参考这篇文章 决策树模型 ID3/C4.5/CART算法比较。 在 GBDT 中采用的决策树是CART (Classification And Regression Tree)，将其当做回归树使用，这里的回归树是一棵在每个树节点进行分裂的时候，给节点设定其在某个特征的的值，若样本对应的特征的值大于这个给定的值的属于一个子树，小于这个给定的值的属于另一个子树。 那么，构建 CART 回归树是 的关键问题就在于选择具体的特征还有这个特征上具体的值了。选择的指标是平方误差最小化准则。对于任意一个切分，其平方误差计算方式如下 假设切分后左子树有 $m$ 个样本，右子树有 $n$ 个 计算左子树样本的目标值的均值为 $y_m = \frac{1}{m}\sum_{i=1}^{m}y_i$, 同样计算右子树样本的目标值的均值为 $y_n = \frac{1}{n}\sum_{j=1}^{n}y_j$ 平方误差和为 $L = \sum_{i=1}^m(y_i - y_m)^2 + \sum_{j=1}^n(y_j - y_n)^2$ 对于每一个可能的切分值，我们都可计算其平方误差和 $L$，选择使得 $L$ 最小的切分点即可。 上面便是 GBDT 中的 “DT” 部分，用于解决一个回归问题，也就是给定一组样本，我们可以通过上面的方式来构建出一棵 CART 来拟合这组样本。下面我们来讲一下 GBDT 中的 “GB” 部分。 简单来说，gradient boosting 就是将若干个模型的输出进行叠加作为最终模型的输出。如下图是一个简单的例子(图片来源于提出 xgb 的论文：XGBoost: A Scalable Tree Boosting System) 下式就是叠加了 $T$ 个 $f_t(x)$ 模型作为最终的模型，$f_t(x)$ 在 GBDT 中就是一棵 CART，当然 $f_t(x)$ 不限于树模型。 $$F(x) = \sum_{t=1}^Tf_t(x)$$ 在构建每棵树的时候，输入的样本不同的地方在于每个样本的目标值 $y$；如构建第 $k$ 棵树，对于原始样本 $(x_i, y_i)$, 其目标值变为 $$y_{ik} = y_i - \sum_{t=1}^{k-1}f_t(x_i)$$ 即输入第 $k$ 棵树的样本变为 $(x_i, y_{ik})$，所以在构建第 $k$ 棵树的时候，实际上是在拟合前 $k-1$ 棵树的输出值的和与样本真实值的残差。 回到我们的 GBDT + LR 模型，首先通过前面提到的 GBDT 训练出一批树模型，然后样本输入每棵树后最终都会落到一个具体的叶子节点上，那我们就将这个节点标为 1，其他叶子节点标为 0，这样每棵树输出的就相当于是一个 one-hot 编码的特征。如下图是摘自 FaceBook 原始论文的图，里面有两棵树，假如输入 $x$ 在第一棵树中落入第一个叶子节点，在第二棵树种落入第二个叶子节点，那么输入 LR 的特征为 [1, 0, 0, 0, 1]. GBDT+LR 方案中每棵决策树从根节点到叶节点的路径，会经过不同的特征，此路径就是特征组合，而且包含了二阶，三阶甚至更多，因此输出的 one-hot 特征是原始特征进行交叉后的结果。而且每一维的特征其实还是可以追溯出其含义的，因为从根节点到叶子节点的路径是唯一的，因此落入到某个叶子节点表示这个特征满足了这个路径中所有节点判断条件。 GBDT 适用的问题刚好与 LR 相反，GBDT 不适用于高纬稀疏特征，因为这样很容易导致训练出来的树的数量和深度都比较大从而导致过拟合。因此一般输入GBDT 的特征都是连续特征。 在 CTR 预估中，会存在大量的 id 特征，对于这种离散特征，一般有两种做法1) 离散特征不直接输入到 GBDT 中进行编码，而是做 one-hot 编码后直接输入到 LR 中即可；对于连续特征，先通过 GBDT 进行离散化和特征组合输出 one-hot 编码的特征，最后结合这两种 one-hot 特征直接输入到 LR。大致框架如下所示 2) 将离散的特征也输入 GBDT 进行编码，但是只保留那些出现频率高的离散特征，这样输入 GBDT 中的 one-hot 特征的维度会遍地，同时通过 GBDT 也对原始的 one-hot 特征进行了组合和交叉。 一些问题1. GBDT 中的 gradient 在哪里体现了？ 推导到现在，好像也没有提及到 gradient，其实前面拟合残差时已经用到了 gradient 的信息。 首先，我们要转换一下思维，我们一般在优化中使用的 gradient descent 都是对某个参数进行的，或者说是在参数空间中进行的，但是除了参数空间，还可以在函数空间中进行。如下图所示对比了两种方式(下面两张图均摘自GBDT算法原理与系统设计简介) 在函数空间中，是对函数直接进行求导的，因此 GBDT 算法的流程如下 上图中的 $\tilde{y_i}$ 就是我们前面说的第 $i$ 个样本的残差，当损失函数为平方损失即 $$L(y,F(x)) = \frac{1}{2}(y-F(x))^2$$ 对 $F(x)$ 求导得出的残差为 $$\tilde{y_i} = y_i - F(x_i)$$ 这正是我们前面说的样本的真实值与前面建的树的输出和的差。如果损失函数改变，这个残差值也会进行相应的改变。 2. GBDT 怎么处理分类问题？ 上面我们讲的 GBDT 是处理回归问题的，但是对于 CTR 预估这一类问题，从大分类上其实还是一个分类问题。那 GBDT 是怎么处理这个问题？ 在回归问题中，GBDT每一轮迭代都构建了一棵树，实质是构建了一个函数 $f$，当输入为x时，树的输出为 $f(x)$。 在多分类问题中，假设有 $k$ 个类别，那么每一轮迭代实质是构建了 $k$ 棵树，对某个样本 $x$ 的预测值为 $f_{1}(x), f_{2}(x), …, f_{k}(x)$, 在这里我们仿照多分类的逻辑回归，使用 softmax来产生概率，则属于某个类别 $j$ 的概率为 $$p_{c} = \frac{\exp(f_{j}(x))}{ \sum_{i=1}^{k}{exp(f_{k}(x))}}$$ 通过上面的概率值，可以分别计算出样本在各个分类下的 log loss，根据上面 GBDT 在函数空间的求导，对 $f_1$ 到 $f_k$ 都可以算出一个梯度，也就是当前轮的残差，供下一轮迭代学习。也就是每一轮的迭代会同时产生 k 棵树。 最终做预测时，输入的 $x$ 会得到 $k$ 个输出值，然后通过softmax获得其属于各类别的概率即可。 更详细的推导可参考这篇文章：当我们在谈论GBDT：Gradient Boosting 用于分类与回归 开源实现直接实现 GBDT + LR 的开源方案不多，但是由于两者的耦合关系并不强，因此可以先训练 GBDT，然后将原始特征通过 GBDT 转换后送入到 LR 中，GBDT 有多个高效的实现，如 xgboost，LightGBM。 FM(Factorization Machine)FM（Factorization Machine）是于2010年在论文 Factorization Machines 中提出，旨在解决稀疏数据下的特征组合问题。其思想是对组合特征的参数所构成的参数矩阵进行矩阵分解，从而得到每个原始特征的隐向量表示，更新特征的隐向量对数据的稀疏性具有鲁棒性。关于 FM 和 FFM ，美团点评这篇文章：深入FFM原理与实践 其实已经写得很详细了，本文主要参考该文章进行修改。 原理FM 可以认为是在 LR 的基础上加入特征的二阶组合，即最多有两个特征相乘，则模型可表示成如下形式 $$y(\mathbf{x}) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n w_{ij} x_i x_j $$ 从模型也可以看出，其实 FM 是在 LR 基础上增加了最后的二阶交叉项。 从上面的公式可以看出，组合特征的参数一共有 $\frac{n(n−1)}{2}$ 个，任意两个参数都是独立的。然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的,原因是每个参数 $w_{ij}$ 的训练需要大量 $x_i$ 和 $x_j$ 都非零的样本；由于样本数据本来就比较稀疏，满足 $x_i$ 和 $x_j$ 都非零的样本将会非常少。训练样本的不足，则会导致参数 $w_{ij}$ 不准确，最终将严重影响模型的性能。 如何解决这个问题？FM 中借鉴了矩阵分解的思想，在推荐系统中，会对 user-item 矩阵进行矩阵分解，从而每个 user 和每个 item 都会得到一个隐向量。如下图所示 类似地，所有二次项参数 $w_{ij}$ 可以组成一个对称阵 $W$，那么这个矩阵就可以分解为 $W=V^TV$，$V$ 的第 $j$ 列便是第 $j$ 维特征的隐向量。换句话说，每个参数可表示成两个隐向量的内积的形式。即 $w_{ij}=$，$v_i$ 表示第 $i$ 维特征的隐向量，这就是FM模型的核心思想。因此，可以将上面的方程改写成如下形式 $$y(\mathbf{x}) = w_0+ \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n x_i x_j $$ 假设隐向量的长度为 $k(k&lt;&lt;n)$，二次项的参数数量减少为 $kn$个，远少于多项式模型的参数数量。另外，参数因子化使得 $x_hx_i$ 的参数和 $x_ix_j$ 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数。 具体来说，$x_hx_i$ 和 $x_ix_j$ 的系数分别为 $$ 和 $$，它们之间有共同项 $v_i$。也就是说，所有包含 $x_i$ 的非零组合特征（即存在某个$j≠i$，使得 $x_ix_j≠0$）的样本都可以用来学习隐向量 $v_i$，这很大程度上避免了数据稀疏性造成的影响。而在多项式模型中，$w_{hi}$ 和 $w_{ij}$ 是相互独立的。 另外，原始论文还对特征交叉项计算的时间复杂度做了优化，具体见如下公式 $$\sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j = \frac{1}{2} \sum_{f=1}^k \left(\left( \sum_{i=1}^n v_{i, f} x_i \right)^2 - \sum_{i=1}^n v_{i, f}^2 x_i^2 \right)$$ 从公式可知，原来的计算复杂度为 $O(kn^2)$，而改进后的时间复杂度为 $O(kn)$ 在 CTR 预估中，对 FM 的输出进行 sigmoid 变换后与 Logistics Regression 是一致的，因此损失函数的求解方法以及优化算法都基本一致，这里不再详细展开。 由于 FM 可以看做是 LR 基础上加上二阶特征组合的模型，同时模型本身对稀疏性有较好的鲁棒性，因此 FM 适用范围跟LR一样，都适用于输入的特征是高纬度稀疏特征。 开源实现FM 在 github 上有单机版本的开源实现 fastFM和pyFM， fastFM 是一个学术项目，发表了相关论文 fastFM: A Library for Factorization Machines, 对 FM 进行了拓展；同时我们前面提到的腾讯的PS Angel 中也实现了这个算法，可参考这里。 FFM(Field-aware Factorization Machine)FFM 发表于论文 Field-aware Factorization Machines for CTR Prediction， 是台大的学生在参加 2012 KDD Cup 时提出的，这个论文借鉴了论文 Ensemble of Collaborative Filtering and Feature Engineered Models for Click Through Rate Prediction 中的 field 的 概念，从而提出了 FM 的升级版模型 FFM。 原理通过引入field的概念，FFM把相同性质的特征归于同一个field。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。 在FFM中，每一维特征 $x_i$，针对其它特征的每一种 field $f_j$，都会学习一个隐向量 $v_{i,f_j}$。因此，隐向量不仅与特征相关，也与field相关。也就是说，假设有 $f$ 个 field，那么每个特征就有 $f$ 个隐向量，与不同的 field 的特征组合时使用不同的隐向量，而原来的 FM 中每个特征只有一个隐向量。 实际上，FM 可以看作 FFM 的特例，是把所有特征都归属到一个 field 时的FFM模型。根据FFM的field敏感特性，同样可以导出其模型方程如下 $$y(\mathbf{x}) = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i} \rangle x_i x_j $$ 其中，$f_j$ 是第 $j$ 个特征所属的 field。如果隐向量的长度为 $k$，那么FFM的二次参数有 $nfk$ 个，远多于FM模型的 $nk$ 个。此外，由于隐向量与 field 相关，FFM二次项并不能够化简，其预测复杂度是 $O(kn^2)$。 其实，FFM 是在 FM 的基础上进行了更细致的分类，增加了参数的个数使得模型更复杂，能够拟合更复杂的数据分布。但是损失函数的推导以及优化的算法跟前面的 FM 还有 LR 都是一样的，因此这里不再赘述。 FFM 适用的场景跟 FM 和 LR 一样，适用于输入的特征是高维稀疏特征。 开源实现FFM 最早的开源实现是台大提供的 libffm，去年开源的 xlearn 中也提供了该算法的实现，提供的 api 比 libffm 更加友好。 另外，由于 FM/FFM 可以看做是 LR 加了特征交叉的增强版本，对输入的特征的特点要求一致，因此上面的 GBDT+LR 也可以直接套到 GBDT+FM/FFM 上，值得一提的是，还是台大的学生，在 2014 由 Criteo 举办的比赛上，通过 GBDT+FFM 的方案夺冠，其实现细节可参考 kaggle-2014-criteo。 小结在非深度学习中，可以看到主流的几个模型基本都是基于 LR 进行的拓展或将 LR 与其他模型结合。原因是 LR 模型简单，具有良好的理论基础，可解释性强，能够获取各个特征的重要性，且能够直接输出概率值。但是应用 LR 过程中无法避免且最为重要的一点就是人工特征工程，特征决定了上限，虽然 FM/FMM 和 GBDT+LR 在一定程度上起到了自动特征工程的作用，但是需要人工特征还是占主要部分。 后面要讲的深度学习的方法在一定程度上能够缓解这个问题，因为深度学习能够通过模型自动学习出有效特征，因此，深度学习也被归类为表示学习( Representation Learning)的一种;但是，没有免费午餐的，特征工程的便利性带来的是特征的不可解释性，所以怎么选取还是要根据具体的需求和业务场景。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ROC 曲线与 PR 曲线]]></title>
      <url>%2F2018%2F06%2F16%2FROC%20%E6%9B%B2%E7%BA%BF%E4%B8%8E%20PR%20%E6%9B%B2%E7%BA%BF%2F</url>
      <content type="text"><![CDATA[ROC 曲线和 PR 曲线是评估机器学习算法性能的两条重要曲线，两者概念比较容易混淆，但是两者的使用场景是不同的。本文主要讲述两种曲线的含义以及应用的场景。 定义ROC 曲线和 PR 曲线都是用在二分类中，且涉及到下图的几个概念(摘自 The Relationship Between Precision-Recall and ROC Curves) 上面四个指标用大白话解释如下 Recall：查全率，正样本中被预测出来是正的比例(越大越好)Precision：查准率，预测的正样本中被正确预测的比例(越大越好)True Positive Rate：跟 Recall 定义一样 （越大越好)FPR : 负样本中被预测为正的比例(越小越好) 对于一个二分类问题，往往要设定一个 threshold，当预测值大于这个 threshold 时预测为正样本，小于这个 threshold 时预测为负样本。如果以 Recall 为横轴，Precision 为纵轴，那么设定一个 threshold 时，便可在坐标轴上画出一个点，设定多个 threshold 则可以画出一条曲线，这条曲线便是 PR 曲线。 PR 曲线是以 Recall 为横轴，Precision 为纵轴；而 ROC曲线则是以 FPR 为横轴，TPR 为纵轴。 那么两者的关系是怎样的？ 对比The Relationship Between Precision-Recall and ROC Curves 中证明了以下两条定理 定理1：对于一个给定的的数据集，ROC空间和PR空间存在一一对应的关系，因为二者包含完全一致的混淆矩阵。我们可以将ROC曲线转化为PR曲线，反之亦然。 定理2：对于一个给定数目的正负样本数据集，曲线 A 在 ROC 空间优于曲线 B ，当且仅当在 PR 空间中曲线 A 也优于曲线 B。 定理 2 中 “曲线A优于曲线B” 是指曲线 B 的所有部分与曲线 A 重合或在曲线 A 之下。而在ROC空间，ROC曲线越凸向左上方向效果越好。与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。 从定理 2 来看，ROC 空间和 PR 空间两个指标似乎具有冗余性，那么为什么还需要这同时两个指标呢？答案是在两者在样本不均衡的情况下表现有较大差异。 下图是ROC曲线和Precision-Recall曲线的对比，摘自 An introduction to ROC analysis 图 (a) 和 (b) 是在样本正负比例为 1:1 下的 ROC 曲线和PR 曲线，图(c) 和 (d) 是在样本正负比例为 1:100 下的 ROC 曲线和PR 曲线。 从结果来看：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。 文章 ROC和AUC介绍以及如何计算AUC 以及An introduction to ROC analysis 中都认为这是个优点，原因是在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而 ROC 这种对不平衡样本的鲁棒性使得其曲线下的面积 AUC 不会发生突变。 那么，AUC 意味这什么？首先 AUC 值是一个概率值，表示随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率。 因此，AUC值实际上反映了模型的 rank 能力，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面。这个指标尤其适用在某些场景下(如 CTR 预估)，每次要返回的是最有可能点击的若干个广告(根据CTR排序, 选择排在前面的若干个)，实际上便是在考验模型的排序能力。除此之外，CTR 中存在着样本不均衡的问题，正负样本比例通常会大于 1:100，如果采用 PR 曲线，则会导致 AUC 发生剧变，无法较好反映模型效果。 然而，ROC 曲线不会随着类别分布的改变而改变的优点在一定程度上也是其缺点。因为 ROC 曲线这种不变性其实影响着的是 AUC 值，或者说是评估分类器的整体性能。但是在某些场景下，我们会更关注正样本，这时候就要用到 PR 曲线了。 比如说信用卡欺诈检测，我们会更关注 precision 和 recall，比如说如果要求预测出为欺诈的人尽可能准确，那么就是要提高 precision；而如果要尽可能多地预测出潜在的欺诈人群，那么就是要提高 recall。一般来说，提高二分类的 threshold 就能提高 precision，降低 threshold 就能提高 recall，这时便可观察 PR 曲线，得到最优的 threshold。 除此之外，Quora 上的问题What is the difference between a ROC curve and a precision-recall curve? When should I use each? 中也举了一下的例子说明了在欺诈检测的问题中，PR 曲线更能反映结果的变化。 Let’s take an example of fraud detection problem where there are 100 frauds out of 2 million samples. Algorithm 1: 90 relevant out of 100 identifiedAlgorithm 2: 90 relevant out of 1000 identified Evidently, algorithm 1 is more preferable because it identified less number of false positive. In the context of ROC curve,Algorithm 1: TPR=90/100=0.9, FPR= 10/1,999,900=0.00000500025Algorithm 2: TPR=90/100=0.9, FPR=910/1,999,900=0.00045502275The FPR difference is 0.0004500225 For PR CurveAlgorithm 1: precision=0.9, recall=0.9Algorithm 2: Precision=90/1000=0.09, recall= 0.9Precision difference= 0.81 The difference is more apparent in PR curve 总结综上，有以下几条结论（参考 机器学习之类别不平衡问题 (2) —— ROC和PR曲线） ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能(通常是会计算AUC，表示模型的rank性能)，相比而言PR曲线完全聚焦于正例。 如果有多份数据且存在不同的类别分布。比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。 如果想要评估在相同的类别分布下正例的预测情况，则宜选PR曲线。类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。(参考上面 Quora 的例子) 最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的precision，recall，f1 score等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[format 函数常用语法]]></title>
      <url>%2F2018%2F06%2F03%2Fformat%20%E5%87%BD%E6%95%B0%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%2F</url>
      <content type="text"><![CDATA[python 的 format 函数能够对输出做格式化从而使得符合输出的要求，这里记录其一些常见用法，主要参考了博客 飘逸的python - 增强的格式化字符串format函数 位置映射下面是通过参数的位置(从0开始)将参数映射到字符串具体位置 123456In [1]: '&#123;0&#125;,&#123;1&#125;'.format('kzc',18) Out[1]: 'kzc,18' In [2]: '&#123;&#125;,&#123;&#125;'.format('kzc',18) Out[2]: 'kzc,18' In [3]: '&#123;1&#125;,&#123;0&#125;,&#123;1&#125;'.format('kzc',18) Out[3]: '18,kzc,18' 字符串的 format 函数可以接受不限个参数，位置可以不按顺序，可以不用或者用多次 名称映射12In [5]: '&#123;name&#125;,&#123;age&#125;'.format(age=18,name='kzc') Out[5]: 'kzc,18' 对象映射1234567class Person: def __init__(self,name,age): self.name,self.age = name,age def __str__(self): return 'This guy is &#123;self.name&#125;,is &#123;self.age&#125; old'.format(self=self) In [2]: str(Person('kzc',18)) Out[2]: 'This guy is kzc,is 18 old' list/tuple 的下标映射123In [7]: p=['kzc',18]In [8]: '&#123;0[0]&#125;,&#123;0[1]&#125;'.format(p)Out[8]: 'kzc,18' 填充与对齐填充常跟对齐一起使用^、&lt;、&gt;分别是居中、左对齐、右对齐，后面带宽度:号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充 123456In [15]: '&#123;:&gt;8&#125;'.format('189')Out[15]: ' 189'In [16]: '&#123;:0&gt;8&#125;'.format('189')Out[16]: '00000189'In [17]: '&#123;:a&gt;8&#125;'.format('189')Out[17]: 'aaaaa189' 精度与类型f精度常跟类型f一起使用12In [44]: '&#123;:.2f&#125;'.format(321.33345)Out[44]: '321.33' 其中.2表示长度为2的精度，f表示float类型。 进制转换主要就是进制了，b、d、o、x 分别是二进制、十进制、八进制、十六进制, 用 , 号还能用来做金额的千位分隔符。12345678910In [54]: &apos;&#123;:b&#125;&apos;.format(17)Out[54]: &apos;10001&apos;In [55]: &apos;&#123;:d&#125;&apos;.format(17)Out[55]: &apos;17&apos;In [56]: &apos;&#123;:o&#125;&apos;.format(17)Out[56]: &apos;21&apos;In [57]: &apos;&#123;:x&#125;&apos;.format(17)Out[57]: &apos;11&apos;In [58]: &apos;&#123;:,&#125;&apos;.format(1234567890)Out[58]: &apos;1,234,567,890&apos;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[强化学习笔记(3)- 从 Policy Gradient 到 A3C]]></title>
      <url>%2F2018%2F05%2F11%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(3)-%20%E4%BB%8E%20Policy%20Gradient%20%E5%88%B0%20A3C%2F</url>
      <content type="text"><![CDATA[在之前的文章 强化学习笔记(2)-从 Q-Learning 到 DQN 中，我们已经知道 Q-Learning 系列方法是基于 value 的方法， 也就是通过计算每一个状态动作的价值，然后选择价值最大的动作执行。这是一种间接的做法，那有没有更直接的做法呢？有！那就是直接更新策略。本文要介绍的 Policy Gradient 就是这类 policy-based 的方法， 除此之外，还会介绍结合了 policy-based 和 value-based 的 Actor-Critic 方法，以及在 Actor-Critic 基础上的 DDPG、A3C方法。 Policy Gradient基本思想Policy Gradient 就是通过更新 Policy Network 来直接更新策略的。那什么是 Policy Network？实际上就是一个神经网络，输入是状态，输出直接就是动作（不是Q值），且一般输出有两种方式：一种是概率的方式，即输出某一个动作的概率；另一种是确定性的方式，即输出具体的某一个动作。 如果要更新 Policy Network 策略网络，或者说要使用梯度下降的方法来更新网络，需要有一个目标函数，对于所有强化学习的任务来说，其实目标都是使所有带衰减 reward 的累加期望最大。即如下式所示 $L(\theta) = \mathbb E(r_1+\gamma r_2 + \gamma^2 r_3 + …|\pi(,\theta))$ 这个损失函数和 Policy Network 策略网络简直没有什么直接联系，reward是环境给出的，跟参数 $\theta$ 没有直接运算上的关系。那么该如何能够计算出损失函数关于参数的梯度 $\nabla_{\theta} L(\theta)$? 上面的问题没法给出更新策略，我们不妨换一个思路来考虑问题。 假如我们现在有一个 Policy Network 策略网络，输入状态，输出动作的概率。然后执行完动作之后，我们可以得到reward，或者result。那么这个时候，我们有个非常简单的想法：如果某一个动作得到reward多，那么我们就使其出现的概率增大，如果某一个动作得到的reward少，那么我们就使其出现的概率减小。 当然，用 reward 来评判动作的好坏是不准确的，甚至用 result 来评判也是不准确的（因为任何一个 reward，result 都依赖于大量的动作才导致的，不能只将功劳或过错归于当前的动作上。但是这样给了我们一个新的思路：如果能够构造一个好的动作评判指标，来判断一个动作的好与坏，那么我们就可以通过改变动作的出现概率来优化策略！ 假设这个评价指标是 $f(s,a)$, 我们的 Policy Network 输出的 $\pi(a|s,\theta)$ 是概率, 那么可以通过极大似然估计的方法来优化这个目标。比如说我们可以构造如下目标函数 $L(\theta) = \sum log\pi(a|s,\theta)f(s,a)$ 比如说，对于某局游戏，假如最终赢了，那么认为这局游戏中每一步都是好的，如果输了，那么认为都是不好的。好的 $f(s,a)$ 就是1，不好的就是-1，然后极大化上面的目标函数即可。 实际上，除了极大化上面的目标函数，还可以直接对 $f(s,a)$ 进行极大化，如这篇博文 Deep Reinforcement Learning: Pong from Pixels 中直接最大化 $f(x)$ 也就是 $f(s, a)$ 的期望，可以看到，最后的结果跟上面的目标函数是一致的。 评判指标的选择从上面的推导也可知道，在 Policy Gradient 中，如何确定评价指标 $f(s,a)$ 是关键。 上面提到了一种简单地根据回合的输赢来判断这个回合中的每一步到底是好是坏。但是其实我们更加希望的是每走一步就能够获取到这一步的具体评价，因此出现了很多其他的直接给出某个时刻的评估的评价方式。如这篇论文 High-dimensional continuous control using generalized advantage estimation 里就对比了若干种评价指标。 上面公式（1）中的 $\Psi_t$ 就是 t 时刻的评价指标。从上图可以看到我们可以使用reward，使用 Q、A 或者 TD 来作为动作的评价指标。那这些方法的区别在哪里？ 根据这篇文章 DRL之Policy Gradient, Deterministic Policy Gradient与Actor-Critic, 本质的区别在于 variance 和 bias 的问题 用reward来作为动作的评价是最直接的，采用上图第3种做法reward-baseline是很常见的一种做法。这样做bias比较低，但是variance很大，也就是reward值太不稳定，会导致训练不会。 那么采用Q值会怎样呢？Q值是对reward的期望值，使用Q值variance比较小，bias比较大。一般我们会选择使用A，Advantage。A=Q-V，是一个动作相对当前状态的价值。本质上V可以看做是baseline。对于上图第3种做法，也可以直接用V来作为baseline。但是还是一样的问题，A的variance比较大。为了平衡variance和bias的问题，使用TD会是比较好的做法，既兼顾了实际值reward，又使用了估计值V。在TD中，TD(lambda)平衡不同长度的TD值，会是比较好的做法。 在实际使用中，需要根据具体的问题选择不同的方法。有的问题reward很容易得到，有的问题reward非常稀疏。reward越稀疏，也就越需要采用估计值。 以上就是 Policy Gradient 的核心思想，通过 policy network 输出的 softmax 概率 和获取的reward(通过评估指标获取)构造目标函数，然后对 policy network 进行更新。从而避免了原来的 reward 和 policy network 之间是不可微的问题。也因为Policy Gradient的这个特点，目前的很多传统监督学习的问题因为输出都是softmax的离散形式，都可以改造成Policy Gradient的方法来实现，调节得当效果会在监督学习的基础上进一步提升。 Actor-Critic上面提到的多种评估指标其实已经涵盖了 Actor-Critic 的思想，原始的 Policy Gradient 往往采用的回合更新，也就是要到一轮结束后才能进行更新。如某盘游戏，假如最后的结果是胜利了，那么可以认为其中的每一步都是好的，反之则认为其中的每一步都是不好的。其更新过程如下，图片摘自 David Silver 的 Policy Gradient 课件 ，这种方法也叫 Monte-Carlo Policy Gradient 图中的 $\log \pi_{\theta}(s_t, a_t)$ 是 policy network 输出的概率，$v_t$ 是当前这一局的结果。这是 policy gradient 最基本的更新形式。但是这个方法显然是有问题的，最后的结果好久并不能说明其中每一步都好。因此一个很直观的想法就是能不能抛弃回合更新的做法，而采用单步更新？Actor-Critic 干的就是这个事情。 要采用单步更新，意味着我们需要为每一步都即时做出评估。Actor-Critic 中的 Critic 负责的就是评估这部分工作，而 Actor 则是负责选择出要执行的动作。这就是 Actor-Critic 的思想。从上面论文中提出的各种评价指标可知，看到 Critic 的输出有多种形式，可以采用 Q值、V值 或 TD 等。 因此 Actor-Critic 的思想就是从 Critic 评判模块(采用深度神经网络居多)得到对动作的好坏评价，然后反馈给 Actor(采用深度神经网络居多) 让 Actor 更新自己的策略。从具体的训练细节来说，Actor 和 Critic 分别采用不同的目标函数进行更新， 如可参考这里的代码 Actor-Critic (Tensorflow)，下面要说的 DDPG 也是这么做的。 Deep Deterministic Policy Gradient(DDPG)上面提到的的 Policy Gradient 处理问题其实还是局限在动作个数是离散和有限的情况，但是对于某些输出的值是连续的问题，上面的方法就不管用了，比如说自动驾驶控制的速度，机器人控制移动的幅度等。 最开始这篇论文 Deterministic Policy Gradient Algorithms 提出了输出连续动作值的 DPG(Deterministic Policy Gradient) ; 然后 论文 Continuous control with deep reinforcement learning 基于 DPG 做了改进，提出了 DDPG(Deep Deterministic Policy Gradient)。 这里 DPG 不详细展开说了，简而言之，主要就是证明了 deterministic policy gradient不仅存在，而且是model-free形式且是action-value function的梯度。因此 policy 不仅仅可以通过 概率分布表示，也就将动作空间推到了无限大的。具体的理论课参考这篇文章 深度增强学习（DRL）漫谈 - 从AC（Actor-Critic）到A3C（Asynchronous Advantage Actor-Critic） DDPG 相对于 DPG 的核心改进是引入了 Deep Learning，采用深度神经网络作为 DPG 中的策略函数 $μ$ 和 $Q$ 函数的模拟，即 Actor 网络和 Critic 网络；然后使用深度学习的方法来训练上述神经网络。两者的关系类似于 DQN 和 Q-learning 的关系。 DDPG 的网络结构为 Actor 网络 + Critic 网络，对于状态 $s$, 先通过 Actor 网络获取 action $a$, 这里的 $a$ 是一个向量；然后将 $a$ 输入 Critic 网络，输出的是 Q 值，目标函数就是极大化 Q 值，但是更新的方法两者又有一些区别。论文中显示 DDPG 算法流程如下 从算法的流程可知，Actor 网络和 Critic 网络是分开训练的，但是两者的输入输出存在联系，Actor 网络输出的 action 是 Critic 网络的输入，同时 Critic 网络的输出会被用到 Actor 网路进行反向传播。 原始论文没有给出两个网路的具体示意图，这里给出一张这篇文章画的示意图，可以看到，Critic 跟之前提到的 DQN 有点类似，但是这里的输入是 state + action，输出是一个 Q 值而不是各个动作的 Q 值。 由于在 DDPG 中，我们不再用单一的概率值表示某个动作，而是用向量表示某个动作，由于向量空间可以被认为是无限的，因此也能够跟无限的动作空间对应起来。 Asynchronous Advantage Actor-Critic(A3C)在提出 DDPG 后，DeepMind 在这个基础上提出了效果更好的 Asynchronous Advantage Actor-Critic（A3C），详见论文 Asynchronous Methods for Deep Reinforcement Learning A3C 算法和DDPG类似，通过 DNN 拟合 policy function 和 value function的估计。但是不同点在于 A3C 中有多个 agent 对网络进行 asynchronous update，这样带来了样本间的相关性较低的好处，因此 A3C 中也没有采用 Experience Replay 的机制；这样 A3C 便支持 online 的训练模式了 A3C 有两个输出，其中一个 softmax output 作为 policy $\pi(a_t|s_t;\theta)$，而另一个linear output为 value function $V(s_t;\theta_v)$ A3C 中的Policy network 的评估指标采用的是上面比较了多种评估指标的论文中提到的 Advantage Function(即A值) 而不是 DDPG 中单纯的 Q 值。 整体的结构如下所示，图片摘自这篇文章。 从上面的如可知，输出包含两部分，value network 的部分可以用来作为连续动作值的输出，而 policy network 可以作为离散动作值的概率输出，因此能够同时解决前面提到的两类问题。 两个网络的更新公式如下 A3C 通过创建多个agent，在多个环境实例中并行且异步的执行和学习，有个潜在的好处是不那么依赖于GPU或大型分布式系统，实际上A3C可以跑在一个多核CPU上，而工程上的设计和优化也是这篇文章的一个重点。 综上，本文主要介绍了 Policy Gradient 这一类的方法，最基础的 Policy Gradient 是回合更新的，通过引入 Critic 后变成了单步更新，而这种结合了 policy 和 value 的方法也叫 Actor-Critic，Critic 有多种可选的方法。对于输出动作为连续值的情形，前面那些输出动作概率分布的方法无能为力，因此提出了 DPG 和 DDPG，DDPG 对 DPG 的改进在于引入深度神经网络去拟合 policy function 和 value function。在 DDPG 基础上又提出了效果更好的 A3C，这个方法在 DDPG 上引入了多个 agent 对网络进行 asynchronous update，不仅取得了更好的效果，而且降低了训练的代价。 参考 深度增强学习之Policy Gradient方法1 Deep Reinforcement Learning: Pong from Pixels 深度增强学习（DRL）漫谈 - 从AC（Actor-Critic）到A3C（Asynchronous Advantage Actor-Critic） 深度强化学习（Deep Reinforcement Learning）入门：RL base &amp; DQN-DDPG-A3C introduction]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[强化学习笔记(2)-从 Q-Learning 到 DQN]]></title>
      <url>%2F2018%2F05%2F09%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)-%E4%BB%8E%20Q-Learning%20%E5%88%B0%20DQN%2F</url>
      <content type="text"><![CDATA[在上一篇文章强化学习笔记(1)-概述中，介绍了通过 MDP 对强化学习的问题进行建模，但是由于强化学习往往不能获取 MDP 中的转移概率，解决 MDP 的 value iteration 和 policy iteration 不能直接应用到解决强化学习的问题上，因此出现了一些近似的算法来解决这个问题，本文要介绍的就是基于 value iteration 而发展出来的 Q-Learning 系列方法，包括 Q-Learning、Sarsa 和 DQN。 Q-LearningQ-Learning 是一个强化学习中一个很经典的算法，其出发点很简单，就是用一张表存储在各个状态下执行各种动作能够带来的 reward，如下表表示了有两个状态 $s_1, s_2$，每个状态下有两个动作 $a_1, a_2$, 表格里面的值表示 reward - a1 a2 s1 -1 2 s2 -5 2 这个表示实际上就叫做 Q-Table，里面的每个值定义为 $Q(s, a)$, 表示在状态 $s$ 下执行动作 $a$ 所获取的reward，那么选择的时候可以采用一个贪婪的做法，即选择价值最大的那个动作去执行。 这样问题就来了，就是 Q-Table 要如何获取？答案是随机初始化，然后通过不断执行动作获取环境的反馈并通过算法更新 Q-Table。下面重点讲如何通过算法更新 Q-Table。 当我们处于某个状态 $s$ 时，根据 Q-Table 的值选择的动作 $a$, 那么从表格获取的 reward 为 $Q(s,a)$，此时的 reward 并不是我们真正的获取的 reward，而是预期获取的 reward，那么真正的 reward 在哪？我们知道执行了动作 $a$ 并转移到了下一个状态 $s’$ 时，能够获取一个即时的 reward（记为$r$）, 但是除了即时的 reward，还要考虑所转移到的状态 $s’$ 对未来期望的reward，因此真实的 reward (记为 $Q’(s,a)$)由两部分组成：即时的 reward 和未来期望的 reward，且未来的 reward 往往是不确定的，因此需要加个折扣因子 $\gamma$,则真实的 reward 表示如下 $$Q’(s,a) = r + \gamma\max_{a’}Q(s’,a’)$$ $\gamma$ 的值一般设置为 0 到 1 之间，设为0时表示只关心即时回报，设为 1 时表示未来的期望回报跟即时回报一样重要。 有了真实的 reward 和预期获取的 reward，可以很自然地想到用 supervised learning那一套，求两者的误差然后进行更新，在 Q-learning 中也是这么干的，更新的值则是原来的 Q(s, a)，更新规则如下 $$Q(s, a) = Q(s, a) + \alpha(Q’(s, a) - Q(s,a))$$ 更新规则跟梯度下降非常相似，这里的 $\alpha$ 可理解为学习率。 更新规则也很简单，可是这一类采用了贪心思想的算法往往都会有这么一个问题：算法是否能够收敛，是收敛到局部最优还是全局最优？ 关于收敛性，可以参考 Convergence of Q-learning: a simple proof，这个文档 证明了这个算法能够收敛，且根据知乎上这个问题 RL两大类算法的本质区别？（Policy Gradient 和 Q-Learning)，原始的 Q-Learning 理论上能够收敛到最优解，但是通过 Q 函数近似 Q-Table 的方法则未必能够收敛到最优解（如DQN）。 除此之外， Q-Learning 中还存在着探索与利用(Exploration and Exploition)的问题, 大致的意思就是不要每次都遵循着当前看起来是最好的方案，而是会选择一些当前看起来不是最优的策略，这样也许会更快探索出更优的策略。 Exploration and Exploition 的做法很多，Q-Learning 采用了最简单的 $\epsilon$-greedy, 就是每次有 $\epsilon$ 的概率是选择当前 Q-Table 里面值最大的action的，1 - $\epsilon$ 的概率是随机选择策略的。 Q-Learning 算法的流程如下，图片摘自这里 上面的流程中的 Q 现实 就是上面说的 $Q’(s,a)$, Q 估计就是上面说的 $Q(s,a)$。 下面的 python 代码演示了更新通过 Q-Table 的算法, 参考了这个 repo 上的代码，初始化主要是设定一些参数，并建立 Q-Table, choose_action 是根据当前的状态 observation，并以 $\epsilon$-greedy 的策略选择当前的动作； learn 则是更新当前的 Q-Table，check_state_exist 则是检查当前的状态是否已经存在 Q-Table 中，若不存在要在 Q-Table 中创建相应的行。 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport pandas as pdclass QTable: def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): self.actions = actions # a list self.lr = learning_rate self.gamma = reward_decay self.epsilon = e_greedy self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64) def choose_action(self, observation): self.check_state_exist(observation) # action selection if np.random.uniform() &lt; self.epsilon: # choose best action state_action = self.q_table.ix[observation, :] state_action = state_action.reindex(np.random.permutation(state_action.index)) # some actions have same value action = state_action.argmax() else: # choose random action action = np.random.choice(self.actions) return action def learn(self, s, a, r, s_): self.check_state_exist(s_) q_predict = self.q_table.ix[s, a] if s_ != 'terminal': q_target = r + self.gamma * self.q_table.ix[s_, :].max() # next state is not terminal else: q_target = r # next state is terminal self.q_table.ix[s, a] += self.lr * (q_target - q_predict) # update def check_state_exist(self, state): if state not in self.q_table.index: # append new state to q table self.q_table = self.q_table.append( pd.Series( [0]*len(self.actions), index=self.q_table.columns, name=state, ) ) SarsaSarsa 跟 Q-Learning 非常相似，也是基于 Q-Table 进行决策的。不同点在于决定下一状态所执行的动作的策略，Q-Learning 在当前状态更新 Q-Table 时会用到下一状态Q值最大的那个动作，但是下一状态未必就会选择那个动作；但是 Sarsa 会在当前状态先决定下一状态要执行的动作，并且用下一状态要执行的动作的 Q 值来更新当前状态的 Q 值；说的好像很绕，但是看一下下面的流程便可知道这两者的具体差异了，图片摘自这里 那么，这两者的区别在哪里呢？这篇文章里面是这样讲的 This means that SARSA takes into account the control policy by which the agent is moving, and incorporates that into its update of action values, where Q-learning simply assumes that an optimal policy is being followed. 简单来说就是 Sarsa 在执行action时会考虑到全局（如更新当前的 Q 值时会先确定下一步要走的动作）， 而 Q-Learning 则显得更加的贪婪和”短视”, 每次都会选择当前利益最大的动作(不考虑 $\epsilon$-greedy)，而不考虑其他状态。 那么该如何选择，根据这个问题：When to choose SARSA vs. Q Learning，有如下结论 If your goal is to train an optimal agent in simulation, or in a low-cost and fast-iterating environment, then Q-learning is a good choice, due to the first point (learning optimal policy directly). If your agent learns online, and you care about rewards gained whilst learning, then SARSA may be a better choice. 简单来说就是如果要在线学习，同时兼顾 reward 和总体的策略(如不能太激进，agent 不能很快挂掉)，那么选择 Sarsa；而如果没有在线的需求的话，可以通过 Q-Learning 线下模拟找到最好的 agent。所以也称 Sarsa 为on-policy，Q-Leanring 为 off-policy。 DQN我们前面提到的两种方法都以依赖于 Q-Table，但是其中存在的一个问题就是当 Q-Table 中的状态比较多，可能会导致整个 Q-Table 无法装下内存。因此，DQN 被提了出来，DQN 全称是 Deep Q Network，Deep 指的是通的是深度学习，其实就是通过神经网络来拟合整张 Q-Table。 DQN 能够解决状态无限，动作有限的问题；具体来说就是将当前状态作为输入，输出的是各个动作的 Q 值。以 Flappy Bird 这个游戏为例，输入的状态近乎是无限的（当前 bird 的位置和周围的水管的分布位置等），但是输出的动作只有两个(飞或者不飞)。实际上，已经有人通过 DQN 来玩这个游戏了，具体可参考这个 DeepLearningFlappyBird 所以在 DQN 中的核心问题在于如何训练整个神经网络，其实训练算法跟 Q-Learning 的训练算法非常相似，需要利用 Q 估计和 Q 现实的差值，然后进行反向传播。 这里放上提出 DQN 的原始论文 Playing atari with deep reinforcement learning 中的算法流程图 上面的算法跟 Q-Learning 最大的不同就是多了 Experience Replay 这个部分，实际上这个机制做的事情就是先进行反复的实验，并将这些实验步骤获取的 sample 存储在 memory 中，每一步就是一个 sample，每个sample是一个四元组，包括：当前的状态，当前状态的各种action的 Q 值，当前采取的action获得的即时回报，下一个状态的各种action的Q值。拿到这样一个 sample 后，就可以根据上面提到的 Q-Learning 更新算法来更新网络，只是这时候需要进行的是反向传播。 Experience Replay 机制的出发点是按照时间顺序所构造的样本之间是有关的(如上面的 $\phi(s_{t+1})$ 会受到 $\phi(s_{t})$ 的影响)、非静态的（highly correlated and non-stationary），这样会很容易导致训练的结果难以收敛。通过 Experience Replay 机制对存储下来的样本进行随机采样，在一定程度上能够去除这种相关性，进而更容易收敛。当然，这种方法也有弊端，就是训练的时候是 offline 的形式，无法做到 online 的形式。 除此之外，上面算法流程图中的 aciton-value function 就是一个深度神经网络，因为神经网络是被证明有万有逼近的能力的，也就是能够拟合任意一个函数；一个 episode 相当于 一个 epoch；同时也采用了 $\epsilon$-greedy 策略。代码实现可参考上面 FlappyBird 的 DQN 实现。 上面提到的 DQN 是最原始的的网络，后面Deepmind 对其进行了多种改进，比如说 Nature DQN 增加了一种新机制 separate Target Network，就是计算上图的$y_j$ 的时候不采用网络 $Q$, 而是采用另外一个网络(也就是 Target Network) $Q’$, 原因是上面计算 $y_j$ 和 Q 估计都采用相同的网络 $Q$，这样使得Q大的样本，y也会大，这样模型震荡和发散可能性变大，其原因其实还是两者的关联性较大。而采用另外一个独立的网络使得训练震荡发散可能性降低，更加稳定。一般 $Q’$ 会直接采用旧的 $Q$, 比如说 10 个 epoch 前的 $Q$. 除此之外，大幅度提升 DQN 玩 Atari 性能的主要就是 Double DQN，Prioritised Replay 还有 Dueling Network 三大方法；这里不详细展开，有兴趣可参考这两篇文章：DQN从入门到放弃6 DQN的各种改进 和 深度强化学习（Deep Reinforcement Learning）入门：RL base &amp; DQN-DDPG-A3C introduction。 综上，本文介绍了强化学习中基于 value 的方法：包括 Q-Learning 以及跟 Q-Learning 非常相似的 Sarsa，同时介绍了通过 DQN 解决状态无限导致 Q-Table过大的问题。需要注意的是 DQN 只能解决动作有限的问题，对于动作无限或者说动作取值为连续值的情况，需要依赖于 policy gradient 这一类算法，而这一类算法也是目前更为推崇的算法，在下一章将介绍 Policy Gradient 以及结合 Policy Gradient 和 Q-Learning 的 Actor-Critic 方法。 参考 Q Learning Sarsa When to choose SARSA vs. Q Learning DQN从入门到放弃5 深度解读DQN算法 深度强化学习（Deep Reinforcement Learning）入门：RL base &amp; DQN-DDPG-A3C introduction]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[强化学习笔记(1)-概述]]></title>
      <url>%2F2018%2F05%2F05%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)-%E6%A6%82%E8%BF%B0%2F</url>
      <content type="text"><![CDATA[本文主要介绍强化学习的一些基本概念：包括MDP、Bellman方程等, 并且讲述了如何从 MDP 过渡到 Reinforcement Learning。 强化学习的任务这里还是放上 David Silver的课程 的图，可以很清楚的看到整个交互过程。这就是人与环境交互的一种模型化表示，在每个时间点，大脑 agent 会从可以选择的动作集合A中选择一个动作 $a_t$ 执行。环境则根据 agent 的动作给 agent 反馈一个 reward $r_t$，同时 agent 进入一个新的状态。 知道了整个过程，任务的目标就出来了，那就是要能获取尽可能多的Reward。Reward越多，就表示执行得越好。每个时间片，agent 根据当前的状态来确定下一步的动作。也就是说我们需要一个state找出一个action，使得 reward 最大，从 state 到 action 的过程就称之为一个策略Policy，一般用 $\pi$ 表示: 强化学习的任务就是找到一个最优的策略Policy从而使Reward最多。 我们一开始并不知道最优的策略是什么，因此往往从随机的策略开始，使用随机的策略进行试验，就可以得到一系列的状态,动作和反馈： $(s_1,a_1,r_1,s_2,a_2,r_2,…s_t,a_t,r_t)$ 这就是一系列的样本Sample。强化学习的算法就是需要根据这些样本来改进Policy，从而使得得到的样本中的Reward更好。由于这种让Reward越来越好的特性，所以这种算法就叫做强化学习Reinforcement Learning。 MDP（Markov Decision Process）强化学习的问题都可以模型化为MDP(马尔可夫决策过程)的问题，MDP 实际上是对环境的建模；MDP 与常见的 Markov chains 的区别是加入了action 和 rewards 的概念。 因此，一个基本的 MDP 可以用一个五元组 $(S,A,P,R, \gamma)$ 表示，其中 $S$ 是一个有限状态集 $A$ 是一个有限动作集 $P$ 是一个状态转移概率矩阵，$P_a(s, s′)=P(s_{t+1}=s′|s_t=s, a_t=a)$ 表示在状态 $s$ 下执行动作 $a$ 后转移到状态 $s′$ 的概率 $R$ 是一个奖励函数，$R_a(s, s′)$ 表示在状态 $s$ 下执行动作 $a$ 后转移到状态 $s′$ 所得到的即时回报(reward) $\gamma$ 是一个折扣因子,一般取值在 [0,1]; 用来区分当前回报和未来回报的重要性，一般会加在未来的回报前，减小未来回报的权重。 因此，MDP 的核心问题就是找到一个策略 $\pi(s)$ 来决定在状态 $s$ 下选择哪个动作，这种情况下MDP就变成了一个 Markov chain，且此时的目标跟我们前面提到的强化学习的目标是一致的。 回报与价值函数状态的好坏等价于对未来回报的期望。因此，引入回报(Return) 来表示某个时刻t的状态将具备的回报： $G_t = R_{t+1} + \gamma R_{t+2} + … = \sum_{k=0}^\infty\gamma^kR_{t+k+1}$ 上面 $R$ 是 Reward 反馈，$\gamma$ 是 discount factor（折扣因子）,跟前面 MDP 中的符号的含义一致。 从上面的式子可以， 除非整个过程结束，否则我们无法获取所有的 reward 来计算出每个状态的 Return，因此，再引入一个概念:价值函数(value function),记为 $V(s)$，通过 $V(s)$ 来表示一个状态未来的潜在价值。从定义上看，value function 就是回报的期望： $V(s) = \mathbb E[G_t|S_t = s]$ 引出价值函数，对于获取最优的策略Policy这个目标，我们就会有两种方法： 直接优化策略 $\pi(a|s)$ 或者 $a = \pi(s)$ 使得回报更高 通过估计 value function 来间接获得优化的策略。道理很简单，通过价值函数可以知道每一种状态的好坏，这样我们就知道该怎么选择了（如选择动作使得下一状态的潜在价值最大），而这种选择就是我们想要的策略。 Bellman方程采用上面获取最优策略的第 2 种方法时，我们需要估算 Value Function，只要能够计算出价值函数，那么最优决策也就得到了。因此，问题就变成了如何计算Value Function？ 根据前面 $G_t$ 和 $V(s)$ 的定义，有 $$\begin{align}V(s) &amp; = \mathbb E[G_t|S_t = s]\\ &amp; = \mathbb E[R_{t+1}+\gamma R_{t+2} + \gamma ^2R_{t+3} + …|S_t = s]\\ &amp; = \mathbb E[R_{t+1}+\gamma (R_{t+2} + \gamma R_{t+3} + …)|S_t = s]\\ &amp; = \mathbb E[R_{t+1}+\gamma G_{t+1}|S_t = s]\\ &amp; = \mathbb E[R_{t+1}+\gamma v(S_{t+1})|S_t = s]\end{align}$$ 则有 $$V(s) = \mathbb E[R_{t+1} + \gamma V(S_{t+1})|S_t = s]$$ 上面这个公式就是Bellman方程的基本形态。从公式上看，当前状态的价值和下一步的价值以及当前的反馈Reward有关, 其中透出的含义就是价值函数的计算可以通过迭代的方式来实现。 从 MDP 到 Reinforcement Learning回到 MDP 问题，如果我们知道了转移概率 $P$ 和奖励函数 $R$，那么便可通过下面的方法求出最优策略 $\pi(s)$, 首先，结合上面提到的价值函数和Bellman方程有 公式1： $$\pi(s):=\arg \max_a\ {\sum_{s’}P_{a}(s,s’)(R_{a}(s,s’)+\gamma V(s’))}$$ 公式2: $$ V(s) := \sum_{s’}P_{\pi(s)}(s,s’)(R_{\pi(s)}(s,s’) + \gamma V(s’)) $$ 公式 1 表示在状态 $s$ 下的采取的最优动作，公式 2 表示在状态 $s$ 下的价值，可以看到两者有依存关系 而在 转移概率 $P$ 和奖励函数 $R$已知的情况下，求解 MDP 问题常见做法有 Value iteration 或 Policy iteration Value iteration在 Value iteration 中，策略函数 $\pi$ 没有被使用，迭代公式如下： $$V_{i+1}(s) := \max_a \sum_{s’} P_a(s,s’)(R_a(s,s’) + \gamma V_i(s’))$$ 下标 $i$ 表示第 $i$ 次迭代，在每轮迭代中需要计算每个状态的价值，并且直到两次迭代结果的差值小于给定的阈值才能认为是收敛。 计算的出收敛的价值函数后，通过公式1就能够得出策略函数 $\pi$ 了，其迭代过程如下图所示 Policy iterationPolicy iteration同时更新价值 $V$ 和策略 $\pi$, 且一般可分成两步： Policy Evaluation，策略评估，就是上面公式2的过程。目的是在策略固定的情况下更新Value Function 直到 value 收敛，从另一个角度来讲就是为了更好地估计基于当前策略的价值 Policy Improvement，策略改进，就是上面公式1的过程。就是根据更新后的 Value Function 来更新每个状态下的策略直到策略稳定 这个方法本质上就是使用当前策略($\pi$)产生新的样本，然后使用新的样本更好的估计策略的价值($V(s)$)，然后利用策略的价值更新策略，然后不断反复。理论可以证明最终策略将收敛到最优 具体的算法流程如下所示 区别与局限问题来了，上面的 Policy Iteration 和 Value Iteration有什么区别, 为什么一个叫policy iteration，一个叫value iteration？ 原因其实很好理解，policy iteration 最后收敛的 value $V$ 是当前 policy 下的 value 值（也做对policy进行评估），目的是为了后面的policy improvement得到新的policy；所以是在显式地不停迭代 policy。 而value iteration 最后收敛得到的 value 是当前state状态下的最优的value值。当 value 最后收敛，那么最优的policy也就得到的。虽然这个过程中 policy 在也在隐式地更新，但是一直在显式更新的是 value 的，所以叫value iteration。 从上面的分析看，value iteration 较 之policy iteration更直接。不过问题也都是一样，都需要知道转移概率 $P$ 和奖励函数 $R$。 但是对于 Reinforcement Learning 这一类问题，转移概率 $P$ 往往是不知道，知道转移概率 $P$ 也就称为获得了模型 Model，这种通过模型来获取最优动作的方法也就称为 Model-based 的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有 Model-free 的方法来寻找最优的动作，像 Q-learning，Policy Gradient，Actor Critic这一类方法都是 model-free 的。 前面的方法问题是需要已知转移概率 $P$, 目的是为了遍历当前状态后的所有可能的状态，因此如果采用贪婪的思想，那么就不需要不遍历后面所有的状态，而是直接采取价值最大的状态动作来执行。 Q-learning 实际上就是采用这种思想的，Q-Learning的基本思想是根据 value iteration 得到的，但要明确一点是 value iteration 每次都对所有的Q值更新一遍，也就是所有的状态和动作。但事实上在实际情况下我们没办法遍历所有的状态，还有所有的动作，因此，我们只能得到有限的系列样本。具体的算法流程会再下一篇文章具体介绍。 综上，本文主要介绍了强化学习的任务和一些概念，以及从 MDP 如何过渡到 Reinforcement，在后续的文章中会陆续介绍Q-learning 类方法，Policy gradient 类方法以及结合两者的 Actor Critic 方法。 参考资料 Markov decision process DQN 从入门到放弃4 动态规划与Q-Learning 强化学习方法汇总]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[梯度裁剪及其作用]]></title>
      <url>%2F2018%2F05%2F01%2F%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%E5%8F%8A%E5%85%B6%E4%BD%9C%E7%94%A8%2F</url>
      <content type="text"><![CDATA[本文简单介绍梯度裁剪(gradient clipping)的方法及其作用，最近在训练 RNN 过程中发现这个机制对结果影响非常大。 梯度裁剪一般用于解决 梯度爆炸(gradient explosion) 问题，而梯度爆炸问题在训练 RNN 过程中出现得尤为频繁，所以训练 RNN 基本都需要带上这个参数。常见的 gradient clipping 有两种做法 根据参数的 gradient 的值直接进行裁剪 根据若干参数的 gradient 组成的 vector 的 L2 norm 进行裁剪 第一种做法很容易理解，就是先设定一个 gradient 的范围如 (-1, 1), 小于 -1 的 gradient 设为 -1， 大于这个 1 的 gradient 设为 1. 第二种方法则更为常见，先设定一个 clip_norm, 然后在某一次反向传播后，通过各个参数的 gradient 构成一个 vector，计算这个 vector 的 L2 norm（平方和后开根号）记为 LNorm，然后比较 LNorm 和 clip_norm 的值，若 LNorm &lt;= clip_norm 不做处理，否则计算缩放因子 scale_factor = clip_norm/LNorm ，然后令原来的梯度乘上这个缩放因子。这样做是为了让 gradient vector 的 L2 norm 小于预设的 clip_norm。 关于 gradient clipping 的作用可更直观地参考下面的图，没有gradient clipping 时，若梯度过大优化算法会越过最优点。 而在一些的框架中，设置 gradient clipping 往往也是在 Optimizer 中设置，如 tensorflow 中设置如下 1234optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)gvs = optimizer.compute_gradients(cost)capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]train_op = optimizer.apply_gradients(capped_gvs) Keras 中设置则更为简单 1optimizer = optimizers.SGD(lr=0.001, momentum=0.9, clipnorm=1.), 除此之外，调试 RNN 是个比较 tricky 的活，可参考知乎上这个问题：此处输入链接的描述你在训练RNN的时候有哪些特殊的trick？ 另外，与 grdient explosion 相反的问题 gradient vanishing 的做法跟 grdient explosion 不同，不能简单地采用 scaling 的方法，具体可参考这个问题 梯度消失问题为什么不通过 gradient scaling 来解决？，实际的处理方法一般是采用 LSTM 或 GRU 这类有记忆的 RNN 单元。 参考： Gradient Clipping caffe里的clip gradient是什么意思？ What is gradient clipping and why is it necessary?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过 Keras 实现 LRCN 模型]]></title>
      <url>%2F2018%2F04%2F18%2F%E9%80%9A%E8%BF%87%20Keras%20%E5%AE%9E%E7%8E%B0%20LRCN%20%E6%A8%A1%E5%9E%8B%2F</url>
      <content type="text"><![CDATA[本文主要介绍了如何通过 Keras 实现 LRCN 模型，模型出自论文 Long-term Recurrent Convolutional Networks for Visual Recognition and Description，最近需要用这个模型做个实验，在网上搜到的实现代码不多，因此这里记录一下，以供参考。 这里的 LRCN 模型的结构如下图所示，输入是 image sequence，然后通过 CNN 提取每帧图像的特征，作为 LSTM 的输入，LSTM可以为每帧预测一个 label，也可在只在最后预测一个 label 作为整个 sequence 的 label。这种想法非常自然，也是 video/image sequence 中的一个 base model。 下面首先讲述如何在 Keras 中构建这个模型，然后讲述数据加载的两种模式：分别对应于不定长的输入序列和固定长度的输入序列。 构建模型在具体的实现中，对于训练数据集不大的情况下， CNN 部分一般可采用预训练的模型，然后选择是否对其进行 FineTunning，这里我采用的是在 ImageNet 上预训练的 VGG16，并且对 VGG16 最上面的5层进行 FineTunning， 其他层的参数不变。 另外，对于输入的每帧图像，通过 CNN 抽取出的 feature map 的 大小为(7,7,512)，而 LSTM 的输入的 size 是 (batch_size, timesteps, input_dim)，因此需要将 (7,7,512) 转为一个一维的 vector，这里我采用最简单的 Flatten() 方法。实际上，在这里可以采用更加灵活的转换，如这篇论文 Diversified Visual Attention Networks for Fine-Grained Object Classification 就提出了一种 attention 机制处理这些feature map。 (7,7,512) 直接 Flatten 后的大小为 25088，直接输入 LSTM 的话比较大，因此这里还加了一个 2048 的全连接层，这样输入 LSTM 的 input_dim 的大小就是 2048. LRCN 模型中的关键点在于为每个 LSTM 的 step 前连上 CNN 网络部分，在 Keras 中可通过 TimeDistributed 层来实现，同时如果需要长度不固定的输入序列时，对应的 sequence length的参数要设为 None，在下面的代码中 input_shape 设为了 (None, 224, 224, 3), None 便是输入序列长度不固定，而 （224, 224, 3）则是预训练 VGG 固定的输入大小。 构建模型的 keras 代码如下，这里为了加快训练速度，将 LSTM 替换成了 GRU 12345678910111213141516171819202122232425262728from keras.applications.vgg16 import VGG16from keras.models import Sequential, Modelfrom keras.layers import Input, TimeDistributed, Flatten, GRU, Dense, Dropoutfrom keras import optimizersdef build_model(): pretrained_cnn = VGG16(weights='imagenet', include_top=False) # pretrained_cnn.trainable = False for layer in pretrained_cnn.layers[:-5]: layer.trainable = False # input shape required by pretrained_cnn input = Input(shape = (224, 224, 3)) x = pretrained_cnn(input) x = Flatten()(x) x = Dense(2048)(x) x = Dropout(0.5)(x) pretrained_cnn = Model(inputs = input, output = x) input_shape = (None, 224, 224, 3) # (seq_len, width, height, channel) model = Sequential() model.add(TimeDistributed(pretrained_cnn, input_shape=input_shape)) model.add(GRU(1024, kernel_initializer='orthogonal', bias_initializer='ones', dropout=0.5, recurrent_dropout=0.5)) model.add(Dense(categories, activation = 'softmax')) model.compile(loss='categorical_crossentropy', optimizer = optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=1., clipvalue=0.5), metrics=['accuracy']) return model 上面 LSTM 的参数初始化参考了知乎上这个答案：你在训练RNN的时候有哪些特殊的trick？，主要就是 initializer 的方式选择、drop-out 和 graddient clipping的设置。在我的实验中也证实了 graddient clipping 的设置会直接影响到最终的进度，而且影响比较大。 数据加载由于 RNN 本身的结构特点，使得其可接受变长的输入，而且往往原始的 img sequence 数据也会符合这一特点，因此这里就有两种数据加载的方式，第一种是对原始数据不做处理, 每个样本的长度不一定相同；第二种是从各个样本中抽取出固定的长度。 但是这两种加载方式的输入的数据的 shape 都遵循着下面的模式(batch_size, sequence_length, width, height, channel) 对于第一种加载方式，根据 这个 issue，有两种处理方法 1) Zero-padding2) Batches of size 1 这里采用的是第二种处理方法，也就是将 batch_size 设置为1， 即每次只用一个样本更新模型。因为第二种处理方法要事先设定一个固定长度(可以是最长序列长度或其他方式获取的长度)，而且padding会让原来较短的序列变得更长，消耗的内存会有所增加。 而对于第二种加载方式，给定固定的长度，需要尽可能“均匀”地从原始序列中抽出固定长度的序列，即帧与帧之间的间隔尽可能相等。但是这个可能会取决于具体的数据集和任务，在我的数据集上这样做是比较合理的。 两种加载方式的比较如下：1）存储数据的方式不一样，固定长度的加载方式由于每个样本的 shape 一样，因此可以直接 concatenate 成一个大的 ndarray，然后在训练时的 batch_size 可设置成任意值；但是对于变长的加载方式，只能每次取一个样本, 然后要通过 np.expand_dims(imgs, axis=0) 的方式为样本添加 batch_size 这个维度（前一种方式不用，因为concatenate 后会自动生成这个维度），然后训练模型同时将 batch_size 和 epoch 设为1。2）训练速度和效果有差别。首先是 batch_size 的不同使得训练速度上固定长度的方式比变长方式要快，这个比较好理解。其次，由于 batch_size 也是一个影响 RNN 性能的重要参数，因此也会影响收敛性和效果。在我的实验中，batch_size 设置大于 1 时效果更好。 两种加载方式实现代码如下，加载的是一个样本的数据， img_dir 目录中包含了一个样本的所有 image sequence，且根据文件名排序后的序列是根据时间序列的。123456789101112131415161718192021222324252627from collections import dequefrom keras.preprocessing import imagedef load_sample(img_dir, categories = 7, fixed_seq_len = None): label = int(img_dir.split('/')[-2].split('_')[0]) - 1 # extract label from name of sample img_names = sorted(os.listdir(img_dir)) imgs = [] if fixed_seq_len: # extract certain length of sequence block_len = round(len(img_names)/fixed_seq_len) idx = len(img_names) - 1 tmp = deque() for _ in range(fixed_seq_len): tmp.appendleft(img_names[idx]) idx = max(idx-block_len, 0) img_names = tmp for img_name in img_names: img_path = img_dir + img_name img = image.load_img(img_path, target_size=(224, 224)) x = image.img_to_array(img) imgs.append(x) imgs = np.array(imgs) label = np_utils.to_categorical(label, num_classes=categories) if not fixed_seq_len: # add dimension for batch_size #（seq_len, width, height, channel）-&gt; (batch_size, seq_len, width, height, channel) imgs = np.expand_dims(imgs, axis=0) label = label.reshape(-1, categories) return imgs, label 最后，在设计网络结构的时候，可通过逐层测试输出的大小来判断每一层是够达到了预期输出的效果，在 keras 中直接通过 model.predict(input) 即可获得当前 model 最后一层的输出。 另外，Keras 虽然能够比较快速地通过其提供的各层 layer 搭建出模型，但是如果要对模型进行更细致的设计的时候， Keras 就不是那么好做了，这时候就要上 tensorflow/pytorch/mxnet 这一类更加灵活的框架了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[共享键鼠神器 Synergy]]></title>
      <url>%2F2018%2F03%2F31%2F%E5%85%B1%E4%BA%AB%E9%94%AE%E9%BC%A0%E7%A5%9E%E5%99%A8%20Synergy%2F</url>
      <content type="text"><![CDATA[最近需要频繁切换使用台式机和笔记本，但是我的小桌子上实在没法同时放得下一个键盘和笔记本 （≧0≦）。哪怕凑合挤下，还得不停在两台电脑之间切换键鼠，因此就想着有没有共享键鼠的方案，结果在网上找到了 Synergy，试了几天后发现这真的是一个共享键鼠的神器。 安装与配置这个软件在2.0版本前支持自己编译的免费模式的，核心源码见github，官方还贴心地提供了各种操作系统下编译的文档:)，这里编译出来的是 stable 版本，如果还需要更高级功能，可以去官网上购买 pro 版本支持一下。网上已经有雷锋帮我们编译了若干个 stable version，详见下面链接 https://www.brahma.world/synergy-stable-builds/ 这个软件的工作模式采用 Client-Server 模式，只允许有一台 Server，但是可有多台client，且只需要在 Server 端配键鼠就行了。我这里以笔记本为server，台式机为client，注意 Server 和 Client 应该在同一局域网内 ServerServer 端的配置过程如下 首先在下图点击设置服务器 出现下图后从右上角将电脑图标拖入网格内，有几台clinet就拖几台，然后进行命名（下图我拖了一台client 并命名为 435LC）这里的命名保持和客户端一致即可。 Client首先要勾选 client 的选项，并输入上图的 Server 的IP，然后需要在编辑中设置屏幕名与 Server 中的屏幕名一致，接着点击启动即可，下面的console 会输出相应的 log 信息 通过这种方式就能够愉快地通过笔记本的键盘和触摸板来控制其他电脑了。 修改快捷键由于笔记本是 mbp，所以复制粘贴用的是 command+c/v, 跟 Windows 的ctrl+c/v 不一样，切换的时候经常搞错，因此这里将 mbp 复制粘贴也改成 ctrl+c/v 如下是打开系统偏好设置-&gt;键盘后的界面，通过 + 号添加相应的快捷键即可 需要注意的是上面的快捷键的名称需要与你系统保持一致，具体可通过任意窗口的菜单中的编辑项查询，如下就是我所用的 mbp 的一个快捷键的名称，比如撤消不能写成撤销，否则快捷键不生效。 文件传输有时候还需要在两台主机上传输一些文件，Synergy 的普通版本不支持这种粘贴复制的传输，但是 pro 版本支持。我这边找不到pro版。。。。。。但是因为我经常要用到 FTP 传输文件， windows 装了 FileZilla 这个FTP 客户端，所以我直接在 windows 上通过 FTP 连到 mbp 上进行传输，在大文件的传输上速度也有保证。 最后祭上一张我的工作台的图片，不得不说 mbp 的触摸板就是好用，有效预防鼠标手～(￣▽￣～)(～￣▽￣)～]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Wand 算法介绍与实现]]></title>
      <url>%2F2018%2F03%2F18%2FWand%20%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
      <content type="text"><![CDATA[本文主要介绍 Wand(Weak And) 算法的原理和实现, Wand 算法是一个搜索算法，应用在 query 有多个关键词或标签，同时每个document 也有多个关键词或标签的情形（如搜索引擎）；尤其是在 query 中的关键词或标签较多的时候，通过 Wand 能够快速的选择出 Top n 个相关的 document，算法的原始论文见 Efficient Query Evaluation using a Two-Level Retrieval Process，本文主要讲述这个算法的原理以及通过 python 实现这个算法。 一般来说，检索往往会利用倒排索引，倒排索引能够根据 query 中的关键词快速检索到候选文档，然而当候选文档集合较大时，遍历整个候选文档所需要的时间也很大。 但是检索需要得到的往往只是 Top n 个结果，在遍历候选文档过程中能否跳过一些与 query 相关性较低的文档，从而加速检索的过程呢？Wand 算法就是干这个事的。 Wand 原理介绍Wand 算法通过计算每个词的贡献上限来估计文档的相关性上限，并与预设的阈值比较，进而跳过一些相关性一定达不到要求的文档，从而得到提速的效果。 上面这句话涵盖了Wand 算法的思想，下面进行详细说明： Wand 算法首先要估计每个词对相关性贡献的上限（upper bound），最简单的相关性就是 TF-IDF，一般IDF是固定的，因此只需要估计一个词在各个文档中的词频TF上限(即这个词在各个文档中最大的TF)，该步骤通过线下计算即可完成。 线下计算出各个词的相关性上限，可以计算出一个 query 和一个文档的相关性上限值，就是他们共同出现的词的相关性上限值的和，通过与预设的阈值比较，如果query 与文档的相关性大于阈值，则进行下一步的计算，否则丢弃。 在上面过程中，如果还是将 query 和一个一个文档分别计算相关性，并没有减少时间复杂度， Wand 算法通过一种巧妙的方式使用倒排索引，从而能够跳过一些相关性肯定达不到要求的文档。 Wand 算法步骤如下 建立倒排索引，记录每个单词所在的所有文档ID(DID)，ID 按照从小到大排序 初始化 posting 数组，使得 posting[pTerm] 为词 pTerm 倒排索引中第一个文档的 index 初始化 curDoc = 0（文档ID从1开始） 接着可以执行下面的 next 函数(摘自原始论文), 上面流程中用到的几个函数的含义如下 1. sort(terms, posting)：根据 posting 数组指向的当前文档 ID，对所有的 terms 从小到大排序。如下是三个 term 及其对应的索引文档的 ID，此时的 posting 数组为 [1, 0, 1], 则根据各个 term 当前文档 ID 排序的结果应该是 t1, t2, t3 t0: [3, 26]t1: [4, 10, 100]t2: [2, 5, 56] 2. findPivotTerm(terms, θ)：按照之前得到的排序，从第一个 term 开始累加各个 term 的相关性贡献的上限（upper bound，UB），这个在之前已经通过离线计算出来；直到累加和大于等于设定的阈值 θ, 返回当前的 term。这里应用这篇文章的一个例子，下面为通过 sort(terms, posting) 后的倒排索引，假设阈值 θ = 8 对于doc 2，其可能的最大得分为28因此，t3 为pivotTerm，doc 23 为pivot 3. pickTerm(terms[0..pTerm])：在0到pTerm(不包含pTerm)中选择一个term，关于选择策略，当然是以可以跳过最多的文档为原则，论文中选择了 IDF 最大的term。以上面的图为例子，此时可以选择 t2, t1 或 t4, 根据其 IDF 值选择最大的 term 即可 4. aterm.iterator.next(n)：返回 aterm 这个单词对应的倒排索引中的文档ID(DID)，这个DID要满足DID &gt;= n。则 posting[aterm] ← aterm.iterator.next(n) 其实就是更新了 aterm 在 posting 数组中的当前文档，从而跳过 aterm 对应的索引中一些不必要计算的文档。 还是以上面的图为例子，假如选择的 aterm 为 t2, 则 t2 中指向 2 的指针要往后移动直至 DID &gt;= 23 ,这样便跳过了部分不必计算文档。 实际上，t1, t4 也可以执行上面这个操作，因为在 doc 23 之前的 doc 的得分不可能达到阈值 θ(因为 DID 是经过排序的) ，所以t2、t1、t4对应的 posting 数组中的项都可以直接跳到大于等于doc23的位置，但是论文中每次只选择一个 term ，虽然多迭代几次也能达到同样效果，但是我认为这里可以三个 Term 可以一起跳。 介绍了上面过程中几个重要函数，下面来看一下上面的几个分支分别表示情况 if (pTerm = null) return (NoMoreDocs)表示当前所有 term 的 upper bound 和达不到阈值 θ ，结束算法 if (pivot = lastID) return (NoMoreDocs) 表示当前已经没有满足相关性大于阈值 θ 的文档，结束算法 if (pivot ≤ curDoc) 表示当前 pivot 指向的 DID 已经计算过相关性，需要跳过，这部分代码会在下面第4步执行后在进入循环时执行 if (posting[0].DID = pivot) 表示当前 pivot 对应的文档的相关性有可能满足大于阈值 θ ，返回这篇文档的 ID 并计算这篇文档和 query 的相关性；posting[0].DID = pivot 表示从第一个term到当前的term所指向的文档都是同一篇 if (posting[0].DID = pivot) 对应的else语句 表示前面遍历过的那些 term 的当前 DID 都不可能满足大于阈值 θ，因此需要跳过，也正是这里大大减少了需要计算相关性的文档数量 Wand 的实现代码实现 Wand 算法的 Python 代码见这里，参考这篇文章的代码进行了修改，并增加了评估文档和query相似性的函数，代码中有以下几点需要注意 当一个 term 对应的所有 document 遍历完后，有两种处理方法。第一种方法是直接删除，这样会降低每次排序的时间复杂度和内存占用率，但是每次删除时候是要在一个有序列表内删除，时间复杂度为 $O(n)$, $n$ 为 terms 的个数；第二种方法是在每个 term 的 document list 最后增加一个比所有文档ID都要大的数(LastID)，这样被遍历完的term会自然被排序到最后，整个代码更加简洁。两种方法都尝试了一下，详细代码可见上面的代码连接的提交历史 pickTerm 方法原论文采用的是选择 idf 最大值的term，这里直接选择第一个，因为代码仅用于阐述算法的流程，各个 term 没有 idf 值。当然，如果有各个 term 的 idf 值，是可以根据 idf 选择的 上面伪代码的算法流程中最后的 else 语句是选择 pivotTerm 中的任意一个并跳过相关性低的文档，但是从前面的解释可知，可以 pivotTerm 前面的所有 term 都可进行这一操作，因此代码里面的这部分跟伪代码不同 这里还是给出完整代码，可以对照着上面的伪代码看，命名方法基本都保持了一致，如有错漏，欢迎指出 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import heapqUB = &#123;"t0":0.5,"t1":1,"t2":2,"t3":3,"t4":4&#125; #upper bound of term's valueLAST_ID = 999999999999 # a large number, larger than all the doc id in the inverted indexTHETA = 2 # theta, threshold for chechking whether to calculate the relevence between query and docTOPN = 3 #max result number class WAND: def __init__(self, InvertIndex): """init inverted index and necessary variable""" self.result_list = [] #result list self.inverted_index = InvertIndex #InvertIndex: term -&gt; docid1, docid2, docid3 ... self.current_doc = 0 self.current_inverted_index = &#123;&#125; #posting self.query_terms = [] self.sort_terms = [] self.threshold = THETA self.last_id = LAST_ID def __init_query(self, query_terms): """init variable with query""" self.current_doc = 0 self.current_inverted_index = &#123;&#125; self.query_terms = [] self.sort_terms = [] for term in query_terms: if term in self.inverted_index: # terms may not appear in inverted_index doc_id = self.inverted_index[term][0] self.query_terms.append(term) self.current_inverted_index[term] = [doc_id, 0] #[ docid, index ] self.sort_terms.append([doc_id, term]) def __pick_term(self, pivot_index): """select the term before pivot_index in sorted term list paper recommends returning the term with max idf, here we just return the firt term, also return the index of the term instead of the term itself for speeding up""" return 0 def __find_pivot_term(self): """find pivot term""" score = 0 for i in range(len(self.sort_terms)): score += UB[self.sort_terms[i][1]] if score &gt;= self.threshold: return [self.sort_terms[i][1], i] #[term, index] return [None, len(self.sort_terms)] def __iterator_invert_index(self, change_term, docid, pos): """find the new_doc_id in the doc list of change_term such that new_doc_id &gt;= docid, if no new_doc_id satisfy, the self.last_id""" doc_list = self.inverted_index[change_term] # new_doc_id, new_pos = self.last_id, len(doc_list)-1 # the case when new_doc_id not exists for i in range(pos, len(doc_list)): if doc_list[i] &gt;= docid: # since doc_list contains self.last_id, this inequation will always be satisfied new_pos = i new_doc_id = doc_list[i] break return [new_doc_id, new_pos] def __advance_term(self, change_index, doc_id ): """change the first doc of term self.sort_terms[change_index] in the current inverted index return whether the action succeed or not""" change_term = self.sort_terms[change_index][1] pos = self.current_inverted_index[change_term][1] new_doc_id, new_pos = self.__iterator_invert_index(change_term, doc_id, pos) self.current_inverted_index[change_term] = [new_doc_id, new_pos] self.sort_terms[change_index][0] = new_doc_id def __next(self): while True: self.sort_terms.sort() #sort terms by doc id pivot_term, pivot_index = self.__find_pivot_term() #find pivot term &gt; threshold if pivot_term == None: #no more candidate return None pivot_doc_id = self.current_inverted_index[pivot_term][0] if pivot_doc_id == self.last_id: # no more candidate return None if pivot_doc_id &lt;= self.current_doc: change_index = self.__pick_term(pivot_index) self.__advance_term(change_index, self.current_doc + 1) else: first_doc_id = self.sort_terms[0][0] if pivot_doc_id == first_doc_id: self.current_doc = pivot_doc_id return self.current_doc # return the doc for fully calculating else: # pick all preceding term instead of just one, then advance all of them to pivot change_index = 0 while change_index &lt; pivot_index: self.__advance_term(change_index, pivot_doc_id) change_index += 1 # print(self.sort_terms, self.current_doc, pivot_doc_id) def __insert_heap(self, doc_id, score): """store the Top N result""" if len(self.result_list) &lt; TOPN: heapq.heappush(self.result_list, (score, doc_id)) else: heapq.heappushpop(self.result_list, (score, doc_id)) def __calculate_doc_relevence(self, docid): """fully calculate relevence between doc and query""" score = 0 for term in self.query_terms: if docid in self.inverted_index[term]: score += UB[term] return score def perform_query(self, query_terms): self.__init_query(query_terms) while True: candidate_docid = self.__next() if candidate_docid == None: break #insert candidate_docid to heap print('candidata doc', candidate_docid) full_doc_score = self.__calculate_doc_relevence(candidate_docid) self.__insert_heap(candidate_docid, full_doc_score) print("result list ", self.result_list) return self.result_listif __name__ == "__main__": testIndex = &#123;&#125; testIndex["t0"] = [1, 3, 26, LAST_ID] testIndex["t1"] = [1, 2, 4, 10, 100, LAST_ID] testIndex["t2"] = [2, 3, 6, 34, 56, LAST_ID] testIndex["t3"] = [1, 4, 5, 23, 70, 200, LAST_ID] testIndex["t4"] = [5, 14, 78, LAST_ID] w = WAND(testIndex) final_result = w.perform_query(["t0", "t1", "t2", "t3", "t4"]) print("=================final result=======================") for i in reversed(range(len(final_result))): print("doc &#123;0&#125;, relevence score &#123;1&#125;".format(final_result[i][1], final_result[i][0])) 参考资料 wand(weak and)算法基本思路WAND算法核心部分梳理]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大规模机器学习框架的四重境界]]></title>
      <url>%2F2018%2F03%2F10%2F%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%9A%84%E5%9B%9B%E9%87%8D%E5%A2%83%E7%95%8C%2F</url>
      <content type="text"><![CDATA[文章为转载，原文链接见这里，作者是 carbon zhang。这篇文章主要介绍了分布式机器学习中的若干重点概念和经典论文，包括数据并行和模型并行、分布式框架的流派、参数服务器以及同步协议的演进等，非常值得一看。 背景自从google发表著名的 GFS、MapReduce、BigTable 三篇paper以后，互联网正式迎来了大数据时代。大数据的显著特点是大，哪里都大的大。本篇主要针对volume大的数据时，使用机器学习来进行数据处理过程中遇到的架构方面的问题做一个系统的梳理。 有了GFS我们有能力积累海量的数据样本，比如在线广告的曝光和点击数据，天然具有正负样本的特性，累积一两个月往往就能轻松获得百亿、千亿级的训练样本。这样海量的样本如何存储？用什么样的模型可以学习海量样本中有用的pattern？这些问题不止是工程问题，也值得每个做算法的同学去深入思考。 简单模型or复杂模型在深度学习概念提出之前，算法工程师手头能用的工具其实并不多，就LR、SVM、感知机等寥寥可数、相对固定的若干个模型和算法；那时候要解决一个实际的问题，算法工程师更多的工作主要是在特征工程方面。而特征工程本身并没有很系统化的指导理论（至少目前没有看到系统介绍特征工程的书籍），所以很多时候特征的构造技法显得光怪陆离，是否有用也取决于问题本身、数据样本、模型以及运气。 在特征工程作为算法工程师主要工作内容的时候，构造新特征的尝试往往很大部分都不能在实际工作中work。据我了解，国内几家大公司在特征构造方面的成功率在后期一般不会超过20%。也就是80%的新构造特征往往并没什么正向提升效果。如果给这种方式起一个名字的话，大概是简单模型+复杂特征；简单模型说的是算法比如LR、SVM本身并不服务，参数和表达能力基本呈现一种线性关系，易于理解。复杂特征则是指特征工程方面不断尝试使用各种奇技淫巧构造的可能有用、可能没用的特征，这部分特征的构造方式可能会有各种trick，比如窗口滑动、离散化、归一化、开方、平方、笛卡尔积、多重笛卡尔积等等；顺便提一句，因为特征工程本身并没有特别系统的理论和总结，所以初入行的同学想要构造特征就需要多读paper，特别是和自己业务场景一样或类似的场景的paper，从里面学习作者分析、理解数据的方法以及对应的构造特征的技法；久而久之，有望形成自己的知识体系。 深度学习概念提出以后，人们发现通过深度神经网络可以进行一定程度的表示学习（representation learning），例如在图像领域，通过CNN提取图像feature并在此基础上进行分类的方法，一举打破了之前算法的天花板，而且是以极大的差距打破。这给所有算法工程师带来了新的思路，既然深度学习本身有提取特征的能力，干嘛还要苦哈哈的自己去做人工特征设计呢？ 深度学习虽然一定程度上缓解了特征工程的压力，但这里要强调两点： 缓解并不等于彻底解决，除了图像这种特定领域，在个性化推荐等领域，深度学习目前还没有完全取得绝对的优势；究其原因，可能还是数据自身内在结构的问题，使得在其他领域目前还没有发现类似图像+CNN这样的完美CP。 深度学习在缓解特征工程的同时，也带来了模型复杂、不可解释的问题。算法工程师在网络结构设计方面一样要花很多心思来提升效果。概括起来，深度学习代表的简单特征+复杂模型是解决实际问题的另一种方式。 两种模式孰优孰劣还难有定论，以点击率预测为例，在计算广告领域往往以海量特征+LR为主流，根据VC维理论，LR的表达能力和特征个数成正比，因此海量的feature也完全可以使LR拥有足够的描述能力。而在个性化推荐领域，深度学习刚刚萌芽，目前google play采用了WDL的结构[1]，youtube采用了双重DNN的结构[2]。 不管是那种模式，当模型足够庞大的时候，都会出现模型参数一台机器无法存放的情况。比如百亿级 feature 的LR对应的权重w有好几十个G，这在很多单机上存储都是困难的，大规模神经网络则更复杂，不仅难以单机存储，而且参数和参数之间还有逻辑上的强依赖；要对超大规模的模型进行训练势必要借用分布式系统的技法，本文主要是系统总结这方面的一些思路。 数据并行vs模型并行数据并行和模型并行是理解大规模机器学习框架的基础概念，其缘起未深究，第一次看到是在姐夫（Jeff Dean）的blog里，当时匆匆一瞥，以为自己懂了。多年以后，再次开始调研这个问题的时候才想起长者的教训，年轻人啊，还是图样图森破。如果你和我一样曾经忽略过这个概念，今天不放复习一下。 这两个概念在这个问题中沐帅曾经给出了一个非常直观而经典的解释，可惜不知道什么原因，当我想引用时却发现已经被删除了。我在这里简单介绍下这个比喻：如果要修两栋楼，有一个工程队，怎么操作？第一个方案是将人分成两组，分别盖楼，改好了就装修；第二种做法是一组人盖楼，等第一栋楼盖好，另一组装修第一栋，然后第一组继续盖第二栋楼，改完以后等装修队装修第二栋楼。咋一看，第二种方法似乎并行度并不高，但第一种方案需要每个工程人员都拥有“盖楼”和“装修”两种能力，而第二个方案只需要每个人拥有其中一种能力即可。第一个方案和数据并行类似，第二个方案则道出了模型并行的精髓。 数据并行理解起来比较简单，当样本比较多的时候，为了使用所有样本来训练模型，我们不妨把数据分布到不同的机器上，然后每台机器都来对模型参数进行迭代，如下图所示 图片取材于TensorFlow的paper[3]，图中ABC代表三台不同的机器，上面存储着不同的样本，模型 P 在各台机器上计算对应的增量，然后在参数存储的机器上进行汇总和更新，这就是数据并行。先忽略synchronous，这是同步机制相关的概念，在第三节会有专门介绍。 数据并行概念简单，而且不依赖于具体的模型，因此数据并行机制可以作为框架的一种基础功能，对所有算法都生效。与之不同的是，模型并行因为参数间存在依赖关系（其实数据并行参数更新也可能会依赖所有的参数，但区别在于往往是依赖于上一个迭代的全量参数。而模型并行往往是同一个迭代内的参数之间有强依赖关系，比如DNN网络的不同层之间的参数依照BP算法形成的先后依赖），无法类比数据并行这样直接将模型参数分片而破坏其依赖关系，所以模型并行不仅要对模型分片，同时需要调度器来控制参数间的依赖关系。而每个模型的依赖关系往往并不同，所以模型并行的调度器因模型而异，较难做到完全通用。关于这个问题，CMU的Erix Xing在这里有所介绍，感兴趣的可以参考。 模型并行的问题定义可以参考姐夫的[4]，这篇paper也是tensorflow的前身相关的总结，其中图 解释了模型并行的物理图景，当一个超大神经网络无法存储在一台机器上时，我们可以切割网络存到不同的机器上，但是为了保持不同参数分片之间的依赖，如图中粗黑线的部分，则需要在不同的机器之间进行concurrent控制；同一个机器内部的参数依赖，即途中细黑线部分在机器内即可完成控制。 黑线部分如何有效控制呢？如下图所示 在将模型切分到不同机器以后，我们将参数和样本一起在不同机器间流转，途中 ABC 代表模型的不同部分的参数；假设C依赖B，B依赖A，机器1上得到A的一个迭代后，将A和必要的样本信息一起传到机器2，机器2根据A和样本对P2更新得到，以此类推；当机器2计算B的时候，机器1可以展开A的第二个迭代的计算。了解CPU流水线操作的同学一定感到熟悉，是的，模型并行是通过数据流水线来实现并行的。想想那个盖楼的第二种方案，就能理解模型并行的精髓了。 上图则是对控制模型参数依赖的调度器的一个示意图，实际框架中一般都会用DAG（有向无环图）调度技术来实现类似功能，未深入研究，以后有机会再补充说明。 理解了数据并行和模型并行对后面参数服务器的理解至关重要，但现在让我先荡开一笔，简单介绍下并行计算框架的一些背景信息。 并行算法演进MapReduce路线从函数式编程中的受到启发，google发布了MapReduce[5]的分布式计算方式；通过将任务切分成多个叠加的Map+Reduce任务，来完成复杂的计算任务，示意图如下 MapReduce的主要问题有两个，一是原语的语义过于低级，直接使用其来写复杂算法，开发量比较大；另一个问题是依赖于磁盘进行数据传递，性能跟不上业务需求。 为了解决MapReduce的两个问题，Matei在[6]中提出了一种新的数据结构RDD，并构建了Spark框架。Spark框架在MR语义之上封装了DAG调度器，极大降低了算法使用的门槛。较长时间内spark几乎可以说是大规模机器学习的代表，直至后来沐帅的参数服务器进一步开拓了大规模机器学习的领域以后，spark才暴露出一点点不足。如下图 从图中可以看出，Spark框架以Driver为核心，任务调度和参数汇总都在driver，而driver是单机结构，所以spark的瓶颈非常明显，就在Driver这里。当模型规模大到一台机器存不下的时候，Spark就无法正常运行了。所以从今天的眼光来看，Spark只能称为一个中等规模的机器学习框架。剧透一句，公司开源的 Angel 通过修改Driver的底层协议将Spark扩展到了一个高一层的境界。后面还会再详细介绍这部分。 MapReduce不仅是一个框架，还是一种思想，google开创性的工作为我们找到了大数据分析的一个可行方向，时至今日，仍不过时。只是逐渐从业务层下沉到底层语义应该处于的框架下层。 MPI技术沐帅在这个问题中对MPI的前景做了简要介绍；和Spark不同，MPI是类似socket的一种系统通信API，只是支持了消息广播等功能。因为对MPI研究不深入，这里简单介绍下优点和缺点吧；优点是系统级支持，性能杠杠的；缺点也比较多，一是和MR一样因为原语过于低级，用MPI写算法，往往代码量比较大。另一方面是基于MPI的集群，如果某个任务失败，往往需要重启整个集群，而MPI集群的任务成功率并不高。阿里在[7]中给出了下图： 从图中可以看出，MPI作业失败的几率接近五成。MPI也并不是完全没有可取之处，正如沐帅所说，在超算集群上还是有场景的。对于工业届依赖于云计算、依赖于commodity计算机来说，则显得性价比不够高。当然如果在参数服务器的框架下，对单组worker再使用MPI未尝不是个好的尝试，[7]中的鲲鹏系统正式这么设计的。 参数服务器历史演进沐帅在[8]中将参数服务器的历史划分为三个阶段，第一代参数服务器萌芽于沐帅的导师Smola的 [9] ，如下图所示： 这个工作中仅仅引入memcached来存放key-value数据，不同的处理进程并行对其进行处理。[10]中也有类似的想法，第二代参数服务器叫application-specific参数服务器，主要针对特定应用而开发，其中最典型的代表应该是tensorflow的前身4。 第三代参数服务器，也即是通用参数服务器框架是由百度少帅李沐正式提出的，和前两代不同，第三代参数服务器从设计上就是作为一个通用大规模机器学习框架来定位的。要摆脱具体应用、算法的束缚，做一个通用的大规模机器学习框架，首先就要定义好框架的功能；而所谓框架，往往就是把大量重复的、琐碎的、做了一次就不想再来第二次的脏活、累活进行良好而优雅的封装，让使用框架的人可以只关注与自己的核心逻辑。第三代参数服务器要对那些功能进行封装呢？沐帅总结了这几点，我照搬如下： 1）高效的网络通信：因为不管是模型还是样本都十分巨大，因此对网络通信的高效支持以及高配的网络设备都是大规模机器学习系统不可缺少的； 2）灵活的一致性模型：不同的一致性模型其实是在模型收敛速度和集群计算量之间做tradeoff；要理解这个概念需要对模型性能的评价做些分析，暂且留到下节再介绍。 3）弹性可扩展：显而易见 4）容灾容错：大规模集群协作进行计算任务的时候，出现Straggler或者机器故障是非常常见的事，因此系统设计本身就要考虑到应对；没有故障的时候，也可能因为对任务时效性要求的变化而随时更改集群的机器配置。这也需要框架能在不影响任务的情况下能做到机器的热插拔。 5）易用性：主要针对使用框架进行算法调优的工程师而言，显然，一个难用的框架是没有生命力的。 在正式介绍第三代参数服务器的主要技术之前，先从另一个角度来看下大规模机器学习框架的演进 这张图可以看出，在参数服务器出来之前，人们已经做了多方面的并行尝试，不过往往只是针对某个特定算法或特定领域，比如 YahooLDA 是针对LDA算法的。当模型参数突破十亿以后，则可以看出参数服务器一统江湖，再无敌手。 首先我们看看第三代参数服务器的基本架构 上图的 resource manager 可以先放一放，因为实际系统中这部分往往是复用现有的资源管理系统，比如yarn、mesos或者k8s；底下的training data毋庸置疑的需要类似GFS的分布式文件系统的支持；剩下的部分就是参数服务器的核心组件了。 图中画了一个 server group 和三个worker group；实际应用中往往也是类似，server group 用一个，而worker group按需配置；server manager 是server group中的管理节点，一般不会有什么逻辑，只有当有server node加入或退出的时候，为了维持一致性哈希而做一些调整。 Worker group中的task schedule则是一个简单的任务协调器，一个具体任务运行的时候，task schedule负责通知每个worker加载自己对应的数据，然后去server node上拉取一个要更新的参数分片，用本地数据样本计算参数分片对应的变化量，然后同步给server node；server node在收到本机负责的参数分片对应的所有worker的更新后，对参数分片做一次update。 这里存在的一个问题就是不同的worker同时并行运算的时候，可能因为网络、机器配置等外界原因，导致不同的worker的进度是不一样的，如何控制worker的同步机制是一个比较重要的课题。详见下节分解。 同步协议本节假设读者已经对随机梯度优化算法比较熟悉，如果不熟悉的同学请参考吴恩达经典课程机器学习中对SGD的介绍，或者我之前多次推荐过的书籍《最优化导论》。 我们先看一个单机算法的运行过程，假设一个模型的参数切分成三个分片k1，k2，k3；比如你可以假设是一个逻辑回归算法的权重向量被分成三段。我们将训练样本集合也切分成三个分片s1，s2，s3；在单机运行的情况下，我们假设运行的序列是（k1，s1）、（k2，s1）、（k3、s1）、（k1、s2）、（k2、s2）、（k3、s2）。。。看明白了吗？就是假设先用s1中的样本一次对参数分片k1、k2、k3进行训练，然后换s2；这就是典型的单机运行的情况，而我们知道这样的运行序列最后算法会收敛。 现在我们开始并行化，假设k1、k2、k3分布在三个server node上，s1、s2、s3分布在三个worker上，这时候如果我们还要保持之前的计算顺序，则会变成怎样？work1计算的时候，work2和worker3只能等待，同样worker2计算的时候，worker1和work3都得等待，以此类推；可以看出这样的并行化并没有提升性能；但是也算简单解决了超大规模模型的存储问题。 为了解决性能的问题，业界开始探索这里的一致性模型，最先出来的版本是前面提到的9中的ASP模式，就是完全不顾worker之间的顺序，每个worker按照自己的节奏走，跑完一个迭代就update，然后继续，这应该是大规模机器学习中的freestyle了，如图所示 ASP的优势是最大限度利用了集群的计算能力，所有的worker所在的机器都不用等待，但缺点也显而易见，除了少数几个模型，比如LDA，ASP协议可能导致模型无法收敛。也就是SGD彻底跑飞了，梯度不知道飞到哪里去了。 在ASP之后提出了另一种相对极端的同步协议BSP，Spark用的就是这种方式，如图所示 每个worker都必须在同一个迭代运行，只有一个迭代任务所有的worker都完成了，才会进行一次worker和server之间的同步和分片更新。这个算法和严格一致的算法非常类似，区别仅仅在于单机版本的batch size在BSP的时候变成了有所有worker的单个batch size求和得到的总的butch size替换。毫无疑问，BSP的模式和单机串行因为仅仅是batch size的区别，所以在模型收敛性上是完全一样的。同时，因为每个worker在一个周期内是可以并行计算的，所以有了一定的并行能力。 以此协议为基础的spark在很长时间内成为机器学习领域实际的霸主，不是没有理由的。此种协议的缺陷之处在于，整个worker group的性能由其中最慢的worker决定，这个worker一般称为straggler。读过GFS文章的同学应该都知道straggler的存在是非常普遍的现象。 能否将ASP和BSP做一下折中呢？答案当然是可以的，这就是目前我认为最好的同步协议SSP；SSP的思路其实很简单，既然ASP是允许不同worker之间的迭代次数间隔任意大，而BSP则只允许为0，那我是否可以取一个常数s？如图所示 不同的worker之间允许有迭代的间隔，但这个间隔数不允许超出一个指定的数值s，图中s=3. SSP协议的详细介绍参见[11]，CMU的大拿Eric Xing在其中详细介绍了SSP的定义，以及其收敛性的保证。理论推导证明常数s不等于无穷大的情况下，算法一定可以在若干次迭代以后进入收敛状态。其实在Eric提出理论证明之前，工业界已经这么尝试过了 顺便提一句，考察分布式算法的性能，一般会分为 statistical performance 和 hard performance 来看, 前者指不同的同步协议导致算法收敛需要的迭代次数的多少，后者是单次迭代所对应的耗时。两者的关系和precision\recall关系类似，就不赘述了。有了SSP，BSP就可以通过指定s=0而得到。而ASP同样可以通过制定s=无穷大来达到。 核心技术除了参数服务器的架构、同步协议之外，本节再对其他技术做一个简要的介绍，详细的了解请直接阅读沐帅的博士论文和相关发表的论文。 热备、冷备技术：为了防止server node挂掉，导致任务中断，可以采用两个技术，一个是对参数分片进行热备，每个分片存储在三个不同的server node中，以master-slave的形式存活。如果master挂掉，可以快速从slave获取并重启相关task。 除了热备，还可以定时写入checkpoint文件到分布式文件系统来对参数分片及其状态进行备份。进一步保证其安全性。 Server node管理：可以使用一致性哈希技术来解决server node的加入和退出问题，如图所示 当有server node加入或退出的时候，server manager负责对参数进行重新分片或者合并。注意在对参数进行分片管理的情况下，一个分片只需要一把锁，这大大提升了系统的性能，也是参数服务器可以实用的一个关键点。 大规模机器学习的四重境界到这里可以回到我们的标题了，大规模机器学习的四重境界到底是什么呢？ 这四重境界的划分是作者个人阅读总结的一种想法，并不是业界标准，仅供大家参考。 境界1：参数可单机存储和更新此种境界较为简单，但仍可以使用参数服务器，通过数据并行来加速模型的训练。 境界2：参数不可单机存储，可以单机更新此种情况对应的是一些简单模型，比如 sparse logistic regression；当feature的数量突破百亿的时候，LR的权重参数不太可能在一台机器上完全存下，此时必须使用参数服务器架构对模型参数进行分片。但是注意一点，SGD的更新公式可以分开到单个维度进行计算，但是单个维度也是需要使用到上一轮迭代的所有参数(即计算预测值$f(w)$)。而我们之所以对参数进行分片就是因为我们无法将所有参数存放到一台机器，现在单个worker有需要使用所有的参数才能计算某个参数分片的梯度，这不是矛盾吗？可能吗？ 答案是可能的，因为单个样本的feature具有很高的稀疏性（sparseness）。例如一个百亿feature的模型，单个训练样本往往只在其中很小一部分feature上有取值，其他都为0（假设feature取值都已经离散化了）。 因此计算$f(w)$的时候可以只拉取不为0的feature对应的那部分w即可。有文章统计一般这个级别的系统，稀疏性往往在0.1%（or 0.01%，记得不是很准，大致这样）以下。这样的稀疏性，可以让单机没有任何阻碍的计算$f(w)$。 目前公司开源的angel和AILab正在做的系统都处于这个境界。而原生spark还没有达到这个境界，只能在中小规模的圈子里厮混。Angel改造的基于Angel的Spark则达到了这个境界。 境界3：参数不可单机存储，不可单机更新，但无需模型并行境界3顺延境界2二来，当百亿级feature且feature比较稠密的时候，就需要计算框架进入到这层境界了，此时单个worker的能力有限，无法完整加载一个样本，也无法完整计算$f(w)$。怎么办呢？其实很简单，学过线性代数的都知道，矩阵可以分块。向量是最简单的矩阵，自然可以切成一段一段的来计算。只是调度器需要支持算符分段而已了。 境界4：参数不可单机存储，不可单机更新，需要模型并行进入到这个层次的计算框架，可以算是世界一流了。可以处理超大规模的神经网络。这也是最典型的应用场景。此时不仅模型的参数不能单机存储，而且同一个迭代内，模型参数之间还有强的依赖关系，可以参见姐夫对 distbelief 的介绍里的模型切分。 此时首先需要增加一个coordinator组件来进行模型并行的concurrent控制。同时参数服务器框架需要支持namespace切分，coordinator将依赖关系通过namespace来进行表示。 一般参数间的依赖关系因模型而已，所以较难抽象出通用的coordinator来，而必须以某种形式通过脚本parser来生产整个计算任务的DAG图，然后通过DAG调度器来完成。对这个问题的介绍可以参考Erix Xing的分享。 Tensorflow目前业界比较知名的深度学习框架有Caffee、MXNet、Torch、Keras、Theano等，但目前最炙手可热的应该是google发布的Tensorflow。这里单独拿出来稍微分解下。 前面不少图片引自此文，从TF的论文来看，TF框架本身是支持模型并行和数据并行的，内置了一个参数服务器模块，但从开源版本所曝光的API来看，TF无法用来10B级别feature的稀疏LR模型。原因是已经曝光的API只支持在神经网络的不同层和层间进行参数切分，而超大规模LR可以看做一个神经单元，TF不支持单个神经单元参数切分到多个参数服务器node上。 当然，以google的实力，绝对是可以做到第四重境界的，之所以没有曝光，可能是基于其他商业目的的考量，比如使用他们的云计算服务。 综上，个人认为如果能做到第四重境界，目前可以说的上是世界一流的大规模机器学习框架。仅从沐帅的ppt里看他曾经达到过，google内部应该也是没有问题的。第三重境界应该是国内一流，第二充应该是国内前列吧。 其他资源管理本文没有涉及到的部分是资源管理，大规模机器学习框架部署的集群往往资源消耗也比较大，需要专门的资源管理工具来维护。这方面yarn和mesos都是佼佼者，细节这里也就不介绍了。 设备除了资源管理工具，本身部署大规模机器学习集群本身对硬件也还是有些要求的，虽然理论上来说，所有commodity机器都可以用来搭建这类集群，但是考虑到性能，我们建议尽量用高内存的机器+万兆及以上的网卡。没有超快速的网卡，玩参数传递和样本加载估计会比较苦逼。 参考文献[1] Cheng H T, Koc L, Harmsen J, et al. Wide &amp; deep learning for recommender systems[C]//Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. ACM, 2016: 7-10. [2] Covington P, Adams J, Sargin E. Deep neural networks for youtube recommendations[C]//Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 2016: 191-198. [3] Abadi M, Agarwal A, Barham P, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems[J]. arXiv preprint arXiv:1603.04467, 2016. [4] Dean J, Corrado G, Monga R, et al. Large scale distributed deep networks[C]//Advances in neural information processing systems. 2012: 1223-1231. [5] Dean J, Ghemawat S. MapReduce: simplified data processing on large clusters[J]. Communications of the ACM, 2008, 51(1): 107-113. [6] Zaharia M, Chowdhury M, Das T, et al. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing[C]//Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012: 2-2. [7] Zhou J, Li X, Zhao P, et al. KunPeng: Parameter Server based Distributed Learning Systems and Its Applications in Alibaba and Ant Financial[C]//Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017: 1693-1702. [8] Li M, Andersen D G, Park J W, et al. Scaling Distributed Machine Learning with the Parameter Server[C]//OSDI. 2014, 14: 583-598. [9] Smola A, Narayanamurthy S. An architecture for parallel topic models[J]. Proceedings of the VLDB Endowment, 2010, 3(1-2): 703-710. [10] Power R, Li J. Piccolo: Building Fast, Distributed Programs with Partitioned Tables[C]//OSDI. 2010, 10: 1-14. [11] Ho Q, Cipar J, Cui H, et al. More effective distributed ml via a stale synchronous parallel parameter server[C]//Advances in neural information processing systems. 2013: 1223-1231.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式机器学习(4)-Implement Your MapReduce]]></title>
      <url>%2F2018%2F02%2F24%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(4)-Implement%20Your%20MapReduce%2F</url>
      <content type="text"><![CDATA[提到 MapReduce，很自然想到的是 Hadoop MapReduce ，但是 MapReduce 只是一个编程范式，而 Hadoop MapReduce 则是这个编程范式的一个比较出名的实现。实际上，可以通过多种方式实现 MapReduce，本文要介绍的就是如何在 Linux 的 bash 下实现一个 MapReduce 程序，并且分别实现了单机版本和多机器版本。原视频见这里，需要自备梯子。 下面以 MapReduce 中的经典例子 WordCount 为例进行讲述, 先实现单机版本，再实现多机版本 单机版 MapReduce下面是在 bash 下通过 MapReduce 范式实现的单机版本的 WordCount 程序 123456789101112text=$(cat &lt;&lt;EOFThis is my CupIt is not your cupMy cup is whiteYour cup is blueEOF)echo $text\| awk '&#123;for (i=0; i&lt;=NF; i++) print $i, 1&#125;' \| sort \| awk '&#123;if ($1 != prev) &#123;print prev, c; c=0; prev=$1&#125; c+=$2&#125;' 上面的 bash 脚本有几个需要了解的语法细节 cat &lt;&lt;EOF 命令在 bash 中主要用于处理与实时多行string相关的任务，实时指的是多行string要在命令执行的时候输入（遇到 EOF 结束），这个命令一般可用于以下几个任务 1. 将变量的值赋为多行 string2. 将多行 string 写入文件3. 将多行 string 传入管道命令 上面的代码中就是将多行的 string赋值给变量 text，这三个任务的例子可参考这里 awk 是一门语言，也是 Linux 下一个常用工具，用于处理文本相关的数据，尤其是表格类的数据。awk 会逐行处理文本直至遍历完整个输入流（可以是标准输入流，也可以是文件流，上面的代码是标准输入流），每一行默认根据空格或tab分割文本为若干的 fields，从下标 1 开始，$1 表示第一个 field 的值，其他同理；NF 则是 awk 中特殊变量，表示这一行共有几个 field。 awk 对每行的操作是包含在 {} 中的命令。 因此，上面的代码中第一个 awk 实现了 map 过程，sort 实现了 shuffling 的过程，而第二个 awk 实现了 reduce 过程。 多机器版本 MapReduce上面是一个单机版本的 MapReduce，然而 MapReduce 在多台机器上更能显示其威力。多机器版本的 MapReduce 首先要考虑的是不同机器间的通信问题，这里采用的是 ssh 通信方式。 ssh 除了可以开一个远程机器的 shell 外，还可以通过命令直接在远程机器起一个进程来运行指定程序。如运行下面的代码会在本地机器上显示 hello world echo &quot;hello world&quot; | ssh 192.168.1.10 &#39;cat&#39; 其通信过程首先是本机通过 ssh 连接到远程机器上，同时将 hello world 作为输入流传到远程机器，远程机器上的 sshd 进程截获了输入流，同时启动 cat 进程读取输入流，并将输出流返回给本地机器，本地机器的 sshd 进程同样会截获输出流，然后在本地机器输出。 这种分布式通信的模式在各个分布式系统中（yarn，mesos，k8s）都非常常见，每个节点都要有一个 deamon 与其他节点进行通信并进行资源管理，在这里 sshd 就相当于 daemon，只是没有资源管理功能，但是基本原理是一样的。 因此，利用这种方式，可将map过程放到其他机器上，如下代码所示就是将 awk &#39;{for (i=0; i&lt;=NF; i++) print $i, 1}&#39; 这段程序放到了 192.168.1.10 这台机器上执行。 123456789101112text=$(cat &lt;&lt;EOFThis is my CupIt is not your cupMy cup is whiteYour cup is blueEOF)echo $text\| ssh 192.168.1.10 'awk '&#123;for (i=0; i&lt;=NF; i++) print $i, 1&#125;'' \| sort \| awk '&#123;if ($1 != prev) &#123;print prev, c; c=0; prev=$1&#125; c+=$2&#125;' 因此可将 map 过程放到其他机器上执行，并将结果存储在其他机器上，因为默认一台机器无法存储所有的数据，而输入的数据也是分布在各台机器上，这个过程具体代码如下所示 12345Map = '&#123;for (i=0; i&lt;=NF; i++) print $i, 1&#125;'ssh worker1 'awk $Map /input*.txt &gt; /tmp/o1 &amp;&amp; echo worker1 ok'ssh worker2 'awk $Map /input*.txt &gt; /tmp/o1 &amp;&amp; echo worker2 ok'ssh worker3 'awk $Map /input*.txt &gt; /tmp/o1 &amp;&amp; echo worker3 ok' 上面的代码分别令三个 worker 处理它们本地的文件并将处理后的文件存储在本地，在处理完后返回消息。注意 awk $Map /input*.txt &gt; /tmp/o1 &amp;&amp; echo worker1 ok 需要用分号括起来，表示整条命令都在远程机器执行。 上面通过 ssh 启动远程程序时，一般会配置密钥访问，从而避免每次都要输入密码。 除了 map 过程，shuffling 过程也需要分布式执行，原因是数据无法容纳在一台机器上。之前shuffling 操作是对所有的数据进行 sort 操作，现在这种方案显然行不通，实际上 shuffling 的一个目的是将相同的 key 交给相同的worker进行处理。因此可以采取下面的方法进行分布式 shuffling 假设有 n 个 reduce worker，则每个 map worker 对其所处理的数据的每条 key,value 记录进行 hash(key)%n 操作，记取模后的值为 i (0&lt;=i &lt; n),并将记录写入到本地第 i 个文件中，则最多会在本地生成 n 个文件，然后分别将这 n 个文件远程复制(scp 等)到 n 个reduce worker的机器上。这样就会令相同的 key 被同一个 worker 处理。reduce worker 只需要对复制到其机器上的若干个文件进行 sort 和 reduce 操作即可，reduce 后的结果也是存储在各台机器上的（也可以考虑存放在一台机器，如果经过 MapReduce 后的数据量能够存放在一台机器上）。 上面的过程需要注意以下两点 map 和 shuffle 可以重叠，但是 map 和 reduce 不能重叠。map 和 shuffling 的重叠方法有很多，其中的一种是每个 map worker 通过上面的方法生成 n 个文件时，不是一次性将所有的 record 传送给 reduce worker，而是达到一定数量后就复制，reduce worker 端则通过插入排序进行 sort 操作，每次接收到 map worker 传过来的文件时，就在已排序的序列上进行插入排序 某个 worker 的可能会被比其他的要慢很多，可能原因 load balance 问题，也就是分到这个 worker 的 record 数量太多，可以对这些 record 进行进一步的切分，但是要保证同一个 key 需要被同一个 reduce worker 处理。 Github 上有个 bashreduce 的项目就是在 bash 上实现了 MapReduce，思路与我们前面讲的差不多，只是还考虑了很多其他细节。 作者本人也实现了一个 C++ 版本的 mapreduce-lite，没有考虑存储问题，速度较快，感兴趣可参考。 另外，Hadoop 项目中也有 Hadoop Streaming，允许用于用其他语言实现 MapReduce 操作，只要指定好 mapper 和 reducer 即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式机器学习(3)-Application Driven]]></title>
      <url>%2F2018%2F02%2F18%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(3)-Application%20Driven%2F</url>
      <content type="text"><![CDATA[本文主要介绍了互联网几项重要业务（在线广告，推荐系统，搜索引擎）背后所需的一项共同技术：语义理解(semantic understanding)，同时介绍了实现语义理解的若干种方法：包括矩阵分解，主题模型(Topic Models)等。原视频见这里，需要自备梯子。 semantic understanding 支持的业务在线广告作者以其在腾讯参与开发的 Peacock 系统为例介绍了在线广告所需的语义理解技术，如下是 peacock 系统对 “红酒木瓜汤” 这个查询关键词返回的语义理解的结果 最下面的 topics 是从 query 中推断出来的主题，每行为一个 topic，weight 是 query 属于这个 topic 的概率，topic_words 则是这个 topic 中的词语，且按照与query的相关度从高到低进行了排序。 如果将广告主的广告描述生成同样的 topics 分布，则可以将 query 和广告对应起来。Peacock 系统是基于 LDA(Latent Dirichlet allocation) 开发的，发表的具体论文是 Peacock: Learning Long-Tail Topic Features for Industrial Applications. 除了用户的 query，同样可得出用户浏览的内容的 topic 分布，进而根据用户的浏览内容为其推荐关联的广告。 推荐系统推荐系统中最耳熟能详的算法是协同过滤，通过 users-items 矩阵中的 user 向量衡量 users 之间的相似性，通过 item 向量衡量 items 之间的相似性，具体可参考这篇文章。但是遇到下面这种情况时，协同过滤是无能为力的 上图所示的状况是不同 user 向量之间没有共同出现的 item，这样无法计算 user 向量间的近似性；而实际中的数据往往是非常稀疏的，不同用户间相交的 items 很少，因此协同过滤在推荐系统中几乎不再被使用。 那么，如何将上面提到的 semantic understanding 应用到推荐系统中呢？如下图所示，假如将原来的 users-items 矩阵划分为两个大的 topics，则每个 user 和每个 items 都可以被映射成一个 topic 向量，这样通过比较 topic 向量即可比较两个 user 或 item 的相似性 在实际中用得更多的是矩阵分解这一类方法，即无须事先对 users-items 矩阵进行划分，而是通过矩阵分解的方式得到每个 user 和每个 item 的 topic 向量。常见的 SVD、NMF 等都是这一类方法。 搜索引擎搜索引擎跟在线广告有点类似，都会根据用户的 query 返回关联的内容，下图中的 text 可以理解为用户的搜索关键词和页面的关键词 图中有几个 topic，最简单的是两个，如下图所示 但是也可以认为有四个，如下图所示 所以这里有个重要问题，就是应该学习什么样粒度的 topic ？作者的观点是学习更小粒度的语义的好处更多。比如说按照第一种大粒度的语义分法时，第一行和第三行是有关联的；而按照第二种小粒度的语义分法时，第一行和第三行直观上没有联系，但可通过第二行联系起来。即更小粒度的语义能够完成大粒度的语义所完成的事情，但是大粒度的语义未必能完成小粒度语义所完成的事情，如从搜索的精确性考虑，肯定是小粒度的语义比大粒度的语义搜索要来得精准。 实现 semantic understanding 的方法实现 semantic understanding 的方法可以分为 unsupervised 和 supervised 两大类，supervised 其实就是分类（即将上面的 user 或 item 分类），但是 supervised 不仅需要人工标注的数据，而且还难以确定准确的类别数；因此实际中往往是采用 unsupervised 的方法，如下所示是一些 unsupervised 方法 Frequent itemset mining 在上一讲中已经讲过，实际中，Frequent itemset mining 和 collaborative filtering 使用得并不多 LSA(Latent semantic analysis) 实际上是对文本矩阵(每行是一篇文本，每列是一个单词) 进行 SVD 分解，且得到的 U 矩阵 和 V 矩阵中分别含有每篇文本或每个单词的 topic 向量。其他矩阵分解类的方法也都是遵循着这个思路。 NMF(Non-negative matrix factorization) 是受限的 SVD，原因是这种方法分解后的矩阵中的元素必须要为正数。 pLSA(Probabilistic latent semantic analysis) 在LSA的基础上加入了概率，使得结果有可解释性 LDA(Latent Dirichlet allocation) pLSA 只能解释输入的数据，对于新来的数据无能为力，因此无法做到实时；因此有了 LDA 的出现，LDA 能够推断出新来的数据的 topic 分布，而smoothed pLSA 指的是 LDA 在 pLSA 基础上加入了先验概率，具体为狄利赫里先验分布。 HDP(Hierarchical Dirichlet process) 与 LDA 效果类似，好处就是训练前不需要指定的 topics 的数量 作者认为，上面虽然列出了很多模型，但是不少模型之间是等价的，如 NFM 和 pLSA 的等价性可参考这篇文章，pLSA 和 LDA 之间的等价性可参考这篇文章； 最后 semantic understanding 应用在上面所述三个互联网业务(在线广告，推荐系统，搜索引擎)时，基本的步骤是一致的，均为 Relevance: information retrieval Ranking: click-through rate prediction 第一步就是借助 semantic understanding 找到与 query 相关的结果，第二步要根据点击率等于具体业务相关的指标对这些相关结果排序。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式机器学习(2)-Infrequent Pattern Mining using MapReduce]]></title>
      <url>%2F2018%2F02%2F11%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(2)-Infrequent%20Pattern%20Mining%20using%20MapReduce%2F</url>
      <content type="text"><![CDATA[这一讲主要介绍了挖掘频繁项集中的经典方法 FP-growth，以及如何通过 MapReduce 实现这个算法，通过MapReduce实现的 FP-growth 也称为 PFP，这个方法不仅能够挖掘频繁项集，还能够挖掘非频繁项集。原视频在这里，需要自备梯子。 频繁项挖掘频繁项集挖掘（Frequent Itemset Mining）指的是将那些共同出现较为频繁的项集找出来，其中一个经典的例子就是啤酒和尿布。下图展示了频繁项挖掘中的一些基本概念 Transaction：每一行就是一个 Transaction，表示一个订单 Item &amp; ItemSet：每个 Transaction 包含了若干个 Item，表示订单中的商品，任意数量的 Item 可组成一个 ItemSet Support：表示某个 ItemSet 出现的次数 挖掘频繁项集有两个基本方法：Apriori 和 FP-growth，其中 Apriori 需要对原始数据进行多次的扫描，导致速度很慢。这里不展开讲述，具体可参考这篇文章。 这里讲述的是 FP-growth 算法，该算法通过构建一棵 prefix tree，使得只需要遍历两次原始的数据，下面先介绍这个算法的流程，再介绍如何通过 MapReduce 实现。 FP-growth 算法FP-growth 算法首先需要构建一棵 FP-tree ，FP-tree 是一棵 prefix tree，构造的方法跟一般的 prefix tree 一样，但是在构造前需要对每个 transaction 中的 items 按照统一的规则排序（可以按照item出现的频率，也可以按照item的名称，总之各个transaction保持一致即可） 由于原始的 items 已经按照items名称排序，因此这里直接按照构建prefix tree 的方法构建 FP-tree 即可，如下图所示是读入前两条记录的构建结果 按照这种方法可以构建出整棵 FP-tree 如下图所示 这里需要注意的是上述的过程忽略了一步，就是在构造 FP-tree 前还还需要设定一个 min-support 值，这个值表示某个 item 至少要出现 min-support 次，如果小于 min-support 次，则将这个 item 从所有的 transaction 中剔除，再按照上面的方法构造 prefix tree。 这么做的原因一是出现频率不高的 itemset 中的item被认为是相关度较低的，二是当输入很大的时候，构造出来的树也会很大，因此可通过设定阈值，去掉 infrequent item。但是从我们前面讲的内容可知，这样做其实已经去掉了部分长尾数据。 构造完 FP-tree 后，挖掘频繁项集时需要先指定一个 item，再挖掘与这个 item 相关的频繁项集，挖掘与这个item相关的频繁项集需要构造 Conditional FP-tree，所谓的 Conditional FP-Tree，就是与该 item 相关的所有 transaction 所组成的 FP-tree。如要挖掘与 e 这个 item 相关的频繁项集，则构造 Conditional FP-tree 过程如下 首先需要找到以 e 为叶子节点的所有 branch 组成的 tree，如下图 (a) 所示 接着更新父节点的计数为子节点的计数之和，并且去掉 e 这个叶子节点，如下图所示 在这个过程中需要去掉那些 support 数不满足 min-support 的item，这里令 min-support 为2，,则由于上图中的 b 的support 数只有 1，需要去掉 b 这个 item，如下图所示 构造出 e 的 conditional FP-tree 后，只需要挖掘出 conditional FP-tree 中的频繁项集，并在所有的频繁项集后加上 e 即可，如从 conditional FP-tree 中挖掘出的频繁项集为 {a:2, d:2} 和 {c:2}, 则关于 e 的频繁项集为 {a:2, d:2, e:2} 和 {c:2, e:2}。这样实际上是在解决一个递归问题了。 MapReduce 实现 FP-growth一般来说，挖掘频繁项集的原始数据都相当庞大，因此构造出来的 FP-tree 也会很大，当内存无法存储 FP-tree 时，一种解决方法就是通过提高 min-support 的值，从而将 FP-tree 的大小降低，但是根据之前讲的长尾效应，这样做会将长尾数据切掉。 另外一种方法就是通过分布式的方式实现该算法，通过分布式使得能够 构建 FP-tree 时支持更小的 min-support，从而能够挖掘非频繁项集。这里采用的分布式框架就是 MapReduce，下面讲述的是这个方法实现的细节。 下面首先介绍一下MapReduce 的编程范式，输入、中间结果和输出均是 key-value pairs, 中间的 shuffling 过程目的就是要将 key 相同的 pairs 交给同一个 worker 处理。 在具体的实现上，shuffling 过程可通过对 key 做 hash 后，再取模（worker 的数量）决定这个 kv 对由哪个 worker 来负责。 另外数据的传输是通过分布式文件系统实现的，也就是需要通过反复写磁盘来进行数据的传输，这导致了 MapReduce 的速度无法快起来。 从上面可知，找到 item A 和 item B 的频繁项集这两个任务是不相关的，只需要分别找出 item A 和 item B 的 conditional FP-tree 即可，因此可以分别交给两个 worker 进行处理。 通过 MapReduce 实现的 FP-growth，每一次的 map + reduce 过程能够输出 item 对应的 conditional FP-tree，下图所示的是 map 过程，能够从 transaction 中输出各个 item 对应的 conditional transaction 上图中最左边的 map input 是最开始的 transactions, 经过排序和过滤掉非频繁项后能够得到各个 item 对应的conditional transaction，作为 map output 输入到 reduce 过程中，reduce 过程如下图所示，reduce 能够将相同的 key(item) 的conditional transactions 聚合在一起，进而构建出这个 item 对应的conditional FP-tree。 得到这个 item 的 conditional FP-tree 后，可对这棵 conditional FP-tree 进行相同的 MapReduce 操作，因此，原始的 FP-growth 算法递归一次在这里就是一个 MapReduce Job。 上面的实现方法实际上就是 PFP(Parallel FP-growth) 算法，目前在 spark 框架中有相应的实现，可以直接调用。 最后需要注意的一点是，PFP 虽然能够挖掘出非频繁项集，也就是长尾数据的尾巴部分，但是却无法判断出这些非频繁项集中那些项集是有效的，那些是噪声。 综上，本文主要介绍了频繁项挖掘中的 FP-growth 算法以及如何通过 MapReduce 实现这个算法，通过 MapReduce 实现的版本能够支持更小的 min-support，因此能够挖掘出非频繁项集，也就是长尾数据中的尾巴部分，但是这个方法不能判断出那些非频繁项集是有效的。 参考 分布式机器学习系列讲座 - 01 Infrequent Pattern Mining using MapReduceFP Tree算法原理总结]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分布式机器学习(1)-A New Era]]></title>
      <url>%2F2018%2F02%2F08%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(1)-A%20New%20Era%2F</url>
      <content type="text"><![CDATA[这个分布式机器学习系列是由王益分享的，讲的是分布式机器学习。正如作者在分享中所说，分布式机器学习与我们今天常听到的机器学习存在比较大的差异，因此分享中的很多观点跟我们从教课书上学到的机器学习是背道而驰的。作者在这方面具有丰富的经验，虽然是三年前的分享，或许分享中提到的部分技术改变了，但是其中的一些观点还是具有一定参考价值的。 笔者对于分享中的一些观点也是存在疑惑的，这里还是按照分享中作者表达的意思记录下来, 也许等到笔者工作后，才有机会去验证这些观点的正误。 本文主要介绍了分布式机器学习中的一些重要概念，如互联网的真实数据是长尾分布的、大比快要重要、不能盲目套用一个框架等，本文对应的视频在这里，需要自备梯子。 机器学习与分布式机器学习机器学习最重要的是数学知识，而这里的分布式机器学习更关注的是工程技法。且一般的机器学习模型假设数据是指数簇分布，而实际的数据是长尾分布的，分布式机器学习需要去对长尾的数据进行建模，对于这点，后面会有详细的描述。 长尾效应根据维基百科，长尾指的是指那些原来不受到重视的销量小但种类多的产品或服务由于总量巨大，累积起来的总收益超过主流产品的现象。在互联网领域，长尾效应尤为显著。下图所示黄色的部分就是长尾部分，之所以要关注长尾的数据，是因为长尾的数据的量并不小，蕴含着重要的价值 以互联网广告为例，长尾现象指的是有大量频次低的搜索词，这些搜索词并非为大多数人所了解。如搜索“红酒木瓜汤”时不应该出红酒、木瓜或者汤的广告，因为这个是一个丰胸健美类的产品，这就是一个长尾的搜索词。 另外一个例子如下所示，下图是从 Delicious 的网页中挖掘出的一些频繁项集，最左边便是这个频繁项集，最右边表示这个频繁项集在所有网页（数亿）中出现的次数，可以看到这些频繁项集是长尾的数据了，然而，这些数据都是有具体意义的。 大比快重要并行计算关注的是更快，而分布式机器学习关注的则是更大，因为数据是长尾的，要涵盖这些数据中的长尾数据，首要的处理的是首先是大量的数据。 在讲述下文前，需要先说明几个概念，Message Passing 和 MapReduce 是两个有名的并行程序编程范式（paradigm），也就是说，并行程序应该怎么写都有规范了——只需要在预先提供的框架（framework）程序里插入一些代码，就能得到自己的并行程序。Message Passing范式的一个框架叫做MPI。MapReduce范式的框架也叫MapReduce。而MPICH2和Apache Hadoop分别是这MPI和MapReduce两个框架的实现（implementations）。 对于框架而言，其中重要的一点是要支持 Fault Recovery，简单来说就是要支持失败的任务的回滚。MPI 无法实现 Fault Recovery，这是因为MPI允许进程之间在任何时刻互相通信。如果一个进程挂了，我们确实可以请分布式操作系统重启之。但是如果要让这个“新生”获取它“前世”的状态，我们就需要让它从初始状态开始执行，接收到其前世曾经收到的所有消息。这就要求所有给“前世”发过消息的进程都被重启。而这些进程都需要接收到他们的“前世”接收到过的所有消息。这种数据依赖的结果就是：所有进程都得重启，那么这个job就得重头做。 虽然很难让MPI框架做到fault recovery，我们可否让基于MPI的应用系统支持fault recovery呢？原则上是可以的——最简易的做法是做 checkpoint——时不常的把有所进程接收到过的所有消息写入一个分布式文件系统（比如HDFS）。或者更直接一点：进程状态和job状态写入HDFS。 与 MPI 相反的框架是 MapReduce，MPI容许进程间在任意时刻互相通信，MapReduce则只允许 进程在 Map 和 Reduce 间的 shuffle 进行通信，MPI 几乎能够将所有的机器学习算法并行化，而部分复杂的算法无法通过 MapReduce 实现。 一些坑这里是作者认为一些需要避开的坑， 下面选择几个展开说明 De-noise data这里的noise data 指的是将数据中出现频率不高的数据，将这些数据去掉，根据前面讲的长尾，这相当于是将长尾的尾巴截掉了，因此会丢失大部分有用的数据。 为什么不能将长尾的尾巴割掉，作者在 描述长尾数据的数学模型 这篇文章是这么说的 那条长尾巴覆盖的多种多样的数据类型，就是Internet上的人生百态。理解这样的百态是很重要的。比如百度和Google为什么能如此赚钱？因为互联网广告收益。传统广告行业，只有有钱的大企业才有财力联系广告代理公司，一帮西装革履的高富帅聚在一起讨论，竞争电视或者纸媒体上的广告机会。互联网广告里，任何人都可以登录到一个网站上去投放广告，即使每日广告预算只有几十块人民币。这样一来，刘备这样织席贩屡的小业主，也能推销自己做的席子和鞋子。而搜索引擎用户的兴趣也是百花齐放的——从人人爱戴的陈老师苍老师到各种小众需求包括“红酒木瓜汤”（一种丰胸秘方，应该出丰胸广告）或者“苹果大尺度”（在搜索范冰冰主演的《苹果》电影呢）。把各种需求和各种广告通过智能技术匹配起来，就酝酿了互联网广告的革命性力量。这其中，理解各种小众需求、长尾意图就非常重要了。 Parallelize models in papers and textbooks教科书或paper中的模型无法应用到大数据环境下，原因是教科书上的模型假设数据是指数簇分布（Exponential family）的，而真实的数据是长尾分布（Long tail）的。 用指数簇分布的模型去拟合长尾分布的数据，会有什么后果？答案就是会将长尾数据的尾巴给割掉了。比如说对于pLSA和LDA这两个Topic model，如果大家尝试着把训练语料中的低频词去掉，会发现训练得到的语义和用全量数据训练得到的差不多。换句话说，pLSA和LDA模型的训练算法没有在意低频数据。 目前大部分的数学模型都假设数据是指数簇分布的，如 Topic model 中的 LDA 和 PLSA 其先验分布和后验分布均是指数簇分布，像 SVD 这种矩阵分解的方法也做了高斯分布的假设，而像Linear Regression 这类方法，从损失函数可以看到也倾向于优化数量多的样本。 既然如此，为什么这类模型还要假设数据是指数族分布的呢？，作者在 描述长尾数据的数学模型 中是这么说的 这么做实在是不得已。指数族分布是一种数值计算上非常方便的数学元素。拿LDA来说，它利用了Dirichlet和multinomial两种分布的共轭性，使得其计算过程中，模型的参数都被积分给积掉了（integrated out）。这是AD-LDA这样的ad hoc并行算法——在其他模型上都不好使的做法——在LDA上好用的原因之一。换句话说，这是为了计算方便，掩耳盗铃地假设数据是指数族分布的。 因此，要对长尾建模，作者提到，Dirichlet process 和 Pitman-Yor process 是一个可能的新方向。 Use existing frameworksMPI 和 MapReduce 是两个极端，介于两者之间的是Pregel（google），spark 等框架，这些主要思想就是在磁盘上做 checkpooint 从而实现灵活的通信和有效的Recovery。 像 PageRank 这种算法在这些框架上能work，但是像 LDA 这种通信量大的算法，做checkpoint 时需要写入磁盘，会导致 buffer 过大，进而出现 out of memeory 的情况。因此，并不能通过一个通用的计算框架来解决所有问题。 下面是一些被广泛使用的开源框架，需要注意的是，在设计机器学习系统时，需要权衡效果与代价，选择甚至是开发合适的框架 MPI MapReduce Pregel GraphLab Spark 另外一个需要注意的问题就是不要将计算框架和分布式操作系统混在一起，如 Hadoop 1.0 设计中将集群管理系统和分布式计算框架混合在一起就显得非常混乱，因此在 2.0 中出现了 Yarn 这个分布式操作系统专门负责任务的调度。 分布式机器学习技术栈最后这张图是分布式技术栈，从下到上依次涵盖了分布式文件系统，分布式操作系统，一些中间件，分布式计算框架，以及构构建在这套分布式系统上的应用。由于是15年的视频了，因此相应的技术也会有所改变，至少我所知的 DNN 的 framework 已经是 tensorflow，pytorch，caffe，mxnet 等占主流了。 参考 分布式机器学习系列讲座 - 00 A New Era分布式机器学习的故事：评价标准分布式机器学习的故事：pLSA和MPI描述长尾数据的数学模型]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[stacking 的基本思想及代码实现]]></title>
      <url>%2F2018%2F01%2F21%2Fstacking%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
      <content type="text"><![CDATA[本文主要介绍机器学习中的一种集成学习的方法 stacking，本文首先介绍 stacking 这种方法的思想，然后提供一种实现 stacking 的思路，能够简单地拓展 stacking 中的基本模型。 stacking 的基本思想stacking 就是将一系列模型（也称基模型）的输出结果作为新特征输入到其他模型，这种方法由于实现了模型的层叠，即第一层的模型输出作为第二层模型的输入，第二层模型的输出作为第三层模型的输入，依次类推，最后一层模型输出的结果作为最终结果。本文会以两层的 stacking 为例进行说明。 stacking 的思想也很好理解，这里以论文审稿为例，首先是三个审稿人分别对论文进行审稿，然后分别返回审稿意见给总编辑，总编辑会结合审稿人的意见给出最终的判断，即是否录用。对应于stacking，这里的三个审稿人就是第一层的模型，其输出（审稿人意见）会作为第二层模型（总编辑）的输入，然后第二层模型会给出最终的结果。 stacking 的思想很好理解，但是在实现时需要注意不能有泄漏（leak）的情况，也就是说对于训练样本中的每一条数据，基模型输出其结果时并不能用这条数据来训练。否则就是用这条数据来训练，同时用这条数据来测试，这样会造成最终预测时的过拟合现象，即经过stacking后在训练集上进行验证时效果很好，但是在测试集上效果很差。 为了解决这个泄漏的问题，需要通过 K-Fold 方法分别输出各部分样本的结果，这里以 5-Fold 为例，具体步骤如下 (1) 将数据划分为 5 部分，每次用其中 1 部分做验证集，其余 4 部分做训练集，则共可训练出 5 个模型(2) 对于训练集，每次训练出一个模型时，通过该模型对没有用来训练的验证集进行预测，将预测结果作为验证集对应的样本的第二层输入，则依次遍历5次后，每个训练样本都可得到其输出结果作为第二层模型的输入(3) 对于测试集，每次训练出一个模型时，都用这个模型对其进行预测，则最终测试集的每个样本都会有5个输出结果，对这些结果取平均作为该样本的第二层输入 上述过程图示如下 除此之外，用 stacking 或者说 ensemble 这一类方法时还需要注意以下两点： Base Model 之间的相关性要尽可能的小，从而能够互补模型间的优势 Base Model 之间的性能表现不能差距太大，太差的模型会拖后腿 代码实现由于需要 stacking 中每个基模型都需要对数据集进行划分后进行交叉训练，如果为每个模型都写这部分的代码会显得非常冗余，因此这里提供一种简便实现 stacking 的思路。 具体做法就是先实现一个父类，父类中实现了交叉训练的方法，因为这个方法对所有模型都是一致的，然后声明两个方法：train 和 predict，由于采用的基模型不同，这两个方法的具体实现也不同，因此需要在子类中实现。下面以 python 为例进行讲解 12345678910111213141516171819202122232425262728293031323334import numpy as npfrom sklearn.model_selection import KFoldclass BasicModel(object): """Parent class of basic models""" def train(self, x_train, y_train, x_val, y_val): """return a trained model and eval metric of validation data""" pass def predict(self, model, x_test): """return the predicted result of test data""" pass def get_oof(self, x_train, y_train, x_test, n_folds = 5): """K-fold stacking""" num_train, num_test = x_train.shape[0], x_test.shape[0] oof_train = np.zeros((num_train,)) oof_test = np.zeros((num_test,)) oof_test_all_fold = np.zeros((num_test, n_folds)) aucs = [] KF = KFold(n_splits = n_folds, random_state=2017) for i, (train_index, val_index) in enumerate(KF.split(x_train)): print('&#123;0&#125; fold, train &#123;1&#125;, val &#123;2&#125;'.format(i, len(train_index), len(val_index))) x_tra, y_tra = x_train[train_index], y_train[train_index] x_val, y_val = x_train[val_index], y_train[val_index] model, auc = self.train(x_tra, y_tra, x_val, y_val) aucs.append(auc) oof_train[val_index] = self.predict(model, x_val) oof_test_all_fold[:, i] = self.predict(model, x_test) oof_test = np.mean(oof_test_all_fold, axis=1) print('all aucs &#123;0&#125;, average &#123;1&#125;'.format(aucs, np.mean(aucs))) return oof_train, oof_test 上面最重要的就是进行 K-fold 训练的 get_oof 方法，该方法最终返回训练集和测试集在基模型上的预测结果，也就是两个一维向量，长度分别是训练集和测试集的样本数。 下面以两个基模型为例进行 stacking，分别是 xgboost 和 lightgbm，这两个模型都只需要实现 BasicModel 中的 train 和 predict 方法 第一个基模型12345678910111213141516171819202122232425262728293031import xgboost as xgbclass XGBClassifier(BasicModel): def __init__(self): """set parameters""" self.num_rounds=1000 self.early_stopping_rounds = 15 self.params = &#123; 'objective': 'binary:logistic', 'eta': 0.1, 'max_depth': 8, 'eval_metric': 'auc', 'seed': 0, 'silent' : 0 &#125; def train(self, x_train, y_train, x_val, y_val): print('train with xgb model') xgbtrain = xgb.DMatrix(x_train, y_train) xgbval = xgb.DMatrix(x_val, y_val) watchlist = [(xgbtrain,'train'), (xgbval, 'val')] model = xgb.train(self.params, xgbtrain, self.num_rounds) watchlist, early_stopping_rounds = self.early_stopping_rounds) return model, float(model.eval(xgbval).split()[1].split(':')[1]) def predict(self, model, x_test): print('test with xgb model') xgbtest = xgb.DMatrix(x_test) return model.predict(xgbtest) 第二个基模型 12345678910111213141516171819202122232425262728293031323334353637import lightgbm as lgbclass LGBClassifier(BasicModel): def __init__(self): self.num_boost_round = 2000 self.early_stopping_rounds = 15 self.params = &#123; 'task': 'train', 'boosting_type': 'dart', 'objective': 'binary', 'metric': &#123;'auc', 'binary_logloss'&#125;, 'num_leaves': 80, 'learning_rate': 0.05, # 'scale_pos_weight': 1.5, 'feature_fraction': 0.5, 'bagging_fraction': 1, 'bagging_freq': 5, 'max_bin': 300, 'is_unbalance': True, 'lambda_l2': 5.0, 'verbose' : -1 &#125; def train(self, x_train, y_train, x_val, y_val): print('train with lgb model') lgbtrain = lgb.Dataset(x_train, y_train) lgbval = lgb.Dataset(x_val, y_val) model = lgb.train(self.params, lgbtrain, valid_sets = lgbval, verbose_eval = self.num_boost_round, num_boost_round = self.num_boost_round) early_stopping_rounds = self.early_stopping_rounds) return model, model.best_score['valid_0']['auc'] def predict(self, model, x_test): print('test with lgb model') return model.predict(x_test, num_iteration=model.best_iteration) 下一个步骤就是将这两个基模型的输出作为第二层模型的输入，这里选用的第二层模型是 LogisticsRegression， 首先需要将各个基模型的输出 reshape 和 concatenate 成合适的大小 1234567891011lgb_classifier = LGBClassifier()lgb_oof_train, lgb_oof_test = lgb_classifier.get_oof(x_train, y_train, x_test) xgb_classifier = XGBClassifier()xgb_oof_train, xgb_oof_test = xgb_classifier.get_oof(x_train, y_train, x_test)input_train = [xgb_oof_train, lgb_oof_train] input_test = [xgb_oof_test, lgb_oof_test]stacked_train = np.concatenate([f.reshape(-1, 1) for f in input_train], axis=1)stacked_test = np.concatenate([f.reshape(-1, 1) for f in input_test], axis=1) 然后用第二层模型进行训练和预测12345from sklearn.linear_model import LinearRegressionfinal_model = LinearRegression()final_model.fit(stacked_train, y_train)test_prediction = final_model.predict(stacked_test) 上述实现的完整代码见下面的链接 https://github.com/WuLC/MachineLearningAlgorithm/blob/master/python/Stacking.py 如有错漏，欢迎交流指正 参考 Introduction to Ensembling/Stacking in Python如何在 Kaggle 首战中进入前 10%]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过 word2vec 与 CNN/RNN 对动作序列建模]]></title>
      <url>%2F2018%2F01%2F15%2F%E9%80%9A%E8%BF%87%20word2vec%20%E4%B8%8E%20CNN-RNN%20%E5%AF%B9%E5%8A%A8%E4%BD%9C%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%2F</url>
      <content type="text"><![CDATA[本文主要讲述如何通过 word2vec 和 CNN/RNN 对动作序列建模，在最近的一个比赛中验证了这个思路，的确有一定效果，在二分类的准确率上能达到0.87.本文主要介绍这个方法的具体步骤，并以比赛和代码为例进行说明。 这里提到的比赛是目前正在进行的精品旅行服务成单预测, 该比赛就是要根据用户的个人信息，行为信息和订单信息来预测用户的下一个订单是否是精品服务。本文提到的方法是仅利用用户的行为信息，主要的思路是：将每个动作通过 word2vec 转化为 embedding 表示，然后将动作序列转化为 embedding 序列并作为 CNN/RNN 的输入。 下面依次介绍通过 word2vec 获得动作 embedding，将 embedding 作为CNN的输入和将embedding作为RNN的输入这三部分内容。 word2vec 获取动作 embeddingword2vec 是一个很著名的无监督算法了，这个算法最初在NLP领域提出，可以通过词语间的关系构建词向量，进而通过词向量可获取词语的语义信息，如词语意思相近度等。而将 word2vec 应用到动作序列中，主要是受到了知乎上这个答案的启发。因为 word2vec 能够挖掘序列中各个元素之间的关系信息，这里如果将每个动作看成是一个单词，然后通过 word2vec 得出每个动作的 embedding 表示，那么这些 embedding 之间会存在一定的关联程度，再将动作序列转为 embedding 序列，作为 CNN 或 RNN 的输入便可挖掘整个序列的信息。 这里训练动作 embedding 的方法跟训练 word embedding 的方法一致，将每个户的每个动作看做一个单词、动作序列看做一篇文章即可。训练时采用的是 gensim, 训练的代码很简单，embedding 的维度设为 300, filter_texts中每一行是一各用户的行为序列，行为之间用空格隔开。 123from gensim.models import word2vecvector_length = 300model = word2vec.Word2Vec(filter_texts, size = vector_length, window=2, workers=4) 由于动作类型只有9种（1~9），也就是共有 9 个不同的单词，因此可将这 9 个动作的 embedding 存在一个 np.ndarray 中，然后作为后面 CNN/RNN 前的 embedding layer 的初始化权重。注意这里还添加了一个动作 0 ，原因是 CNN 的输入要求长度一致，因此对于长度达不到要求长度的序列，需要在前面补 0（补其他的不是已知的动作也可以）。代码如下 1234import numpy as npembedding_matrix = np.zeros((10, vector_length))for i in range(1, 10): embedding_matrix[i] = model.wv[str(i)] CNN 对动作序列建模CNN 采用的模型是经典的 TextCNN, 模型结构如下图所示 这里通过 Keras 实现，具体代码如下 首先需要处理序列，使得所有序列长度一致，这里选择的长度是 50，具体代码如下，代码中的 x_original 是一个 list[list[int]] 类型，表示所有用户的所有动作序列，对于长度比 max_len 长的，从后往前截取50个最近时间的动作，而短的则在前面补0. 12345from keras.preprocessing import sequencemax_len = 50x_train = sequence.pad_sequences(x_original, maxlen=max_len)y_train = np.array(y_original)print(x_train.shape, y_train.shape) 然后通过前面得到的 embedding_matrix 初始化 embedding 层 12345678910from keras.models import Sequential, Modelfrom keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding, BatchNormalization, Activationfrom keras.layers.merge import Concatenatefrom keras import optimizersembedding_layer = Embedding(input_dim=embedding_matrix.shape[0], output_dim = embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True) 然后建立模型并训练, 这里用了四种不同步长的卷积核，分别是 2、3、5、8，比起原始的 TextCNN, 用了两层的卷积层(在这个任务上经过测试比一层的要好), 后面的全连接层也拓展到了三层，具体代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344NUM_EPOCHS = 100BATCH_SIZE = 64DROP_PORB = (0.5, 0.8)NUM_FILTERS = (64, 32)FILTER_SIZES = (2, 3, 5, 8)HIDDEN_DIMS = 1024FEATURE_DIMS = 256ACTIVE_FUNC = 'relu'sequence_input = Input(shape=(max_len, ), dtype='int32')embedded_seq = embedding_layer(sequence_input)# Convolutional blockconv_blocks = []for size in FILTER_SIZES: conv = Convolution1D(filters=NUM_FILTERS[0], kernel_size=size, padding="valid", activation=ACTIVE_FUNC, strides=1)(embedded_seq) conv = Convolution1D(filters=NUM_FILTERS[1], kernel_size=2, padding="valid", activation=ACTIVE_FUNC, strides=1)(conv) conv = Flatten()(conv) conv_blocks.append(conv)model_tmp = Concatenate()(conv_blocks) if len(conv_blocks) &gt; 1 else conv_blocks[0]model_tmp = Dropout(DROP_PORB[1])(model_tmp)model_tmp = Dense(HIDDEN_DIMS, activation=ACTIVE_FUNC)(model_tmp)model_tmp = Dropout(DROP_PORB[0])(model_tmp)model_tmp = Dense(FEATURE_DIMS, activation=ACTIVE_FUNC)(model_tmp)model_tmp = Dropout(DROP_PORB[0])(model_tmp)model_output = Dense(1, activation="sigmoid")(model_tmp)model = Model(sequence_input, model_output)opti = optimizers.SGD(lr = 0.01, momentum=0.8, decay=0.0001)model.compile(loss='binary_crossentropy', optimizer = opti, metrics=['binary_accuracy'])model.fit(x_tra, y_tra, batch_size = BATCH_SIZE, validation_data = (x_val, y_val)) 由于最后要求的是 auc 指标，但是 Keras 中并没有提供，而 accuracy 与 auc 还是存在一定差距的，因此可以在每个epoch后通过 sklearn 计算auc，具体代码如下 123456from sklearn import metricsfor i in range(NUM_EPOCHS): model.fit(x_tra, y_tra, batch_size = BATCH_SIZE, validation_data = (x_val, y_val)) y_pred = model.predict(x_val) val_auc = metrics.roc_auc_score(y_val, y_pred) print('val_auc:&#123;0:5f&#125;'.format(val_auc)) 这种方法最终的准确率约为 0.86，auc 约为0.84 RNN 对动作序列建模通过 RNN 进行建模与 CNN 类似，不同的是 RNN 可接受不同长度的输入，但是根据这里的说明，对于输入也需要 padding 的操作，只是RNN 会将其自动忽略。 因此，数据的预处理和构建 embedding 层的代码与 CNN 中基本一致，这里只给出建立模型的代码，模型比较简单，首先是将输入通过 embedding 层的映射后，作为以 LSMT/GRU 为基础单元构建的 RNN 的输入, 最后通过 sigmoid 进行分类，具体代码如下 123456789101112131415# RNNs are tricky. Choice of batch size is important,# choice of loss and optimizer is critical, etc.model = Sequential()model.add(embedding_layer)model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2)))# model.add(LSTM(256))# model.add(Bidirectional(GRU(256)))model.add(Dense(1, activation='sigmoid'))opti = optimizers.SGD(lr = 0.01, momentum=0.8, decay=0.0001)# try using different optimizers and different optimizer configsmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 通过 RNN 得出的最终效果比 CNN 要好一点，准确率约为 0.87，auc 约为0.85。但是训练起来非常慢，且参数非常的 tricky，需要精调，这里我没有细调参数，模型也没有搞得很复杂，应该还有提升空间。 综上，本文提供了一种对动态序列建模的思路：将动作序列通过 word2vec，得到每个动作的 embedding 表示，然后将动作序列转化为 embedding 序列并作为 CNN/RNN 的输入。希望能够起到抛砖引玉的作用，如果您有更好的想法，欢迎交流。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2017 小结]]></title>
      <url>%2F2018%2F01%2F02%2F2017%20%E5%B0%8F%E7%BB%93%2F</url>
      <content type="text"><![CDATA[一个多月没写文章了，这个月主要是被各种焦头烂额的事情所烦扰：比赛、数据的采集与筛选、各种无聊的报告等等。一眨眼就踏入了2018，本来也不打算写年度总结，但是后来想想还是做一下简单的记录，一是因为自己本来就有总结的习惯，要不也不会一直在写这个博客；二是因为不总结下，都不知道自己这一年过得有多烂（捂脸）。言归正传，下面主要写一下在这一年里干了啥。 关于课程研究生的第一年还是以上课为主，当初从电信转到计算机一个原因就是对计算机更有兴趣，所以这一年的课程也是学得挺顺利的。计算机的基本素养课程：操作系统、计算机网络和数据库都有，虽然本科上过，但是研究生的课是对某些知识点进行了更深入的讲解，操作系统和计算机网络都在本站点上做了相应的总结，虽说当时总结起来一顿操作猛如虎，考试也考了90+，但是现在的内容却忘得七七八八了，似乎是水过鸭背。但我还是觉得这个东西虽然不能被我清晰记起，但是当要再次捡起来的时候，还是会比较快的。现在回想如果我本科时候没那么认真学习操作系统和计算机网络，研究生这两门课也不会学得这么顺利，知乎有个高票答案就说到知识或者技能这种东西，学到了就跟你一辈子，也许说的就是这种情况吧。 除了这几门常规课，其他让我觉得最有用的两门课就是 最优化基础 和 凸优化了，两门都是关于优化的数学课，以往基本没接触过，搞数学建模的时候接触过一点线性规划，这里则是更详细地介绍了各种优化问题和解决方法。非常有用的两门课，尤其是对于我这种接触过机器学习和计算广告的人，因为从本质上来讲，这种优化思想在生活中无处不在：资源往往是有限的，我们总是想借助着有限的资源来最大化我们所希望的获取的利益。 将这类问题量化成一个最优化的问题，就有了一个目标，然后通过优化算法，就有了一个方向，这样或早或晚都能走到局部最优或全局最优。 还有一门就是模式分类，这门课是跟本科生一起上的，因此讲的内容并不是很深入，都是传统的机器学习算法，但是让我收获最大的是课程论文的阅读，读的论文是12年提出AlexNet 的经典论文 ImageNet Classification with Deep Convolutional Neural Networks，当时为了讲好 PPT，做了较多的调研，还写了阅读笔记，这篇论文涵盖了很多深度学习的重要概念，毕竟是开山之作。详细了解了这篇论文后使得我后面上手深度学习的项目时比较快，还是印证了前面的那句话：知识或者技能这种东西，学到了就跟你一辈子。 学校安排的课程中，能被我记住的基本就是上面提到的，其他的那些如上得不痛不痒的数据库，让人备受煎熬的中特，都没有多大印象了。这一年的课程成绩也还可以，最后的评优中虽然没有专利、活动之类的加分，但是因为成绩优势，最终也拿了一等奖学金，比刚入学时候的二等奖学金要好了，虽然我觉得如果我不跨院保研的话入学的时候也能拿一等（捂脸逃）。 除了学校的课程，主要还学习了与机器学习和计算广告相关的课程。 机器学习的几门课程包括吴恩达在斯坦福的公开课，台大林轩田的机器学习基石和机器学习技法；这几门课都是理论为主，数学偏多，硬着头皮也算啃了下来，其中让我印象最为深刻的不是各种各样的模型算法，而是 NFL(No Free Lunch) 定理和 VC 维理论，NFL 定理指出了模型之间并的差距必须要到某一个具体的数据集上才能体现出来，也就是说要解决一个机器学习问题时，必须要先对数据的分布等信息有较好的理解，才能选出适合这个数据的模型；VC 维理论需给我们揭示了一个很直观的概念：要取得较好的泛化能力，用于训练模型的样本数目应该至少是模型参数数目的10倍。这个能够很好的解释复杂模型容易过拟合的问题。但是对于目前如火如荼的深度学习，VC 维理论并不适用，因此 2017 ICLR 的最佳论文 Understanding deep learning requires rethinking generalization 通过实验指出了这个问题，同时深度学习目前也还没有公认的理论基础，因此这个问题也是亟需解决。 计算广告算是今年看到的一门比较新的课程，之前对广告的认知只限于弹窗、强制推送等，后来看了刘鹏的计算广告学，才发现这门学科集理论知识（主要是优化方面）和工程技法于一体，而且广告可以说是大数据为数不多的正真落地的一个产业。看了视频后又买了跟视频配套的书计算广告又看了一遍，对于书中众多概念及需要解决问题才有了初步了解。 这两方面的课程的内容虽然都看完了一遍，也做了一些笔记，但是还是感觉理解得还不够深入，还要重温一遍。 关于项目研一基本在上课，直到研一的暑假才被大老板叫去做项目，之前一直是跟小老板做 NLP 方向的研究，但是大老板的项目是图像相关的，具体的就是做人脸的表情识别。由于很久没接触图像相关知识了，刚开始还有点害怕做不来。但是后来才发现有了一些机器学习的知识，上手也是挺快的。 在这个过程中，对比了人工特征+传统机器学习方法和深度学习方法，传统的人工特征基本就是人脸的68个特征点以及特征点衍生出来的特征，在几个数据集上深度学习的效果基本上都要优于传统的机器学习方法，也许是我们提特征不够好，但是这也是深度学习的强大之处，将特征抽取和分类器的训练融合到一个模型中。 暑假做了大概一个月的算法研究，开学后被派去搭建系统了，主要就是实现从监控获取图像，对图像中的人脸进行表情识别并可实时观察具体的识别效果。首先要解决的是数据传输问题，就是图像从摄像头传到服务器，服务器处理后送到展示端，展示端为了维护的便利性，采用的网页展示方式。因为之前已经听说过 kafka 这个工具，知道这个工具的大概作用，因此这里就做了一下调研，没想到这个工具还是挺好用的，吞吐量高且拓展性强，部署起来也不麻烦，因此系统中有数据传输的部分都用了 kafka，需要存储的部分用了redis，在数据传输存储中将图像按照 base64 编码后，能够避免很多问题。而网页端的显示则是用了 multipart/x-mixed-replace 这个content type，简单来说这个 content type 能够替换掉原位置上的数据，如果将图像一帧一帧地传过来，便可达到动态视频的效果。 下面是具体效果 在这个过程中需要用到人脸检测、图像处理的工具，因此也接触了 opencv， dlib 这两个功能强大的库， 在人脸检测上 dlib 的效果要优于 opencv，opencv 则主要用在图像处理，如标注、裁剪等。 后面由于模型在已有的数据集(如CK+, KDEF等)上的效果很好，但是人肉测试时效果并不好，因此就考虑数据扩充，除了常规的在已有的数据集上进行裁剪、翻转、颜色抖动等操作。还通过爬虫在网上采集人脸数据库，主要就是通过关键词在谷歌中搜索对应的图片，然后获取其下载链接并下载图片，这个小工具已开源，具体地址见 https://github.com/WuLC/GoogleImagesDownloader 通过这个工具搜集了一定数量的图片，通过预处理和人工筛选后得到最终的图片，其中人工筛选过这个步骤由于每个人的标准都不一样，因此最后出现某些类别很少的情况。但是在一定程度上也算是扩充了原有的数据集（7种类别仅有2000多张图片） 因为之前的测试都是内部的测试，没有一个对比的标准，因为我们做的这个项目是以实用性为主，而目前提供表情识别服务且比较有名公司有 微软、谷歌、Face++、竹间智能等，因此就想到到了构建一个公有的数据集，来对比一下我们的模型和商用的差距。经过讨论后，决定去采集真实的人脸表情，这样一来所有的模型都不可能接触过这些数据集，因而能够比较公平地验证各个模型的泛化能力。因此就用 python 写了一个采集程序，去采集一个人的 7 种表情，并以视频形式存储。下图是采集的页面 采集完了需要将视频转为图像帧序列，然后人工选出若干帧作为表情变化序列进行后续操作。不得不说人工筛选就是累。 采集完验证集后，测试了上述的四个提供表情识别服务的公司的 API 在验证集上的准确率，结果显示准确率大概在 52%～58%（七分类） ，我们的模型最好的效果能达到 61%，且通过 confusion matrix 可以看到所有模型基本上都有这个问题，就是 angry、fear、disgust、sad 这几类表情被误分为 neutral，原因就是正常人在做这些表情的时候幅度并不会太大，而训练集中的数据却都是动作幅度较大，表情比较明显。 最后，还需要将从监控中获取到的不同人脸分来，就是一个人脸聚类问题。最开始采用的是人脸识别经典做法，就是每个人采用已知的人脸图片，然后通过预训练的 FaceNet 抽取图像的 128 维特征，对于未知的人脸图片，也用 FaceNet 抽取出 128 维特征，并和已知人脸的 128 维特征计算相似度，这种做法对于人脸的角度鲁棒性不好，就是只能识别出与已知的人脸图片中人脸角度差别不大的人脸。后来改用了 Chinese whispers 聚类算法，同样也是要通过预训练的 FaceNet 抽取图像的 128 维特征，但是不需要提供已知的人脸图片，且算法对于人脸角度的鲁棒性较好。 去年就只做了这个项目，虽然做的内容比较杂，但是也算是学到了不少东西。 关于比赛上课的时候参加了一些比赛，但是由于课程作业、考试等原因，基本都半途而废；暑假以后主要参加了两个比赛：AI Challenger 的图像中文描述 和 CCF 的计算智能大赛。 参加第一个比赛主要是这个方向可能是我的毕设方向，通过这个比赛，也算是基本入门了这个方向，代码主要是参考了 tensorflow 中提供的 im2txt 例子，基本看懂后做了一些改动，代码见这里，参数没有细调，因为到后面去搞 CCF 的比赛了,最后 B 榜大概 30 名左右 。 CCF 的比赛中主要参加了法海风控的比赛，做的是命名实体识别+文本分类，我主要负责的是文本分类，就是判断抽取出来的实体到底是正向、负向还是中性的，尝试了一些开源的工具，也实现了一些模型如 TextCNN 等，综合效率和准确率，fastText 是最好的，了解这个工具也是我觉得是比赛中一个较大的收获。比赛过程很繁琐，最终以 Top5 的成绩去了江苏答辩，最后第四名，也算是收获了一个奖项。 总的来说，2017 年里主要完成的就是上面这三个方面的事了，其他琐碎的也基本记不起。2018 就要找工了，时间真的过得好快, 希望在2018里能够继续保持 stay hungry，stay foolish 的状态吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker 使用小记]]></title>
      <url>%2F2017%2F11%2F25%2Fdocker%20%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[由于最近需要在几台电脑上配置相同的环境，几台电脑的操作系统不一样，而且配置一台所需要的步骤是比较繁琐的，因此就想到了用 docker，下面是使用 docker 构造镜像并且在不同的电脑上使用这个镜像的记录。 docker 支持多个平台，各个平台上具体的安装步骤可参考官方文档。 docker 提供了一个镜像仓库，当从docker镜像仓库中下载的镜像不能满足我们的需求时，我们可以通过以下两种方式对镜像进行更改 从已经创建的容器中更新镜像，并且提交这个镜像 使用 Dockerfile 指令来创建一个新的镜像 这里采用的是第一种方法，由于我这里需要的是python环境，因此先 pull 一个 python 镜像作为基础镜像（可以通过 docker search python 找到相关的镜像，这里 pull 的是官方的pytohn 3.5 镜像）, 命令如下 docker pull python:3.5 等到镜像 pull 下来后，可以通过以下命令进入镜像中 docker run -t -i python:3.5 /bin/bash 这里 -t -i 含义如下 -t : 在新容器内指定一个伪终端或终端。-i : 允许你对容器内的标准输入 (STDIN) 进行交互 这样便可进入装有 python 3.5的系统（默认是ubuntu），然后在其中像普通的系统一样通过 apt 和 pip 配置所需要的软件和库即可 配置完成后注意不能马上退出这个容器，因为在这个容器中的修改默认是不会影响到原来的镜像的，也就是说如果退出后在进入 python:3.5 镜像所创建的容器中，所安装的这些库会完全消失，因此需要将这个配置过的容器另存为一个新的镜像，具体做法如下 首先原来的容器不能退出，另开一个终端，通过 docker ps 命令获得修改过的容器的 id ，如下图所示 然后 docker commit id new_image 命令将这个修改过的容器存为新的镜像，这里的 id 不一定要写全，只要能跟其他的容器id区分开来，写前几个字符也可以。 如下所示是将前面的容器另存为镜像 modified_python, 并且通过 docker images 查看到该创建的镜像的时间和大小。 如果要将这个容器分发到其他机器，可以先将这个镜像上传到 docker 官方的中央仓库(需要注册账号），其他机器再从中央仓库 pull 下来，但是这样可能会存在着网速过慢的问题，因此可以在本地导出容器，然后直接拷贝到其他机器导入，具体操作如下 首先通过 docker ps -a 查看本地使用的容器，然后通过 docker export id &gt; tar_file 将容器导出到 tar_file 中, 其中容器 id 的书写规则同上。如下是将上面配置过的 python 环境导出到 ubuntu_python.tar 通过这个文件导入为镜像也很简单，通过 cat tar_file | docker import image_name 即可将 tar_file 导入为 image_name 镜像 其他的一些值得注意的地方就是 -v 参数可以将本地的目录挂到 docker 的目录中，从而可以在容器中写入本地磁盘，具体语法为 -v local_dir:contain_dir, 当容器中的 container_dir 不存在时会自动创建这个目录。 参考资料： Install DockerDocker 教程关于Docker目录挂载的总结保存对容器的修改导出和导入容器]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[分批训练过大的数据集]]></title>
      <url>%2F2017%2F11%2F18%2F%E5%88%86%E6%89%B9%E8%AE%AD%E7%BB%83%E8%BF%87%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
      <content type="text"><![CDATA[在深度学习中训练网络时，往往会出现训练数据过于庞大从而无法全部加载到内存中的情况，这里讲述的就是如何分批训练一个庞大的数据集，下面会以Keras 中的训练为例进行讲述。 分批处理的思路就是先将那个较大的数据处理成若干个较小的数据文件（如共1000000 条记录，处理成 1000 个小文件，每个小文件 1000 条记录），然后依次读取各个小的数据文件到内存中进行训练，这里的利用了 python 的 generator 特性来依次读取各个文件的内容。 如下代码所示,就是每次读取 num_files 个文件并合并成 X_train 和 Y_train 并返回，直到整个目录下的文件都被遍历一遍。 123456789101112131415161718def train_batch_generator(train_data_dir = './processed_data/train/', num_files = 1): files = sorted(os.listdir(train_data_dir)) count = num_files embeddings, labels = [], [] for file in files: print('Reading file &#123;0&#125;...........'.format(file)) gc.collect() with open(train_data_dir + file, 'rb') as rf: data = pickle.load(rf) embeddings.append(data['embedding']) labels.append(data['label']) count -= 1 if count == 0: X_train, Y_train = np.concatenate(embeddings), np.concatenate(labels) gc.collect() count = num_files embeddings, labels = [], [] yield (X_train, Y_train) 这样读取文件对应的训练方法如下（以Keras中的模型训练为例进行说明） 12345678NUM_EPOCHS = 10BATCH_SIZE = 32for i in range(NUM_EPOCHS): print('################&#123;0&#125; epochs#############'.format(i+1)) for x_train, y_train in train_batch_generator(num_files = 3): print(x_train.shape, y_train.shape) gc.collect() model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = 100, validation_data = (x_test, y_test)) 另一种方法不需要将大文件分成若干个小文件，而是直接打开整个大文件逐行读取，然后读取了一定数目的行后通过 yield 返回, 这种方法的一个问题就是训练过程中必须要保持整个文件为打开状态，此时如果发生系统故障等异常可能会损坏文件，而将大文件分为若干的小文件则能够很大程度上避免这个问题。 这种方法的另外一个问题就是不能对训练样本进行的 shuffle，由于这深度神经网络的训练都是基于 SGD 模式的，因此需要对其训练样本进行 shuffle，具体可参考这个问题。但是当训练样本很大时，显然无法读取整个文件后在内存进行 shuffle，但是如果将大文件分成小文件后，可在系统内存能够承受的范围内对几个小文件进行shuffle(如相邻的三个小文件)，虽然这种shuffle是一种局部的shuffle，但是可以通过改变进行shuffle的小文件的间隔并进行多次的shuffle（如间隔分别从0递增），从而近似全局的shuffle。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning with Spark 简介]]></title>
      <url>%2F2017%2F11%2F05%2FMachineLearning%20with%20Spark%20%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[本文主要介绍 《Machine Learning with Spark》这本书各章节的主要内容，以及提供该书各章节对应的 python 代码。 这本书主要介绍了如何通过 spark 处理大规模的数据，以及利用这些处理过的数据通过 MLlib 进行模型的训练。全书共分为10章，涵盖了数据的预处理，推荐模型，分类模型，回归模型，聚类模型，数据降维，文本处理以及 Spark 流式处理等内容。 书中的代码大部分是 scala， 某些章节是 python，这里全部通过 python 重写，全部代码参见 github，为了便于交互，采用了 Jupyter Notebook 的形式，且通过 HDFS 存储数据文件，关于环境的搭建可参考这篇文章。 下面简单介绍各个章节的内容（从第三章开始），更多细节可参考具体代码 Chapter 3：Obtaining, Processing, and Preparing Data with Spark这章主要介绍了如何通过 spark 提供的 API 从原始数据中提取特征，如一些常用的函数 map, filter, reduceByKey 等，以及如何通过 spark 进行特征的归一化和处理以下几种 feature category feature：one-hot 编码 text feature：抽取文本 -&gt; 分词 -&gt; 创建字典，为每个单词分配一个唯一的id -&gt; 将文本转为向量 derived feature：这个主要是指从非结构化的特征中抽取出结构化的特征，如给出一个日期，可以提取出其中的小时部分，然后进行分段，分为 morning，noon，afternoon，evening四个阶段，然后再进行 one-hot 编码 Chapter 4：Building a Recommendataion Engine with Spark这一章主要介绍如何通过 spark 构建一个推荐系统，采用了电影评分数据集 MovieLens 100k，使用的是经典的协同过滤技术，而 spark 的 MLlib 则提供了 alternating least squares (ALS) 这种基于矩阵分解的方法用于求解协同过滤问题，通过 ALS对用户-物品评分矩阵进行分解，能够为每个用户或物品推荐 top k 个最相似的用户或物品。度量 ALS 算法效果的指标为 MSE，RMSE等。 Chapter 5：Building a Classifcation Model with Spark这一章主要介绍分类模型，采用了 Kaggle 上的一个数据集，通过 MLlib 提供的 LogisticsRegression，SVM，NaiveBayes，DecisionTree 等分类器对其进行分类，并且比较了进行 feature standardization 前后的效果。 Chapter 6：Building a Regression Model with Spark这一章主要介绍了回归模型，采用了 bike sharing 数据集，主要通过 MLlib 提供的回归模型 Linear Regression 和 Decision Tree进行了预测，并且对目标变量进行了log变换，目的是让目标变量更加接近 正态分布，因为像 Linear Regression 这一类模型对目标函数值的分布做了正态分布的假设。 Chapter 7：Building a Clustering Model with Spark这一章主要介绍了聚类模型，采用的是前面提到的 MovieLens 100k 数据集，通过 ALS 进行矩阵分解，为每个用户(物品)提取出一个隐含属性向量作为用户(物品)的特征向量。然后通过 K-Means 进行聚类。 聚类结果的评估有两种方法：一种是 Internal evaluation，也就是只用数据本身进行评估，一般通过 WCSS (within-cluster sums of squares) 评估；第二种则是 External evaluation，即还通过数据的标签进行评估，评估的方法就是常见的准确率等指标。由于聚类往往是无监督方法，因此数据往往是不带标签的， 因此第一种评估方法比较常见。 Chapter 8：Dimensionality Reduction with Spark这一章主要介绍了spark中的降维技术，采用了 LFW（LabeledFaces in the Wild） 人脸数据集，因此也介绍了图像处理的一些基本操作(基于python的opencv库)，然后通过两种方法：PCA和SVD，进行了数据的降维，实际上这两种方法的关系非常密切，且可以达到相同的效果。同时介绍了 Eigenface 的概念，Eigenface实际上就是PCA提取出来的特征向量再变为人脸图像。 Chapter 9：Advanced Text Processing with Spark这一章主要介绍了 Spark 中的文本处理技术，采用了 20 Newsgroups 数据集，介绍了文本处理的基本操作 分词 过滤停止词，低频词 构建词典 基于词典为每篇文本构造一个向量 在向量的基础上加上 TF-IDF 便可算出表示文本的 TF-IDF 向量，基于这个 TF-IDF 向量可做文本相似性的比较，而如果文本本身是有标签的，可以将 TF-IDF 向量作为文本特征，进而训练一个分类模型。 最后这章还简单介绍了 Word2Vec 模型及其简单应用。 Chapter 10：Real-time Machine Learning with Spark Streaming这章主要介绍了用于处理实时流的 Spark Streaming, 模拟了产生实时流的 producer ，通过 Spark Streaming 处理这些信息流后训练了一个 streaming regression 模型，并通过 online 的方式评估了这个模型的效果。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(684,685,721)-并查集介绍及应用]]></title>
      <url>%2F2017%2F10%2F12%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(684%2C685%2C721)-%E5%B9%B6%E6%9F%A5%E9%9B%86%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
      <content type="text"><![CDATA[本文主要以LeetCode上的几道题目： 684. Redundant Connection 、 685. Redundant Connection II 和 721. Accounts Merge 为例讲解并查集（merge–find set）这种数据结构的应用。 说到并查集，不得不提的是最小生成树，因为并查集的最经典的应用地方便是解决最小生成树的 Kruskal 算法。 最小生成树有两个经典的算法可用来解决 最小生成树 问题： Kruskal 算法 和 Prim 算法。其中 Kruskal 算法中便应用了并查集这种数据结构，该算法的步骤如下 新建图G，G中拥有原图中相同的节点，但没有边 将原图中所有的边按权值从小到大排序 从权值最小的边开始，如果这条边连接的两个节点于图G中不在同一个连通分量中，则添加这条边到图G中 重复3，直至图G中所有的节点都在同一个连通分量中 该算法的动图显示如下(摘自维基百科) Kruskal 算法很简单，实际上 Kruskal 算法是一种贪心算法，并且已被证明最终能够收敛到最好结果。而在实现 Kruskal 算法时，则需要用到并查集这种数据结构来减小算法的时间复杂度。下面将详细介绍这种数据结构。 在介绍并查集前，顺便介绍一下 Prime 算法，Prime 算法也是一种贪心算法，而且也被证明了最终能够得到最好的结果，只是两者的侧重点不同， Kruskal 算法维护的是一个边的集合，而 Prime 算法则同时维护了一个边的集合和一个点的集合，Prim 算法的过程如下 输入：一个加权连通图，其中顶点集合为V，边集合为E； 初始化：Vnew = {x}，其中x为集合V中的任一节点（起始点），Enew = {}； 重复下列操作，直到Vnew = V： 在集合E中选取权值最小的边（u, v），其中u为集合 Vnew 中的元素，而v则是V中没有加入Vnew的顶点（如果存在有多条满足前述条件即具有相同权值的边，则可任意选取其中之一）； 将v加入集合Vnew中，将（u, v）加入集合Enew中； 输出：使用集合Vnew和Enew来描述所得到的最小生成树。 其动图描述如下(摘自维基百科) 并查集在上面描述的 Kruskal 算法中，第三步是 从权值最小的边开始，如果这条边连接的两个节点于图G中不在同一个连通分量中，则添加这条边到图G中 而判断这条边连接的两个节点是否在同一个连通分量中， 实际上就是判断加入了这条边后，是否会与原来已经添加的边形成环路，并查集正是高效的实现了这个功能。 并查集主要有三种操作：MakeSet，Find 和 Union。 MakeSet 是初始化操作，即为每个 node 创建一个连通分量，且这个 node 为这个连通分量的代表，这里连通分量的代表指的是当连通分量中有多个点时，需要从这些点中选出一个点来代表这个连通分量，而这个点也往往被称为这个连通分量的 parent（意思即指这个点是其他点的 parent） Find 是指找到这个点所属的连通分量的 parent Union 是指将两个连通分量合并成一个连通分量，并选出代表这个连通分量的新的 parent 那么怎么通过上面这几种操作判断某条边是否会与原来的边形成环路呢？具体操作如下 给定一条边，为这条边的两个顶点执行 Find 操作，假如两个顶点的 parent 一样，那么说明这两个点已经在同一个连通分量中，再添加就会导致闭环 当两个点的 parent 不同，即两个点在不同的连通分量时，需要通过 Union 操作将这两个连通分量连起来 重复 1、2 步操作直到所有边遍历完 在具体实现，往往并不需要集合这种数据结构，而是仅仅通过数组即可，比如说有 n 个点，那么就创建一个长度为 n 的数组，每个下标代表一个点，而下标对应的值则代表这个点的 parent。 并查集还有两个重要的概念 path compression 和 union by rank，目的均是降低时间复杂度，下面会详细说明。 现在通过具体的题目来讲解上面提到若干概念 Redundant Connection684. Redundant Connection 这道题目实际上就是要找到一个无向图中形成环路的最后那条边(输入保证了所有边会形成回路)。首先，看一种最简单的解决方法 123456789101112class Solution: def findRedundantConnection(self, edges): parents = range(1001) for edge in edges: v1, v2 = edge[0], edge[1] if parents[v1] == parents[v2]: return edge tmp = parents[v2] for i in xrange(len(parents)): if parents[i] == tmp: parents[i] = parents[v1] return None 这种方法中每次 Find 的时间复杂度为 $O(1)$（即 parents[v1] 操作）, 每次 Union 则需要遍历所有的点，时间复杂度是 $O(n)$，总体时间复杂度是 $O(mn)$, $m$ 为边的数目，而 $n$ 为点的数目。 而我们也可以改变思路，就是进行 Union 操作时不再将某个连通分量中所有点的 parent 改为另一个连通分量的 parent，而是只改变那个连通分量的代表；这样进行 Find 操作的时候只需要递归的查找即可，下面为这种思路对应的代码 12345678910111213141516171819202122232425262728class UnionFindSet(object): def __init__(self): self.parents = range(1001) def find(self, val): if self.parents[val] != val: return self.find(self.parents[val]) else: return self.parents[val] def union(self, v1, v2): p1, p2 = self.find(v1), self.find(v2) if p1 == p2: return True else: self.parents[p1] = p2 return False class Solution(object): def findRedundantConnection(self, edges): """ :type edges: List[List[int]] :rtype: List[int] """ ufs = UnionFindSet() for edge in edges: if ufs.union(edge[0], edge[1]): return edge 这个方法每次 Union 的时间复杂度为 $O(1)$, 但是每次 Find 的时间复杂度是 $O(n)$，所以总体时间复杂度还是 $O(mn)$, 那么有没有一种改进总体时间复杂度的方法呢？ 答案就是上面提到的 path compression 和 union by rank。 path compression 指的是在上面的递归的 Find 操作中，将最终得到的结果赋给递归过程中经过的所有点，从而降低连通分量的高度，实际上可以将一个连通分量当做一颗树，树的每个节点都连着其 parent，而 path compression 则相当于将搜寻路径中的所有点直接连到最终的那个 parent 上，因此能够降低树的高度。 降低树的高度有什么好处？那就是能够降低查找的时间复杂度，从 $O(n)$ 降为了 $O(logn)$, 因为原来的递归搜索实际上是在一颗每个节点只有一个子节点的树上进行搜索，树的高度即为点的个数，而通过 path compression 则能够有效降低树的高度。 另外一个问题就是进行 Union 操作时，需要将高度低的树连接到高度较高的树上，目的是为了减少 Union 后的整棵树的高度，这就是 union by rank, rank 代表的就是树的高度。 采用 path compression 和 union by rank 后，Find 的时间复杂度变为了 $O(logn)$, Union 的时间复杂度为 $O(1)$, 因此总体时间复杂度是 $O(mlogn)$, $m$ 为边的数目，而 $n$ 为点的数目。改进后的代码如下 1234567891011121314151617181920212223242526272829303132333435class UnionFindSet(object): def __init__(self): self.parents = range(1001) self.rank = [0] * 1001 def find(self, val): """find with path compression""" if self.parents[val] != val: self.parents[val] = self.find(self.parents[val]) return self.parents[val] def union(self, v1, v2): """union by rank, check whether union two vertics will lead to a cycle""" p1, p2 = self.find(v1), self.find(v2) if p1 == p2: return True elif self.rank[p1] &gt; self.rank[p2]: self.parents[p2] = p1 elif self.rank[p1] &lt; self.rank[p2]: self.parents[p1] = p2 else: self.rank[p2] += 1 self.parents[p1] = p2 return False class Solution(object): def findRedundantConnection(self, edges): """ :type edges: List[List[int]] :rtype: List[int] """ ufs = UnionFindSet() for edge in edges: if ufs.union(edge[0], edge[1]): return edge Redundant Connection II685. Redundant Connection II 从前面的无向图升级到了有向图，对应的要求从原来的仅要求不形成环路升级到在不形成环路的基础上，拓扑必须要是一棵合法树，也就是每个点只能有一个父节点，例如 [[2,1],[3,1]] 这两条边虽然没有形成环路，但是 1 有两个父亲节点（2和3），因此不是一棵合法的树。 由于题目说明了输入只有一条不合法的边，因此首先可以统计一下这些边中是否存在某个点有两个父亲节点，假如有，则需要移除的边必定为连着这个点的两条边中的一条，通过上面 Union-find 的方法，可以判断出假如移除掉连着这个点的第一条边时，是否会形成回路。如果会，则说明需要移除第二条边，否则直接移除第一条边。 如果统计的结果中没有点含有两个父亲节点，那么可以直接通过第一题的方法直接找到形成回路的最后那条边。AC的代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class UnionFindSet(object): def __init__(self): self.parents = range(1001) self.rank = [0] * 1001 def find(self, val): """find with path compression""" if self.parents[val] != val: self.parents[val] = self.find(self.parents[val]) return self.parents[val] def union(self, v1, v2): """union by rank, check whether union two vertics will lead to a cycle""" p1, p2 = self.find(v1), self.find(v2) if p1 == p2: return True elif self.rank[p1] &gt; self.rank[p2]: self.parents[p2] = p1 elif self.rank[p1] &lt; self.rank[p2]: self.parents[p1] = p2 else: self.rank[p2] += 1 self.parents[p1] = p2 return False class Solution(object): def findRedundantDirectedConnection(self, edges): """ :type edges: List[List[int]] :rtype: List[int] """ redundant_edges = None count = &#123;&#125; for e in edges: if e[1] not in count: count[e[1]] = [] count[e[1]].append(e) if len(count[e[1]]) == 2: redundant_edges = count[e[1]] break if redundant_edges: ufs = UnionFindSet() for edge in edges: if edge == redundant_edges[1]: continue if ufs.union(edge[0], edge[1]): return redundant_edges[0] return redundant_edges[1] else: ufs = UnionFindSet() for edge in edges: if ufs.union(edge[0], edge[1]): return edge Accounts Merge这道题目虽然也用到了并查集的数据结构，但是与前面的两道题目又有点不同，主要体现在两个方面 节点不再以数字标识，因此标识 parents 的数据结构要从 array 变为 map 不需要判断是否形成闭环，而要返回最终各个集合内的元素；在这个操作中需要注意的是不能直接利用存储各个节点的 parent 的 map 直接为每个节点找到其 parent， 因为并非各个节点都进行了 path compression。对应有两种方法 (1)借助 find 方法找到各个节点的parent (2) 对存储各个节点的 parent 的 map 再进行一次 path compression, 然后直接在 map 中找到各个节点的 parent 对应的方法入下 方法(1) 123456789101112131415161718192021222324252627282930313233class Solution(object): def accountsMerge(self, accounts): """ :type accounts: List[List[str]] :rtype: List[List[str]] """ owners, parents = &#123;&#125;, &#123;&#125; for account in accounts: owners[account[1]] = account[0] for i in xrange(1, len(account)): parents[account[i]] = account[i] for account in accounts: p = self.find(account[1], parents) for i in xrange(1, len(account)): parents[self.find(account[i], parents)] = p unions = &#123;&#125; for account in accounts: for i in xrange(1, len(account)): p = self.find(account[i], parents) unions.setdefault(p, set()) unions[p].add(account[i]) result = [] for k, v in unions.items(): result.append([owners[k]] + sorted(v)) return result def find(self, email, parents): if parents[email] != email: parents[email] = self.find(parents[email], parents) return parents[email] 方法(2) 12345678910111213141516171819202122232425262728293031323334353637class Solution(object): def accountsMerge(self, accounts): """ :type accounts: List[List[str]] :rtype: List[List[str]] """ owners, parents = &#123;&#125;, &#123;&#125; for account in accounts: owners[account[1]] = account[0] for i in xrange(1, len(account)): parents[account[i]] = account[i] for account in accounts: p = self.find(account[1], parents) for i in xrange(1, len(account)): parents[self.find(account[i], parents)] = p # not all paths are compressed currently for k, v in parents.items(): if k!=v: parents[k] = self.find(parents[v], parents) unions = &#123;&#125; for k, v in parents.items(): if v not in unions: unions[v] = set() unions[v].add(k) result = [] for k, v in unions.items(): result.append([owners[k]] + sorted(v)) return result def find(self, email, parents): if parents[email] != email: parents[email] = self.find(parents[email], parents) return parents[email]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Google 图片爬虫]]></title>
      <url>%2F2017%2F09%2F23%2FGoogle%20%E5%9B%BE%E7%89%87%E7%88%AC%E8%99%AB%2F</url>
      <content type="text"><![CDATA[这里的 Google 图片爬虫指的是爬取在 Google 上通过关键词搜索得到的图片，由于最近需要一些特定领域的图片，而且现有的数据库满足不了要求，因此就想通过 Google 搜索筛选出这些特定领域的图片，然后下载下来后再进行人工筛选。这里采用了两种方法，区别在于是否需要解析网页端的 JS 代码。该项目的代码已经放到了 Github 上，详细代码参见这里。 整体的思路就是先获取 Google 搜索结果页面的 html 代码，然后从中提取出图片的真实 URL，因为实际上 Google 也是通过爬虫来爬取这些图片的地址以及根据描述归类，所以图片并不存储在 Google 的服务器中。 这个过程中的一个关键点就是通过关键词 search_query 搜索得到的页面可直接通过下面这个 url 访问 https://www.google.com/search?q=search_query&amp;source=lnms&amp;tbm=isch 将 search_query 换成需要搜索的关键词即可。 这样便可以直接获取这个页面的源码，然后通过正则表达式解析出所有的图片的链接，这个方法只需要 python 的 urllib 库即可，对应的源码文件为 download_with_urllib.py 上面是第一种方法，这种方法比较简单，当时有一个问题就是每个关键词最多只能下载 100 张图片，原因是 访问上面的链接返回的 HTTP Response 中限制了最多只有前100张图片的URL，如果需要显示更多，则需要在解析页面中相应的 JS 代码，这就是接下来要介绍的第二种方法。 所谓解析 JS 代码，其实就是浏览器的滚动条向下滚动时，浏览器引擎解析了页面的 JS 代码，从而向 Google 的服务器发出新的请求，从而返回更多的图片，当向下滚动时到底时，会出现一个显示为 Show more results需要点击的按钮，只有点击后才能加载更多的图片，这些操作要通过浏览器完成，而 python 提供了一个 selenium 库，这个库通过相应的浏览器驱动来启动浏览器（不同的浏览器对应于不同的驱动），并在代码中制定具体的浏览器操作。 这里采用的是 FireFox 浏览器，对应的驱动为 geckodriver，具体的安装步骤就是先安装 FireFox 浏览器，然后在这里下载与浏览器版本相应的 geckodriver，并且在环境变量 PATH 中添加 geckodriver 的路径。 第二种方法对应的源码文件为 download_with_selenium.py 第二种方法的源码中主要有两个方法: get_image_links和，，第一个方法是获取所有图片链接并写入到文件中，第二个方法则是下载第一个方法获取的文件中的图片；在测试时，第二个方法常常会卡住，原因可能是网络的不稳定，也可能是图片服务器那边的反爬虫机制，代码中没有捕获到 Exception, 因此后面对第二个方法进行了改进，并将新的方法写到文件 download_images_with_time_limit.py 新的方法主要是限制 HTTP 请求的最大耗时，超过这个时间就会抛出异常，实现是通过系统的 signal 来中断进程的，并且用了 SIGALRM 这个信号，而这个信号只在 unix-like 系统中，因此 Windows 无法运行这个脚本。 而 SIGALRM 在 python 中的用法如下： 123456789101112131415import signal, osdef handler(signum, frame): print 'Signal handler called with signal', signum raise IOError("Couldn't open device!")try: # Set the signal handler and a 5-second alarm signal.signal(signal.SIGALRM, handler) signal.alarm(5)exception IOERROR: # This open() may hang indefinitely fd = os.open('/dev/ttyS0', os.O_RDWR)finally: signal.alarm(0) # Disable the alarm]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark 集群部署和 Jupyter Notebook 配置注意事项]]></title>
      <url>%2F2017%2F09%2F13%2FSpark%20%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E5%92%8C%20Jupyter%20Notebook%20%E9%85%8D%E7%BD%AE%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
      <content type="text"><![CDATA[文章主要记录了 spark 2.1.0 集群部署注意事项以及如何通过 Jupyter Notebook 访问 spark 集群。 spark 集群安装注意事项安装过程主要参考了 spark 官方文档 https://spark.apache.org/docs/latest/spark-standalone.html 一些需要注意的事项如下 每台机器需要先安装 Java，并配置环境变量 JAVA_HOME 和 PATH 安装包解压到每台机器上，保持目录一致性，master 能够通过密钥 ssh 到其他 workers 修改 /etc/hosts 文件，保证每台主机能够通过主机名称 ping 通其他机器，注意，如果原来 /etc/hosts 文件中有类似于 127.0.0.1 hostname 的解释需要注释掉（因为会与前面设置 ip hostname冲突） master 创建 conf/slaves,每行包含一个 worker 的 hostname 各个主机的 python 路径和版本要一致，在各台主机上的conf/spark-env.sh中定义如下 12export PYSPARK_PYTHON=/opt/miniconda2/bin/pythonexport PYSPARK_DRIVER_PYTHON=ipython 通过 sbin 中的script start-all.sh 启动整个集群 默认情况下 worker(slave) 所有的 cores 都会使用，但是内存默认只会使用 1g(可通过 http://SparkMaster:8080/查看，SparkMaster 为 master 的ip)，如果需要修改可用的资源，需要修改 conf/spark-defaults.conf 文件，如下是设置了每个 worker 分配 4 个 core 和 6g 内存给 executor，同时设定了 driver program 使用的 core 的数量和内存大小。1234spark.executor.cores 4spark.executor.memory 6gspark.driver.cores 4spark.driver.memory 6g 这里需要注意的是 conf/spark-env.sh 也有内存和cores相关的设定，但是设定的是可使用的最大值，并不是实际的使用值，设定实际的使用值必须要在文件 conf/spark-defaults.conf 中设置，且当 conf/spark-defaults.conf 设定值大于 conf/spark-env.sh 时，该项不生效，也就是实际的资源值会变为0。 另外，spark 只是一个计算框架，并不提供存储的功能，往往需要结合其他的分布式数据库 HBase 或分布式文件系统 HDFS 等使用， 从中读取数据并进行将结果保存在其中。 jupyter notebook 配置jupyter notebook 的前身是 ipython notebook，网上的基本是通过 ipython 建立 profile 文件来解决，但是 jupyter notebook 已经不支持 profile 参数了，因此这种方法无效。 通过 Apache Toree 可以建立 jupyter notebook 使用的 kernel，从而将 kernel 连接到spark上，本来通过pip install toree可以简单地安装toree，但是由于使用 spark 的版本是 2.1.0, 其对应的Scala的版本是2.11, 这个版本的 Scala 与 toree 中的 2.10 的 Scala 版本不符。因此需要重新编译toree并安装，编译toree并安装可参考以下教程。 主要过程如下 1.如果没有安装sbt，需要先安装sbt1234echo &quot;deb https://dl.bintray.com/sbt/debian /&quot; | sudo tee -a /etc/apt/sources.list.d/sbt.listsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823sudo apt-get updatesudo apt-get install sbt 2.下载并编译toree的源码 12git clone https://github.com/apache/incubator-toreecd incubator-toree/ 编译前需要先修改 MakeFile 文件中的 APACHE_SPARK_VERSION、SCALA_VERSION为对应的版本。我这里使用spark 2.1.0，因此修改成如下 12APACHE_SPARK_VERSION?=2.1.0SCALA_VERSION?=2.11 然后执行 make dist &amp;&amp; make release 这时会报错 /bin/sh: 1: docker: not found,主要原因是没安装docker，可以忽略这个错误，因为docker主要作用是将安装文件打包在一起再通过pip安装，但是这些文件已经生成在 dist 目录下，可以直接安装，因此可以执行下面的命令进行安装 12cd dist/toree-pip/python setup.py install 通过 toree 生成相应的 kernel 供jupyter notebook使用 可以通过 jupyter toree install --spark_home= path_to_spark-2.1.0 --interpreters=PySpark/SparkR/SQL/Scala 安装对应的kernel，然后在web界面选择相应的kernel即可。 最后的问题，toree 默认使用的是单机模式，如果需要使用集群模式，需要通过环境变量设置提交时的参数，从这个脚本文件 /usr/local/share/jupyter/kernels/apache_toree_pyspark/bin/run.sh 可以看出提交任务时的参数通过 SPART_OPTS 获取，因此可以将这个换件变量的值设置为 --master spark://ip:7077 从而通过集群运行任务。 通过上面搭建的环境与 Spark 集群进行交互的一些 Jupyter Notebook 样例可参考 https://github.com/WuLC/MachineLearningWithSpark 参考资料 Machine Learning with Jupyter using Scala, Spark and Python: The SetupInstalling Toree+Spark 2.1 on Ubuntu 16.04]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度学习在表情识别中的应用]]></title>
      <url>%2F2017%2F09%2F03%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E6%8E%A2%E7%B4%A2%2F</url>
      <content type="text"><![CDATA[本文的表情识别指的是给出一张图片，检测其中人脸的表情（如果含有人脸的话）。所有可能的表情种类往往事先约定好,粗分可以分为 positive、negative、neutral 三种，细分可以分为 neutral, angry, surprise, disgust, fear, happy, sad 7种或者更多种，这里的类别可根据具体采用的数据集进行调整，从机器学习的角度来说，这实际上就是一个多分类问题。 本文主要讲述如何将深度学习应用在表情识别中，以及在图像分类中深度学习一些常用方法，如采用预训练的模型进行特征的提取，用数据集对预训练的模型进行 Fine-tunning，而这实际上又牵涉到了迁移学习。 之所以采用深度学习的方法，是因为深度学习中的网络（尤其是CNN）对图像具有较好的提取特征的能力，从而避免了人工提取特征的繁琐，人脸的人工特征包括常用的 68 个 Facial landmarks 等其他的特征，而深度学习除了预测外，往往还扮演着特征工程的角色，从而省去了人工提取特征的步骤。下面首先讲述深度学习中常用的网络类型，然后讲述通过预训练的网络(经过ImageNet进行预训练)对图像提取特征，以及对预训练的网络采用自己的数据进行微调的 Fine-Tunning。 数据集表情识别中常用的数据集有 CK+， MMI， JAFFE， KDEF等，这些数据有些是短视频，有些是图片序列（记录一个表情的若干张图片），有些则是单张表情图片。 在训练时，需要根据实际的应用场景以及采用的模型的输入格式将这些数据集处理成相关格式，这里不在详细说明。 网络类型假如采用深度学习中常用的网络层 cnn，rnn， fully-connect 等层组合成网络，那么具有非常多种的选择，这些网络的性能需要在实际任务中检验，而经过实践发现，某些网络结构往往在图像分类上具有较好的结果，如ImgeNet比赛中提出的一些列模型：AlexNet，GoogleNet（Inception), VGG， ResNet 等。这些网络已经经过了 ImageNet 这个数据集的考验，因此在图像分类问题中也常被采用。 至于网络的结构，往往是先通过若干层 CNN 进行图像特征的提取，然后通过全连接层进行非线性分类，这时的全连接层就类似与MLP，只是还加入了 dropout 等机制防止过拟合等，最后一层有几个分类就连接几个神经元，并且通过 softmax 变换得到样本属于各个分类的概率分布。如下是 AlexNet 的网络结构图 AlexNet 关于AlexNet 更详细的介绍可参考这篇文章。 Inception而 Inception 是 Google 研发的一个深度神经网络，经历了四个版本 （也叫GoogLeNet），各个版本及其对应的论文如下，各个版本 [v1] Going Deeper with Convolutions, 6.67% test error, http://arxiv.org/abs/1409.4842[v2] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 4.8% test error, http://arxiv.org/abs/1502.03167[v3] Rethinking the Inception Architecture for Computer Vision, 3.5% test error, http://arxiv.org/abs/1512.00567[v4] Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, 3.08% test error, http://arxiv.org/abs/1602.07261 Inception 的结构与前面的 AlexNet 的大同小异，其核心是多了 Inception 模块，而 Inception 模块的结构如下 Inception 模块使得网络的当前层可以通过多种方式从前一层网络提取特征，如上图通过了三种不同大小的卷积层以及一个池化层，然后将这些特征进行 concate 送到下一层。 如下是 Inception V3 的结构 VGGVGG 并没有采用很新颖的结构，整个网络只是采用了 3X3 的卷积层以及 2X2 的池化层，但是层数比较深，就是这么一个靠两种简单网络层堆叠起来的网络，却在 ImageNet 比赛上取得了非常好的结果。VGG 主要工作是证明了增加网络的深度能够在一定程度上影响网络最终的性能，越深的网络能够容纳更多数据的信息，对于更大的数据具有更好的效果，VGG整个网络的结构如下 ResNet前面 VGG 提到了网络的深度（层数）起到了一个非常重要的作用，但是如果只是简单地将层堆叠在一起，增加网络的深度并不会起太大作用。这是由于梯度消失和爆炸（vanishing/exploding gradient）问题，深层的网络很难训练。因为梯度反向传播到前一层，重复相乘可能使梯度无穷小。结果就是，随着网络的层数更深，其性能趋于饱和，甚至开始迅速下降。 而为了解决因深度增加而产生的性能下降问题， ResNet 引入一个“身份捷径连接”（identity shortcut connection），直接跳过一层或多层，如下图所示： 提出 ReNet 的这篇论文指出，假设目标映射为 $H(x)$，这个模块并不是让 stacked layers 去直接拟合这个目标映射，而是去拟合残差 $F(x) := H(x)-x$，则拟合的 $H(x)$ 则变为了 $F(x) + x$, 论文假设优化残差 $F(x)$ 比优化 H(x) 更容易。 上面这段话基本翻译自提出 ResNet 这篇论文，更直观的理解就是每一层不仅仅只是能从前一层获取信息了，而是还可以从更前面的几层获取，而 ResNet 最开始的只是建单地将更前面几层的 x 直接加到当前层的输出，也就是 $F(x) + x$， 而 ResNet 的若干变体则对这部分直接传递的 $x$ 进行了处理，如通过卷积层等操作，但是其核心思想还是跨层连接从而获得更多的信息。 最开始提出的 ResNet 的结构如下 CNN-LSTM上面的模型均是对单张图像进行处理，但是还有一种模型是对连续图片 sequence 进行处理的，由于连续图片 sequence 包含了时序信息，因此通过将 CNN 与 RNN 进行结合对时序图片列进行预测。在表情识别中 CNN-LSTM 是将 CNN 与 LSTM 结合起来的一种模型，其基本结构如下，图片出自该论文 其思想就是首先通过 cnn 提取每张图片的特征，然后将这些带时序的特征传入 LSTM 中，可以取每一个 LSTM 的输出进行平均后连接 softmax 进行输出，也可以直接取最后一个 LSTM 的输出连接 softmax 作为输出。 上面这些网络训练的时候均是通过 SGD 进行反向传播，某些会加入 momentum 等其他改进。 这些网络理解起来可能问题不大，但是如果要代码实现起来的话工作量并不小，好在已经有了若干框架实现了这些模型，在使用时直接调用即可，这里以最简单的框架 Keras 为例说明，Kreas 已经在这里列出了一些实现的网络，可以参考其文档直接进行调用。 预训练模型提取特征上面提到的这些模型少则十几层，多则上百层，其参数数目也达到了百万级别，要训练这么庞大的一个网络，如果数据量不足，很容易会会导致整个网络过拟合（当然，如果训练epoch次数少，也会直接导致欠拟合）。 而在实际中，像 ImageNet 这种庞大的数据集很少，而且某些只是归少数大公司所有，假如个人或缺乏数据的小公司需要用到上面提到的网络时，那就是无米之炊了，因此在实际中使用时，往往不是从头开始训练一个很大的模型，而是采用下面提到的通过模型提取特征以及对模型进行 Fine-Tunning 的方法。 深度学习中的网络的一个好处就是，经过大规模数据集训练过后的，网络具有了抽取图像特征的特性，而抽取出来的图像的特征，跟实际要处理任务没有关系，也就是说经过 ImageNet 训练过后的网络，也可以用于表情识别中抽取人脸的特征，然后用这些特征再训练一个小一点的模型，如 Logistics Regression， SVM等，这时候的网络完全就是在扮演着一个自动特征工程的角色。 这里“通过网络提取的特征”往往指的是网络最后的某个全连接层的输出值，具体采用哪一层取决于后续处理所需的特征维数。 Keras 也提供了经过 ImageNet 预训练的一些模型，通过这些模型抽取图像特征的样例代码如下 123456789101112131415from keras.applications.vgg19 import VGG19from keras.preprocessing import imagefrom imagenet_utils import preprocess_inputfrom keras.models import Modelbase_model = VGG19(weights='imagenet')model = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)img_path = 'elephant.jpg'img = image.load_img(img_path, target_size=(224, 224))x = image.img_to_array(img)x = np.expand_dims(x, axis=0)x = preprocess_input(x)block4_pool_features = model.predict(x) 在实际测试时，用 VGG16 抽取图像的特征后再经过带 L1 正则化的Logistics Regression，再CK+上进行10-fold cross validation， 得到的准确率约为85%, 也说明了这种方法的有效性。 对预训练模型进行微调(Fine-Tunning)上面通过预训练的网络提取特征的确有效果，但是这些经过预训练的网络基本都是在 ImageNet 数据集上进行训练的，而实际中的各种任务是千差万别的，光凭 ImageNet 是难以涵盖各个领域的需求的。 因此很自然会想到在预训练的网络基础上，用涉及到的具体任务中的数据集再次训练这个网络，从而让这个网络能够学习到这个数据集内的信息，这种方法也称为 Fine-Tunning。 Keras 的官方blog也写了一篇文章专门阐述这种方法，文章链接为 https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html 在这篇文章中，并没有调整整个网络的参数，而是只调整了最后的几层卷积层和全连接层，文章称原因是越底层的卷积层所提取到的图像的特性越是有共性的特征，而越上层的卷积层提取的特征则越是跟具体的领域相关的，当然，到底要调整多少层，还取决于所拥有的数据量，另外，往往还会去掉网络最后的若干层，并根据实际的图像分类数目构建最后一层大小。 通过 Keras 进行 Fine-Tunning 的样例代码如下 123456789101112131415161718192021from keras.applications.vgg19 import VGG19height, width, categoriees = 128, 128, 7model_vgg19_conv = VGG19(weights = 'imagenet', include_top = False)# just fine-tune the top five convulutional layersfor layer in model_vgg19_conv.layers[:-5]: layer.trainable = False#Create your own input format (here 128x128X3)input = Input(shape=(height, width, 3),name = 'image_input')#Use the generated model output_vgg19_conv = model_vgg19_conv(input)#Add the fully-connected layers x = Flatten(name='flatten')(output_vgg19_conv)x = Dense(feature_dim, activation='relu', name='fc1')(x)# x = Dense(feature_dim, activation='relu', name='fc2')(x)x = Dense(categories, activation='softmax', name='predictions')(x)#Create your own model model = Model(inputs = input, outputs = x) 最后讲的利用了预训练模型的两部分实际上可以归入到迁移学习的范畴了，原因是我们利用了模型在 ImageNet 上学到的知识，迁移到了一个新的领域(表情识别），同理，也可以将其推广至医学影像等领域，当然，迁移学习远不止这点内容，有兴趣的可以去查找相关资料，这里不在论述。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 中的字符串与编码]]></title>
      <url>%2F2017%2F08%2F28%2Fpython%20%E4%B8%AD%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%8E%E7%BC%96%E7%A0%81%2F</url>
      <content type="text"><![CDATA[本文主要参考了这篇文章，该文章比较清楚地讲述了字符串的编码问题，并且在 python2 和 python3 中如何区分使用两者。 字符编码因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。最早的计算机在设计时采用8个比特（bit）作为一个字节（byte），所以，一个字节能表示的最大的整数就是255（二进制 11111111 = 十进制 255），如果要表示更大的整数，就必须用更多的字节。比如两个字节可以表示的最大整数是65535，4个字节可以表示的最大整数是4294967295。 由于计算机是美国人发明的，因此，最早只有127个字母被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为 ASCII 编码，比如大写字母A的编码是65，小写字母z的编码是122。 但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和 ASCII 编码冲突，所以，中国制定了 GB2312 编码，用来把中文编进去。 你可以想得到的是，全世界有上百种语言，日本把日文编到 Shift_JIS 里，韩国把韩文编到 Euc-kr 里，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。 因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。 Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。 现在，捋一捋ASCII编码和Unicode编码的区别：ASCII编码是1个字节，而Unicode编码通常是2个字节。 字母A用ASCII编码是十进制的65，二进制的01000001； 字符0用ASCII编码是十进制的48，二进制的00110000，注意字符’0’和整数0是不同的； 汉字中已经超出了ASCII编码的范围，用Unicode编码是十进制的20013，二进制的01001110 00101101。 你可以猜测，如果把ASCII编码的A用Unicode编码，只需要在前面补0就可以，因此，A的Unicode编码是00000000 01000001。 新的问题又出现了：如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。 所以，本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间；注意UTF-8并不是唯一对Unicode进行编码的编码方式。 字符 ASCII Unicode UTF-8 A 01000001 00000000 01000001 01000001 中 x 01001110 00101101 11100100 10111000 10101101 从上面的表格还可以发现，UTF-8编码有一个额外的好处，就是ASCII编码实际上可以被看成是UTF-8编码的一部分，所以，大量只支持ASCII编码的历史遗留软件可以在UTF-8编码下继续工作。 搞清楚了ASCII、Unicode和UTF-8的关系，我们就可以总结一下现在计算机系统通用的字符编码工作方式： 在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。 如用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件： 而浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器： 所以你看到很多网页的源码上会有类似 &lt;meta charset=&quot;UTF-8&quot; /&gt; 的信息，表示该网页正是用的UTF-8编码。 python中的字符串搞清楚了令人头疼的字符编码问题后，我们再来研究Python对Unicode的支持。 因为Python的诞生比Unicode标准发布的时间还要早，所以最早的Python只支持ASCII编码，普通的字符串’ABC’在Python内部都是ASCII编码的。Python提供了 ord() 和 chr() 函数，可以把字母和对应的数字相互转换： 1234&gt;&gt;&gt; ord('A')65&gt;&gt;&gt; chr(65)'A' Python在后来添加了对Unicode的支持，以Unicode表示的字符串用 u&#39;...&#39; 表示，比如： 1234&gt;&gt;&gt; print u'中文'中文&gt;&gt;&gt; u'中'u'\u4e2d' 写 u&#39;中&#39; 和 u&#39;\u4e2d&#39; 是一样的，\u后面是十六进制的Unicode码。因此，u&#39;A&#39;和u&#39;\u0041&#39;也是一样的。 两种字符串如何相互转换？字符串’xxx’虽然是ASCII编码，但也可以看成是UTF-8编码，而u’xxx’则只能是Unicode编码。 把 u&#39;xxx&#39; 转换为UTF-8编码的 &#39;xxx&#39; 用 encode(&#39;utf-8&#39;) 方法： 1234&gt;&gt;&gt; u'ABC'.encode('utf-8')'ABC'&gt;&gt;&gt; u'中文'.encode('utf-8')'\xe4\xb8\xad\xe6\x96\x87' 英文字符转换后表示的UTF-8的值和Unicode值相等（但占用的存储空间不同），而中文字符转换后1个Unicode字符将变为3个UTF-8字符，上面的\xe4就是其中一个字节，因为它的值是228，没有对应的字母可以显示，所以以十六进制显示字节的数值。len()函数可以返回字符串的长度： 12345678&gt;&gt;&gt; len(u'ABC')3&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; len(u'中文')2&gt;&gt;&gt; len('\xe4\xb8\xad\xe6\x96\x87')6 反过来，把UTF-8编码表示的字符串&#39;xxx&#39;转换为Unicode字符串u&#39;xxx&#39;用decode(&#39;utf-8&#39;)方法：123456&gt;&gt;&gt; 'abc'.decode('utf-8')u'abc'&gt;&gt;&gt; '\xe4\xb8\xad\xe6\x96\x87'.decode('utf-8')u'\u4e2d\u6587'&gt;&gt;&gt; print '\xe4\xb8\xad\xe6\x96\x87'.decode('utf-8')中文 两者的转换方式总结来说就是从 Unicode 到 UTF-8 通过 encode() 方法， 而从 UTF-8 到 Unicode 通过 decode() 方法, 而且UTF-8并不是唯一的编码方式 在编写 python 程序时，如果代码中有中文，那么就需要在代码的开始地方加上 # -*- coding: utf-8 -*-, 这是为了是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。而且这样声明了UTF-8编码并不意味着.py文件就是UTF-8编码的，必须并且要确保编辑器在保存文件的时候使用 UTF-8 编码方式。 最后需要注意的是，由于历史遗留问题，python 2.x里的字符串用 &#39;xxx&#39; 表示 str，Unicode字符串用 u&#39;xxx&#39; 表示unicode，而在3.x中，所有字符串都被视为unicode，因此，写 u&#39;xxx&#39; 和 &#39;xxx&#39; 是完全一致的，而在2.x中以 &#39;xxx&#39; 表示的str就必须写成 b&#39;xxx&#39;，以此表示“二进制字符串”.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过 sklearn 进行大规模机器学习]]></title>
      <url>%2F2017%2F08%2F08%2F%E9%80%9A%E8%BF%87%20sklearn%20%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
      <content type="text"><![CDATA[sklearn 是 python 中一个非常著名的机器学习库，但是一般都是在单机上使用而不支持分布式计算，因此往往跟大规模的机器学习扯不上关系。这里通过 sklearn 进行的大规模机器学习指的也不是分布式机器学习，而是指当数据量比内存要大时怎么通过 sklearn 进行机器学习，更准确来说是 out-of-core learning， 这里涉及到的一个核心思想是将数据转化为流式输入，然后通过 SGD 更新模型的参数，当然其中还涉及到一些其他的细节和trick，下面会详细描述。 out-of-core learning上面的问题也称为 out-of-core learning， 指的是机器的内存无法容纳训练的数据集， 但是硬盘可容纳这些数据，这种情况在数据集较大的时候比较常见，一般有两种解决方法：sampling 与 mini-batch learning。 第一种方法是 sampling（采样），采样 可以针对样本数目或样本特征，能够减少样本的数量或者feature的数目，两者均能减小整个数据集所占内存，但是采样无可避免地会丢失掉原来数据集中的一些信息（当数据没有冗余的时候），这会导致 variance inflation 问题，也就是进行若干次采样，每次训练得出的模型之间差异都比较大。用 bias-variance 来解释就是出现了high variance，原因是每次采样得到的数据中都有随机噪声，而模型拟合了这些没有规律的噪声，从而导致了每次得到的模型都不一样。 解决采样带来的 high-variance 问题，可以通过训练多个采样模型，然后将其进行集成，采用这种思路典型的方法有 bagging。 第二种方法是 mini-batch learning，这种方法不同于 sampling， 利用了全部的数据， 只是每次只用一部分样本（可以是一个样本，也可以是多个样本）来训练模型，通过增加迭代的次数可以近似用全部数据集训练的效果，这种方法需要训练的算法的支持，SGD 恰好就能够提供这种模式的训练，因此 SGD 是这种模式训练的核心。下面也主要针对这种方法进行讲述。 通过 SGD 进行训练时，需要流式（streaming）读取训练样本，同时注意的是要将样本的顺序随机打乱,以消除样本顺序带来的信息，如先用正样本训练，再用负样本训练，模型会偏向于将样本预测为负。下面主要讲述如何将磁盘上的数据流式化并送入到模型中进行训练。 流式读取数据文件读取这里读取的数据的格式是每行存储一个样本。最简单的方法就是通过 python 读取文件的 readline 方法实现 123456with open(source_file, 'rb') as f: line = f.readline() while line: # data processing # training line = f.readline() 而往往训练文件都是 csv 格式的，此时需要丢弃第一行，同时可通过 csv 模块进行读取，下面以这个数据文件为例说明：https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip 12345678910111213SEP = ',' with open(source_file, 'rb') as f: iterator = csv.reader(f, delimiter = SEP) for n, row in enumerate(iterator): if n == 0: header = row else: # data processing # training pass print ('Total rows: %i' % (n+1)) print ('Header: %s' % ', '.join(header)) print ('Sample values: %s' % ', '.join(row)) 输出为 123Total rows: 17380Header: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cntSample values: 17379, 2012-12-31, 1, 1, 12, 23, 0, 1, 1, 1, 0.26, 0.2727, 0.65, 0.1343, 12, 37, 49 在上面的例子中，每个样本是就是一个 row（list 类型），样本的 feature 只能通过 row[index] 方式获取，假如要通过 header 中的名称获取， 可以修改上面获取 iterator 的代码，用 csv.DictReader(f, delimiter = SEP) 来获取 iterator，此时得到的 row 会是一个 dictionary，key 为 header 中的列名，value 为对应的值; 对应的代码为 12345678with open(source_file, 'rb') as R: iterator = csv.DictReader(R, delimiter=SEP) for n, row in enumerate(iterator): # data processing # training pass print ('Total rows: %i' % (n+1)) print ('Sample values: %s' % row) 输出为 12Total rows: 17379Sample values: &#123;&apos;mnth&apos;: &apos;12&apos;, &apos;cnt&apos;: &apos;49&apos;, &apos;holiday&apos;: &apos;0&apos;, &apos;instant&apos;: &apos;17379&apos;, &apos;temp&apos;: &apos;0.26&apos;, &apos;dteday&apos;: &apos;2012-12-31&apos;, &apos;hr&apos;: &apos;23&apos;, &apos;season&apos;: &apos;1&apos;, &apos;registered&apos;: &apos;37&apos;, &apos;windspeed&apos;: &apos;0.1343&apos;, &apos;atemp&apos;: &apos;0.2727&apos;, &apos;workingday&apos;: &apos;1&apos;, &apos;weathersit&apos;: &apos;1&apos;, &apos;weekday&apos;: &apos;1&apos;, &apos;hum&apos;: &apos;0.65&apos;, &apos;yr&apos;: &apos;1&apos;, &apos;casual&apos;: &apos;12&apos;&#125; 除了通过 csv 模块进行读取，还可以通过 pandas 模块进行读取，pandas 模块可以说是处理 csv 文件的神器，csv 每次只能读取一条数据，而 pandas 可以指定每次读取的数据的数目，如下所示 12345678910import pandas as pdCHUNK_SIZE = 1000with open(source_file, 'rb') as R: iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) for n, data_chunk in enumerate(iterator): print ('Size of uploaded chunk: %i instances, %i features' % (data_chunk.shape)) # data processing # training pass print ('Sample values: \n%s' % str(data_chunk.iloc[0])) 对应的输出为12345678910111213141516171819202122232425262728293031323334353637Size of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 379 instances, 17 featuresSample values: instant 17001dteday 2012-12-16season 4yr 1mnth 12hr 3holiday 0weekday 0workingday 0weathersit 2temp 0.34atemp 0.3333hum 0.87windspeed 0.194casual 1registered 37cnt 38Name: 17000, dtype: object 数据库读取上面的是直接从文件中读取的数据，但是数据也可能存在数据库中，因为通过数据库不经能够有效进行增删查改等操作，而且通过 Database Normalization 能够在不丢失信息的基础上减少数据冗余性。 假设上面的数据已经存储在 SQLite 数据库中，则流式读取的方法如下 12345678910111213141516import sqlite3import pandas as pdDB_NAME = 'bikesharing.sqlite'CHUNK_SIZE = 2500conn = sqlite3.connect(DB_NAME)conn.text_factory = str # allows utf-8 data to be stored sql = "SELECT H.*, D.cnt AS day_cnt FROM hour AS H INNER JOIN day as D ON (H.dteday = D.dteday)"DB_stream = pd.io.sql.read_sql(sql, conn, chunksize=CHUNK_SIZE)for j,data_chunk in enumerate(DB_stream): print ('Chunk %i -' % (j+1)), print ('Size of uploaded chunk: %i istances, %i features' % (data_chunk.shape)) # data processing # training pass 输出为 1234567Chunk 1 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 2 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 3 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 4 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 5 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 6 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 7 - Size of uploaded chunk: 2379 istances, 18 features 样本的读取顺序上面简单提到了通过 SGD 训练模型的时候，需要注意样本的顺序必须要是随机打乱的。 假如给定一批样本，然后用整批的样本来更新，那么就不存在样本的读取顺序问题；但是由于像 SGD 这种 online learning 的训练模式，越是后面才读取的样本，模型一般会拟合得更好，因为这是模型最近看到了这些样本且针对这些样本进行了调整。 这样的特性有其好处，如处理时间序列的数据时，由于对最近时间的数据拟合得更好，因此不会受到时间太久远的数据的影响，但是在更多的情况下，这种由样本顺序带来的是有弊无益的，如上面提到的先用全部的正样本训练，再用全部的负样本训练。因此有必要对数据先进行 shuffle ，然后再通过 SGD 来进行训练。 假如内存能够容纳这些数据， 那么所有的数据可以在内存中进行一次 shuffle；假如无法容纳，则可以将整个大的数据文件分为若干个小的文件，分别进行 shuffle ，然后再拼接起来，拼接时也不按照原来的顺序，而是进行 shuffle 后再拼接， 下面是这两种 shuffle 方法的实现代码。 在内存中进行 shuffle 之前可以通过 zlib 对样本先进行压缩，从而让内存可以容纳更多的样本，实现代码如下 1234567891011121314import zlibfrom random import shuffledef ram_shuffle(filename_in, filename_out, header=True): with open(filename_in, 'rb') as f: zlines = [zlib.compress(line, 9) for line in f] if header: first_row = zlines.pop(0) shuffle(zlines) with open(filename_out, 'wb') as f: if header: f.write(zlib.decompress(first_row)) for zline in zlines: f.write(zlib.decompress(zline)) 基于磁盘的 shuffle 方法首先将整个文件划分为若干个小文件，然后再进行 shuffle， 为了能够实现整个数据集更彻底的 shuffle ，可以将上面的过程重复几遍，同时每次都改变划分的文件的大小，实现的代码如下 12345678910111213141516171819202122232425from random import shuffleimport pandas as pdimport numpy as npimport osdef disk_shuffle(filename_in, filename_out, header=True, iterations = 3, CHUNK_SIZE = 2500, SEP=','): for i in range(iterations): with open(filename_in, 'rb') as R: iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) for n, df in enumerate(iterator): if n==0 and header: header_cols =SEP.join(df.columns)+'\n' df.iloc[np.random.permutation(len(df))].to_csv(str(n)+'_chunk.csv', index=False, header=False, sep=SEP) ordering = list(range(0,n+1)) shuffle(ordering) with open(filename_out, 'wb') as W: if header: W.write(header_cols) for f in ordering: with open(str(f)+'_chunk.csv', 'r') as R: for line in R: W.write(line) os.remove(str(f)+'_chunk.csv') filename_in = filename_out CHUNK_SIZE = int(CHUNK_SIZE / 2) sklearn 中的 SGD通过前面的步骤可以将数据以流式输入，下面接着就是要通过 SGD 进行训练，在 sklearn 中， sklearn.linear_model.SGDClassifier 和 sklearn.linear_model.SGDRegressor 均是通过 SGD 实现，只是一个用于分类，一个用于回归。下面以 sklearn.linear_model.SGDClassifier 为例进行简单说明，更详细的内容可参考其官方文档。这里仅对其几个参数和方法进行简单的讲解。 需要注意的参数有： loss : 表示具体的分类器，可选的值为 hinge、log、modified_huber、squared_hinge、perceptron；如 hinge 表示 SVM 分类器，log 表示logistics regression等 penalty：正则项，用于防止过拟合(默认为 L2 正则项) learning_rate: 表示选择哪种学习速率方案，共有三种：constant、optimal、invscaling，各种详细含义可参考官方文档 需要注意的方法主要就是 partial_fit(X, y, classes), X 和 y 是每次流式输入的数据，而 classes 则是具体的分类数目, 若 classes 数目大于2，则会根据 one-vs-rest 规则训练多个分类器。 需要注意的是 partial_fit 只会对数据遍历一次，需要自己显式指定遍历的次数，如下是使用 sklearn 中的 SGDClassfier 的一个简单例子。 12345678910111213from sklearn import linear_modelimport pandas as pdCHUNK_SIZE = 1000n_iter = 10 # number of iteration over the whole datasetn_class = 7model = linear_model.SGDClassifier(loss = 'hinge', penalty ='l1',)for _ in range(n_iter): with open(source_file, 'rb') as R: iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) for n, data_chunk in enumerate(iterator): model.partial_fit(data_chunk.x, data_chunk.y, classes = np.array(range(0, n_class))) 流式数据中的特征工程feature scaling对于 SGD 算法，特征的 scaling 会影响其优化过程，也就是只有将特征标准化（均值为0，方差为1）或归一化（处于 [0,1] 内）才能加快算法收敛的速度，但是由于数据不能一次读入内存，如果需要标准化或归一化，需要对数据遍历 2 次，第一次遍历是为了求特征的均值和方差（标准化需要）或最大最小值（归一化需要），第二次遍历便可以用上面的均值、方差、最大值，最小值等值进行标准化。 由于数据是流式输入的，求解均值、最大值、最小值都没有什么问题，但是求解方差的公式为( $\mu$ 为均值） $$\sigma^2 = \frac{1}{n} \sum_x(x-\mu)^2$$ 只有知道均值才能求解， 这意味着只有遍历一次求得 $\mu$ 后才能求 $\sigma^2$, 这无疑会增加求解的时间，下面对这个公式进行简单的变换，使得 $\mu$ 和 $\sigma^2$ 能够同时求出 假如当前有 $n$ 个样本，当前的均值 $\mu’$ 可以简单求出，而当前的方差 $\sigma’^2$ 可通以下公式求解 $$\sigma’^2 = \frac{1}{n} \sum_x(x^2 - 2x\mu’ + \mu’^2) = \frac{1}{n} \sum_x(x^2) - \frac{1}{n}(2n\mu’^2 - n\mu’^2) = \frac{1}{n} \sum_x(x^2) - \mu’^2$$ 通过这个公式，可以遍历一次便求出任意个样本的方差，下面通过这个公式求解均值和方差随着样本数量变化而变化的情况，并比较进行 shuffle 前后两者在均值和方差上的区别。 12345678910111213141516171819202122232425262728293031323334# calculate the running mean,standard deviation, and range reporting the final resultimport os, csvraw_source = 'bikesharing/hour.csv' # unshuffleshuffle_source = 'bikesharing/shuffled_hour.csv'def running_statistic(source): SEP=',' running_mean = list() running_std = list() with open(local_path+'/'+source, 'rb') as R: iterator = csv.DictReader(R, delimiter=SEP) x = 0.0 x_squared = 0.0 for n, row in enumerate(iterator): temp = float(row['temp']) if n == 0: max_x, min_x = temp, temp else: max_x, min_x = max(temp, max_x),min(temp, min_x) x += temp x_squared += temp**2 running_mean.append(x / (n+1)) running_std.append(((x_squared - (x**2)/(n+1))/(n+1))**0.5) # DATA PROCESSING placeholder # MACHINE LEARNING placeholder pass print ('Total rows: %i' % (n+1)) print ('Feature \'temp\': mean=%0.3f, max=%0.3f, min=%0.3f,sd=%0.3f' \ % (running_mean[-1], max_x, min_x, running_std[-1])) return running_mean, running_stdprint '===========raw data file==========='raw_running_mean, raw_running_std = running_statistic(raw_source)print '===========shuffle data file==========='shuffle_running_mean, shuffle_running_std = running_statistic(shuffle_source) 输出如下123456===========raw data file===========Total rows: 17379Feature &apos;temp&apos;: mean=0.497, max=1.000, min=0.020,sd=0.193===========shuffle data file===========Total rows: 17379Feature &apos;temp&apos;: mean=0.497, max=1.000, min=0.020,sd=0.193 两者的统计数据一致，符合要求，下面再看看两者的均值和方差随着时间如何变化，也就是将上面得到的 running_mean 和 running_std 进行可视化 12345678910111213# plot how such stats changed as data was streamed from disk# get an idea about how many instances are required before getting a stable mean and standard deviation estimateimport matplotlib.pyplot as plt%matplotlib inlinefor mean, std in ((raw_running_mean, raw_running_std), (shuffle_running_mean, shuffle_running_std)): plt.plot(mean,'r-', label='mean') plt.plot(std,'b-', label='standard deviation') plt.ylim(0.0,0.6) plt.xlabel('Number of training examples') plt.ylabel('Value') plt.legend(loc='lower right', numpoints= 1) plt.show()# The difference in the two charts reminds us of the importance of randomizing the order of the observations. 得到的结果如下 原始的文件 shuffle 后的文件 可以看到，经过 shuffle 后的数据的均值和方差很快就达到了稳定的状态，可以让 SGD 算法更快地收敛，这也从另一个角度验证了 shuffle 的必要性。 hasing trick对于 categorial feature， 往往要对其进行 one-hot 编码，但是进行 one-hot 编码需要知道这个 feature 所有可能的取值的数量，对于流式输入的数据，可以先遍历一遍数据得到 categorial feature 所有可能取值的数目。除此之外，还可以利用接下来要讲的 hashing trick 对categorical feature 进行 one-hot 编码，这种方法对只能遍历一遍的数据有效。 hahsing trick 利用了 hash 函数，通过hash后取模，将样本的值映射到预先定义好的固定长度的槽列中的某个槽中，这种方法需要对 categorical feature 的所有可能取值有大概的估计，而且可能会出现冲突的情况，但是如果对categorical feature 的所有可能取值有较准确的估计时，冲突的概率会比较低。下面是利用 sklearn 中的 HashingVectorizer 进行这种编码的一个例子 1234from sklearn.feature_extraction.text import HashingVectorizerh = HashingVectorizer(n_features=1000, binary=True, norm=None)sparse_vector = h.transform(['A simple toy example will make clear how it works.'])print(sparse_vector) 输出如下123456789(0, 61) 1.0(0, 271) 1.0(0, 287) 1.0(0, 452) 1.0(0, 462) 1.0(0, 539) 1.0(0, 605) 1.0(0, 726) 1.0(0, 918) 1.0 这里定义的槽列的长度为1000，即假设字典中的单词数目为 1000， 然后将文本映射到这个槽列中，1 表示有这个单词，0表示没有。 总结本文主要介绍了如何进行 out-of-core learning，主要思想就是将数据以流式方式读入，然后通过 SGD 算法进行更新，在读入数据之前，首先需要对数据进行 shuffle 操作，消除数据本来的顺序信息等，同时可以让样本的特征的方差和均值更快达到稳定状态。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[隐马尔可夫模型的三大问题及求解方法]]></title>
      <url>%2F2017%2F07%2F14%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98%E5%8F%8A%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95%2F</url>
      <content type="text"><![CDATA[本文主要介绍隐马尔可夫模型以及该模型中的三大问题的解决方法。 隐马尔可夫模型的是处理序列问题的统计学模型，描述的过程为：由隐马尔科夫链随机生成不可观测的状态随机序列，然后各个状态分别生成一个观测，从而产生观测随机序列。 在这个过程中，不可观测的序列称为状态序列(state sequence), 由此产生的序列称为观测序列(observation sequence). 该过程可通过下图描述 上图中， $X_1,X_2,…X_T$ 是隐含序列，而 $O_1, O_2,..O_T$ 是观察序列 隐马尔可夫模型由三个概率确定 初始概率分布，即初始的隐含状态的概率分布，记为 $\pi$ 状态转移概率分布，即隐含状态间的转移概率分布, 记为 $A$ 观测概率分布，即由隐含状态生成观测状态的概率分布, 记为 $B$ 以上的三个概率分布可以说就是隐马尔可夫模型的参数，而根据这三个概率，能够确定一个隐马尔可夫模型 $\lambda = (A, B, \pi)$ 而隐马尔科夫链的三个基本问题为 概率计算问题。即给定模型 $\lambda = (A, B, \pi)$ 和观测序列 $O$，计算在模型 $\lambda$ 下观测序列出现的最大概率 $P(O|\lambda)$ 学习问题。即给定观测序列 $O$，估计模型的参数 $\lambda$, 使得在该参数下观测序列出现的概率最大，即 $P(O|\lambda)$ 最大 解码问题。给定模型 $\lambda = (A, B, \pi)$ 和观测序列 $O$，计算最有可能产生这个观测序列的隐含序列 $X$, 即使得概率 $P(X|O, \lambda)$ 最大的隐含序列 $X$ 概率计算问题概率计算问题理论上可通过穷举法来解决，即穷举所有可能的隐含状态序列，然后计算所有可能的隐含序列生成观测序列的概率，假设观测序列的长度为 $n$, 且每个观测状态对应的可能的隐含状态长度为 $m$, 则这种方法的时间复杂度就是 $O(m^n)$, 这样的时间复杂度显然是无法接受的，因此在实际中往往不采用这种方法，而是采用前向算法和后向算法, 前向算法和后向算法都是通过动态规划来减少计算的时间复杂度，两者的不同点就是计算的方向不同。 前向算法前向算法需要先定义前向概率 前向概率定义为到时刻 $t$ 为止观测序列为 $o_1,o_2,o_3…o_t$，且时刻 $t$ 的隐含状态为所有的隐含状态中的第 $i$ 个（记为 $q_i$），则前向概率可记为 $$\alpha_t(i) = P(o_1,o_2,o_3…o_t,x_t = q_i|\lambda)$$ 定义了前向概率，便可递归地求解前向概率和观测序列的概率 $P(o_1,o_2,o_3…o_t|\lambda)$ 初始的状态为 $$\alpha_1(i) = \pi_ib_i(o_1)~~i=1,..m$$ 上式中的 $m$ 表示隐含状态的数目， $\pi_i$ 表示各个隐含状态的初始概率， $b_i(o_1)$ 表示第 $i$ 个隐含状态生成观测状态 $o_1$ 的概率 则递推的公式为 $$\alpha_{t+1}(i) = (\sum_{j=1}^N \alpha_t(j) a_{ji} )b_i(o_{t+1})~~i=1,..m$$ 上式中的 $a_{ji}$ 表示从隐含状态 $j$ 转移到隐含状态 $i$ 的状态转移概率。通过下图可较直观地看到前向递推公式的过程 最终计算得到的概率为（其中 $T$ 为观测序列的长度） $$P(O|\lambda) = \sum_{i=1}^{m}\alpha_T(i)$$ 后向算法类似前向算法，后向算法也可用于求解这个问题，只是方向是从后到前的。同样的，需要先定义后向概率 后向概率指时刻 $t$ 的隐含状态为所有隐含状态中的第 $i$ 个(记为 $q_i$), 且时刻$t+1$ 到 $T$ 的观测序列为 $o_{t+1}, o_{t+2},….o_T$的概率,记为$$\beta_t(i) = P(o_{t+1}, o_{t+2},….o_T, x_t = q_i|\lambda)$$ 初始状态定义为 $$ \beta_T(i) = 1~~i=1,2,…m $$ 这里概率为 1 的原因是概率为1的原因是本来还需要看看时刻 $T$ 后面有什么东西，但因为最后一个时刻 $T$ 后面已经没有时刻，即不需要再观测某个东西，所以随便给个什么状态都可以 递推公式为 $$\beta_t(i) = \sum_{j=1}^ma_{ij}b_j(o_{t+1})\beta_{t+1}(j)~~i=1,2,…m$$ 上面的式子中的符号与前向算法中的一致，其过程可通过下图更直观理解 最终计算得到的概率为 $$P(O|\lambda) = \sum_{i=1}^m \pi_ib_i(o_1)\beta_1(i)$$ 分析可知，前向算法和后向算法的时间复杂度均是 $O(m^2T)$, $m$ 为隐含状态的个数，$T$ 为序列长度 学习问题学习问题要根据观测序列来推导模型参数，这一类的问题对应到概率论中的极大似估计问题。但是这里是有隐含变量的极大似然估计，因此直接无法通过直接求导进行求解，而要通过 EM 算法 来求解这一类问题。 EM 算法是一类算法，用于解决有隐含变量的概率模型参数的极大似然估计，具体到隐马尔可夫模型中的具体算法是 Baum-Welch 算法。 注：这里采用 EM 算法的前提是问题仅给出了观测序列，假如同时给出了观测序列和隐含序列，可直接通过最大似然估计求解。 问题的描述如下：给定的训练数据只包含 S 个长度为 T 的观测序列 $O = \lbrace O_1, O_2, O_3…O_T \rbrace$ 而没有对应的状态序列 $X$，目标是学习隐马尔可夫模型 $\lambda = (A,B,\pi)$ 的参数，则隐马尔可夫模型此时变为了一个含有隐含变量的概率模型，表示为$$P(O|\lambda) = \sum_I P(O|I,\lambda)P(I|\lambda)$$ 这里只给出 Baum-Welch 算法的流程，而省去其推导过程 1.初始化模型参数：选取 $a_{ij}^{(0)}, b_j^{(0)}, \pi_i^{(0)}$, 得到模型 $\pi^{(0)} = (A^{(0)}, B^{(0)}, \pi^{(0)})$2.E 步，求解两个中间变量 $\gamma_t(i), \xi_t(i,j)$,两者的含义如下$\gamma_t(i)$：给定模型 $\lambda$ 和观测序列 $O$，在时刻 $t$ 的隐含状态为 $q_i$ 的概率, 即 $\gamma_t(i) = P(x_t = q_i | O, \lambda)$$\xi_t(i,j)$：给定模型 $\lambda$ 和观测序列 $O$，在时刻 $t$ 的隐含状态为 $q_i$ , 时刻 $t+1$ 的隐含状态为 $q_j$ 的概率, 即 $\xi_t(i,j) = P(x_t = q_i, x_{t+1} = q_j | O, \lambda)$ 结合前面的前向概率和后向概率的定义，计算这两个中间变量的公式如下($m$ 表示隐含状态的总个数) $$\gamma_t(i) = \frac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^m \alpha_t(j) \beta_t(j)}$$ $$\xi_t(i,j) = \frac{\alpha_t(i) a_{ij} b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{p=1}^m \sum_{q=1}^m \alpha_t(p) a_{pq} b_q(o_{t+1})\beta_{t+1}(q)}$$ 3.M 步，同时 E 步求解出的两个中间变量来求解模型的参数，求解公式如下 $$a_{ij} = \frac{\sum_{t=1}^T \xi_t(i,j)}{\sum_{t=1}^T \gamma_t(i)} $$ $$b_j(k) = \frac{\sum_{t=1}^T \gamma_t(j)I(o_t = v_k)}{\sum_{t=1}^T \gamma_t(j)}$$ $$\pi_i = \gamma_1(i)$$ 上式中的 $I(o_t = v_k)$ 表示时刻 $t$ 的观察状态为 $v_k$ 时, $I(o_t = v_k)$ 才为1，否则为0 迭代进行 E 步骤和 M 步，将最终收敛的结果作为模型的参数 解码问题解码问题理论上也可以通过穷举法来解决，就是穷举所有可能的隐含序列并计算在这个隐含序列下观测序列的概率，并选择概率最大的那个隐含序列，但是穷举所有可能的隐含序列的时间复杂度也是指数级别的，跟第一个问题一样，在实际中往往也是不常用的。 实际中，解码问题通过动态规划来降低时间复杂度，并且已经有了成熟的解决方法，就是著名的维特比算法，具体的算法流程可参考这篇文章：维特比算法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习中样本比例不平衡的处理方法]]></title>
      <url>%2F2017%2F07%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%A0%B7%E6%9C%AC%E6%AF%94%E4%BE%8B%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%2F</url>
      <content type="text"><![CDATA[在机器学习中，常常会遇到样本比例不平衡的问题，如对于一个二分类问题，正负样本的比例是 10:1。这种现象往往是由于本身数据来源决定的，如信用卡的征信问题中往往就是正样本居多。样本比例不平衡往往会带来不少问题，但是实际获取的数据又往往是不平衡的，因此本文主要讨论面对样本不平衡时的解决方法。 样本不平衡往往会导致模型对样本数较多的分类造成过拟合，即总是将样本分到了样本数较多的分类中；除此之外，一个典型的问题就是 Accuracy Paradox，这个问题指的是模型的对样本预测的准确率很高，但是模型的泛化能力差。其原因是模型将大多数的样本都归类为样本数较多的那一类，如下所示 category Predicted Negative Predicted Positive Negative Cases 9700 150 Positive Cases 50 100 准确率为 $$\frac{9700+100}{9700 + 150 + 50 + 100} = 0.98$$ 而假如将所有的样本都归为预测为负样本，准确率会进一步上升，但是这样的模型显然是不好的，实际上，模型已经对这个不平衡的样本过拟合了。 针对样本的不平衡问题，有以下几种常见的解决思路 搜集更多的数据 改变评判指标 对数据进行采样 合成样本 改变样本权重 搜集更多的数据搜集更多的数据，从而让正负样本的比例平衡，这种方法往往是最被忽视的方法，然而实际上，当搜集数据的代价不大时，这种方法是最有效的。 但是需要注意，当搜集数据的场景本来产生数据的比例就是不平衡时，这种方法并不能解决数据比例不平衡问题。 改变评判指标改变评判指标，也就是不用准确率来评判和选择模型，原因就是我们上面提到的 Accuracy Paradox 问题。实际上有一些评判指标就是专门解决样本不平衡时的评判问题的，如准确率，召回率，F1值，ROC（AUC），Kappa 等。 根据这篇文章，ROC 曲线具有不随样本比例而改变的良好性质，因此能够在样本比例不平衡的情况下较好地反映出分类器的优劣。 关于评判指标更详细的内容可参考文章： Classification Accuracy is Not Enough: More Performance Measures You Can Use 对数据进行采样对数据采样可以有针对性地改变数据中样本的比例，采样一般有两种方式：over-sampling 和 under-sampling，前者是增加样本数较少的样本，其方式是直接复制原来的样本，而后者是减少样本数较多的样本，其方式是丢弃这些多余的样本。 通常来说，当总样本数目较多的时候考虑 under-sampling，而样本数数目较少的时候考虑 over-sampling。 关于数据采样更详细的内容可参考 Oversampling and undersampling in data analysis 合成样本合成样本(Synthetic Samples)是为了增加样本数目较少的那一类的样本，合成指的是通过组合已有的样本的各个 feature 从而产生新的样本。 一种最简单的方法就是从各个 feature 中随机选出一个已有值，然后拼接成一个新的样本，这种方法增加了样本数目较少的类别的样本数，作用与上面提到的 Over-sampling 方法一样，不同点在于上面的方法是单纯的复制样本，而这里则是拼接得到新的样本。 这类方法中的具有代表性的方法是 SMOTE（Synthetic Minority Over-sampling Technique），这个方法通过在相似样本中进行 feature 的随机选择并拼接出新的样本。 关于 SMOTE 更详细的信息可参考论文 SMOTE: Synthetic Minority Over-sampling Technique 改变样本权重改变样本权重指的是增大样本数较少类别的样本的权重，当这样的样本被误分时，其损失值要乘上相应的权重，从而让分类器更加关注这一类数目较少的样本。 参考： 8 Tactics to Combat Imbalanced Classes in Your Machine Learning DatasetIn classification, how do you handle an unbalanced training set?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(343, 377)-动态规划求解数字的组合方案]]></title>
      <url>%2F2017%2F06%2F28%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(343%2C%20377)-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%B1%82%E8%A7%A3%E6%95%B0%E5%AD%97%E7%9A%84%E7%BB%84%E5%90%88%E6%96%B9%E6%A1%88%2F</url>
      <content type="text"><![CDATA[LeetCode 上的这两道题 343. Integer Break 和 377. Combination Sum IV 从名字上来看没有什么联系，但是实际上两个题目都是通过动态规划来降低了求解时间复杂度，并且面对这种题目一开始往往难以往动态规划方向去想，特此记录。 对我而言，之所以不会一开始就想到动态规划，原因是这两个题目要求解的问题均是以某种方式构造某个数字时，总共的方案有几种或者最优的方案是哪种。一开始往往就是往数学或者 dfs 方向去想，而不会想到通过动态规划建立一个从 1 到目标数字的数组来记录各个状态的答案。 对于问题 343. Integer Break, 该问题属于求解组成某个数字的最优方案的问题，首先需要注意的是只把数字分为两部分，这样能够简化问题，然后对于这两部分都取最优时，则这两部分的乘积就是这种分法的最优解。那对于这两部分，可以很自然地想到均通过递归去求解最优，但是这种方法或导致很多重复的计算，比如说将 8 分为 6 和 2 时，需要计算 6 和 2 的最优解，但是进一步将 6 分为 4 和 2 时，还要计算 2 的最优解，这样显然会导致 多次计算 2 的最优解。 解决这个问题就是通过动态规划建立的数组记录数字 2 的最优解，需要求解时直接从数组中取即可，这时候动态规划的数组就有点类似 cache 的作用，实现的 java 代码如下,其中 dp[i] 表示数字 i 的最优解（即分解的数字的乘积最大） 123456789101112131415161718public class Solution &#123; public int integerBreak(int n) &#123; int[] dp = new int[n+1]; dp[1] = 1; for (int i = 2; i &lt;= n; i++) &#123; int max = 0; for(int j = 1; j &lt;= (i &gt;&gt; 1); j++) &#123; max = Math.max( Math.max(dp[j], j) * Math.max(i - j, dp[i-j]), max); &#125; dp[i] = max; &#125; return dp[n]; &#125;&#125; 针对 LeetCode 的评测方法（建立一个对象，输入多组评测数据），上面的代码还能该继续优化，就是为这个对象建立一个全局的数组记录各个数字的最优解，供多组评测数据使用，当评测数字在数组中已经有答案了就不必再求解，反之递增数组。实现的 Java 代码如下，代码中的 cache list 与上面方法中的 dp 数组的作用相同 1234567891011121314151617public class Solution &#123; private List&lt;Integer&gt; cache = new ArrayList&lt;Integer&gt;(); public int integerBreak(int n) &#123; for (int i = cache.size(); i &lt;= n; i++) &#123; int max = 0; for(int j = 1; j &lt;= (i &gt;&gt; 1); j++) &#123; max = Math.max( Math.max(cache.get(j), j) * Math.max(i - j, cache.get(i-j)), max); &#125; cache.add(max); &#125; return cache.get(n); &#125;&#125; 针对问题 377. Combination Sum IV，一开始想的是通过 dfs 来求解，但是题目中允许重复的数字，并且不同的顺序被认为是不同的组合方案，这样就使得如果通过 dfs 求解，时间复杂度会非常大。 通过动态规划求解，通过 dp 数组中的 dp[i] 表示数字 i 的组合方式的数目，可以同时解决上面提到的重复数字和数字的顺序问题。 则 dp[i+1] 可以考虑所有数字与已有的 dp[j] (j=0，1，...i) 的组合，实现的 Java 代码如下 123456789101112131415161718192021public class Solution &#123; public int combinationSum4(int[] nums, int target) &#123; int[] dp = new int[target+1]; dp[0] = 1; Arrays.sort(nums); int count; for (int i = 1; i &lt;= target; i++) &#123; count = 0; for (int j = 0; j &lt; nums.length; j++) &#123; if (nums[j] &gt; i) break; count += dp[i - nums[j]]; &#125; dp[i] = count; &#125; return dp[target]; &#125;&#125; 往往会听到说动态规划的难点在构造递推关系，这没错，但是还有一个难点就是想到题目可以通过动态规划去求解，其实只要能往动态规划方向去想，递推关系往往也不难得到。因此，本文的主要作用还是要提醒面对这种求解目标数字的最优构造方案或者构造方案数目的时候，可以往动态规划方向去想，建立一个从1到目标数字的数组记录各个数字的结果，而不是仅仅拘泥于目标数字。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何用数据武装运营工作]]></title>
      <url>%2F2017%2F06%2F18%2F%E5%A6%82%E4%BD%95%E7%94%A8%E6%95%B0%E6%8D%AE%E6%AD%A6%E8%A3%85%E8%BF%90%E8%90%A5%E5%B7%A5%E4%BD%9C%2F</url>
      <content type="text"><![CDATA[这篇文章的内容主要来源于 该知乎live，主要介绍了利用数据获取了用户后如何运营，从而能够尽可能长时间地留存用户，介绍了这方面的三个具体方法：建立用户转化漏斗、通过多维报表找到问题和建立实验框架。 本文主要关注下图左半部分，利用了别人的数据和流量获取了用户后该怎么运营以留存用户。 为了达到这个目标，有三个重要的方法 建立用户转化漏斗：总体的数据发生变化时，到底是哪个环节起了作用。 通过多维报表找到问题：经过上一步确定某个环节出现问题后，对这一个环节需要更细致的分解，这是需要从各个维度去分析这个环节中出现问题的原因。 建立灵活的实验框架：上面两步是被动地发现问题，实际中更要主动的探索新方案，新的实验框架有利于测试的快速迭代 建立用户转化漏斗把商业目标转化为用户转化的一系列步骤，下面是两个具体例子 每一步都有一定的损失，且需要关注的点是前后两步的用户的比率，因为通过比率更容易看到具体的变化 漏斗设计的原则与作用：整个漏斗过程用于优化一个唯一的目标，并将该目标分解为若干比率的乘积，便于发现问题并优化，下面是一个优化总用户时长的例子 设计用户漏斗需要涉及到具体的度量指标，下面是移动应用中一些常见的度量指标 转化率/激活率：激活数和点击数的比留存率：某日激活的用户中，经过一段时间还活跃的用户所长比例；根据设定的时间不同可分为次日留存、七日留存、月留存等活跃用户：活跃的独立用户数，根据时间的不同可分为日活跃用户（DAU）、月活跃用户（MAU）用户时长：每个活跃用户平均消耗的时间 对于网站分析，其优化目标本质上跟移动的应用一样，就是尽可能吸引并留存更多的用户，但是由于两者具体提供服务的不同，两者的度量指标也有不同的叫法，下面是网站分析中常见的度量 UV (User View)：独立访客数PV (Page View)：所有浏览量页面停留时长：页面浏览时间跳出率 (Bounce Rate)：指单页会话（用户打开了网站上的一个网页，然后就退出了网站）次数在所有会话次数中所占的比例，也就是用户在您网站上仅查看一个网页的会话次数所占的百分比网站热力图：热力图是指以特殊高亮的形式显示访客热衷的页面区域和访客所在的地理区域的图示，简单来说就是分析出一个页面内各个部位的点击情况，如下是一个热力点击图 关于跳出率，高跳出率有时是网站本身的属性决定的，这时候就没必要做更多的优化， Google Analytics 描述如下： 高跳出率不是件好事？ 这需要视情况而定。 如果您网站的成功取决于用户是否查看多个网页，那么高跳出率就不是一件好事。例如，如果您的首页是通往您网站的其他部分（例如新闻报道、产品页、结帐流程）的入口，而大部分用户仅查看您的首页，那么您一定不希望跳出率处于较高的水平。 另一方面，如果您拥有类似博客那样的单页网站，或提供预计会产生单页会话的其他类型的内容，则高跳出率完全属于正常现象。 分析工具 网站分析工具：Google Analytics、百度统计、CNZZ应用分析工具：TalkingData、友盟+、Flurry、Google Analytics应用归因工具：Appsflyer、Tune、Adjust、TalkingData 分析工具是用于分析已有用户的行为，而归因工具是用于分析用户的来源。 因此，总结上面的过程为：建立漏斗并利用分析工具将漏斗的每一步的数据找出来 下面是一个通过漏斗分析的页游的例子 通过这样的步骤可以有目的地去排查具体的问题，如上面的例子中可能是程序在 chrome 浏览器上的不兼容导致了注册率的较低。 通过多维报表找到问题上面的例子中统计各个浏览器的注册率时已经涉及到了多维报表查询的问题，因为除了从浏览器，还可能从地域、时间段、时间段和浏览器组合等其他维度去统计，这时候就需要一个灵活的查询统计工具来提供这样的多维度报表，这个工具就是数据仓库（Data Warehouse） OLAP (Online Analytical Processing) 和 OLTP(Online Transaction Processing) 软件：Saiku、Tableau 上面的流程： 一个商业目标 -&gt; 建立一个转化漏斗 -&gt; 用分析工具在应用或网站埋点 -&gt; 分析工具得到的数据建立数据仓库 -&gt; 用数据仓库做精细查询 建立灵活的实验框架实验框架其实就是常听说的 A/B 测试，就是把用户分成两部分（两部分的量不一定相等），然后对这两部分的用户分别采用不同的方案，比较哪种的效果好，从而采取效果好的那个方案。这里的不同方案的不同点就是我们需要验证的产品的特性，且往往是单变量的，也就是验证某个新的特性与旧特性哪个好时需要保持其他环境一样，而仅仅改变这个特性。在这个过程中需要注意划分的用户要有代表性，也就是两部分用户的分布情况应该一致（如年龄等） A/B 测试中的一个重要方法是分层实验框架，其目的是为了能够在同样的流量情况下容纳更多的 A/B 测试，下面以一个简单的例子讲解 上图所示的实验层中，在UI层，广告检索层和广告排序层均有 A/B 测试的需求，假如要测试UI层的一个新特征，同时也要测试广告检索层的一个新特征，当需要同时进行这两个测试时，必须要确保UI层的流量划分不会影响到广告检索层的测试，也就是说在广告检索层中划分的两部分流量中，只存在着广告检索层的特征的差异。因此如果同时在两个层进行AB测试，需要将流量划分为四份，分别是 UI层原特征+广告检索层原特征、UI层原特征+广告检索层新特征、UI层新特征+广告检索层原特征、UI层新特征+广告检索层新特征。即需要测试的特征数为 $n$ 时，需要划分的流量数为 $2^n$, 显然这样的增长级数带来了流量分割的困难。分层实验框架就是解决这个问题的。 具体的做法就是采用正交的哈希函数来为每一层进行流量划分，正交的哈希函数避免了不同层间的干扰问题。如对于上面的问题，采用两个相互正交的哈希函数，分别在UI层和广告检索层将流量划分为两部分，因为两个哈希函数是正交的，因此在UI层所划分的两部分流量U1和U2中，U1所包含的广告检索层的原特征流量和U2所包含的广告检索层的原特征流量比例相等，同时新特征的比例也相等，这样就避免了上面的指数级增长的划分方式。更详细的信息可参考这篇文章Overlapping experiment infrastructure: more, better, faster experimentation 上图中的非重叠测试域指的是测试的特性贯穿了三个层，就是同时测试三个层的特性组合带来的效果，这时候就用不上分层实验框架了。 上面这种采用正交的哈希函数来为每一层进行流量划分思路同样可用在灰度发布中，进行分层的灰度发布测试。 通过 A/B 测试能够将目前的产品的特性调到最优，但是需要注意的是 A/B 测试并不是万能的，因为过度依赖于数据会丧失对关键创新的把握，这里有一句很形象的话：汽车无法从跑得更快的马进化而来，也就是无论我们利用 A/B 测试把马（现有的产品）训练成跑得更快，依然是没法比得过汽车（更有创意的产品）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[有价值的数据应该如何交易]]></title>
      <url>%2F2017%2F06%2F16%2F%E6%9C%89%E4%BB%B7%E5%80%BC%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E4%BA%A4%E6%98%93%2F</url>
      <content type="text"><![CDATA[本文的内容主要来源于该知乎 live，主要介绍了哪些行为数据是有价值的，以及广告领域中数据是如何交易的，最后还讨论了数据隐私的问题。 有价值的数据分类数据有价值密度之分，并不是说数据量越多就一定越有效 这里的数据主要指的是用户的行为数据和用户的标识数据，而有价值的用户行为数据主要有: 决策行为、主动行为、半主动行为、被动行为，这些行为的价值递减，但是数据的量递增，因为有价值的数据一般量都不大 决策行为决策行为对应着转化(conversion)或预转化(pre-conversion), 也就是购买了商品或将商品加入了购物车。这些行为对应着非常明确的用户兴趣，价值也非常高 主动行为主动行为对应着搜索（search）、广告点击（Ad Click)、搜索点击（Search Click），这种行为表明了用户已经有了明确的意图，但是最终决定还不清楚，价值也很高。 需要注意的是，这些行为里面往往会有作弊的流量在里面，需要去除掉。 半主动行为半主动行为对应着分享（share），网页浏览（page view),这种数据的量最大，用户意图较弱，因为用户可能只是随意在浏览，这些数据也有一定价值。 被动行为被动行为是强加给用户的行为，如广告的浏览（注意不是点击，而是强推给用户浏览），这种行为甚至会有负面作用，价值基本可以忽略 社交关系社交关系指的是不直接利用用户的行为数据（有可能是用户的行为数据过于稀疏），而是利用与其在社交网络（微博、Facebook 等）上有关联的的用户的信息进行定向。 这种方法在某个人的行为不足而无法进行精准的行为定向时有效。 用户 ID用户 ID ，也就是用户标示，是最重要的数据，因为所有的行为数据有效的前提是需要先确认这些行为数据是属于哪个用户的，标识一个用户的 ID 在不同的环境下有不同的方法，下面是常见的场景和方法 web/wap 环境：使用 cookie，生命周期短（1~2周），存续性差，但是跨域名的时候需要映射 ios 应用：使用 IDFA（ID For Advertiser)，存续性好于 cookie，但 ios10 有更严格的政策 安卓应用：使用 Android ID，存续好于 IDFA；有些也使用 IMEI（手机标识），但是 Google Play 上是不给用的 无以上 ID 场景：使用 FingerPrint（IP+UserAgent -&gt; hash），存在 http 头中，可作缺省标识; 但是在移动端使用效果不是很好，因为几乎每个应用都有一个内置的浏览器 三方数据划分下面以广告中用到的用户数据为例讲述三方数据的划分，在广告中根据数据来源的不同可以将数据划分为第一方数据，第二方数据和第三方数据。 如下图所示，第一方和第二方分别是指广告主和广告平台，而不直接参与广告交易的其他数据提供方统称为第三方。 数据管理平台(DMP)第一方数据的收集和加工是广告市场上非常重要的环节，不过对于没有这方面技术积累的广告主而言，专门设团队进行数据加工是没有必要的，因此市场上出现了 数据管理平台（DMP),专门从事此业务，而 DMP 又可划分为第一方 DMP 和第三方 DMP。 第一方 DMP第一方 DMP 的目的是对广告主提供的第一方数据（也可结合公开市场第三方数据）进行加工，进而得到广告主指定的用户标签，用于支持网站业务运营和广告投放。 需要注意的是第一方 DMP 只能加工第一方数据，而不能使用第一方的数据，也就是不能把数据进行售卖（除非与广告主达成协议） 因此，第一方的 DMP 的商业模式如下 第三方 DMP对于中小网站，其规模不大，没有利用数据的能力，只是单纯想将数据卖给需要数据的广告主，同时也没有加工数据的能力，因此产生了满足中小网站的这项需求的第三方 DMP。 第三方 DMP 与第一方 DMP 的一个不同点在于服务对象的不同，另外一个不同点则是两者的加工标签的逻辑不一样，第一方 DMP 是根据广告主的需求进行标签的加工，而第三方 DMP 则是根据 DMP 其自己的逻辑进行加工然后售卖。 第三方的 DMP 的商业模式如下所示 数据的交易上面提到的 DMP 的一个重要功能就是售卖标签，实际上就是一种数据交易，这些标签一般售卖的对象是广告主，而广告主往往由于缺乏相应的技术而将手中定向委托给其他平台也就是 DSP（Demand Side Platform），因此交易发生在 DMP 和 DSP 之间。 同时由于往往存在着多个 DMP 和多个 DSP，假如 DMP 和 DSP 间都要一一连接的话，那么通信的代价会非常大，因此在实际中往往是通过广告交易平台也就是 ADX（AD Exchange) 将两者联系起来，从而降低通信代价。整个数据交易的过程如下所示 通过 ADX 进行 DMP 和 DSP 间的通信避免了 DMP 和 DSP 直接通信的开销，因为实时竞价的时候 ADX 本来就要跟 DSP 发生通信，因此没有增加二次通信。 上面简单提到数据交易时的收费是按照实际的广告展示次数付费的，目前来说这种市场化的定价方式是唯一的选择，这种方式并没有限制数据供给次数，直觉上似乎是利润最大化的。 但是这有可能间接地抬高了流量价格，而低估了数据价格。因为不限量地售卖标签，会导致竞价同一次展示的广告主的数目增加，因为有了标签，各个广告主能够更精准地定向到更多用户，因此更多的广告主的竞价抬高了流量的价格， 而假如广告主的预算是一定的情况下，购买流量需要更多的钱，因此用于购买数据支出会变少。当然这只是宏观上的探讨，目前业界对此并没有一套完善的理论来指导。 如果采用限量的售卖，那就要采用竞价的方式，而有了竞价，整个市场的活跃程度和价值会最大化。 数据隐私在数据交易过程中不可避免地会设计到数据隐私的问题。针对数据隐私，欧盟负责隐私保护条例指定的委员会 A29 制定了以下相关原则 Personal Identifiable Information（PII）不能使用，PII指的是可以主动接触到用户的信息，比如手机号、QQ号、微信号、e-mail等都不能使用 用户可以要求系统停止记录和使用自己的行为数据，比如说网站会在网页上说明收集到的用户数据的作用，同时可让用户选择是否允许收集其数据，实际上对商业影响非常小，因为选择不允许收集的用户比例不大 不能长期保存和使用用户的行为数据，实际上数据也具有时效性，时间太久远的数据基本上无价值 另外一个数据隐私问题就是稀疏的行为数据带来的挑战，一个典型的例子就是 Netflix 推荐大赛中，有人从数据集里面发现了自己的同时是同性恋，原因是数据的稀疏性使得个人的行为数据更加容易被熟悉这个人的其他人所辨识。 目前对于这个领域相关的研究课题是差分隐私（differential privacy）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[怎样用数据洞察你的用户]]></title>
      <url>%2F2017%2F06%2F10%2F%E6%80%8E%E6%A0%B7%E7%94%A8%E6%95%B0%E6%8D%AE%E6%B4%9E%E5%AF%9F%E4%BD%A0%E7%9A%84%E7%94%A8%E6%88%B7%2F</url>
      <content type="text"><![CDATA[本文内容主要来源于该知乎 live，主要介绍了受众定向（用户画像）的分类和方法、具体介绍标签体系建立以及如何进行行为定向。 受众定向与用户画像原始行为数据无序杂乱，不能直接应用于业务，因此要将用户的原始行为转化为对用户的描述，也就是受众定向（用户画像），但是这里需要注意的是不能想当然地描述用户，而是要根据需求方（如广告主）的需求来为用户打上相应标签 受众定向与常听到的用户画像的差别不大，均是研究如何描述用户，为用户打上标签，两者只是在着重点上有细微区别 受众定向重点是可优化，也就是跟侧重于效果，而用户画像则更侧重于可解释性，然而在实际中对用户的描述往往是两者混合的，如对广告主需要可解释性强的标签，而对于模型则更侧重那些有效的标签（可解释性不一定好） 受众定向的分类下面以计算广告为例阐述可受众定向需要在哪些维度上做打标签 1）用户维度 $t(u)$, 描述用户的固有属性2）上下文维度 $t(c)$，描述用户浏览的内容3）用户-广告维度 $t(a,u)$，描述用户在某个广告主下的特有属性 实际中需要为广告打上标签 $t(a)$，用于描述广告的固有属性，从而与用户匹配 如上图所示，受众定向建立的标签一般有两种作用，而第一种作用的标签需要可解释性，第二种作用的标签则更强调其效果 受众定向的方法同样以计算广告为例，下图展示了受众定向常用的方法，左边表示广告的生命周期，右边表示各个阶段的受众定向的方法和效果，其中越往左效果越好，具体方式的含义可参考这里 重定向只需要用到第一方数据look-alike 需要用到第一方数据和第二方数据shyper-local 表示根据更精细的位置做定向(移动端) 标签体系的建立上面讲述了该在那些方面建立标签体系以及建立标签体系的方法，下面要讲一个重点的内容，就是该建立怎样的标签体系 如下图所示，标签体系可以分为两大类，其中一类是结构化标签，可以认为这一类的标签是一个大的树状结构；另外一类则是非结构化的，也就是根据效果和需求驱动的标签体系，关键词就是一个典型的非结构化标签体系。 这里需要注意的是，由于结构化标签结构上的完备性，往往会被大部分人采用，但是这种标签体系的效果未必就好，原因是这些结构化标签将重点放在了标签体系的完备性而往往忽略了广告主的具体需求。 结构化的标签体系的一个典型例子是雅虎的 GD 系统的标签体系，这个标签体系根据主观的判断来分类标签，并没有结合广告主的具体需求，在实际的效果并不好 下面的标签体系中虽然形式上类似于结构化标签，但是是根据各个广告主的需求来定制各个标签的，因此是非结构化的，也更实用 非结构化标签体系的建立过程如下1）确定行业2）了解行业里面用户的决策流程3）根据决策流程定制各个流程中的标签 如对于汽车行业，一般用户购买时会先考虑预算(价格区间标签)，然后考虑车的用途（车型、大小等标签），最后考虑品牌（品牌标签）。 实际中，具体的标签可通过不同的方法获取，下文要讨论的行为定向中也提供了一种获取具体标签的方法。 行为定向下面讲述受众定向中的一个重点：行为定向，就是根据用户的历史行为给用户打标签。 行为定向首先要将用户的各种行为转化为标签，同时对各种行为进行加权，如下图所示就是通过三种行为（广告点击，搜索，浏览）为用户打标签，其过程都是根据用户操作对象的具体内容提取出标签（关键词，主题等）并进行叠加。 上面采用的方法并不复杂，其中一个很重要的原因就是行为定向的实时性要求，因为用户的行为的有效期一般不长，即从关注到最终的购买所持续的时间往往并不长，其次是因为要处理的用户的量级非常大。 此外，由于各种行为所反映的用户的意图不一，因此需要对不同行为进行加权，而且对于不同的标签，加权的方式还不完全一样，如下是对不同标签的不同行为加权的一种方法 上面建模采用泊松分布来处理，因为泊松分布就是描述某段时间内，具体数量的事件发生的概率，如上式表示在时间 $t$ 内，用户点击广告次数为 $h$ 的概率。其中 $\lambda$ 表示用户点击广告的频繁程度，也叫频繁性参数，该值越大，表示点击越频繁。 $\lambda$ 可描述为各种原始行为的加权和, 其中 $w_{tn}$ 为权重系数, $x_{tn}$ 为原始行为（N种，如浏览、搜索等）的统计量，求解时通过 $h$ 的历史数量进行极大似然估计即可求出权重系数 $w_{tn}$ ，然后直接在线上使用权重系数。 此外，上面式子中 $t$ 表示对不同的标签建立不同的权重体系。 上图中将各类行为变为具体的标签方法有以下几种，主要思路是找到用户的行为对应的内容（一般是具体文本），然后借助 NLP 技术将内容转为标签 主要方法可以分为两种1）针对浏览行为、点击行为，相应的都会有具体内容，如浏览的页面的内容，点击的广告的具体内容，通过 NLP 技术对内容提取关键词或主题分布，作为标签2）针对搜索行为，可分为通用搜索和垂直搜索，要解决的问题都是如何将搜索关键词变为标签。对于通用搜索，可以利用已有搜索引擎（百度，谷歌等）模拟用户搜索行为，从得到的搜索结果作为内容，从中提取标签方法可以采用与第一种相同的 NLP 技术；对于垂直搜索，可以通过淘宝、携程、汽车之家等垂直搜索引擎，直接从关键词返回的结果中提取标签，因为像这种垂直搜索引擎一般都会自定义好一套标签了，因此这里是采用了垂直网站已经分好的类。 实际工程中，往往会从不同渠道获取用户不同的行为数据，将各种方式获取的日志整理在一起，称为 Session log。Session log 中一行表示一个用户的数据，这样通过 MapReduce 进行计算时，通过 map 过程即可完成某个用户的 targeting 过程，也就是下图中的局部计算。 除此之外，行为定向中往往要用到用户过去一段时间的数据（7的整数倍，避免周六日带来的偏差），处理的数据是一个时间序列数据，处理的方法有以下两种：滑动窗口和时间衰减。 实际中一般采用时间衰减方式，因为时间衰减的方式计算的效率较高 场景定向场景定向指的是判断出用户当前所处的场合和状态（地铁上、开会中、健身房等），针对的是移动设备。利用移动设备的传感器等搜集的信息，可以判断用户当前所处场景。 以早餐推送为例，根据用户当前的速度可以判断出用户是否在坐地铁，根据时间判断用户是要去上班，根据用户的位置可以为用户推荐附近的早餐店，实际上这是一个真实的例子，在东京的一次肯德基推广活动中，通过移动设备行为数据的分析，活动方准确找到了那些从地铁出来准备吃早餐的人群，实时给他们送出了早餐优惠券。 这里需要注意的是场景定向不是上下文定向，上下文定向针对的是媒体内容，而场景针对的是用户。 人口属性定向人口属性定向中主要是要预测性别和年龄阶段，实际上就是分类问题 受众定向的评判标准上述的受众定向的过程其实就是依据某个用户的各种历史行为，为该用户在所有的标签上打出一个分数，然后线上设定分数阈值，大于这个阈值则判断用户是这个标签的人群。因此一般来说阈值越大，标签人群越少，但是结果也会越可信。 下图展示了该如何评测受众定向的有效性，横轴表示被打上标签的人群的比例，纵轴表示被打上标签的人群对广告的点击率，一般来说，某个标签圈定的人数越少，效果越好，也就是点击率越高，违反了这一变化规律则说明受众定向不起作用。 综上，本文主要讲了受众定向中的行为定向 确定具体行业 研究行业用户决策过程，制定用户标签体系 制定标签体系不要刻意追求规整、结构化的标签体系 把用户的原始行为映射到标签体系中，并求出各种行为类型的权重 根据 4 可为用户在各个标签上打出一个分数，为标签设定阈值，则通过比较分数和阈值可以判断用户是否属于该标签人群 通过 Reach/CTR 曲线评测定向是否有效]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何用数据来挣钱]]></title>
      <url>%2F2017%2F06%2F05%2F%E5%A6%82%E4%BD%95%E7%94%A8%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%8C%A3%E9%92%B1%2F</url>
      <content type="text"><![CDATA[本文内容主要来源于 该知乎 live，主要简单介绍了互联网中免费的商业模式所带来的资产（数据），以及如何通过这些资产进行变现。 免费模式与变现资产 免费模式带来的后向变现资产包括 其中数据依附于流量进行变现，数据提高了流量变现的价值，如下图通过数据进行人群划分后再进行广告的投放能够提高收益 数据变现产品的发展历程数据变现可以说是经历四个发展阶段，由于广告在这方面的发展历程较为完备，因此以计算广告中的数据变现的发展历程来介绍，但是拓展到其他领域也是大同小异 1）变现人口属性数据2）变现行为数据3）变现第一方和第三方数据：在此阶段前只利用了广告平台提供的数据，这个阶段用了广告主自身的数据（第一方数据）4）变现场景数据 变现人口属性数据（合约广告）这个阶段对应于计算广告中的合约广告，即广告主要求在符合某些人口属性的人群中投放一定量的广告，并与提供广告位方达成一定的合约。如下图中的需求节点所展示的是一些广告主的具体需求，供给节点则是一些媒体网站等的流量信息，所展示的整个系统称为担保式投放系统（Guarantee Delivery，GD）。在这个过程中要解决在线分配（Online delivery）的问题。 上述这种方法在粗略定向中有效，对更精细数据的定向力不从心，因此有了竞价广告的出现 变现行为数据（竞价广告）上面提到的精细数据（行为数据）其实代表的就是长尾数据，也就是频次很低但是数量很多的数据。实际中无法去掉长尾流量，因为长尾所占的总体比例还是很大的，在广告中的通过竞价交易模式来变现这一部分数据 以搜索广告为例， A 个广告主在争取 S 个广告位 在这个过程中有个需要注意的点就是竞价是要采用广义第二高价的原则，就是广告位拍卖最终收的钱是出价中排第二的那个价格，这是由经济学博弈论得到的一个结论。 变现第一方和第三方数据（程序化交易）产生的原因是广告主的定制化的需求，因为竞价广告中提供的定向还不能满足广告主的个性需求，需要更精细化的定向。如广告主（称为第一方）要向其流失用户投放优惠广告，这时候流失的用户的数据只存在于第一方数据中，像 Google、百度这些是不可能为广告主找出其流失用户的。 程序化交易则是将用户数据展示给广告主，让广告主自己决定是否要这个广告，大大提高了中小广告主的参与度。这种方式使得第一方的数据得到应用，同时也让第一方（广告主）有动力去购买其他渠道的用户数据（第三方数据），也就是促进了数据交易。 下图展示的是一个程序化交易的过程 2.1 用户访问媒体2.2 媒体网站上有 ADX 的代码，将请求发送给 ADX2.3 ADX 将该用户信息和询价请求发送给 DSP，DSP 进行竞价 在这个过程中，DSP 需要根据广告主提供的数据来判断 ADX 所发送的用户是否符合广告主的需求，这样广告主的数据（第一方数据）便得到了应用 下如所示是第一方（广告主）数据的使用的一个具体例子：重定向，也就是找到访问过广告主网站的用户并投放广告 上面的例子仅仅是使用第一方数据，下面的例子联合使用了第一方（广告主）和第三方（媒体）的数据。 Look-alike 指的是本来广告主的用户基数人群就不多，需要借助广告平台的数据找到可能的新的用户，该过程也被称为新客推荐。这个过程需要广告主提供一部分的种子用户，DSP 分析这些用户的共同特点，并结合媒体网站上其他用户的数据为广告主推荐与种子用户相似的用户。 变现场景数据(移动广告)主要是与移动端广告有关，通过手机可以了解到人当前所处的场景 比如说每天 9 点进行地点采样一次，那么一个月下来便可知道其工作地点，便可向其推送附近商家的广告。 有比如说早上上班时间检测到手机从高速移动的状态变为低速移动的状态，那么就是从地铁出来了，这是便可向其推荐附件早餐店的广告。 这个方向比较前沿，目前还没有成熟的方案 从电商角度看数据利用的方法下面以电商为例，讲述电商将其数据变现的几种途径 站内推荐 站外推荐：重定向 新客推荐：Look-alike]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大数据是否能够改造你的行业]]></title>
      <url>%2F2017%2F06%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%98%AF%E5%90%A6%E8%83%BD%E5%A4%9F%E6%94%B9%E9%80%A0%E4%BD%A0%E7%9A%84%E8%A1%8C%E4%B8%9A%2F</url>
      <content type="text"><![CDATA[本文内容主要来源于该知乎 live，主要介绍了深度学习为什么能在大数据的环境下有效，并描述了大数据的三个特点：行为数据、全量加工和自动化应用。 深度学习为何有效 深度学习属于表示表示学习的一种，将特征提取和模型训练放到一起，消除了领域知识(特征工程)的影响 深度学习有效的原因 1）深度学习的表达能力更强，模型能够容纳更多的数据2）深度学习的模型很早就提出了，但是一直缺乏有效的优化方法（求解方法），无法将桶灌满，直到 GPU 的出现，相当于图中的水管的出现3）能够获取的数据量变得更大（水源） 行为数据、全量加工、自动化应用能够利用大数据改造的产业必须要有以下三个特点 1）具有行为数据2）需要进行全量加工3）能够部署自动化应用 行为数据 v.s 交易数据 由于两者的特点不同，交易数据和行为数据的加工方式差别很大 全量加工 v.s 采样加工问题属性决定采用哪种加工 全量加工是大数据的一个根本特点 CTR 预估是一个全量加工的问题，但是实际中往往要对负样本抽样以解决正负样本不平衡问题 洞察应用 v.s 全自动化应用洞察指的是根据大量数据生成报表，然后通过人观察这些报表并作出决策 全自动化指的是数据的产生，加工，交易形成闭环 因此，要将大数据应用到业务中，需要回答这三个问题 1）行为数据从哪里来？2）要全量加工的问题是什么？3）如何做到自动化 下面是根据大数据的三个特点介绍的三个应用场景，其中广告行业是已经发展的比较成熟的了，而保险行业和医疗行业则是未来有这种发展趋势的 自动化系统一般框架上面提到了在大数据的环境下需要将处理自动化，下面以发展得较为成熟的计算广告为例讲述自动化系统的一般框架 这个系统的分解以及各部分的作用如下所示，更详细的可参考这里 由于开源软件的发展，搭建这样的系统难度不大，开源软件的几个优势和顾虑如下所示 核心业务的迭代应该是非常快而且非常重要的，不能被开源软件的开发进度控制。 最后，在具体的业务中应用到数据时，一定要遵循以下准则：数据高于经验，让数据来决策，不能只是先入为主做假设，有些现象是想不到的]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(382, 398)--随机采样算法 Reservoir Sampling]]></title>
      <url>%2F2017%2F05%2F28%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(382%2C%20398)--%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95%20Reservoir%20Sampling%2F</url>
      <content type="text"><![CDATA[Reservoir sampling 是一个随机采样算法，简单来说就是从 $n$ 个 items 中随机选择 $k$ 个items，并且每个 item 被选择的概率应该都一样。这个算法的优点在于时空复杂度都不高，其中时间复杂度为 $O(n)$, 空间复杂度为 $O(1)$。下面介绍该算法的过程，并且以 leetcode 上的两道题目为例讲解。 假设现在要从 $n$ 个数里面随机选择一个数，那么通过 Reservoir sampling 选择的流程如下 记最终选择的数为 result 遍历数组，对于数组第 i 个数，以 $1/i$ 的概率将其赋给result（i从1开始，所以第一个数肯定会赋给result） 遍历完数组后得到的 result 即为产生的随机数 假设现在有数组 [1, 2, 3], 随机产生一个数，那么按照上面的流程有 遍历第一个数时，result = 1 遍历第二个数时，result = 2 的概率为 1/2, 即 result = 1 的概率也是 1/2 遍历第三个数时，result = 3 的概率为 1/3, result = 1 的概率为 (1 - 1/3) * 1/2 = 1/3, 同理 result = 2 的概率也是 1/3 上面的 (1 - 1/3) * 1/2 指的是这一次没有选择第三个数且之前 result 的值为1的概率，通过数学归纳法可以很容易的证明遍历完整个数组后每个数被选择的概率是 1/n (n 为数组的长度) 而假如要从 $n$ 个数里面随机选择 $k$ 个数时，Reservoir sampling 的过程类似上面的 选择前 $k$ 个数作为 result 从第 $k+1$ 个数开始遍历数组，对于数组第 $k+i （i = 1,2,…..）$个数，以 $\frac{k}{k+i}$ 的概率选择这个数加入result并替换掉 result 中的任意一个数 遍历完数组后得到的 result 即为产生的 $k$ 个随机样本 下面通过数学归纳法证明通过上面的算法过程最终每个数被选择的概率为 $k/n$ 1) $i = 1$ 时，选择第 $k+1$ 个数的概率为 $\frac{k}{k+1}$，而在 result 中 $k$ 个数里面的一个(记为 $x$) 能够保留下来的概率为是 $x$ 原来在 result中且这一次没有被替换的概率，而这一次没有被替换掉又可分为两种情况，一种是根本没有选择到第 $k+i$ 个数，一种是选择了第 $k+i$ 个数，但是替换 $k$ 个数中的一个时没有替换掉 $x$。公式表示为 $$p( x 上一次在 result 中) * p( x 没有被替换掉) = 1 *（\frac{k}{k+1} * (1-\frac{1}{k}) + (1 - \frac{k}{k+1})）= \frac{k}{k+1}$$ 即每个数被选择的概率为 $\frac{k}{k+1}$ 2) 因此当 $i = m$ 时，每个数被选择的概率为 $k/(k+m)$ 3) 则当 $i = m+1$ 时，选择第 $k+m+1$ 个数的概率为 $\frac{k}{k+m+1}$, 在 result 中 $k$ 个数里面的一个(记为 $x$) 能够保留下来的概率为: $$p( x 上一次在 result 中) * p( x 没有被替换掉) = \frac{k}{k+m} *（\frac{k}{k+m+1} * (1- \frac{1}{k}) + (1 - \frac{k}{k+m+1})）= \frac{k}{k+m+1}$$ 从上可知，遍历到第 $i$ 个数的时候，前 $k+i$ 每个数被选择的概率为 $k/(k+i)$,则遍历完 $n$ 个数后，每个数被选择的概率为 $k/n$ LeetCode 上的题目 382. Linked List Random Node 和 398. Random Pick Index 均用到了 Reservoir Sampling 的技巧，上面的依概率选择可以通过产生随机数并与概率值比较来实现，下面分别是 解决 382. Linked List Random Node 和 398. Random Pick Index 的 Java 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 382. Linked List Random Nodepublic class Solution &#123; private ListNode dummy = new ListNode(0); public Solution(ListNode head) &#123; dummy.next = head; &#125; public int getRandom() &#123; ListNode curr = dummy.next; int count = 0, result = 0; while (curr != null) &#123; count ++; if (Math.random() &lt; 1.0/count) result = curr.val; curr = curr.next; &#125; return result; &#125;&#125;// 398. Random Pick Indexpublic class Solution &#123; private int[] numbers; public Solution(int[] nums) &#123; numbers = nums; &#125; public int pick(int target) &#123; int index = 0, count = 0; for(int i = 0; i &lt; numbers.length; i++) &#123; if (numbers[i] == target) &#123; count++; if(Math.random() &lt; 1.0/count) index = i; &#125; &#125; return index; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习中模型优化不得不思考的几个问题]]></title>
      <url>%2F2017%2F05%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E4%B8%8D%E5%BE%97%E4%B8%8D%E6%80%9D%E8%80%83%E7%9A%84%E5%87%A0%E4%B8%AA%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[文章为转载，原文链接在这里，文章从业界的角度出发介绍了机器学习如何发挥其价值，非常接地气，值得一看，以下为原文 机器学习工程师的知识图谱图1 机器学习工程师的知识图谱 图1列出了我认为一个成功的机器学习工程师需要关注和积累的点。机器学习实践中，我们平时都在积累自己的“弹药库”：分类、回归、无监督模型、Kaggle上面特征变换的黑魔法、样本失衡的处理方法、缺失值填充……这些大概可以归类成模型和特征两个点。我们需要参考成熟的做法、论文，并自己实现，此外还需要多反思自己方法上是否还可以改进。如果模型和特征这两个点都已经做得很好了，你就拥有了一张绿卡，能跨过在数据相关行业发挥模型技术价值的准入门槛。 在这个时候，比较关键的一步，就是高效的技术变现能力。 所谓高效，就是解决业务核心问题的专业能力。本文将描述这些专业能力，也就是模型优化的四个要素：模型、数据、特征、业务，还有更重要的，就是它们在模型项目中的优先级。 模型项目推进的四要素项目推进过程中，四个要素相互之间的优先级大致是：业务&gt;特征&gt;数据&gt;模型。 图2 四要素解决问题细分+优先级 业务一个模型项目有好的技术选型、完备的特征体系、高质量的数据一定是很加分的，不过真正决定项目好与坏还有一个大前提，就是这个项目的技术目标是否在解决当下核心业务问题。 业务问题包含两个方面：业务KPI和deadline。举个例子，业务问题是在两周之内降低目前手机丢失带来的支付宝销赃风险。这时如果你的方案是研发手机丢失的核心特征，比如改密是否合理，基本上就死的很惨，因为两周根本完不成，改密合理性也未必是模型优化好的切入点；反之，如果你的方案是和运营同学看 bad case，梳理现阶段的作案通用手段，并通过分析上线一个简单模型或者业务规则的补丁，就明智很多。如果上线后，案件量真掉下来了，就算你的方案准确率很糟、方法很low，但你解决了业务问题，这才是最重要的。 虽然业务目标很关键，不过一般讲，业务运营同学真的不太懂得如何和技术有效的沟通业务目标，比如： 我们想做一个线下门店风险评级的项目，希望运营通过反作弊模型角度帮我们给门店打个分，这个分数包含的问题有：风险是怎么定义的、为什么要做风险评级、更大的业务目标是什么、怎么排期的、这个风险和我们反作弊模型之间的业务关系你是怎么看的？ 做一个区域未来10min的配送时间预估模型。我们想通过运营的模型衡量在恶劣天气的时候每个区域的运力是否被击穿（业务现状和排期？运力被击穿可以扫下盲么？运力击穿和配送时间之间是个什么业务逻辑、时间预估是刻画运力紧张度的最有效手段么？项目的关键场景是恶劣天气的话，我们仅仅训练恶劣天气场景的时间预估模型是否就好了？）。 为了保证整个技术项目没有做偏，项目一开始一定要和业务聊清楚三件事情： 1. 业务核心问题、关键场景是什么。 2. 如何评估该项目的成功，指标是什么。 3. 通过项目输出什么关键信息给到业务，业务如何运营这个信息从而达到业务目标。 项目过程中，也要时刻回到业务，检查项目的健康度。 数据与特征要说正确的业务理解和切入，在为技术项目保驾护航，数据、特征便是一个模型项目性能方面的天花板。garbage in， garbage out 就在说这个问题。 这两天有位听众微信问我一个很难回答的问题，大概意思是，数据是特征拼起来构成的集合嘛，所以这不是两个要素。从逻辑上面讲，数据的确是一列一列的特征，不过数据与特征在概念层面是不同的：数据是已经采集的信息，特征是以兼容模型、最优化为目标对数据进行加工。就比如通过word2vec将非结构化数据结构化，就是将数据转化为特征的过程。 所以，我更认为特征工程是基于数据的一个非常精细、刻意的加工过程。从传统的特征转换、交互，到embedding、word2vec、高维分类变量数值化，最终目的都是更好的去利用现有的数据。之前有聊到的将推荐算法引入有监督学习模型优化中的做法，就是在把两个本不可用的高维ID类变量变成可用的数值变量。 观察到自己和童鞋们在特征工程中会遇到一些普遍问题，比如，特征设计不全面，没有耐心把现有特征做得细致……也整理出来一套方法论，仅供参考： 图3 变量体系、研发流程 在特征设计的时候，有两个点可以帮助我们把特征想的更全面： 1. 现有的基础数据2. 业务“二维图” 这两个方面的整合，就是一个变量的体系。变量（特征），从技术层面是加工数据，而从业务层面实际在反应RD的业务理解和数据刻画业务能力。“二维图”，实际上未必是二维的，更重要的是我们需要把业务整个流程抽象成几个核心的维度，举几个例子： 外卖配送时间业务（维度甲：配送的环节，骑手到点、商家出餐、骑手配送、交付用户；维度乙：颗粒度，订单粒度、商家粒度、区域城市粒度；维度丙：配送类型，众包、自营……）。 反作弊变量体系（维度甲：作弊环节，登录、注册、实名、转账、交易、参与营销活动、改密……；维度乙：作弊介质，账户、设备、IP、WiFi、银行卡……）。 通过这些维度，你就可以展开一个“二维图”，把现有你可以想到的特征填上去，你一定会发现很多空白，比如下图，那么哪里还是特征设计的盲点就一目了然： 图4 账户维度在转账、红包方面的特征很少；没有考虑WiFi这个媒介；客满与事件数据没考虑 数据和特征决定了模型性能的天花板。deep learning当下在图像、语音、机器翻译、自动驾驶等领域非常火，但是 deep learning在生物信息、基因学这个领域就不是热词：这背后是因为在前者，我们已经知道数据从哪里来，怎么采集，这些数据带来的信息基本满足了模型做非常准确的识别；而后者，即便有了上亿个人体碱基构成的基因编码，技术选型还是不能长驱直入——超高的数据采集成本，人后天的行为数据的获取壁垒等一系列的问题，注定当下这个阶段在生物信息领域，人工智能能发出的声音很微弱，更大的舞台留给了生物学、临床医学、统计学。 模型图5 满房开房的技术选型、特征工程roadmap 模型这件事儿，许多时候追求的不仅仅是准确率，通常还有业务这一层更大的约束。如果你在做一些需要强业务可解释的模型，比如定价和反作弊，那实在没必要上一个黑箱模型来为难业务。这时候，统计学习模型就很有用。 这种情况下，比拼性能的话，我觉得下面这个不等式通常成立：Glmnet&gt;LASSO&gt;=Ridge&gt;LR/Logistic。相比最基本的LR/Logistic，ridge通过正则化约束缓解了LR在过拟合方面的问题，lasso更是通过L1约束做类似变量选择的工作。 不过两个算法的痛点是很难决定最优的约束强度，Glmnet是Stanford给出的一套非常高效的解决方案。所以目前，我认为线性结构的模型，Glmnet的痛点是最少的，而且在R、Python、Spark上面都开源了。 如果我们开发复杂模型，通常成立第二个不等式 RF（Random Forest，随机森林）&lt;= GBDT &lt;= XGBoost 。拿数据说话，29个Kaggle公开的winner solution里面，17个使用了类似GBDT这样的Boosting框架，其次是 DNN（Deep Neural Network，深度神经网络），RF的做法在Kaggle里面非常少见。 RF和GBDT两个算法的雏形是CART（Classification And Regression Trees），由L Breiman和J Friedman两位作者在1984年合作推出。但是在90年代在发展模型集成思想the ensemble的时候，两位作者代表着两个至今也很主流的派系：stacking/ Bagging &amp; Boosting。 一种是把相互独立的CART（randomized variables，bootstrap samples）水平铺开，一种是深耕的Boosting，在拟合完整体后更有在局部长尾精细刻画的能力。同时，GBDT模型相比RF更加简单，内存占用小，这都是业界喜欢的性质。XGBoost在模型的轻量化和快速训练上又做了进一步的工作，也是目前我们比较喜欢尝试的模型。 作者简介胡淏，美团算法工程师，毕业于哥伦比亚大学。先后在携程、支付宝、美团从事算法开发工作。了解风控、基因、旅游、即时物流相关问题的行业领先算法方案与流程。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[凸优化总结]]></title>
      <url>%2F2017%2F05%2F20%2F%E5%87%B8%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
      <content type="text"><![CDATA[之前曾写过一篇最优化课程总结， 涉及到的内容较多也较细。而在最优化中，凸优化是最为常见而又最为重要的，因为凸优化有一个良好的性质：局部最优是全局最优，这个性质使得我们不需要去证明解是否会收敛到全局最优，或者如何避免局部最优。因此凸优化有广泛应用，在优化问题不是凸的时候，往往也会尝试将其变为凸问题便于求解。本文着重讲凸优化，算是对之前写的文章的一个拓展和补充。 本文主要讲述下面内容，凸优化的概念以及凸优化中的三类常见解法：梯度类方法，对偶方法和ADMM方法。 凸集，凸函数与凸优化凸集凸集的定义非常清楚 对于集合 $K$ ，$\forall x_1,x_2 \in K$,若 $\alpha x_1 + (1-\alpha)x_2 \in K$,其中$\alpha \in [0,1])$,则 $K$ 为凸集 即集合中任意两点的连线均在凸集中，如在下图中左边的是凸集而右边的不是 有时候需要对某个凸集进行放缩转换等操作，对凸集进行以下操作后，得到的集合依然是凸集 凸集的重叠（intersection）部分任然为凸集 若 $C$ 为凸集，则 $$aC+b = \lbrace ax+b , x \in C, \forall a, b\rbrace$$ 也为凸集 对于函数 $f(x) = Ax+b$, 若 $C$ 为凸集，则下面得到的转换也为凸集，注意这里的 $A$ 是矩阵$$f(C) = \lbrace f(x):x\in C\rbrace$$而当 $D$ 是一个凸集的时候，下面得到的转换也是凸集$$f^{-1}(D) = \lbrace x: f(x)\in D\rbrace$$这两个转换互为逆反关系 常见的凸集有下面这些(下式中 $a, x, b$ 均为向量, $A$ 为矩阵) 点（point）、线（line）、面（plane） norm ball: $\lbrace x: ||x|| \le r\rbrace$ hyperplane: $\lbrace x: a^Tx=b\rbrace$ halfspace: $\lbrace x: a^Tx \le b\rbrace$ affine space: $\lbrace x: Ax = b\rbrace$ polyhedron: $\lbrace x: Ax &lt; b\rbrace$ polyheron 的图像为 凸函数凸函数的定义如下 设$f(x)$为定义在n维欧氏空间中某个凸集S上的函数，若对于任何实数$\alpha(0&lt;\alpha&lt;1)$以及S中的任意不同两点 $x$ 和 $y$，均有$$f(\alpha x+ (1-\alpha)y) \le \alpha f(x) + (1-\alpha)f(y)$$则称$f(x)$为定义在凸集 S 上的凸函数 凸函数的定义也很好理解，任意两点的连线必然在函数的上方，如下是一个典型的凸函数 严格凸(strictly convex)与強凸(strongly convex) 严格凸指的是假如将上面不等式中的 $\le$ 改为 $\lt$， 则称该函数为严格凸函数。 严格凸指的是$\forall m &gt; 0, f - \frac{m}{2}||x||_2^2$ 也是凸的，其含义就是该凸函数的“凸性”比二次函数还要强，即使减去一个二次函数还是凸函数 凸函数有几个非常重要的性质，对于一个凸函数 $f$, 其重要性质 一阶特性（First-order characterization）： $$f(y) \ge f(x) + \nabla f(x)(y - x)$$ 二阶特性（Second-order characterization）： $$\nabla^2f(x) \succeq 0$$这里的 $\succeq 0$ 表示 Hessian 矩阵是半正定的。 Jensen不等式（Jensen’s inequality）：$$f(E(x)) \le E(f(x))$$这里的 $E$ 表示的是期望，这是从凸函数拓展到概率论的一个推论，这里不详细展开。 sublevel sets，即集合 $\lbrace x:f(x) \le t\rbrace$ 是一个凸集 其中，一阶特性或二阶特性是一个函数为凸函数的充要条件，通常用来证明一个函数是凸函数。 常见的凸函数有下面这些 仿射函数( Affine function ): $a^Tx + b$ 二次函数( quadratic function),注意这里的 $Q$ 必须为半正定矩阵: $\frac{1}{2}x^TQx + b^Tx+c(Q \succeq 0)$ 最小平方误差( Least squares loss ): $||y-Ax||_2^2$ (总是凸的，因为 $A^TA$ 总是半正定的) 示性函数（Indicator function）：$$I_C(X) = \begin{cases} 0&amp;x \in C\\\infty &amp; x \notin C\end{cases}$$ max function: $f(x) = max \lbrace x_1,…x_n \rbrace$ 范数（Norm）：范数分为向量范数和矩阵范数，任意范数均为凸的，各种范数的定义如下 向量范数 0范数：$||x||_0 $= 向量中非零元素的个数1范数： $||x||_1 = \sum_{i=1}^n |x_i|$$p$ 范数：$||x||_p = (\sum_{i=1}^nx_i^p)^{1/p}~~(p &gt; 1)$无穷范数: $||x||_{\infty} = max_{i=1,…n} |x_i|$ 矩阵范数 核(nuclear)范数: $||X||_{tr} = \sum_{i=1}^{r}\sigma_i(X)$ , ($\sigma_i(X)$是矩阵分解后的奇异值,核范数即为矩阵所有奇异值之和)谱（spectral）范数：$||X||_{op} = max_{i=1,…r}\sigma_i(X)$, 即为最大的奇异值 凸优化对于下面的优化问题 $$\begin{align*}&amp;\min_x\quad f(x)\\&amp;\begin{array}\\s.t.&amp;g_i(x) \le 0,~i=1,\ldots,m\\&amp;h_j(x)=0,~j=1,\ldots,r\end{array}\end{align*}$$ 当 $f(x), g_i(x)$ 均为凸函数， 而$h_j(x)$ 为仿射函数（affine function）时，该优化称为凸优化,注意上面的 $\min$ 以及约束条件的符号均要符合规定。 凸优化也可以解释为目标函数 $f(x)$ 为凸函数而起约束围成的可行域为一个凸集。 常见的一些凸优化问题有：线性规划（linear programs），二次规划（quadratic programs），半正定规划（semidefinite programs），且 $LP \in QP \in SDP$, 即后者是包含前者的关系。 线性规划问题一般原型如下($c$为向量，$D,A$为矩阵) $$\begin{align*}&amp;\min_x\quad c^Tx\\&amp;\begin{array}\\s.t.&amp;Dx \le d\\&amp;Ax=b\end{array}\end{align*}$$ 二次规划问题一般原型如下（要求矩阵 $Q$ 半正定） $$\begin{align*}&amp;\min_x\quad \frac{1}{2}x^TQx+c^Tx\\&amp;\begin{array}\\s.t.&amp;Dx \le d\\&amp;Ax=b\end{array}\end{align*}$$ 而半正定规划问题一般原型如下($X$ 在这里表示矩阵)$$\begin{align*}&amp;\min_X\quad CX\\&amp;\begin{array}\\s.t.&amp;A_iX \le b_i, i=1,…m\\&amp;X \succeq 0\end{array}\end{align*}$$ 梯度类方法梯度类方法是无约束优化中非常常用的方法，其依据的最根本的事实就是梯度的负方向是函数值下降最快的方向。但是常用的 gradient descent 必须要求函数的连续可导，而对于某些连续不可导的问题（如lasso regression），gradient descent 无能为力，这是需要用到subgradient descent和proximal gradient descent. gradient descent梯度下降法的迭代公式为 $$x^{(k)} = x^{(k-1)} - t_k\nabla f(x^{(k-1)} )$$ 上式中上标 $(k)$ 表示第 $k$ 次迭代, 而 $t_k$表示步长，$\nabla f(x^{(k-1)} )$表示在点 $x^{(k-1)}$ 的梯度。 这里对于梯度下降主要讨论其步长选择的问题， 最简单直接的方式是固定每次的步长为一个恒定值，但是如果步长过大或过小时，可能会导致结果难以收敛或者收敛速度很慢。因此提出了可变长步长的方法，可变长步长的方法指的是根据每次迭代依照一定的规则改变步长，下面介绍两种：backtracking line search 和 exact line serach。 backtracking line search backtracking line search 需要先选择两个固定的参数 $\alpha, \beta$, 要求 $0 &lt; \beta &lt; 1, 0&lt;\alpha&lt;1/2$ 每次迭代的时候，假如下式成立 $$f(x - t\nabla f(x)) &gt; f(x) - \alpha t||\nabla f(x)||_2^2$$ 则改变步长为 $t = \beta t$, 否则步长不变。 这种方法的思想是当步长过大的时候(即跨过了最优点)，减小步长，否则保持步长不变，如下式是一个简单的例子 exact line serach exact line serach 则是得到先计算出梯度 $\nabla f(x^{(k-1)} )$,然后代入下面的函数中，此时只有步长 $t_k$ 是未知，因此可对 $t_k$ 进行求导并令其为0，求得的 $t_k$ 即为当前的最优的步长，因为这个步长令当前迭代下降的距离最大。 $$f(x^{(k-1)} - t_k\nabla f(x^{(k-1)} ))$$ 这种方法也被称为最速下降法。 subgradient descentsubgradient 可以说是 gradient 的升级版，用于解决求导时某些连续不可导的函数梯度不存在的问题，我们知道，对于可微的凸函数有一阶特性，即 $$f(y) \ge f(x) + \nabla^T f(x)(y - x)$$ 加入将上面的 $\nabla^T f(x)$ 换成 $g^T$ 且不等式恒成立，则 $g$ 被称为 subgradient，当函数可微时，$\nabla f(x) = g$ ，但是若函数不可微，subgradient 不一定存在，下面是几个特殊函数的subgradient例子。 对于函数 $f(x) = |x|$,其图像如下 其subgradient为 $$g = \begin{cases}sign(x) &amp;x \neq 0\\[-1,1] &amp; x=0\end{cases}$$ 对于函数 $f(x) = ||x||_2$,其图像如下 其subgradient为 $$g = \begin{cases} x/||x||_2&amp;x \neq 0\\\lbrace z: ||z||_2 \le 1\rbrace &amp; x=0\end{cases}$$ 对于函数 $f(x) = ||x||_1$,其图像如下 其subgradient为 $$g = \begin{cases} sign(x_i) &amp;x_i \neq 0\\[-1,1] &amp; x_i=0\end{cases}$$ 对于两个相交的函数 $f(x) = \max \lbrace f(x_1), f(x_2)\rbrace$,设其函数图像如下 则其subgradient为$$g = \begin{cases} \nabla f(x_1) &amp;f(x_1) &gt; f(x_2) \\\nabla f(x_2) &amp;f(x_1) &lt; f(x_2) \\[\min \lbrace \nabla f(x_1),\nabla f(x_2)\rbrace, \max \lbrace \nabla f(x_1), \nabla f(x_2)\rbrace] &amp;f(x_1) = f(x_2)\end{cases} $$ 而 subgradient descent 与 gradient descent 的不同地方就是当函数不可微的时候，将 gradient descent 中更新公式中的 gradient 换成 subgradient。下面看一个经典的 lasso regression 问题。 $$\min_{\beta \in \mathbb {R}^n} \frac{1}{2} ||y-\beta||_2^2 + \lambda ||\beta||_1~~(\lambda \ge 0)$$ 对目标函数求导并令其为0，其中 $||\beta||_1$ 项不可导，用前面提到的subgradient代替，则有以下等式 $$\begin{cases} y_i - \beta_i = \lambda sign(\beta_i) &amp;\beta_i \neq 0\\|y_i - \beta_i| \le \lambda &amp;\beta_i = 0\end{cases}$$ 则解可表示成 $$\beta_i = \begin{cases} y_i - \lambda &amp; y_i &gt; \lambda\\0 &amp;-\lambda \le y_i \le \lambda \\y_i + \lambda &amp; y_i &lt; -\lambda\end{cases}$$ 上面实际上是简化了的 lasso regression， 因为更一般的lasso 问题表示如下 $$\min_{\beta \in \mathbb {R}^n} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||\beta||_1~~(\lambda \ge 0)$$ 这时如果采用上面的解法，那么会得到 $$\begin{cases} X_i^T(y - X\beta) = \lambda sign(\beta_i) &amp;\beta_i \neq 0\\|X_i^T(y - X\beta)| \le \lambda &amp;\beta_i = 0\end{cases}$$ 上式并没有为这个 lasso 问题提供一个明确的解，这个问题可以通过下面要提到的 proximal gradient 进行求解，但是上面的式子一定程度上解释了L1 regularization的导致的参数的稀疏性的特点，从上面的表达式可知，只有当 $X_i^T(y - X\beta) = \lambda sign(\beta_i)$ 时，对应的 $\beta_i$ 才不为0，而其他大多数的情况下 $\beta_i$ 为0. proximal gradient descentproximal gradient descent 也可以说是 subgradient 的升级版，proximal 通过对原问题的拆分并利用 proximal mapping，能够解决 subgradient descent 无法解决的问题（如上面的一般化 lasso 问题）。 一般来说，这类方法将目标函数描述成以下形式 $$f(X) = g(x) + h(x)$$ 上面的 $g(x)$ 是凸且可微的， 而 $h(x)$ 也是凸的，但是不一定可微。则 proximal gradient descent 的迭代公式为 $$x^{(k)} = x^{(k-1)} - t_kG_{t_k}(x^{(k-1)})\\G_{t}(x) = \frac{x - prox_{th}(x-t\nabla g(x))}{t}\\prox_{th}(x) = argmin_{z\in \mathbb {R}^n} \frac{1}{2t}||x-z||_2^2+h(z)$$ 上面 $prox_{th}(x)$ 表示对函数 $h$ proximal mapping。这里仅给出结论，证明过程略，接下来以上面没有解决的一般化的 lasso 问题为例讲述这种方法的应用。 $$\min_{\beta \in \mathbb {R}^n} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||\beta||_1~~(\lambda \ge 0)$$ 记 $g(\beta) = \frac{1}{2} ||y-X\beta||_2^2, h(\beta) = \lambda ||\beta||_1$ 则有$$prox_{th}(\beta) = argmin_{z \in \mathbb {R}^n} \frac{1}{2t}||\beta - z||_2^2+h(z) $$ 这个问题通过前面的 subgradient 方法已经解出来，结果为 $$z_i = \begin{cases} y_i - \lambda t &amp; y_i &gt; \lambda t\\0 &amp;-\lambda t \le y_i \le \lambda t\\y_i + \lambda t &amp; y_i &lt; -\lambda t\end{cases}$$ 将上面的解记为 $S_{\lambda t}(y)$, 同时 $\nabla g(\beta) = - X^T(y-X\beta)$ 代取上面列出的 proximal gradient descent 列出的迭代公式，则 $\beta$ 的迭代公式如下 $$\beta^{(k)} = S_{\lambda t}(\beta^{(k-1)} + t_kX^T(y-X\beta^{(k-1)}))$$ 其中 $t_k$ 为步长。 这种方法之所以比 subgradient 方法更加一般化，是因为 $prox_{th}(x)$ 对于绝大部分的 $h$ 是易求的。 对偶方法与KKT条件对偶理论在最优化中非常重要，其中具有代表性的两条定理是弱对偶定理和强对偶定理，弱对偶定理告诉了我们最优化的目标的上界( max 问题)或下界(min 问题)，而强对偶定理告诉了当 KKT 条件满足的时候，可以通过对偶问题的解推出原问题的解。 弱对偶条件总是成立，而强对偶需要在 Slater&#39;s condition 成立时才成立，该条件描述如下 对于优化问题 $$\begin{align*}&amp;\min_x\quad f(x)\\&amp;\begin{array}\\s.t.&amp;g_i(x) \le 0,~i=1,\ldots,m\\&amp;h_j(x)=0,~j=1,\ldots,r\end{array}\end{align*}$$ 假如存在可行解 $x’$ 使得 $g_i(x’) &lt; 0(i=1,…m)$, 即不等式约束严格成立（注意同时等式约束也要成立，否则就不是可行解了），那么称Slater&#39;s condition 成立，同时强对偶也成立。 线性规划对偶对于形如下面的线性规划问题 $$\begin{align*}&amp;\min_x\quad c^Tx\\&amp;\begin{array}\\s.t.&amp;Ax = b\\&amp;Gx \le h\end{array}\end{align*}$$ 其对偶问题为 $$\begin{align*}&amp;\min_{u,v}\quad -b^Tu - h^Tv\\&amp;\begin{array}\\s.t.&amp;-A^Tu - G^Tv = c\\&amp; v \ge 0\end{array}\end{align*}$$ 对于 LP 问题，其对偶的特殊性在于只要存在原问题存在可行解，那么 Slater&#39;s condition 一定成立，因此强对偶也成立. LP 对偶问题的一个经典的例子是最大流(max flow)问题和最小割(min cut)问题. 拉格朗日对偶对于更一般的非 LP 问题的对偶问题，需要用到拉格朗日对偶理论得到，并称该问题为拉格朗日对偶问题。 这个方法可以说是对求解等式约束的拉格朗日乘子法的一个推广。 对于下面的优化问题 $$\begin{align*}&amp;\min_x\quad f(x)\\&amp;\begin{array}\\s.t.&amp;g_i(x) \le 0,~i=1,\ldots,m\\&amp;h_j(x)=0,~j=1,\ldots,r\end{array}\end{align*}$$ 其増广拉格朗日函数为 $$L(x,u,v) = f(x) + \sum_{i=1}^m u_ig_i(x) + \sum_{j=1}^r v_jh_j(x)~~(u_i \ge 0)$$ 则原问题的拉格朗日对偶函数为 $$ g(u,v) = \min_x L(x,u,v)$$ 且原问题的对偶问题为 $$\begin{align*}&amp;\max_{u,v}\quad g(u,v)\\&amp;\begin{array}\\s.t.&amp;u_i \ge 0,~i=1,\ldots,m\end{array}\end{align*}$$ 上面得到对偶问题的简单推导流程如下： 首先原问题的目标函数加上约束可以表示为 $$f(x) = \max_{u,v} L(x,u,v)$$ 原因是在增广拉格朗日函数中，假如 $x$ 违反了约束条件(即 $ g(x) &gt; 0 $ )，那么 $f(x)$ 会趋向无穷大；而不违反约束条件时，$u$ 必须取0才能使得目标最小。 进一步，原问题可以表示为 $$\min_x \max_{u,v} L(x,u,v) $$ 而由于下式恒成立（弱对偶定理） $$\min_x \max_{u,v} L(x,u,v) \ge \max_{u,v} \min_xL(x,u,v) $$ 因此将 $\min_xL(x,u,v)$ 作为对偶函数， $\max_{u,v} \min_xL(x,u,v) $ 作为对偶问题。 假设现在找到了对偶问题，并且对偶问题比原问题要容易求解得多，也求出了对偶问题的解，那么该转化去求原问题的解，这里就要用到下面的KKT条件。 KKT 条件KKT 条件是非线性规划领域中最重要的理论成果之一，是确定某点是最优点的一阶必要条件，只要是最优点就一定满足这个条件，但是一般来说不是充分条件，因此满足这个点的不一定是最优点。但对于凸优化而言，KKT条件是最优点的充要条件。 同样地 对于下面的优化问题 $$\begin{align*}&amp;\min_x\quad f(x)\\&amp;\begin{array}\\s.t.&amp;g_i(x) \le 0,~i=1,\ldots,m\\&amp;h_j(x)=0,~j=1,\ldots,r\end{array}\end{align*}$$ 其増广拉格朗日函数为 $$L(x,u,v) = f(x) + \sum_{i=1}^m u_ig_i(x) + \sum_{j=1}^r v_jh_j(x)~~(u_i \ge 0)$$ 则 KKT 条件为 $$\begin{cases} \nabla_x L(x,u,v) = 0\\u_ig(x) = 0\\u_i \ge 0\\g_i(x) \le 0, h_j(x)=0\end{cases}$$ 因此其实只要能够求解出上面的联立方程组，得到的解就是最优解（对于凸优化而言，非凸的问题一般用KKT来验证最优解）。 但是上面的方程组往往很难求解，一些特殊情况下解是有限的，可以分类讨论；但是更一般的情况下可能的解是无限的，因此无法求解。这里要结合上面的拉格朗日对偶问题得到的解进行求解。 求解之前，首先要知道下面的定理 假如一个问题满足强对偶，那么 $x’,u’,v’$ 是原问题和对偶问题的最优解 $\longleftrightarrow$ $x’,u’,v’$ 满足KKT条件。 因此通过对偶问题求得 $u’,v’$ 后，带入上面的 KKT 条件即可求出 $x’$ 。 svm 是利用拉格朗日对偶和KKT条件进行求解的经典问题，这里不详细展开，有兴趣的可以参考 Andrew Ng 公开课中关于 svm 的那章或这篇文章。 ADMMADMM(Alternating Direction Method of Multipliers) 是解决带约束的凸优化问题的一种迭代解法，当初提出这个算法最主要的目的是为了在分布式环境(Hadoop, MPI 等)中迭代求解这个问题，关于这方面的资料可参考这里 ADMM 将要解决的问题描述成以下形式 $$\begin{align*}&amp;\min_x\quad f_1(x_1) + f_2(x_2)\\&amp;\begin{array}\\s.t.&amp; A_1x_1+A_2x_2 = b\end{array}\end{align*}$$ 这里省略证明过程，直接给出 ADMM 的迭代公式 $$\begin{align*}&amp;x_1^{(k)} = argmin_{x_1} f_1(x_1) + \frac{\rho}{2}||A_1x_1+A_2x_2^{(k-1)} - b + w^{(k-1)}||_2^2\\&amp;x_2^{(k)} = argmin_{x_2} f_2(x_2) + \frac{\rho}{2}||A_1x_1^{(k)} + A_2x_2 - b + w^{(k-1)}||_2^2\\&amp;w^{(k)} = w^{(k-1)} + A_1x_1^{(k)} + A_2x_2^{(k)} - b\end{align*}$$ 上式中的 $\rho$ 是事先选择的参数，可以选择定值，选择定值的时候会遇到跟梯度下降固定步长的类似问题，因此也可以根据每次迭代情况改变 $\rho$ 的值，具体可参考这篇文献。 实际中，难点在于把一个问题变为 ADMM 求解的形式，即上面列出的优化问题的形式。下面给出一个例子说明这个问题，这个例子是一个更一般的 lasso regression 问题，称为 fused lasso regression. $$\min_{\beta \in \mathbb {R}^n} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||D\beta||_1~~(\lambda \ge 0)$$ 一般的 lasso regression 问题中 $D = I$.下面用 ADMM 的形式表示上面的问题 $$\min_{\beta \in \mathbb {R}^n，\alpha \in \mathbb {R}^n} \frac{1}{2} ||y-X\beta||_2^2 + \lambda ||\alpha||_1~~(\lambda \ge 0)\\s.t. D\beta - \alpha = 0$$ 将这个问题映射到上面的优化问题的模式有 $$\frac{1}{2} ||y-X\beta||_2^2\rightarrow f_1(x1) \\\lambda ||\alpha||_1 \rightarrow f_2(x_2)\\D\beta - \alpha = 0 \rightarrow A_1x_1+A_2x_2 = b$$ 则根据上面的迭代公式计算（对问题变量求导且令结果为0）可得到下面的迭代公式 $$\begin{align*}&amp;\beta^{(k)} = (X^TX+\rho D^TD)^{-1}(X^Ty+\rho D^T(\alpha^{(k-1)}-w^{(k-1)})\\&amp;\alpha^{(k)} = S_{\lambda/\rho}(D\beta^{(k)}+w^{(k-1)})\\&amp;w^{(k)} = w^{(k-1)} + D\beta^{(k)} - \alpha^{(k)}\end{align*}$$ 而对于无约束的优化也可以通过 ADMM 求解，例如对于下面的问题 $$\min_x \sum_{i=1}^B f_i(x)$$ 将其表示为ADMM的形式如下 $$\min_{x_1,..x_B,x} \sum_{i=1}^B f_i(x_i)\\s.t. x_i = x,~~i=1,..B$$ 则ADMM的迭代规则如下 $$\begin{align*}&amp;x_i^{(k)} = argmin_{x_i} f_i(x_i)+\frac{\rho}{2}||x_i-x^{(k-1)}+w_i^{(k-1)}||~(i=1,..B)\\&amp;x^{(k)} = \frac{1}{B} \sum_{i=1}^B (x_i^{(k)} + w_i^{(k-1)})\\&amp;w_i^{(k)} = w_i^{(k-1)} + x_i^{(k)} - x^{(k)}~(i=1,..B)\end{align*}$$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《ImageNet Classification with Deep Convolutional Neural Networks》阅读笔记]]></title>
      <url>%2F2017%2F05%2F15%2FImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[ImageNet Classification with Deep Convolutional Neural Networks 这篇论文可以说是多层CNN用在图像领域的首次尝试（此前的LeNet也将CNN用在手写数字的识别上，但是没有用到连续多层CNN）。文中提出的网络模型 AlexNet(设计者的名字叫 Alex) 在 ImageNet 2010、2012 年的比赛上取得的效果远远地优于传统方法，这篇文献最重要的工作是设计并验证了这样一个有多层卷积层的网络的有效性，对学术界和工业界的影响都很大。 本文主要介绍这篇文章中提出的网络模型 AlexNet，以及其他涉及到的一些知识，主要的介绍的内容有：CNN的基础知识，AlexNet 的设计，训练和效果，以及对网络泛化（generalization）性能的一些探讨。 CNN 简介CNN 全称是 Convolutional Neural Network，翻译做卷积神经网络，类似于我们常见的神经网络，CNN 也是一层一层连接起来的。顺便一提，这种 layer by layer 的网络叫做 feed-forward network，特点是无环，以区别于贝叶斯网络这种有环的网络 但是与传统的多层神经网络不同点在于,组成 CNN 的各个网络层不是常见的神经网络的里的网络层，而是由以下四种特殊的网络层组成 卷积层(convolutional layer) 池化层(pooling layer) ReLU层(ReLU layer) 全连接层(fully connected layer) 下面分别介绍各个层的具体结构与作用 卷积层(Convolutional layer)卷积层是 CNN 中最重要的层，也是 CNN 名称的来源，卷积层里面有几个重要概念：核（kernel/filter)、卷积（convolution）、输出（activation map）。对照下图可以比较清晰的理解 核（kernel/filter): 移动的橙色正方形卷积（convolution）：kernel 和 image 相应区域的点积输出（activation map）：卷积的输出，图中的粉色区域（convolved feature） 卷积层的作用可以理解为特征抽取，这种设计来源于人脑里面的机制，但是我们也能够直观地理解这类操作的意义，假设说我们现在要观察一幅图像，往往是从左到右，从上到下来观察的，而相邻的区域往往具有相似性，对于小面积区域（也就是 kernel 覆盖的区域）可以用更少的数据来概括这部分的特征。 上面的图像只用了一个 kernel，我们将其看做是一个人观察这张图片得到的信息，那假如有更多的人观察这张图片，并将所有人观察到的信息综合起来，得到的信息是否会更加完备呢?答案是肯定的，这就涉及到了多个kernel的情况，具体如下图所示 除了多个 kernel，图片也会有多个channel，上图显示的 input image 只有一个channel，但是实际中用于分类的图片往往是 RGB 图片，有3个channel，分别是red， blue，green，如下图所示 因此当输入的图片有多个channel，并且用多个 kernel 去进行卷积的时候，过程会如下图所示（来源） 上图的左边是 input image 的三个 channel，中间是两个 kernel，右边是输出的两个 activation map。对于有多个 channel 的情况，每个 kernel 就不再是二维的，而是三维，除去表示 kernel 大小的两个维度，剩下的一个维度也叫深度，大小就是输入的 channel 的大小。将从各个 channel 得到的值加起来，就得到了对应的 activation map 相应位置的输出。这里需要注意的的是，无论输入有多少个 channel，每个 kernel 最终只产生一个 activation map。 池化层(pooling layer)池化类似于一种下采样（down sampling), 目的是要较少参数数量和计算量，如下所示是一个 max polling 的例子，其步长（stride）为2，kernel 为 2 X 2。 max pooling 指的是每次取 kernel 中的最大值最为输出，除了 max pooling，还有 average pooling 等其他方式。除了上面提到的减少参数数量和计算量，池化还可以避免过拟合。 当上面的步长改为1后，kernel 移动过的区域会有重叠，我们称之为 overlapping pooling，如下图所示 ReLU 层(ReLU layer)ReLU 的全称是 Rectified Linear Units，从严格意义上来讲，ReLU 只是一个激活函数，而不能称之为一个层。其函数表达式为 $f(x) = max(0, x)$，其图像如下所示 ReLU的主要作用是提升整个网络的非线性判别能力。关于选择 ReLU 作为激活函数而不是 sigmoid 或 tanh，后面会有详细说明。 全连接层(fully connected layer)全连接层就是我们常见的神经网络中的网络层，每个神经元都与前面或后面的各个神经元有连接，如下图所示 由于全连接层的参数过多，在 CNN 中全连接层往往是作为最后几层用于输出。 网络的设计上面介绍的四类 layer 是构成这篇论文中的 CNN 网络四种 layer，下面介绍论文中的 CNN 网络的结构及其特点。 网络结构总览文中提出的 CNN 网络结构如下 上图有以下几个特点 由5层卷积层+3层全连接层构成，并且整个网络在两个GPU上训练 在第 1、2、5 层卷积层后添加了最大池化的操作 在每层卷积层和全连接层后都有 ReLU 激活函数 上图中网络就是 AlexNet，网络结构可以这样理解，首先上下两部分表示网络在两个GPU上训练，前五层表示5层卷积层，后三层表示3层全连接层；而立方体（最左边的是输入的图像，这里不算入）表示每层卷积层的输出，立方体里面的小立方体表示kernel的大小。 第一层卷积层采用了 48+48 共96个 kernel，输入的图像有三个 channel，但是前面提到无论有多少个channel，一个 kernel 只会产生一个 activation map，所以图中的第一个立方体 48 表示输出的 48 个 activation map，而这48个 activation map 作为第二层卷积层的输入又成为了 48 个 input channels，依次类推，第二层卷积层采用了 128+128 共 256 个kernel。 网络的特点这个网络有三个特点并没有在上图中并不是非常显式地展示出来：分别是 ReLU Nonlinearity、Local Response Normalization 和 Overlapping Pooling。 ReLU Nonlinearity ReLU 在前面已经简单地进行了介绍，这里要讨论的是为什么采用了 ReLu 作为激活函数而不是 其他的如 sigmoid 或 tanh。主要原因是 ReLU 能够更快地收敛，因为其能够在一定程度上避免梯度消失（vanishing gradient）的现象。 要解释这个原因首先需要看看这三个函数的图像（ReLU 的图像上面已经给出） 这两个激活函数的图像非常相似，均是两边平，中间陡。当通过反向传播（backpropgation）训练时，需要通过链式法则求出总的梯度，而当激活值很大或很小的时候，也就是对应到上面图像两边平缓的地方是，对这两个激活函数的求导结果几乎为0， 从而导致相乘得到的总的梯度也几乎为0，错误不能有效地传播到前面的层，修正前面层的参数。这种现象就称为梯度消失。 而对于 ReLU 函数，当激活值小于0的时候，也存在着相同问题，而且这时候导数完全是0；但是大于0的时候 ReLU 的导数总是1，因此大于0的时候不存在梯度消失的现象。也有人说当激活值小于0的时候会带来稀疏性的好处。 Local Response Normalization Local Response Normalization 指的是对网络中经过 ReLU 层输出的结果进行正规化， 其正规化的公式如下所示： $$b_{x,y}^i = a_{x,y}^i/(k + \alpha \sum_{j=\max(0, i-n/2)}^{\min(N-1, i+n/2)}(a_{x,y}^j)^2)^{\beta}$$ 上式中的 $a_{x,y}^i$ 表示第 $i$ 个kernel 在位置 $(x,y)$ 的原始输出，而 $b_{x,y}^i$表示正规化后的输出，$N$ 表示所有 kernel 的数目。上式表明对某个 kernel 在某个位置的输出的正规化利用了与这个 kernel 相邻的 $n$ 个 kernel 在相同位置的值进行。 而其他参数 $k, n, \alpha, \beta$ 则是通过 cross-validataion 获得的参数，Local Response Normalization 分别被应用到第一层和第二层卷积层，文章里说这种方法分别将 top-1 error 和 top-5 error 降低了 1.4% 和1.2%。 Overlapping Pooling 这个机制我们在前面谈到池化层的时候已经提到，文章里说这种方法分别将 top-1 error 和 top-5 error 降低了 0.4% 和 0.3%。 网络的训练目标函数文章中的问题是一个图像多分类的问题，多分类问题有若干种方法，在神经网络中最常用的就是 softmax 单纯从数学的角度来讲，softmax 只是一种向量变换方式，假设现在有一个长度为 $k$ 的向量 $z = (z_1,…z_k)$,对其进行 softmax 变换后得到向量 $\sigma(z)$, 其变换公式如下 $$\sigma(z)_j = \frac{e^{z_j}}{\sum_{l=1}^k e^{z_l}}~~~(j=1,…k)$$ 变换后的向量 $\sigma(z)$ 有一个重要特征，就是所有元素之和加起来为 1；从概率论的角度来考虑，很自然地可以将这个向量作为属于各个分类的一个概率分布，选择值最大的那个项对应的分类作为其分类。 这种“自然”也是有数学支撑的，实际上，softmax 的这个特性可以从 Generalized Linear Model 中推导出来。这里就不详细展开论述了。 有了概率分布，很自然地一个想法就是做极大似然估计，如下是一个 $k$ 分类问题中，最大化 $m$ 个sample 的联合概率分布，其中 $1 \lbrace . \rbrace$ 的含义为 $1 \lbrace True \rbrace = 1, 1 \lbrace False \rbrace = 0$，如$1 \lbrace 2=2 \rbrace = 1, 1 \lbrace 2=3 \rbrace = 0$ $$\max \sum_{i=1}^{m} \sum_{j=1}^{k} 1 \lbrace y^{(i)} = j \rbrace \log \frac{e^{z_j}}{\sum_{l=1}^k e^{z_l}}$$ 在其前面添加一个负号和一个常数 $\frac{1}{m}$ 可以将其转为如下的极小化问题 $$\min -\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{k} 1 \lbrace y^{(i)} = j \rbrace \log \frac{e^{z_j}}{\sum_{l=1}^k e^{z_l}}$$ 实际上，上面要最小化的目标函数是交叉熵损失（cross-entropy error），这个目标函数也可以通过交叉熵的定义推导出来。 训练算法上面得到的最后是一个无约束的最优化问题，对于这类最优化问题有多种方法可用，其中最常用的是随机梯度下降（Stochastic Gradient Descent），但是这里没有用原始的SGD， 而是采用了带有 momentum, weight decay 和 mini-batch 的SGD。 momentum, weight decay 和 mini-batch 是三个非常重要的概念，这里简单说明他们的作用 momentum 指的是每次更新参数的梯度除了用当前迭代得到的梯度，还要加上前一次迭代得到的梯度，起作用是为了加快收敛，避免局部最优（如果问题非凸的话） weight decay 实际上是 L2 regularization 微分后得到的项，其目的是为了防止过拟合。 mini-batch 则是指每个更新时不仅采用一个样本，而是采用多个样本，这种方法介于 BGD 和 SGD 之间。 其更新的规则如下所示,参数 ，$v_i$ 被称作 momentum variable， $- 0.0005 \epsilon w_i $ 是 weight decay 项，$&lt;\frac{\partial L}{\partial w}|_{w_i}&gt;_{D_i}$ 则是从 mini-batch 为 $D_i$ 中得到的gradient， $\epsilon$ 为步长。 $$\begin{align*}&amp;v_{i+1} = 0.9v_i - 0.0005 \epsilon w_i - \epsilon &lt;\frac{\partial L}{\partial w}|_{w_i}&gt;_{D_i}\\&amp;w_{i+1} = w_i + v_{i+1}\end{align*}$$ 多GPU训练前面已经提到了整个网络在两个 GPU 上训练， 原因是 GPU 能够并行的处理数据，训练速度较快，而同时一个 GPU 限制了模型的大小，因此用到了两个，两个 GPU 是并行的训练网络的，除了在第三层的卷积层两个 GPU 交换了数据用于cross validation。与一个GPU训练的模型相比，两个GPU训练的模型分别将 top-1 error 和 top-5 error 降低了 1.7% 和 1.2%。 防止过拟合为了防止过拟合，文章采用了两种方法，data augmentation 和 dropout。 data augmentation data augmentation 指的是如何从提供的数据集中得到更多的数据，文中也采用了两种途径，其中一种是从原始图像（大小为 256 × 256）中抽出多个大小为 224 × 224 的块作为图像，因此一幅原始图像能够生成多个图像；另外一种途径就是在原始图像的像素上加上通过PCA从图像中抽取出来的信息，从而生成新的图像。这两种方法将 top-1 error 降低了1%。 dropout droupout 指的是每个神经元每次传递值时只有 50% 的概率工作，如下图所示，灰色的神经元指的是该神经元并没有工作。 这种方法的好处是降低了神经元间的依赖性，使得每个神经元更加 robust。droupout 被添加在第一和第二层全连接层中。 网络的效果文中采用的数据集是 ImageNet，这是一个有着约 14 million 张 labeled image 的图片集，每一年通过这个数据集会举办一次名为 ImageNet Large-Scale Visual Recognition Challenge(ILSVRC) 的比赛，就是一个图片多分类比赛，文中展示了上面提到的 cnn 网络在 2010 年和2012年比赛中的表现结果，结果如下所示 2010年 2012年 其中 2012 年的表格中 5 CNNs 表示用了 5 个CNN做投票 ensemble 后的效果，CNN* 表示在原来的5层卷积层的基础上再增加一层卷积层。可以看到CNN所得到的结果要远远优于第二名的，而这也是当年这篇文章震惊了学术界和工业界的原因。 网络泛化能力的探讨从上面的论述中可知，我们将多个卷积层和全连接层连在一起，然后加上pooling，dropout等操作，就构建了一个取得非常好效果的网络，很自然我们会问，这个网络为什么能够取得这么好的效果？或者说这个网络的泛化能力为什么会这么好，是不是有什么保证了其泛化误差不会过大？ 在统计机器学习中，有一个叫 VC dimension 的概念，用于描述模型的复杂度，这个概念中的 VC bound 为泛化误差约束了一个bound，但是这个概念需要很复杂的数学推导，这里我们略去这些推导。只说一个 VC dimension 给我们揭示的一个很直观的概念：要取得较好的泛化能力，用于训练模型的样本数目应该至少是参数数目的10倍。 这个理论在统计机器学习的svm等模型中都得到了较好的验证，但是文中提出的网络有 60 million的参数和1.25 million 的样本，因此这个条件远远没得到满足。但是网路却取得了很好的效果，这样看来，VC dimension这个理论并适用于这个网络，实际上，不仅仅是这个网络，VC dimension 在深度学习中多个网络中也不适用。 而这一点，也被 2017 ICLR 的最佳论文 Understanding deep learning requires rethinking generalization指出，下图是从这篇论文的 presentation 中抽取的一张图片。 图中四个宠物小精灵代表了四个著名的网络，随着 p/n 值越来越大，也就是“样本/参数” 的比值越来越小，越不满足 VC dimension 提出的条件，但是泛化的误差却越来越小。 这篇最佳论文还做了很多其他实验，这里就不详细展开，但是从这篇文章并没有从理论上说明了这个网络泛化误差小的理论依据，也就是没有提出在深度学习领域适用的 “VC dimension”，而这一工作将会是未来深度学习发展中非常重要和有意义的工作。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算广告(6)--广告交易市场(Ad Exchange)]]></title>
      <url>%2F2017%2F05%2F14%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(6)--%E5%B9%BF%E5%91%8A%E4%BA%A4%E6%98%93%E5%B8%82%E5%9C%BA(Ad%20Exchange)%2F</url>
      <content type="text"><![CDATA[本文是刘鹏老师的计算广告学中的一些笔记。本文是第六章: 广告交易市场(Ad Exchange)。主要介绍目前最常见的实时竞价的广告交易模式，分别介绍了这一模式下的供给方平台(SSP) 和 需求方平台(DSP), 并且重点介绍了 DSP， 因为在这一交易模式下，DSP 负责了绝大多数的关键任务。 广告交易平台(Adx)广告交易平台的关键特征是用实时竞价(RTB)方式连接广告和 (上下文，用户)，按照展示上的竞价收取广告主费用 实时竞价指的是事先不对广告做retrieval，而是在需要展示的时候向下游的 DSP 展示目前的 context，让各个 DSP 通过竞价的方式获取这次展示的机会。其流程如下 从上图可知，RTB 是一个多方参与的过程，而从实现上可以分为两个阶段 1）cookie mapping：目的是将 supply 网站和 demand 端的用户对应起来；实际中通过 Adx（代表supply） 与 DSP（代表demand） 来进行直接的mapping，后面会提到这方面的具体技术2）ad call：指有广告展示机会到来，这时 Adx 通知各个 DSP 进行竞价，DSP 根据自身需求出价，出价最高者获得投放机会。 这种模式的出现是由于广告主需要的定制化受众定向在原来的广告网络上无法得到满足，举例来说：京东商城需要对其用户进行重定向投放广告，但是在广告网络上是采买不到这种流量的，因为哪些用户是京东的只有京东自己知道，因此需要让 demand 端对流量进行选择才能达到深度定制化的要求。 这种主要技术难点在于1） Adx与DSP之间的用户身份同步，一般通过 cookie mapping 实现2） DSP数量较多时的服务和带宽成本优化 但是存在着的潜在问题是：1） 存在浏览数据的泄露风险。指的是某些DSP目的并不是获取广告，而是获取 Adx 展示的用户浏览数据；即每次出价都很低，保证自己不会赢得广告的展示机会，但是却能获得了 Adx 提供的用户 cookie 及其访问的url，为其日后进行受众定向积累数据基础。 2） 多一次 round trip，对 latency 有较大影响。在之前的广告体系（合约广告，竞价广告）中，supply 端只需要先广告网络进行请求，广告网络根据其 retrieval 以及相关广告主的出价返回相关的广告即可，但是在实时竞价中，Adx 还需要等待 DSP 的出价以及通知获得展示机会的 DSP 进行广告的投放，这样无疑会增加投放的 latency 。 除了实时竞价，广告交易平台另外一个重要的特点是按展示计费而不是按照点击收费，其商业逻辑在于将点击率和点击价值的预估全放到 demand 端。 Adx 的系统架构图Ad Exchange 的系统架构很简单，原因是其不需要存储广告之类的信息，只需要将广告展示机会告诉给每个 DSP ，然后进行竞价拍卖选出出价最高的那个 DSP。其难点在于系统的吞吐量大且对 latency 的要求严格。其系统架构图如下 其流程如下1）某个 User 访问某个网站，网站通知 Adx 目前有一次展示机会2）Adx 通过 RTBD 接口向各个 DSP 询价，从而获取 DSP提供的各个广告（相当于图中的 Ad retrieval）3）Adx 通过出价进行排序，并选择出价最高的广告展示给用户（相当于图中的 Ad ranking）4）Adx 在日志中记录该广告的展示记录 cookie mapping 的技术cookie mapping 需要弄清楚三个问题：谁发起 cookie mapping 的请求？在哪里发起的请求？谁存mapping表？ 下面以两个例子说明，第一个例子是 DSP 与 Adx 间的cookie mapping，第二个例子是 DMP 与媒体的 cookie mapping。 下面是 DSP 与 Adx 的 cookie mapping 的过程 这个例子中 cookie mapping 由 DSP 发起，在有 DSP 代码的广告主网站发起，mapping表存在DSP。其具体流程为 1）DSP 在广告主网站中嵌入 JS 代码，有用户访问时，选择性加载一个 DSP 域名下的 iframe2）DSP 根据其记录判断该用户是否需要 cookie mapping，返回包含多个 beacon 的动态 HTML，这里的多个 beacon 目的是为了与多个 Adx 交换 cookie。3）通过某个beacon 向对应的 Adx 发送 cookie 映射请求，并带有 Adx 标识（xid），DSP标识（did）和 DAP cookie（dck） 三个参数。4）Adx 通过302 重定向向 DSP 返回 Adx 标识（xid）及其域名下的cookie（xck)5) DSP 返回一个 1x1 的 beacon 并记录下 Adx 方的 cookie（xcd） 与己方的 cookie（dck） 的对应关系。 下面是媒体与 DMP 的 cookie mapping 的过程 这个例子中 cookie mapping 由媒体方发起，在媒体的页面上发起，并且 cookie mapping 的表由 DMP 保存。其过程如下 1) 用户访问媒体网站时，媒体网站向媒体的 cookie 映射服务请求一段负责此功能的 JS 代码2）媒体的 cookie 映射服务返回该段 JS 代码3）该 JS 代码判断是否需要映射，如果需要，则先 DMP 发送映射请求，并传送两个参数：媒体的标识（mid）以及媒体方的 cookie（mck）4）DMP 返回一个 1x1 的beacon，并记录下媒体方 cookie(mck) 和己方的 cookie(dck) 对应关系。 供应方平台（supply side platform）SSP 完全代表着媒体方的利益 广告市场中，媒体变现流量方式一般有三种 合约广告，与广告主签订合约进行投放（CPM 结算） 竞价广告，将广告位托管给广告网络，广告网络根据人群售卖给广告主（CPC 结算） 实时竞价（按展示结算） 而 SSP 应该能够为媒体提供上面的所有变现方式，通过组合以上方式达到收益最大化(称为收益管理，Yield Optimizer) 综合以上可知，SSP的关键特征是 提供媒体端的用户划分和售卖能力 可以灵活接入多种变现方式 收益管理 整个行业有代表性的公式有： AdMeld，Rubicon，Pubmatic 需求方平台（Demand Side Platform）将决策交到 demand 端, 维基上对 DSP 的定义如下 A demand-side platform (DSP) is a system that allows buyers of digital advertising inventory to manage multiple ad exchange and data exchange accounts through one interface. DSP 为广告主提供的便利之处在于，广告主主需要通过 DSP 的一个接口就可以获取各式各样的流量，从而进行定制化广告投放。 DSP的关键特征如下 定制化用户划分 跨媒体流量采购 通过 ROI 估计来支持 RTB DSP 的代表公司有 InviteMedia 和 MediaMath DSP 的架构DSP 的架构示意图如下 复杂部分在于计算部分，就是要估计 eCPM = CTR * clickValue，CTR 表示点击率，clickValue 表示点击价值，后面会详细介绍，在这里 DSP 需要同时计算 CTR 和 clickValue，复杂度自然大大提升。 DSP 流量预测流量预测指的是给定一组受众定向标签组合以及一个 eCPM 的阈值，估算在将来某个时间段内符合这些受众标签组合的条件、且市场价在该 eCPM 阈值以下的广告展示量。 DSP 也需要预测流量以决定采买策略，因为 DSP 与广告主是 CPM 结算的，DSP只要将市场中符合广告主的那部分低价的流量买下来才能获取更大的利润。 但是由于 DSP 无法拿到所有的流量情况，因此无法像攻击方那样通过历史流量那样进行流量预测，这个问题目前没有一个公认的较好的解决方法 DSP 点击价值预估点击价值指的是上面提到 eCPM = CTR * clickValue 中的 clickValue。一般衡量该点击的价值的指标是点击后的转化率 这个问题的挑战有 1）训练数据非常稀疏（最终得到的训练数据的比例是 点击率*转化率）2）价值的预估与广告主类型强烈相关的行为模式（比如说游戏领域与电商领域不能相同的方法预估） 这个问题目前也没有很好的解决方法，但是有以下两点原则供参考 1）模型估计的时候，用较大的 bias 换较小的 variance，以达到稳定估计的目的2）充分利用广告商类型的层级结构以及转化流程上的特征 DSP 重定向（retargeting）什么是重定向呢？假如有一个电商网站，每天有几千人访问。当然，这几千人里面大部分当时并没有买东西就离开了。而重定向就是说这些人到了别的网站，就在该网站的广告位上投放该电商的广告，这样一来，用户点击广告和进而下单购买的比例都会相当高。 如下是一个重定向的例子 重定向的分类 1.网站重定向。就是上面提到的方法2.搜索重定向。根据用户在搜索引擎上与广告主相关的搜索行为3.个性化重定向。对用户购买流程的追踪和推荐，即根据用户在广告主网站上关注的具体产品和购买阶段，推送商品粒度的广告。对于广告主而言，可以视为一个站外推荐引擎。 上面提到了推荐算法，这里对推荐算法做一个简单的介绍，推荐算法可以分为两大类：基于协同过滤的算法和基于内容算法 基于协同过滤的算法又可分为两种1）内存方法（非参数方法）：neighbor-based methods，User-based/Item-based top-N2）模型方法（参数方法）:matrix factorization, bayesian belief nets 基于内容的推荐算法指的是从 user 和 item 提取出相同的特征向量，或者说将两者映射到相同的向量空间中再比较两者的相似性。 推荐算法的本质是对 user-item 这一系数矩阵的参数化或非参数化的描述，而推荐算法选择的关键也是探索合适的 bias 与 variance 的平衡，以适应问题的数据的稀疏性。 重定向的典型代表公司是： Magnetic 和 Criteo 新客推荐（look-alike）上面的重定向针对的是广告主已经有的客户，但是对于中小电商，仅仅对老用户定向营销远远不够；而对于某些类型的广告商如银行，大多数用户无法通过重定向渠道捕获。这时候就产生了新客推荐（look-alike）的需求。 新客推荐指的是由广告商提供一部分种子用户，DSP 通过网络行为的相似性为其找到潜在用户。需要注意的是尽量利用非 demand 端的数据，避免在广告主之间倒卖用户。 广告流量交易方式目前为止介绍了多种广告流量的交易形式，可以说交易的方式发展趋势为：合约广告-&gt;竞价广告-&gt;实时竞价广告，对应着下图从右到左的顺序。而发展到竞价广告之后，更加强调了 supply 端和 demand 端的分工的专业化。 优先销售有两种模式1）CPT结算，即传统广告的销售方式，技术要求低2）GD：CPM 结算 + 人群定向 程序交易包括竞价广告（Ad network）和实时竞价广告（Adx），其对应的 supply 端和 demand 端负责的任务也不同，但是其目的均是如何将自身的资源或需求分发到多个点（网络）上从而获取最大的利益。 1）demad：network optimization + RTBS2）supply：portfolio selection + RTBS]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算广告笔记(5)--搜索广告与广告网络Demand技术]]></title>
      <url>%2F2017%2F05%2F10%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(5)--%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A%E4%B8%8E%E5%B9%BF%E5%91%8A%E7%BD%91%E7%BB%9CDemand%E6%8A%80%E6%9C%AF%2F</url>
      <content type="text"><![CDATA[本文是刘鹏老师的计算广告学中的一些笔记。本文是第五章: 搜索广告与广告网络Demand技术。主要介绍搜索广告中的几个典型问题以及广告网络中demand端需要用到的技术 搜索广告的特点搜索广告与显示广告不同的特点在于 用户定向标签 $f(u)$: 远远弱于上下文影响（query），一般可以忽略 Session内的短时用户搜索行为作用很重要 上下文定向标签 $f(c)$: 关键词 搜索广告是一种典型的位置竞价模式，如下是搜索广告中常见的三种位置 根据上图，可知搜索广告中的位置一般分为北，南，东三个广告区块，根据各位置的reference ctr决定，各位置在竞价系统中的位次 reference ctr 可以通过ε流量较准确测定出 搜索广告的典型问题搜索广告中需要考虑的几个典型问题如下 查询词扩展(Query Expansion) 用户相关的搜索广告决策 短时用户行为反馈 查询词扩展(Query Expansion)目的是 supply 端为了赚取更多的利润，同时拓展了广告主的竞价范围，常见的思路有以下三种 （1）基于推荐的方法：挖掘(session, query)矩阵找到相关query, 可类比(user, item)矩阵，这种方法利用的是搜索数据（2）基于语义的方法：用topic model或概念化的方法中找语义相关query，这种方法利用的是其他文档数据（3）基于收益的方法：根据实际eCPM统计得到变现能力最好的相关query，这种方法利用的是广告数据 用户相关的搜索广告决策首先需要明确结果个性化对于搜索广告作用有限，原因是上下文信息(c)太强, 个人兴趣基本可以忽略；同时搜索页上的结果需要保证主题上某种一致性 但是广告展示条数是可以深度个性化的，因为约一半的用户无法明确区分广告与搜索结果，在平均广告条数的约束下，可以对每个用户的广告条数进行个性化，以最大化营收。因此这又一个约束优化问题！ 另外可以根据同一 session 内的行为调整广告结果，如在第一页没点的广告是否要放到第二页。 短时用户行为反馈短时用户行为的定义如下，狭义来说是用户在一个session内的行为，广义来说是 用户在短时间(一般为一到两天)内的行为 通过短时用户行为反馈，可以实现：（1）短时受众定向: 根据短时行为为用户打上的标签（2）短时点击反馈: 根据短时广告交互计算的动态特征 而短时用户行为计算需要准实时(分钟级)对用户行为进行加工，不适合在Hadoop上进行可以利用流式计算(stream computing)平台, 如S4（雅虎开源的一个流式计算平台）, Storm等 流式计算平台前面提到了流式计算平台，下面以 storm 为例简单讲述 Storm 是一个大规模实时数据处理框架，能够自动完成数据分发和可靠性管理,开发者只需要关注处理逻辑，数据流基本在网络和内存进行（极端情况下会有磁盘调度） Storm 计算逻辑类似Map/Reduce, 区别在调度数据而非调度计算，其拓扑及任务分配如下（spout 是输入，根据输入的 key 分发到不同的 Bolt 上处理，最后将结果组成） 广告网络 demand 端技术广告购买平台 (Trading Desk) 是 demand 端的一种产品，其目的是 Allows advertisers buy audience across publishers and ad networks 其关键特征有 连接到不同媒体和广告网络，为广告商提供universal marketplace 非实时竞价campaign的ROI优化能力 经常由代理公司孵化出来 ROI优化能力ROI优化目标是给定总预算，在多广告网络中采买并优化ROI ROI 优化其中若干关键问题有 （1）在合适的流量segment上投放广告；如SEM中的选词、显示广告网络中的标签组合选择等 （2）在每个投放上合理地出价以优化ROI；与实时竞价不同，采买方无法控制每次展示的出价(因此一般采用每次点击固定价格的策略)，但是因为 $u, c$ 的取值未知，需要在各流量分割上估计其分布并合理出价 （3）对每个segment的量以及Market price进行预估，以完成整体的优化 在这个领域有代表性的公司有 EfficientFrontier，这个公司的核心业务是为搜索广告主提供大量关键词情形下的 ROI 优化服务，并收取固定比例的提成；广告主只需要提供预算、关键词、受众类型等信息即可，EfficientFrontier 会通过计算为其提供最优方案 其核心技术为 Portfolio Optimization，原是金融领域内的一个优化算法，目前正在向显示广告领域扩张，需要注意的是除了算法以外，长时间数据积累也很重要]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算广告笔记(4)--竞价广告系统]]></title>
      <url>%2F2017%2F05%2F07%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(4)--%E7%AB%9E%E4%BB%B7%E5%B9%BF%E5%91%8A%E7%B3%BB%E7%BB%9F%2F</url>
      <content type="text"><![CDATA[本文是刘鹏老师的计算广告学中的一些笔记。本文是第四章: 竞价广告系统。主要介绍竞价系统理论，广告网络的概念，以及点击率预测设计到的一些技术。 合约广告系统两个核心问题：在线分配和受众定向。 竞价广告系统是广告系统发展的里程碑。从搜索引擎的关键词 的竞价延伸到展示广告，标签精细化的必然发展。 不保量，但是保质，量由 demand 端的 agency 保证。 竞价系统理论竞价广告系统可以被描述成一个位置拍卖问题(Position auctions)，该问题描述如下 该问题要解决的是将广告对象 $ a = \lbrace 1, 2, … A \rbrace$ 排放到位置 $ s = \lbrace 1, 2, …, S \rbrace$，这里的位置主要针对搜索广告而言 假设对象 $a$ 的出价(bid)为 $b_a$ , 而其对位置 $s$ 的计价为 $u_{as} = v_ax_s ,(x_1&gt;x_2 &gt;…&gt;x_S)$， $v_a$ 视为点击价值，$x_s$ 视为点击率，该模型可近似描述广告系统竞价问题(对显示广告，可以认为S = 1，但是与搜索广告的不同点在于搜索广告可以一个都不出，而显示广告至少要出一个) 可以将问题描述成一个对称纳什均衡(Symmetric Nash equilibrium)其目标是 $(v_s – p_s)x_s &gt;= (v_s – p_t)x_t $, 其中 $p_t = b_{s+1}$,这里的 $p_s$ 表示根据广告的位置实际的收费情况，$b_{s+1}$ 在位置 $s+1$ 的广告的出价；寻找收入最大化且稳定的纳什均衡状态是竞价系统设计的关键 上面的问题建模比较复杂，需要较多的数学推导，这里主要关注上面问题建模求解以后得到的结论：就是定价机制该如何设置。这里讲的定价机制有两种：VCG(Vickrey–Clarke–Groves)机制和广义第二高价(Generalized second pricing)机制。 从理论上来讲，VCG 机制是最优的，VCG 认为某对象的收费应等于给他人带来的价值损害，而且其有一个很好的性质：整体市场是truth-telling的，也就是每个广告主只需要根据自己真实想法出价，避免广告主间的博弈。 但是实际的定价机制是广义第二高价，就是每个位置收取的费用是其下一个位置的出价加1（比下一个位置高即可）即$$p_s = b_{s+1} + 1$$这种做法看似会降低收益，但是实际上与VCG机制相比，会收取广告主更多的费用。关于原因，视频讲解时给了一个例子 假如说现在第一高价的广告是5块，第二高价的是3块。假如采用第一高价的机制，那么出了第一高价的广告主就会设法调低自己的价格，比如说调到3块1毛，因为这时候仍然可以拿到第一位且支出更少，而如果再来一个人要第一的广告位，那么他可能就会调到3块2；而采用第二高价的时候，出了第一高价的广告主不会设法调低自己的价格，因为收取的是第二高价的价格，而此时如果来了一个想要第一的广告位的广告主，那么他出的价格必须要在5元以上了 广义第二高价的整体市场不是truth-telling的，从上面的例子也可看到，商家之间会存在着博弈，但是由于其简单易行，为在线广告系统广泛采用，而VCG则用得较少 广告网络(Ad Network)维基百科对广告网络的定义如下： Connects advertisers to web sites that want to host advertisements 在竞价广告系统中，其主要的特征有： 竞价系统(Auction system) 淡化广告位概念：卖的是人群，而不是媒体，媒体已经变成了一个载体 最合适的计价方式为CPC（Cost Per Click）: 非实时竞价方式，广告网络估计CTR，广告主估计每个广告的价值 不足：不易支持定制化用户划分：广告主只能购买广告网络指定的关键词 广告网络的系统架构示意如下 该架构的工作原理大致如下： User 开始浏览某个页面时，Ad retrieval 根据 User 的 User Attributes 和页面的 Page Attributes 从 Ad Index 中找到相关的广告(如果有广告主将广告新加进来时，也要通过 Real-time indexing 将广告加入 Ad Index 中) 对相关广告根据 eCPM 进行排序，由于点击的 value 已经由广告主决定，这时只需要预估点击率即可。 进行排序后需要将排序的结果及其用户后续的点击行为记录到日志中，从而便于改进点击率预估模型，同时通过流式计算平台进行反作弊监测以及对广告主收费（如果用户有点击行为的话）。 下面要介绍以上广告网络中提到的一些技术， 广告检索(Ad Index)规模较大的时候才用。检索是一种搜索技术，这里主要介绍两个重点：布尔表达式检索和长Query情况下的相关性检索。 布尔表达式检索 将每篇文档看做一个布尔表达式，同时将每个查询也看做一个布尔表达式。 如下面是一个表示文档的布尔表达式 只有当文档满足查询的布尔表达式条件时，才能出这个广告，为了加快这个检索过程，布尔表达式检索通过建立两层索引来实现加速。其涉及到的一些概念和具体方法如下(摘自PPT） 长 Query 情况下的相关性检索指的是当查询的布尔表达式中的条件较多时，如果采用传统的搜索引擎的搜索方法，会导致计算量非常大，这里提供的思路是：在查找候选文档的过程中做一个近似的评估，切掉那些理论上不需要再考虑的文档，只对进候选的文档进行相关性计算，比Top-N最小堆最小值大的插入 采用的具体算法是WAND算法，其细节如下（摘自PPT） 一致性问题ZookeeperZookeeper 是在基于消息传递通信模型的分布式环境下解决一致性问题的基础服务 用层次式Namespace维护同步需要的状态空间 保证实现特性：Sequential Consistency, Atomicity, Single System Image, Reliability, Timeliness，较复杂的同步模式需要利用API编程实现。Zookeeper的实现利用了Paxos算法。 Paxos 算法概念目的是解决分布式环境下，怎么分布式地决策某些状态使得所有机器都处于一致性。 节点角色： P(roposer): (提出提案(n, value)), A(cceptor), L(earner) 三个约束： value只有在被提出后才能被批准 在一次算法的执行实例中，只批准一个value learners只能获得被批准的value 准备阶段 P选择某提案编号n并将 prepare请求发给A中的某多数派； A收到消息后，若n大于它已经回复的所有消息，则将自己上次接受的提案回复给P，并承诺不再回复小于n的提案； 批准阶段： 当P收到了多数A回复后，进入批准阶段。它要向回复请求的 A发送 accept 请求，包括编号 n 和根据约束决定的 value 在不违背向其他P的承诺的前提下，A收到请求后即接受。 点击率预测从广告检索出相关的广告后，需要根据 eCPM 对相关广告进行排序，由于出价由广告主决定，因此实际中需要估算的往往是广告的点击率。 基于统计的模型 $$ u(a,u,c) = p(click|a,u,c)$$ 在这个问题上，Regression比Ranking合适一些，因为广告的实际排序是根据eCPM，而eCPM由点击率和出价相乘决定，因此需要尽可能准确估计CTR，而不仅仅是各候选的CTR排序正确 冷启动问题：指的是新的广告非常多，这种情况下利用广告层级结构(creative, solution, campaign, advertiser)，以及广告标签对新广告点击率做估计 捕获点击率的动态特性，两种方案：动态特征: 快速调整特征在线学习: 快速调整模型 逻辑回归逻辑回归是工程中常用的点击率预估方法。 动态特征在线广告的三个维度 $(u,a,c)$ 上均有不同的特征，可以通过组合这些特征构造高纬特征 上面的组合特征均为静态特征，如果在这些组合的静态特征上加上这个特征的历史数据就变成了动态特征，和某个静态特征为“年龄为25~35且为男性”，如果这个特征的取值为在某段时间的下单量而不是单纯的0或1，那么这个特征就是一个动态特征。因此动态特征即在标签组合维度上聚合点击反馈统计作为CTR预测的特征。 优势 工程架构扩展性强，变 features 不变 model(与在线学习相比) 对新 $(a, u, c)$ 组合有较强back-off能力 缺点 在线特征的存储量大，更新要求高 组合维度举例: cookie(u) and creative(a) gender(u) and topic(c) location(u) and advertiser(a) Category(a) and category(u) cookie(u) creative(a) gender(u) 优化方法L-BFGS基于梯度的方法在工程上的收敛性不好，因为工程上的问题总是病态的。用二阶的方法，一般用 Quasi-Newton 方法。 BFGS (Broyden, Fletcher, Goldfarb, and Shanno) 是Quasi-Newton方法的一种， 思路为用函数值和特征的变化量来近似Hession矩阵，以保证正定性，并减少计算量。 BFGS方法Hession计算公式如下 (空间复杂度为 $O(n^2)$ )： L(imited memory)-BFGS 是为了解决 BFGS 的空间复杂度问题。将 nxn 的Hession阵用下图方式加以近似($B_k$ 为Hession近似) 这样的方法将空间复杂度降为 $O(nk)$, 在特征量大时比BFGS实用 可以非常容易地用 map/reduce 实现分布式求解，这种方法也适用于梯度法：mapper 求部分数据上的梯度，reducer 求和并更新参数 ADMMAlternating Direction Method of Multipliers 的形式 $$\min f(x)+g(z)\\s.t. Ax + Bz = c$$ Augmented Lagrangian及迭代解法如下 上面的迭代方法也可以用下面的迭代公式描述，其效果是一致的，但是下面的描述更加简便，而且在实际中也更常用 ADMM这种迭代的解法能够很容易地通过 MapReduce 模式迭代进行求解。 下面介绍逻辑回归的ADMM分布式解法，这种方法将将样本划分为 N 份，每个 mapper 负责一份，其描述的最优化问题如下 $$\min \sum_{i=1}^{N}\sigma (w_ix_i)+r(z)\\s.t. w - z = 0$$ 分布式的迭代解法入下: $$w_i^{k+1} \leftarrow \arg \min_{w_i} (\sigma(w_ix_i) + \frac{\rho}{2}||w_i - z^k + \mu_i^k||_2^2)\\z^{k+1} \leftarrow \arg \min_{z} (r(z) + \frac{N\rho}{2}||z^k - \overline w^{k+1}+ \overline \mu_k||_2^2)\\\mu_i^{k+1} \leftarrow \mu_i^k + w_i^{k+1} - z^{k+1}$$ 探索与利用(Explore and exploit， E&amp;E)这一问题主要是为长尾的 $(a, u, c)$ 组合创造合适的展示机会以积累统计量，从而更准确地估计其CTR 原因是真实的环境中，数据总是长尾的，总体集没法通过采样获得，实际上大批广告主的广告是没有机会展示，为了让更多的广告主的广告能够得到恰当的展示，需要做一些探索（即不选择当前出价最高的广告，而是选择一些符合要求的长尾广告），但是最终的目的仍是提升整体的广告收入，即需要严格控制探索的量和有效性 基本方法思路 通常描述为 Multi-arm Bandit (MAB) 问题: 有限个 arms(或称收益提供者) $a$, 每个有确定有限的期望收益 $E(r_{t,a})$ 在每个时刻t,我们必须从arms中选择一个,最终目标是优化整体收益基本方法为 $\epsilon$–greedy: 将 $\epsilon$ 比例的小部分流量用于随机探索 上面的方法应用在广告中的主要挑战有： 海量的组合空间需要被探索 各个arm的期望收益是动态变化的 因此提出了以下两个思路， UCB 和 Contextual Bandit UCB在时刻t，通过以往的观测值以及某种概率模型,计算每个arm的期望收益的upper confidence bound (UCB)，并选择UCB最大的arm。 实际上是将每个arm的收入看作一个分布，选择所有分布中可能达到最大的那个arm作为最终的选择。 我们不可能一直选择非最优的arm, 原因是我们选择的此arm次数越多, 其UCB就越接近于其期望收益 具体UCB策略有以下两种： β-UCB策略: 依一个很大的概率, 我们选择非最优arms的次数存在着一个上界, 该上界与总的选择次数无关 UCB-tuned策略: 我们已选择的次数越多, 就越可以自信地抛弃不太有前途(但仍有可能最优)的arm. Contextual Bandit 解决 arm 数目过多问题，降维，映射到另外的特征空间 对每次展示，每个 arm (广告) $a$ 有一个对应的特征矢量 $x(u,a)$ 用此特征矢量代替 arm 本身进行 Bandit 决策]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算广告笔记(3)--受众定向]]></title>
      <url>%2F2017%2F04%2F28%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(3)--%E5%8F%97%E4%BC%97%E5%AE%9A%E5%90%91%2F</url>
      <content type="text"><![CDATA[这一讲主要介绍了受众定向的概念和若干种定向的方法。 受众定向概念受众定向是目前广告系统的核心部分，要做的就是根据人群划分进行广告售卖和优化。 受众定向可以认为是为 AUC（Ad，User，Context）打标签的过程，上下文标签可以认为是即时受众标签，如下所示是AUC上的标签类别 标签的两大主要作用 建立面向广告主的流量售卖体系，注意这些特征需要有具体的意义才能展现给广告主，一般通过事先定义而不是通过文本聚类方法获取 为各估计模块(如CTR预测)提供原始特征 下图的定向方式从右到左效果逐渐变好，从上到下表示不同阶段的定向方式； 上图中的 $f(u)$, $f(c)$, $f(a,u)$ 含义如下： $f(u)$：用户标签，即根据用户历史行为给用户打上的标签$f(c)$：上下文标签，即根据用户当前的访问行为得到的即时标签$f(a,u)$：定制化标签，也是一种用户标签，但是是针对某一广告主而言 除了上面提到的三种标签还有一中标签 $f(a)$ 表示广告标签，便于与上下文标签或用户标签做匹配。广告标签常常有两种选择1）直接将广告投放中的广告主、广告计划、广告组、关键词等直接作为标签2）用人工的方式归类 除此之外，各种定向方式中的数字表示该方式有利于某个个阶段需要遵循的原则，各个阶段及其需要遵循原则如下所示 各种定向方式含义如下所示：重定向：如果用户曾经访问过广告主的网站，就会给用户推该广告主的广告。（品牌广告较为常用）上下文：用户目前浏览的网页内容相关的广告行为：根据用户的历史行为进行推荐网站/频道：根据网站的属性投放与该网站内容相关的某一领域的广告（如汽车广告）Hyper-local：将定位做得更细（如定位到某条街道的小饭馆，一般在移动广告较容易实现）Look-alike：一般来说重定向的量比较少，该方式可以找到与广告主提供的种子用户相似的用户进行广告投放人口属性：人口，性别，教育水平，收入水平等，主要给广告商看，效果不是很好地域：主要给广告商看，效果不是特别好 下面介绍一下audience targeting 在业界的一种商业模式 Audience Science 是一个做audience targeting的第三方发公司，其核心业务有两个： 主要提供面向publisher（如NewYork Times）的数据加工服务，从publisher提供的数据中提取出用户标签供 publisher 使用 直接运营ad network，并帮助广告主进行campaign管理和优化；该过程中会通过上面提取出来的用户标签优化效果，同时使用标签创造的营收按照一定比例跟publisher分成 行为定向根据用户的历史行为给用户打上标签(上面提到的$f(u)$)，下面是九种重要原始行为(按信息强度排序，往往强度越大，量越少) Transaction, 就是用户购买行为 Pre-transaction, 用户购买前的一些行为，如商品浏览、加入购物车等 Paid search click, 搜索广告中的点击行为 Ad click, 广告的点击行为 Search click, 搜索引擎上的点击行为 Search, 搜索引擎上的搜索行为 Share, 分享行为，如微博等 Page View, 浏览页面的行为，注意这个页面不是上面搜索出来的页面，而是用户在某个站点中只能看到的页面（如贴吧），量大但是效果一般 Ad view，看某个广告的次数，但是带来的效果往往是负面的 行为定向计算的一种方式如下 ($t^{(i)}(u)$表示用户 $u$ 在标签 $i$ 上的强度) 上面的方式比较简单，但是在海量数据中首先要做的是shallow的挖掘。 行为定向的还有其他一些问题：如 session log 和 long-term 行为定向 Session log session log 关系到怎么从日志文件中提取出所需数据，有以下两点建议 将各种行为日志整理成以用户ID为key的形式，完成作弊和无效行为标注，作为各数据处理模块的输入源 可以将targeting变成局部计算，大大方便整个流程 Long-term 行为定向 Long-term 行为定向指的是如何从用户的长期行为中提取出用户的标签，常用的有两种方法：滑动窗口方式和时间衰减方式。 滑动窗口方式：直接将前面 $T$ 天的行为标签进行相加。下面的公式中 $f$ 为 long-term 标签，下标为日期 $$f_d^{(i)}(u) = \sum_{j=0}^T t_{d-j}^{(i)}(u)$$ 时间衰减方式：按照时间衰减方式，对前n天的标签进行相加，时间越长，权重越小。这种方式空间复杂度低，仅需昨天的 $f$ 和今天的 $t$ $$f_d^{(i)}(u) = t_{d}^{(i)}(u) + \alpha f_{d-1}^{(i)}(u)$$ 实际中上面两种方法的效果差异不大，但是时间衰减方式的计算代价较小。$T$ 的选择与实际商品相关，如汽车的 $T$ 往往较大，而运动鞋的 $T$ 往往不大，目前这个值主要是根据经验来设。 如何评判为用户打的标签？ 受众定向评测可以借助Reach/CTR曲线，该曲线如下所示，横轴表示reach到的用户，纵轴表示广告点击率 上图从左到右阈值设置逐渐变小，设为0时，可以reach到所有人，但是这个时候就相当于无定向投放了 注意拐点以及该曲线是否服从一个递减的趋势；如果不是递减，可能标签没意义，而拐点可以知道设置在哪一个值能够将有更强的购买倾向的人群与其他人群分开。 上下文定向上下文定向指的是根据用户的访问内容给用户打标签 (上面提到的 $f(c)$)，这样的定向中有一些根据广告请求中的参数经过简单运算就可以得到，如地域定向，频道/URL 定向、操作系统定向等。另外一类则是根据上下文页面的一些特征标签，如关键词、主题、分类等进行定向，下面重点讨论这种上下文定向方式。 上下文定向（打标签）主要有以下几种思路：1）用规则将页面归类到一些频道或主题分类，如将 auto.sohu.com 归类到“汽车”的分类中2）提取页面的关键词3）提取页面的入链锚文本中的关键词，这需要一个全网的爬虫作支持4）提取网页流量来源中的搜索关键词，这种方法除了页面内容，也需要页面访问的日志数据作支持5）用主题模型将页面内容映射到语义空间的一组主题上 半在线抓取系统确定了对上下文页面打标签的方法后，在在线广告投放时，页面标签系统需要某个查询 url 返回其对应的标签。在广告系统中，可以通过半在线(Near-line)上下文定向系统实现这个事情，如下就是一个 Near-line 上下文定向系统 该系统用一个缓存(Redis)来保存每个 URL 对应的标签，当在线广告请求到达的时候，执行如下操作 如果请求的上下文 URL 在缓存中存在，直接返回其对应的标签 如果 URL 在缓存中不存在，为了广告请求能够及时得到处理，立刻返回空的标签集合，同时向后台的抓取队列中加入此 URL 进行抓取和存储 考虑到页面的内容会不定期更新，可以设置缓存合适的 TTL 以自动更新 URL 对应的标签 步骤 2 中能够返回空标签的原因是在广告系统中，某一次展示时标签的缺失带来的影响是可以忍受的，因为对于广告系统而言，这部分只是起到一个锦上添花的作用。 主题模型除了直接提取页面内容的关键词作为页面的特征以外，还可以通过主题模型（Topic models）这一类模型对文本进行聚类得到文本的主题分布情况。 主题模型分两大类：有监督和无监督的。有监督指的是预先定义好主题的集合，用监督学习的方法将文档映射到这一集合的元素上；无监督指的是不预先定义主题集合，而是控制主题的总个数或聚类程度，用非监督的方法自动学习出主题集合以及文档到这些主题的映射函数。 广告中的主题挖掘有两种用途 用于广告效果优化的特征提取 用于售卖给广告主的标签体系 对于第一种用途, 用有监督或非监督的方法都可以；对于第二种用途，应该优先考虑采用监督学习的方法，因为这样可以预先定义好对广告主有意义而且可解释的标签体系。下面先介绍非监督方法，再介绍监督方法。 非监督方法 非监督的主题模型的发展经历了 LSA -&gt; PLSA -&gt; LDA 的过程，下面简单介绍这三种模型。 LSA(Latent Semantic Analysis) 有时也叫 LSI(Latent Semantic Indexing)，这种方法实际上是将 SVD 分解应用到了 “文本-单词” 矩阵中，即 $$X \approx U \Sigma V^T$$ $X$ 矩阵中的值有多种选择：0-1，出现次数，TF-IDF值。则 $U$ 矩阵的一行对应的就是一篇文本在各个主题上的分布， $V$ 矩阵每行对应的就是一个单词在各个主题上的分布，而选择奇异值的个数则决定了隐含主题的个数，也就是代表文本或词语的向量的维度。通过比较向量间的余弦相似性，便可比较文本或单词间的相似性。 这样的方法虽然直观，但是有几个问题，一是分解后矩阵 $U$、$V$ 中可能存在着负值，二是这些数值在概率上没有意义。为了解决这些问题，便提出了PLSA。 PLSA(Probabilistic Latent Semantic Analysis)可以说是概率化了的LSA，但是采用的方法与 LSA 完全不同，PLSA 没有涉及到 SVD，而是采用混合模型的做法。 PLSA 方法假设文本包含多个主题，这些主题服从多项式分布，而每个主题下的有多个词，这些词也服从多项式分布。假设有 M 篇文档，每篇文档有 N 个词，则生成这 M 篇文档的过程通过有向图模型表示如下 在有向图模型中，灰色的点表示能够观察到的点，其他白色的点表示需要求解的点，而最后需要求解的点是没有入度的点，其他的有入度和出度的点会被积分积掉，框及其框内符号表示框里面的内容重复若干次 而上图中 $d$ 表示文本，$c$ 表示主题， $w$ 表示词语，且文本 $d$ 中生成词语 $w_i$ 的概率是 $$P(w_i|d) = \sum_c P(c|d) P(w_i|c)$$ 其中 $P(c|d)、 P(w|c)$均服从多项式分布 则整篇文本生成的概率为 $$P(d) = \prod_i P(w_i|d) $$ 这个模型跟混合高斯模型（mixture of Gaussian）非常相似，都是融合了多个指数族分布的模型，这一类模型也可以称为混合模型，而求解这一类的问题的方法便是 EM 算法，这里不详细展开。除此之外，假如将上面 PLSA 中文本主题服从的多项式分布改为伽马分布，将主题下的词语服从的多项式分布改为泊松分布，那么 PLSA 就变为了GaP(Gamma-Poisson)模型。 LDA(Latent Dirichlet Allocation) 则是在 PLSA 的基础上为其两个多项式分布加上了贝叶斯先验, 先验选为 Dirichelet 分布，原因是更多是数学上的便利性，因为Dirichelet 是 multinational 的共轭先验，容易求解。 上图中 $\alpha$, $\beta$ 表示参数为 $\alpha$, $\beta$ 的狄利赫里分布，这两个分布分别是文本的主题概率分布和主题下词语的概率分布的先验分布。文本的主题概率分布的先验分布为 $$ \theta_i \sim Dir(\alpha),~i=1, 2…M $$ $\theta_i$ 表示第 $i$ 篇文档的主题分布，而 $\theta_{i,k}（k=1,2…K）$ 表示第 $i$ 篇文档中包含第 $k$ 个主题的概率 而主题下的词语的概率分布的先验分布为 $$ \phi _k \sim Dir(\beta),~k=1,2…K $$ $\phi_k$ 表示第 $k$ 个主题下的词语分布，而 $\phi_{k,j}（j=1,2…V）$ 表示第 $k$ 个主题中包含第 $j$ 个词的概率，$V$ 为语料库的词表的大小 确认先验分布后，文档中的主题分布以及主题下的词语分布均服从多项式分布，与 PLSA 相同，求解 LDA 得思路是先求解出其最终的联合概率分布，然后通过 Gibbs Sampling 收敛到该概率。 经验贝叶斯(Empirical Bayes)如下图模型, 如何确定hyperparameter $\eta$? 用 Empirical Bayes 估计的解为：$$\widehat \eta = arg \max_{\eta} \int \prod_{k=1}^{K}p(D_k|\theta_k)p(\theta_k|\eta)d\theta_k$$ 当 $p(x|\theta)$ 为指数族分布，$p(\theta|\eta)$ 为其共轭先验时，可用EM求解, 其中E-step为Bayesian inference过程, 由 $\eta^{old}$ 得到后验参数 $\widetilde \eta_k^{old}$ , 而M-step为:$$(\theta, ln[g(\theta)])_{\eta^{new}} = \frac{1}{K}\sum_{k=1}^{K}(\theta, ln(g(\theta)))_{\widetilde \eta_k^{old}}$$ 从经验贝叶斯看LDALDA可以视为PLSI的经验贝叶斯版本，由于PLSI不是指数族分布，而是其混合分布，因此其贝叶斯版本不能使用前面的EM算法。工程上常用的求解方法有两种：Deterministic inference 和 Probabilistic inference Deterministic inference： 可用变分近似，假设z和θ的后验分布独立迭代求解过程与EM非常相似，称为VBEM，但是在大多数问题上无法保证收敛到局部最优 Probabilistic inference: 可用Gibbs-sampling(Markov-chain Monte-Carlo, MCMC 的一种)，以概率1收敛到局部最优值；还有一种方法是 Collapsed Gibbs-sampling Topic model的并行化 EM及VBEM的并行化较为简单 E-step(mapper): 可以方便地并行计算 M-step(reducer): 累加E-step各部分统计量后更新模型 将更新后的模型分发到新的E-step各个计算服务器上 AD-LDA: Gibbs Sampling的并行化 Mapper: 在部分data上分别进行Gibbs sampling Reducer: 全局Update $$n_{i,j} \leftarrow n_{i,j}+\sum_p(n_{i,j,p} - n_{i,j}),~n_{i,j,p}\leftarrow n_{i,j}$$ 文档的Topic model抽取可以认为是一个大量(而非海量)数据运算，采用类MPI架构的分布式计算架构(例如spark)会比 MapReduce 效率更高 虽然LDA能够聚类，但是supervisord learning 对标签体系更有意义。 数据加工和交易精准的广告业务是什么？下面的图将数据的加工过程类比于石油的加工和提炼的过程，在这个过程中，实际上与媒体的关系已经不大了。 精准广告业务的若干值得探讨的观点 越精准的广告，给市场带来的价值越大 媒体利益与广告主利益是相博弈的关系 精准投放加上大数据可以显著提高营收 人群覆盖率较低的数据来源是不需要的（长尾？） 不同的广告产品应该采用不同的投放机 有价值的数据下面列出了一些在广告系统中有价值的数据 用户标识 除上下文和地域外各种定向的基础，需要长期积累和不断建设 可以通过多家第三方ID绑定不断优化 用户行为 业界公认有效行为数据（按有效性排序） 交易，预交易，搜索广告点击，广告点击，搜索，搜索点击，网页浏览，分享，广告浏览 需去除网络热点话题带来的偏差 越靠近demand的行为对转化越有贡献 越主动的行为越有效 广告商(Demand)数据 简单的cookie植入可以用于retargeting。 对接广告商种子人群可以做look-alike，提高覆盖率。 用户属性和精确地理位置 非媒体广告网络很难获取，需通过第三方数据对接。 移动互联和HTML5为获得地理位置提供了便利性。 社交网络 朋友关系为用户兴趣和属性的平滑提供了机会 实名社交网络的人口属性信息相对准确 数据管理平台(Data Management Platform) 目的: 为网站提供数据加工和对外交易能力(如Audience Science) 加工跨媒体用户标签，在交易市场中售卖 是否应直接从事广告交易存在争议 关键特征: 定制化用户划分 统一的对外数据接口：demand端提供给supply端 代表: Bluekai, AudienceScience DMP的系统架构示意图如下所示 DMP主要是Data highway部分，主要完成两个工作： 挖掘出各个用户的标签 利用挖掘出来的用户的标签，售卖或使用 这里介绍一个Data Highway 的工具：Scribe 大规模分布式日志收集系统，可以准实时收集大量日志到 HDFS，利用Thrift实现底层服务 类似工具: Flume, Chukwa 这个工具在 Facebook 经过实践，验证了其面对大规模数据时的可靠性。 下面介绍一下audience targeting 在业界的一种商业模式上面提到了Bluekai这个公司，其核心业务主要有以下两个： 为中小网站主提供数据加工和变现的方式 通过汇聚众多中小网站用户资料和行为数据，加工成受众定向标签，通过Data exchange对外售卖 Bluekai 提供大量细分类别、开放体系上的标签，如“对宝洁洗发水感兴趣的人”，“想去日本旅游的人”；靠数据出售变现，并与提供数据的网站主分成，但是并不直接运营广告业务；对于设计用户隐私的问题，用户可以看到自己的资料被谁使用，也可以选择“捐给慈善机构”]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算广告笔记(2)--合约广告系统]]></title>
      <url>%2F2017%2F04%2F25%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(2)--%E5%90%88%E7%BA%A6%E5%B9%BF%E5%91%8A%E7%B3%BB%E7%BB%9F%2F</url>
      <content type="text"><![CDATA[本系列文章是刘鹏老师的计算广告学中的一些记录。本文是第二章:合约广告系统，主要介绍了广告系统中一些常用的开源工具，合约广告系统的概念以及在线分配问题的建模与求解。 常用广告系统开源工具广告系统中常用的工具有以下这些 总体可分为两类工具：离线与在线 离线 HBASE（列存储的NOSQL数据库，类似的有BigTable，HyperTable，Cassandra） Pig: 一种脚本语言 Elephant-bird：将二进制文件转为 pig 可处理的文本文件 Hive: 一种脚本语言 mahout: 分布式机器学习 在线 ZooKeeper/Chubby: 分布式环境下解决一致性问题 Avro/Thrift: 分布式环境下跨语言的通信问题 S4/Storm: 流式计算平台 Chuhwa/Scribe/Flume: data highway, 分布式日志收集工具，并送到其他平台 合约广告系统合约广告有两个重要组成部分：广告位合约和展示量合约。 广告位合约：实际上是将线下的模式搬到了线上，指的是广告主与媒体约定在某一时间段内，在某些广告位上固定投放该广告主的广告，相应的结算方式是 CPT(Cost Per Time)（也就是按照展示时间结算），这种模式下售卖给广告主的是广告位。 展示量合约：展示量合约广告指的是约定某种受众条件下的展示量，然后按照事先约定好的单位展示量价格来结算，这种结算方式是 CPM(Cost Per Mille)（也就是每一千次展示的付费）。这种方式也叫担保式投放(Guarantee Delivery, GD)，意思就是先广告主担保其提出的广告展示量会被满足。这种模式下售卖给广告主的是广告位+人群。 广告位合约广告位合约对供给方和需求方的技术的要求都不高。 供给方即媒体往往会使用一种在合同确定以后自动的执行合同的广告管理工具：广告排期系统。广告排期系统能够帮助媒体自动执行多个合同的排期，可以将广告素材直接插入页面，且对于图片等静态资源，会放到 CDN 上进行加速。 需求方即广告主往往会通过代理商（agency）进行媒介采买（也就是广告位采买），代理商帮助广告主策划和执行排期，而对于广告的质和量，是根据代理公司人员对媒体广告位的历史经验以及对广告主业务的了解通过人工优化的方式来满足。这样的代理公司的代表有我们前面一讲提到的 4A 公司。 展示量合约媒体从前面的广告位售卖变为按 CPM 的售卖，初衷是为了在受众定向的基础上提高单位流量的变现能力，可是面向的任然是原来的品牌广告主。广告主按广告位采买时，比较容易预估到自己能够拿到的流量，可是按照人群定向的方式采买，流量有诸多不确定的因素，因此，需求方希望在合约中加入对量的保证，才能放心购买。而假如约定的量未完成，则需要向广告商补偿。 因此，这种方式卖的不仅仅是广告位，而是人群。 后面半句我们很容易理解，而前面半句话的指的是在 CPM 这种结算方式下，任然没有摆脱广告位这一标的物，原因是无法将多个差别很大的广告位打包成统一售卖标的（这样的话每个广告主都会抢着要那些曝光率更高的广告位）；因此实际中的展示量合约往往是以一些曝光量很大的广告位为基础，再切分人群售卖，最典型的例子就是视频网站的铁片位置或者门户网站首页的广告位。 展示量合约这种模式的出现实际上已经反映了互联网广告计算驱动的本质：分析得到用户和上下文的属，并且由服务器端根据这些属性以及广告库情况动态决定广告候选。这一商业模式的出现，需要有一系列技术手段的支持，这些技术手段主要包括受众定向、流量预测、在线分配等。下面主要介绍流量预测和在线分配，受众定向会在下一讲中详细讲解。 流量预测流量预测(traffic forecasting) 简单来说就是预测某个标签的人群访问某个站点的量。流量预测其目的有多种，典型的有售前指导，在线流量分配，出价指导，前面两个是合约广告中的内容，而后面一个是竞价广告中的内容。 1）售前指导指的是在展示合约广告系统中，由于要约定曝光总数，事先尽可能准确地预测个人群标签的流量非常重要。因为如果流量低估，会出现资源售卖量不足的情形；而如果流量严重高估，则会出现一部分合约不能达成的状况。 2）在线流量分配指的是在展示量合约广告系统中，由于合约之间在人群的选择上会有很多的交集，因此一次的曝光往往会满足多个合约的要求，这时候就需要在多个合约之间进行分配，目的是达到整体满足所有合约的目的。这也是下面要详细探讨的在线流量分配的问题。 3）出价指导是竞价广告中的内容，在竞价广告中，没有了量的保证，广告主往往需要根据自己预计的出价先了解一下可能获得多少流量，以判断自己的出价是否合理。与前面在合约广告中的应用不同，这里还多了出价这一因素。 综上，广告里一般的流量预测问题，可以描述为对流量 $t(u,b)$ 这个函数的估计，其中 $u$ 是给定的人群标签或这些标签的组合，而 $b$ 是具体的出价。在展示量合约中，由于没有竞价，可以看成是 $b \rightarrow \infty$ 的特例。 在线分配在合约广告系统中主要讨论在线分配（Online Allocation）问题，在线分配问题指的是在通过对每一次广告展示进行实时在线决策，从而达到在满足某些量的约束的前提下，优化广告产品整体收益的过程。 在线分配是广告中比较关键的算法框架之一，适用于许多量约束下的效果优化问题，而这实际上是广告业务非常本质的需求。 问题建模在线匹配可看作是一个二部图匹配问题，二部指的是代表广告库存的供给节点（集合记为 $I$）和代表广告合约的需求节点（集合记为 $A$）。如下图所示，上边为集合 $A$, 下边为集合 $I$, 如果供给节点的受众标签能够满足某个需求节点的要求时，就在相应的两个节点之间建立一条连接边，所有边的连接记为集合 $E$。 在线分配技术并不仅仅适用于展示量合约中的担保投放（GD）问题，还适用于 AdWords 问题，展示广告问题等。下面主要介绍 GD 问题和 AdWords 问题。 GD 问题 前面已经提到了GD的概念，在这里如果不考虑合约 $a$ 未完成的惩罚，收益一定是常数。那么 GD 的优化问题可以表示为 $$\begin{align*}&amp;\max \quad C\\&amp;\begin{array}\\s.t. &amp;\sum_{a \in \Gamma(i)} x_{ia} \le 1 &amp;\forall i \in I\\&amp;\sum_{i \in \Gamma(a)} s_i x_{ia} \ge d_a &amp;\forall a \in A\\&amp;x_{ia} \ge 0 &amp;\forall (i,a) \in E\end{array}\end{align*}$$ 上面各式的符号含义如下 $C$ 是一个常数，指的是总收益$I$、$A$、$E$ 上面已经提到，分别表示供给点集合，需求点集合，边的集合$\Gamma(i)$ 表示与供给节点 $i$ 连接的所有需求节点的集合$x_{i,a}$ 表示供给节点 $i$ 分配给需求节点 $a$ 的流量的比例$\Gamma(a)$ 表示所有与需求节点 $a$ 连接的供给节点 $i$ 的集合$s_i$ 表示供给节点 $i$ 的总流量$d_a$ 表示需求节点 $a$ 的展示量需求 Adwords 问题 AdWorks 问题，也被称为有预算约束的出价问题，是竞价广告领域内的问题。简单来说，这个问题讨论的是在按照 CPC 方式结算的广告环境下，给定广告主的预算，整体化市场营收问题。需要注意的是，竞价广告中已经没有了量的约束，广告主给的约束是其预算费用。因此可以将这个问题表示为如下的优化问题 $$\begin{align*}&amp;\max_{(i,a) \in E} \quad q_{ia} s_i x_{ia}\\&amp;\begin{array}\\s.t. &amp;\sum_{a \in \Gamma(i)} x_{ia} \le 1 &amp; \forall i \in I\\&amp;\sum_{i \in \Gamma(a)} q_{ia}s_i x_{ia} \le d_a &amp; \forall a \in A\\&amp;x_{i,a} \ge 0 &amp;\forall (i,a) \in E\end{array}\end{align*}$$ 上式大部分的符号跟 GD 问题相同，不同的地方主要是以下两个符号$q_{ia}$ 表示需求节点(广告主) $a$ 对供给节点（某个人群标签） $i$ 的出价$d_a$ 表示则表示广告主 $a$ 的总预算 问题求解上面的两个最优化问题均是线性规划问题，未知量是 $s_i$（供给节点 $i$ 的流量） 和 $x_{ia}$（供给节点 $i$ 分配给需求节点 $a$ 的流量的比例）。但是对于 $s_i$ ，常常利用历史流量去估计它的值，因此上面的优化问题变成了仅仅需要求解 $x_{ia}$ 的问题。下面解决这个问题的几种思路 直接求解对于这类线性规划问题，可以通过内点法或单纯形法直接进行求解，但是在大型的广告合约系统中，供给节点和需求节点的数目都很大，因此边 $|E|$ 的数目也会非常大（百万级以上),这样会使得对应的分配问题变得过于复杂而无法直接有效求解。令 $n$ 为变量的个数，则内点的时间复杂度为 $n$ 的多项式级别，单纯形法的时间复杂度为 $O(n^2)$, 这样直接求解的解参数正比于 $|E|$ 的数量，规模有可能过于庞大，无法进行实时的在线分配。因此有必要探索更新效率更高的的在线分配方案。 对偶求解通过拉格朗日对偶可将原问题装化为对偶问题，但是对偶问题的变量数目仍然正比于约束的数目（供给约束和需求约束），前者的变量的量级为十万甚至百万千万，但后者的量级在数千级别。 为了减少所需求解的变量，这篇文献 Optimal Online Assignment with Forecasts 提出了一个方法：只保留需求约束对应的对偶变量，然后通过数学变换恢复出供给约束对应的对偶变量和分配率。具体的算法过程可参考上面提到的文献。 上面的方法在求解对偶问题时代价仍然比较高，因此在文献 SHALE: an efficient algorithm for allocation of guaranteed display advertising 提出了 SHALE 算法，优化了求解对偶变量的步骤，采用了原始对偶方法迭代进行求解，求解出对偶变量后，通过数学变换恢复出供给约束对应的对偶变量和分配率跟上面的方法一致。 启发式分配方案 HWM上面根据历史流量数据来求解的分配方案原理上可行，但是在实际的工程应用中仍然显得有些复杂，比如离线仍然要消耗大量的时间求解对偶解。因此，人们希望实现一种快速算法，保持前述方法的紧凑分配的特性，效果上也能够近似最优。前述方法中通过合同节点的对偶变量即可恢复最优解，受其讨论启发，可以发现，只要大体确定好每个合同在分配中的相对优先级以及分配时得到某次展示的概率，就可以构造出一种直觉上可行的在线分配方案。 文献 Ad serving using a compact allocation plan 提出的 HWM（High Water MArk） 算法便是这样一种方案，虽然在数学上并不完全严谨，但是由于根据历史数据来指定的分配方案本身就具有相当程度的近似，因此其实际效果也不错，而工程上的便利性则是这个算法的一大优点。 HWM 分配算法有两个关键点：1）根据历史流量确定每个广告合约资源的稀缺程度，通过可满足各合约的供给节点总流量的升序排列进而得到分配优先级2）根据优先级确定每个广告合约的分配比例 具体过程可参考文献内容 合约广告系统主要模块在前面提到的广告提供架构图中，合约广告系统主要表示为以下模块 上面是竞价广告系统中截取出来的，但是截取出来的部分在概念上与合约广告系统相差不大；主要由以下几个部分构成 Ad retrieval：搜索页面内容相关的广告Ad Ranking：计算 CTR，根据CTR排序Yield management（Allocation）：广告分配的问题流量预测模块：跟 Allocation 打交道，在上图中没有画出来Billing 和 Anti-spam：实时计算部分，用于计价和防作弊，任何广告系统都有 Hadoop 介绍上面简单介绍了一些计算广告系统中常用的开源工具，下面主要介绍 Hadoop 这个使用最为广告的分布式系统。 Hadoop 源于Lucene项目一部分, 2006年成为子项目, 后来成为Apache顶级项目 Hadoop最为重要的两个部件： HDFS：一个高可靠性, 高效率的分布式文件系统 MapReduce: 一个海量数据处理的编程框架 HDFSHDFS的架构如下： HDFS中主要有 Namenode 和 Datanodes 两种角色，其中 Namenode 存储的的是 metadata，其包含的信息是组成文件的各个block存储在哪个 Datanode 上，而 DataNodes 是真正存储文件数据的地方，并且为了达到高可用的效果，文件的一个block会以多个replication的方式存在多个Datanode上。 当 client 访问 HDFS 上的文件时，会先访问 NameNode，得到 文件所在的 DataNode 的后再去对文件进行具体操作。 MapReduceMapReduce 是一个分布式计算框架，整个过程包括一个Map过程和一个Reduce过程。相比于MPI,Map 过程处理之间的独立性使得整个系统的可靠性大为提高；并且分布式操作和容错机制由系统实现, 应用级编程非常简单。 MapReduce的计算流程非常类似于简单的Unix pipe 12Pipe: cat input | grep | sort | uniq -c &gt; outputM/R: Input | map | shuffle &amp; sort | reduce | output MapReduce中进程间通信的时间只能是在map和reduce间的 shuffle &amp; sort 过程，该过程主要是将经过map操作后 key 相同的那些记录聚合到一起，已进行后面的reduce操作。其过程如下图示： MapReduce 与 分布式机器学习在分布式机器学习中常用的统计模型有两种：指数族分布和指数族混合分布 指数族分布(Exponential Family)是条件概率服从某种形式的，常见的很多分布（如高斯分布，泊松分布，多项式分布等）通过变换都能够转化为这种形式，也就是常见的分布很多都是指数族分布。 这种分布的一个好处是通过最大似然(Maximum likelihood, ML)估计可以通过充分统计量(sufficient statistics)链接到数据，或者说得到数据的规律。这里的充分统计量(sufficient statistics)是指数族分布中的一个组成部分，一般来说就是模型的参数（如高斯分布的均值和方差）。 而当单个的指数族分布无法刻画数据的分布的时候，就要考虑多个指数族分布混合在一起的分布，也就是指数族混合分布；常见的指数族混合分布有Mixture of Gaussians, Hidden Markov Models, Probabilistic Latent Semantic Analysis (PLSI)等。 在指数族混合分布中， ML估计一般通过EM算法迭代得到. 每个迭代中, 我们使用上一个迭代的统计量更新模型。 而一般的 ML 和 EM 算法能够通过 MapReduce 过程较好地刻画 对于 ML 过程只需要一个 mapper 和 一个 reducer 即可，上面从 reducer 经过 model 返回到 mapper 的过程代表 EM 的迭代过程。 这样刻画的好处使得在 mapper 中仅仅生成比较紧凑的统计量, 其大小正比于模型参数量, 与数据量无关；同时这样的流程可以抽象出来, 而具体的模型算法只需要关注统计量计算和更新两个函数。 但是对于需要迭代的算法，MapReduce 需要与 HDFS 进行多次交互从而导致性能不佳，而这也是 Spark 等框架致力于解决的问题之一。 MapReduce 的多种实现方式MapReduce提供了多样的编程接口，除了上面介绍的直接通过 Java 写MapReduce程序外；通过 Streaming 可以利用标准输入输出模拟以上 pipeline；而通过Pig只需关注数据逻辑，无须考虑M/R实现 Hadoop 的 Streaming 模拟 Pipe 方式执行Map/Reduce Job, 并利用标准输入/输出调度数据；开发者可以使用任何编程语言实现map和reduce过程, 只需要从标准输入读入数据, 并将处理结果打印到标准输出. 其限制是只支持文本格式数据, 数据缺省配置为每行为一个Record, Key和value之间用\t分隔,如生成大量文本上的字典可通过下面的linux命令模拟map操作和reduce操作 12map： awk '&#123;for (i=1; i &lt;=NF; i ++)&#123;print $i&#125;&#125;'reduce: uniq Pig 通过类SQL操作在Hadoop上进行数据处理，如下是一段 pig 代码实例：1234567891011Users = load ‘users’ as (name, age); Fltrd = filter Users by age &gt;= 18 and age &lt;= 25; Pages = load ‘pages’ as (user, url); Jnd = join Fltrd by name, Pages by user; Grpd = group Jnd by url; Smmd = foreach Grpd generate group, COUNT(Jnd) as clicks; Srtd = order Smmd by clicks desc; Top5 = limit Srtd 5; store Top5 into ‘top5sites’; Pig 解释器会进行整体规划以减少总的map/reduce次数，而如果需要通过 Java 来写 MapReduce 程序的话，会非常冗长，如下所示是实现相同功能的 MR 代码 pig 常用的一些语句如下所示 Hadoop上还有一个工作流引擎：Oozie，用于连接多个Map/reduce Job, 完成复杂的数据处理，处理各Job以及数据之间的依赖关系（可以依赖的条件：数据,时间,其他Job等); Oozie使用hPDL(一种XML流程语言) 来定义DAG工作流。这里需要注意的是工作流引擎在线上环境中非常重要，原因是当执行任务多了以后，任务间的关系依赖关系不能出错，否则会带来各种意想不到的后果。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算广告笔记(1)--广告的基本知识]]></title>
      <url>%2F2017%2F04%2F20%2F%E8%AE%A1%E7%AE%97%E5%B9%BF%E5%91%8A%E7%AC%94%E8%AE%B0(1)--%E5%B9%BF%E5%91%8A%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
      <content type="text"><![CDATA[本系列文章是刘鹏老师的计算广告学中的一些记录。本文是第一章的相关笔记：广告的基本知识，主要介绍广告的定义与模型，目前在线广告的特点、技术架构与市场形态，可以说简要地概括了整个课程的内容。 什么是广告？ 广告有商业上的诉求和目的，并不是简单的技术的堆砌。 广告的定义 广告是由已确定的出资人通过各种媒介进行的有关产品(商品、服务和观点)的，通常是有偿的、有组织的、综合的、劝服性的非人员的信息传播活动。 重点： 广告的主体：出资人(sponsor)即广告主(advertiser)，媒介(medium)，受众(audience)广告的本质功能：是借助某种有广泛受众的媒介的力量，完成较低成本的用户接触(reach) 品牌广告(Brand Awareness)与效果广告(Direct Response)品牌广告：创造独特良好的品牌或产品形象, 目的在于提升较长时期内的离线转化率 效果广告：有短期内明确用户转化行为诉求的广告。用户转化行为例如：购买, 注册, 投票, 捐款等。大部分互联网广告都是这种类型。 广告的有效性模型三个大阶段，同时可分为6个小阶段 这里值得注意的几点有 1.广告的天然属性（如广告的位置）很重要，远远强于技术带来的效果2.引起用户关注时需要遵循一定的原则：以上列出了三点3.解释阶段包括用户对广告的理解和认可两方面；理解阶段4.保持（retention）主要指品牌在用户中树立起其形象，但是点击率往往不会高 下面一些广告策略的效果,这些广告策略都是有利有弊的，下面的 + 表示对该项有正面效果， - 表示对该项有负面效果。 在线广告在线广告是与传统的线下广告对比而言的，下面主要介绍在线广告的特点、目前的市场、核心计算问题。 在线广告的特点当前的在线广告行业的有以下特点 1） 技术和计算导向。原因是数字媒体的特点使在线广告可以进行精细的受众定向，而技术使得广告决策和交易朝着计算驱动的方向发展2） 可衡量性。指的是可以用量化的方式来衡量广告的效果，广告的点击是效果的直接收集途径3） 标准化。指的是有行业制定了相关的规则来指导广告市场。如下是一些美国广告行业相关的机构。 Interactive Advertising Bureau 在线广告供给方的行业协会，推动数字化市场营销行业的发展 制定市场效果衡量标准和在线广告创意的标准 会员: Google, Yahoo, Microsoft, Facebook等 American Association of Advertising Agencies 主要的协议是关于广告代理费用的收取约定(17.65%)，以避免恶意竞争 主要集中在创意和客户服务，在线业务是一部分 会员：Ogilvy &amp; Mather, JWT, McCann等,Dentsu等非4A会员的大公司但也被列为4A公司 Association of National Advertisers 主要代表广告需求方的利益(也有媒体和代理会员) 会员：AT&amp;T, P&amp;G, NBA等 在线广告市场在线广告市场发展情况如下所示，主要分为了三大部分：需求方，供给方和连接两者的平台，需求方指的是需要投放广告的广告主，供给方指的是提供广告位的媒体。 从媒体也就是supply端来看，其变现的手段有以下三种 方式一：将广告位托管给广告网络(Ad net1)方式二：将广告位对接到广告交易平台(Adx), 以实时竞价的方式变现方式三：将广告位托管给SSP（supply side platform), 通过SSP可以对接多个广告网络和DSP，按照动态分配的逻辑选择变现最高的需求方。 上面简单的说明了媒体方通过广告位进行变现的方式，涉及到多个概念，可以参考刘鹏的《计算广告》中的第六章内容。 在线广告核心计算问题在线广告的核心计算问题是 ROI（Return On Investment，投资回收率）其定义如下 Find the best match between a given user u, in a given context c, and a suitable ad a.$$\max\sum_{i=1}^{T} ROI(a_i, u_i, c_i)$$ 上面的 $T$ 表示广告共展示 $T$ 次,ROI 主要有两部分构成: Investment 和 Return，一般来说主要优化的目的在于Return， 其计算公式如下 $$Return = \sum_{i=1}^{T} \mu(a_i, u_i,c_i)v(a_i,u_i) = \sum_{i=1}^{T} e(a_i, u_i,c_i)$$ 其中 $\mu(a_i, u_i,c_i)$ 表示点击率$a_i, u_i,c_i$ 分别表示广告，用户和广告上下文$v(a_i,u_i)$ 表示点击价值$e(a_i, u_i,c_i)$ 表示 eCPM（expected CPM，预期每次展示能够带来的价值） 而对上式的不同的分解对应不同的市场形态： CPM(Cost per mille)市场: 按照千次展示结算。是需求方与供应方约定好千次展示的计费。在这种方式下，点击率和点击价值都需要需求方预估。 CPC(Cost per click)市场: 按照点击结算，最早产生于搜索广告。在这种方式下，点击率估计交给供给方，点击价值的估计交给需求方，并通过点击出价的方式向市场通知自己的估价。 CPA(Cost per action)/CPS(Cost per Sale)/ROI市场: 按照转化行为数、销售订单数和投入产出比来结算。这三个都是按照转化付费的一些变种。在这种方式下，点击率和点击价值都需要供给方预估。 CPA/CPS/ROI市场中需要注意广告主可能会有作弊行为： 如隐瞒订单，卖高价物品（品牌得到了展示，但是转化率低，不用向平台付费） 优化 ROI 的问题可从以下两个角度来考虑，每个角度都有其重点关注的点，下面简单列出 从优化角度来看，主要的关注点在于 特征提取：受众定向 微观优化：CTR预测 宏观优化：竞价市场机制 受限优化：在线分配 强化学习：探索与利用 个性化重定向：推荐技术 从系统角度来看，主要的关注点在于 候选查询：实时索引 特征存储：No-sql技术 离线学习：Hadoop 在线学习：流计算 交易市场：实时竞价 在线广告计算的主要挑战有 大规模 (Scale)：百万量级的页面，十亿量级的用户，需要被分析处理; 高并发在线投放系统 (例: Rightmedia 每天处理百亿次广告交易); Latency 的严格要求 (例: ad exchange 要求竞价在 100ms 内返回) 动态性 (Dynamics)：用户的关注和购物兴趣非常快速地变化 丰富的查询信息 (Rich query)： 需要把用户和上下文中多样的信号一起用于检索广告候选 探索与发现 (Explore &amp; exploit)：用户反馈数据局限于在以往投放中出现过的 $(a, u, c)$ 组合，需要主动探索未观察到的领域，以提高模型正确性 搜索、广告与推荐搜索，广告和推荐可以说是联系紧密同时又有各自特点的三个领域。 比起搜索，广告不需要爬虫，索引数也比较少。 推荐不等于个性化，个性化是推荐的一个准则，其他准则还包括新鲜性，多样性等。 广告与推荐系统：文字广告点击率高于图片广告点击率，但是推荐系统刚好相反 推荐与广告的一个重要区别在于 Downstream 优化，推荐出来的物品还可顺带其他的推荐物品，优化的目的是一系列用户可能会点击的物品；而广告的推送只是要优化用户对这个广告的点击率。 在线广告系统结构下面的在线广告系统结构图，需要注意这并非实际设计图，只是概念性的结构图 从上图可以看到，整个系统可以分为四大部分 高并发的投送系统(Ad Server)：在线部分，根据 $u,c$ 决定出 $a$,特点是高并发 受众定向平台：离线部分，分布式机器学习，用于预估点击率等信息，常用的是 Hadoop 平台 数据高速公路：收集线上日志文件等供其他部分使用 流式计算平台：重点在于实时性，比 Hadoop 要快，包括反作弊，计价，实时索引(广告的加入和删除)等任务 将上面的架构图各部分做更细致的划分时，可以得到如下的划分图 Ad Serveing: 主要指 Ad Server 接受两种请求，一种来源于用户（USer），另外一种是广告交易市场发过来的（RTBS） Ad retrival：找出与页面和用户相关的广告 Ad ranking：有多个广告满足要求时，根据某种指标 (如eCPM) 来排序，选出最符合要求的广告 Streaming Computing：流式计算平台 Data highway：把线上数据传到 Hadoop 平台或流式计算平台 Session log generation: 搜集用户的浏览、搜索的行为整理成一份标准日志，提供给其他的系统 Customized audience segmentation：受众的定制化，不由平台固定受众分类，而是由广告主选择具体的受众类型，因为业务的需求是各式各样的 Page attribute system：爬取有广告展示的页面，用于广告的 retrieval Audience targeting：受众定向，根据用户及其浏览的上下文决定出推送哪个广告 Ad management system: 供广告主投放广告的平台]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[遗传算法简介]]></title>
      <url>%2F2017%2F04%2F10%2F%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[最近由于课程需要去调研了一下遗传算法，本文是调研结果的总结，主要介绍遗传算法的背景、基本流程、理论基础、变种及其应用。 连续优化与组合优化连续优化连续优化(continuous optimization) 在某些教材中也被称为函数优化，其特点是变量的取值是连续的，或者说是变量的范围是实数域(绝大多数情况)。对于一个可行域，其中包含着无数个可行解，这类问题一般利用目标函数可导的性质，通过如梯度，海塞矩阵等进行求解（如常见的梯度下降法，牛顿法等）。更详细的关于连续优化的方法可参考这篇文章 组合优化相对于连续优化的另外一种优化是组合优化，组合优化的变量是离散的，也就是变量的空间只有有限多个的可行解。比如说经典的背包问题就是一个组合优化问题 既然只有有限多个解，那么最直接想到的求解方法就是通过穷举这些解(exhaustive search)找到最优解，然而分析可知，穷举所需要的时间复杂度一般是指数级别的，比如说对于背包问题，有 $n$个物品的时候，共有 $n^2$ 种可能性，时间复杂度是 $O(2^n)$。这样的问题已经是一个 NP-hard 问题了，这里先简单介绍一下P问题，NP问题和NP-hard问题的区别。 一般来说，P 问题指的是能够在多项式时间复杂度内找到解的问题，而 NP 问题指的是无法在多项式时间复杂度内找到答案的问题，但是能够在多项式时间复杂度内验证一个解是否正确的问题（如大素数的判断问题）；而 NP-hard 问题则是指那种不仅无法在多项式时间复杂度内找到答案, 甚至无法在多项式时间复杂度内验证一个解是否正确的问题（如背包问题，如果要找到最优，就必须对所有的可能进行比较，这样的时间复杂度就是指数级别了）。 实际上，大多数的组合优化问题都是 NP-hard 问题，虽然存在着某些特殊问题可以用多项式时间的方法来解决，如通过维特比算法求解 HMM 中最大概率的隐含状态转移路径。但是更多的情况下组合优化问题仍然是一个 NP-hard 问题。 于是为了解决这类得到最优解就需要耗时非常长的问题，人们退而求其次，去找一个接近最优解但是耗时在可接受范围内的解。进而诞生了一类方法: 元启发式方法(Metaheuristic)。 元启发式方法(Metaheuristic)元启发式方法(Metaheuristic) 是一类方法， 维基上对的定义如下 a metaheuristic is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem 元启发式方法的定义中已经说得很清楚了，这一类方法不确定能够提供最优解，但是会提供一个足够好的解，当然这个足够好好到什么程度也是与具体算法相关的，需要通过数学证明。但是这类方法能够在可接受时间内返回这个解。 我们经常听到的一些算法都属于这种元启发式算法，包括： 遗传算法(Genetic Algorithms，GA) 模拟退火(Simulated Annealing，SA) 蚁群算法(Ant Colony Optimization，ACO) 粒子群算法(Particle Swarm Optimization，PSO)…… 这一类算法都是从自然现象中得到启发的，如遗传算法就是从进化论中得到启发的，认为“物竞天择，适者生存”，一代代进化的基因是逐渐往好的方向发展的，对应到优化问题中就是逐渐收敛到比较好的解；模拟退火则是从金属加热后再冷却，金属中的分子最终有可能会到达一个内能比原来更低的位置，表示为一个更优解；蚁群算法和粒子群算法则是通过一个群体去搜索最优解，如对于非凸的优化，往往具有多个局部最优解，通过一个群体去搜索能够扩大搜索范围，从而以更大的概率收敛于全局最优解。 关于元启发式方法在组合优化中的更详细的应用可这篇文献：Metaheuristics in Combinatorial Optimization: Overview and Conceptual Comparison 遗传算法从前面可知，遗传算法是元启发式方法的一种，维基上对其的定义如下: Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection。 从维基的描述可知，遗传算法是收生物进化论的影响，因此遗传算法中的很多概念跟生物进化中的很多概念类似，而遗传算法主要的三个操作就是选择(selection)，交叉(crossover)和编译(mutataion). 下面先介绍遗传算法中的基本概念： 基本概念 个体（individual） : 一个候选解 种群（generation） : 一组候选解，代表某一代的所有个体 染色体（chromosomes） : 候选解的染色体表示，常见的用01字符串表示 适应度（fitness） : 个体的适应度，一般与优化问题中的目标函数值相关 选择（selection）: 根据个体适应性从一个 generation 中选出 适应性好的进入下一代 交叉（crossover） : 类似于生物中两个个体进行杂交，两个 chromosomes 进行相应的交叉 变异（mutation）: chromosomes 的某一位进行突变，如从0变到1或从1变到0 在执行遗传算法前，有两个东西必须先定义好 可行解的表示方法，也就是常说的编码方式，上面说到了常用的有 01 编码，除此之外，根据问题的不同，还存在其他的一些编码方式 适应度函数，也就是计算上面适应度的函数，这个函数一般yong就是用目标函数，如要最大化某个函数，则将这个函数作为目标函数即可，反之最小化的时候加个负号即可。 遗传算法是一个迭代算法，在进行初始化后按照 “选择，交叉，变异” 的顺序进行迭代，当满足设定的结束条件(例如达到一定的代数)，便将得到的解作为问题的解。其流程如下所示 算法流程初始化(Initialization)初始化主要是确定编码方式以及初始种群的数目，初始种群的数目由人为设定然后随机产生，设定的数目如果太小，可能会一开始就限制了搜索域较小，而如果设定的数目太大，计算量会比较大。 编码方式则根据问题的约束条件和希望得到的解的精度确定，后面会给出一个详细的例子说明这个问题。 选择(Selection)选择模拟了生物进化中的“适者生存”，根据当代的个体的适应度按概率选出下一代，适应度越高，则被选择的概率也越大。 之所以不直接将适应度最大的若干个个体作为下一代，是因为这样可能会导致算法过早陷入局部最优解。在遗传算法里面这种现象称为”早熟（premature）”。 假如当代的个体总数是 $M$，而个体 $a_i$ 的适应度是 $f_i$，则个体 $a_i$被选择到下一代的概率是$$p_i = \frac{f_i}{\sum_{j=1}^{M} f_j}$$ 交叉(crossover)交叉跟生物上的杂交是一样的，只是生物中是双螺旋结构，而遗传算法中只有一条链。原始的遗传算法只会选择一个点进行交叉，如下如所示 而假如对遗传算法进行改进，也可以在多个点进行交叉的操作 交叉（crossover）的目的是从目前的所有解中组合出更优的解，但是尝试获得更优解的同时，也可能丢掉目前得到的最优解。而这也是传统的遗传算法无法收敛到全局最优的一个原因（详见后面的理论证明，对其进行了简单改进后可以收敛到全局最优）。 交叉是按照一定的概率进行的，这个概率也需要人为设定，假如设得过大边很容易将当前的最优解破坏掉，并且容易陷入早熟(premature)，而设得过小时则很难从当前的可行解中组合出更优的解。 变异(mutation)变异（mutation）跟生物中的变异也一样，随机改变染色体中的某一位，如下所示就是一个变异的示例 变异也是按照一定的概率进行的，这个概率也需要人为设定，而且这个概率一般会设置得比较小，如果设得较大时，就变成了类似随机搜索（random search）的方法；但是如果设得太小的时候，就会造成生物中的基因漂变(genetic drift)的现象，从而导致收敛到一个局部最优。 变异的主要作用是为了防止算法收敛到一个局部最优解。假如将找到最优解比作在多座山峰中找到最高的那一座，那么交叉就类似同一座山峰下的多个基因合力爬上山顶，而变异就类似于从一座山峰调到另外一座山峰，目的就是为了防止当前的山峰是一个局部最优解而将算法其作为一个全局最优解。 终止(termination)算法的终止条件由人为设定，一般是限制迭代的最大次数，或能够使用的最大的计算资源，或者是两次的解小于某个设定的阈值，下面是从维基百科中摘录的终止条件 A solution is found that satisfies minimum criteria Fixed number of generations reached Allocated budget (computation time/money) reached The highest ranking solution’s fitness is reaching or has reached a plateau such that successive iterations no longer produce better results Manual inspection Combinations of the above 例子下面给出两个例子：分别是遗传算法解决背包问题和一个连续优化问题。 遗传算法解决背包问题背包问题是一个很经典的组合优化问题，通过遗传算法可以求解，但是目前求解这类问题的最优方法是动态规划，是一个伪多项式时间复杂度问题。这里采用背包问题为例子是因为这个例子的编码方式有较好的解释。 假设现在背包的容量为 $W$, 共有 15 个物品，每个物品的大小为 $w_i$, 价值为 $v_i$, 则可以得到下面的最优化问题 $$max \sum_{i=1}^{15} x_iv_i\s.t. \sum_{i}w_ix_i \le W\x_i \in \lbrace 0,1 \rbrace$$ $x_i \in \lbrace 0,1 \rbrace$ 表示是否将第 $i$ 件物品放到背包中。 则通过遗传算法解决这个问题的流程如下 初始化由于有15个物品，而每个物品只有两个状态，因此用一个长度为15的01字符串表示一个解，0表示不拿该物品，1表示那该物品。随机产生10个候选解如下所示，这里要注意每个候选解必须要满足约束条件 由于没有给出每个物品具体的价值，这里用 $V_i$ 表示第 $i$ 个候选解通过公式 $\sum_{j=1}^{15}x_jv_j$求得的总价值。 选择 由于每个个体被选择到下一代的概率与其适应度成正比，因此第 $i$ 个候选解被选择到下一代的概率为$$p_i = \frac{V_i}{\sum_{j=1}^{15}V_j}$$ 如下为模拟的过程，注意一个个体可能会被选择多次 交叉 对选择出来的个体重新进行编号，然后进行交叉的操作，按照设定的交叉概率选择个体进行交叉的操作，交叉的位置也是随机产生的，如下所示是设定交叉概率为 0.5 后的交叉结果 变异 变异也是按照变异概率选择个体来进行的，被选中的个体变异的位数可以是一位也可以是多位，如下图所示是设定变异概率为0.2的编译过程 终止 设定一个终止条件，如最大的迭代次数，当满足这个条件时边终止算法，将当前得到的最优解作为问题的最优解；没满足时便按照 “选择-&gt;交叉-&gt;编译”进行迭代。 利用遗传算法求解背包问题更详细的信息可参考这篇文献：A genetic algorithm for the multidimensional knapsack problem 遗传算法解决连续优化问题该例子来源于知乎， 优化的问题如下所示 $$\max f(x) = x + 10sin(5x) + 7cos(4x)， x \in [0,9]$$ 这个函数的图像如下所示 显然这已经不是一个凸函数了，通过遗传算法解决这个问题的步骤跟背包问题的一样，最主要也是最重要的区别在于编码方式的不同。 由于现在可行解是实数域了，仍旧采用01编码，则要解决的问题就是如何用01串表示一个实数。假设现在希望解精确到小数点的后四位， 假如设定求解的精度为小数点后4位，可以将x的解空间划分为 $(9-0)×10^4=90000$ 个等分。 由于 $2^{16}&lt;90000&lt;2^{17}$，则需要17位二进制数来表示这些解。换句话说，一个解的编码就是一个17位的二进制串。 一开始，这些二进制串是随机生成的。一个这样的二进制串代表一条染色体串，这里染色体串的长度为17。对于任何一条这样的染色体chromosome，如何将它复原(解码)到 [0,9] 这个区间中的数值呢？对于本问题，我们可以采用以下公式来解码： x = 0 + decimal(chromosome)×(9-0)/(2^17-1) decimal( ): 将二进制数转化为十进制数 更一般化的解码公式： x∈[lower_bound, upper_bound]1x = lower_bound + decimal(chromosome)×(upper_bound-lower_bound)/(2^chromosome_size-1) lower_bound: 函数定义域的下限upper_bound: 函数定义域的上限chromosome_size: 染色体的长度通过上述公式，我们就可以成功地将二进制染色体串解码成[0,9]区间中的十进制实数解。 后面的选择，交叉，变异的过程与上面的背包问题一样，这里不在给出，在经过 100 次迭代后，最终选出的最优个体为00011111011111011，其适应度，也就是目标函数的值为24.8554，对应的 $x$ 为7.8569,从图像来看，已经是在最优解的位置了。 理论基础上面介绍的内容仅仅是遗传算法的基本流程，但是经过这些仿生的操作能确保最终收敛吗？如果收敛的话能够收敛到全局最优还是局部最优？收敛到全局最优的概率是多少？ 在遗传算法刚提出来的时候为不少问题找到了一个较好的解，对于很多问题都能找到一个比较好的解，但是缺乏数学理论基础的保证，往往会让人产生上面这些问题。 很多时候，实践的发展往往是超前于理论的，于是在遗传算法提出 5 年后，它的提出者便发表了模式定理(Schema Theory)来说明遗传算法中发生“优胜劣汰”的原因。而后又有人通过马氏链证明了算法的收敛性，结果是原始的遗传算法并不能收敛到全局最优，但是经过简单改进的遗传算法能够收敛到全局最优。 模式定理(Schema Theory)基本概念问题引入： 求解方程 $F(x) = x^2$ 在 [0,31] 上的最大值，使用固定长度二进制编码对种群中的个体进行编码，在计算适应度值时会发现一个规律，当个体编码的最左边为1时，适应度普遍较大，可以记为 1***1，同理 0**** 的个体适应度偏低。由此可以引入以下一些基本概念： 模式(Schema)：编码的字符串中具有类似特征的子集。 例如上述五位二进制字符串中，模式 *111* 可代表4个个体。个体和模式的一个区别就是，个体是由{0，1}组成的编码串，模式是由{0，1，*}组成，* 为通配符。 模式阶(Schema Order)：表示模式中已有明确含义的字符个数，记为o(s)，s代表模式。例如o(111)=3; 阶数越低，说明模式的概括性越强，所代表的编码串个体数也越多。其中阶数为零的模式概括性最强。 模式定义长度(Schema Defining Length)：指第一个和最后一个具有含义的字符之间的距离，其可表示该模式在今后遗传操作中被破坏的可能性，越短则越小，长度为0最难被破坏。 下面就是模式定理的具体定义： 适应度高于群体平均适应度的，长度较短，低阶的模式在遗传算法的迭代过程中将按指数规律增长。 下面将推导经过选择，交叉和变异后如何得到上面的模式定理 选择假设当前是第 $t$ 代， 这一代中模式 $s$ 的个体数记为 $m(s,t)$，整个种群的数目为 $M$，个体 $a_i$ 的适应度为 $f_i$ 则个体 $a_i$ 被选择的概率为 $$p_i = \frac{f_i}{\sum_{j=1}^{M} f_j}$$ 因此经过选择后， 模式 $s$ 在下一代的数目为 $$m(s, t+1) = M\frac{m(s,t) \overline {f(s)}}{\sum_{j=1}^{M} f_j} = m(s,t) \frac{\overline {f(s)}}{\overline {f}}$$ 其中 $\overline {f(s)}$表示模式 $s$ 中个体的平均适应度, $\overline f$表示整个种群的平均适应度，也就是 $$\overline f = \frac{\sum_{j=1}^{M} f_j}{M}$$ 可知只有当$\overline {f(s)} &gt; \overline f$, 模式 $s$ 中的个体才能增长, 将上面的公式做简单的变化，则有 $$m(s, t+1) = m(s,t) \frac{\overline {f(s)}}{\overline {f}} = m(s,t) \frac{(1+c)\overline f }{\overline {f}} = m(s,t)(1+c) $$ 则从开始到第 $t+1$ 代，模式 $s$ 的个体的数目为:$$m(s, t+1) = m(s, t)(1+c)^t$$ 可以看到当模式平均适应度高于种群是适应度时，模式中的个体会呈指数形式增长。 交叉记模式定义长度(schema defining length)为 $\delta(s)$，染色体的长度记为 $\lambda$ 则模式被破坏，也就是在定义长度内进行交叉的概率为 $p_d = \frac{\delta(s)}{\lambda - 1}$ 因此模式存活的概率为 $p_s = 1 - p_d = 1 - \frac{\delta(s)}{\lambda - 1}$ 假设交叉概率为 $p_c$，则存活概率为 $p_s = 1 - p_c \frac{\delta(s)}{\lambda - 1}$ 在经过选择，交叉后，模式 $s$ 在下一代数目为 $$m(s, t+1) = m(s,t) \frac{\overline {f(s)}}{\overline {f}}[1 - p_c \frac{\delta(s)}{\lambda - 1}]$$ 从这里可以看到，模式的定义长度(schema defining length) 越小，则其存活的概率越大 变异记模式的阶(Schema Order) 为 $o(s)$，变异的概率为 $p_m$ 则原来的基因存活，也就是那些确定的位置都不发生变异的概率为 $$p_s = (1-p_m)^{o(s)} \approx 1 - p_mo(s)$$ 上式最后做了泰勒展开，因此经过选择，交叉和变异后，模式 $s$ 在下一代种群的数目为 $$m(s, t+1) = m(s,t) \frac{\overline {f(s)}}{\overline {f}}[1 - p_c \frac{\delta(s)}{\lambda - 1} - p_mo(s)]$$ 从上式也可以看到，模式的阶越低，模式约容易存活。 因此经过上面的分析，可以得到模式定理的定义 适应度高于群体平均适应度的，长度较短，低阶的模式在遗传算法的迭代过程中将按指数规律增长。 该定理阐明了遗传算法中发生“优胜劣汰”的原因。在遗传过程中能存活的模式都是定义长度短、阶次低、平均适应度高于群体平均适应度的优良模式。遗传算法正是利用这些优良模式逐步进化到最优解。 收敛性分析虽然上面的模式定理说明了遗传算法有“优胜劣汰”的趋势，但是对于其收敛性并没有给出一个证明，这篇文献 Convergence Analysis of Canonical Genetic Algorithms 对遗传算法的收敛性进行了证明。 证明利用了有限状态的时齐马氏链的遍历性和平稳分布的性质，因为遗传算法可以被描述成一个有限状态的时齐马氏链，假如种群大小为 $M$, 染色体的长度为 $\lambda$,采用01编码方式，将所有个体的染色体连起来作为一个状态，则种群的所有状态数目为 $2^{M\lambda}$。同时选择概率，变异概率和交叉概率可作为状态转移概率。 由于证明的定义和引理较多，因此这里不详细描述证明过程，只给出最后的结论。 证明给出的主要结论有两个： 传统的遗传算法不能收敛到全局最优 在传统的遗传算法基础上每次选择的时候保留前面所有迭代里面的最优个体，最终能收敛到全局最优。 从证明最终给出的结论可知，传统的遗传算法并不能收敛到全局最优，证明过程指出的是收敛到全局最优的概率小于1，原因就是选择、交叉和变异中都带有一定的随机性，这种随机性导致了最优解可能会被抛弃；而改进后的遗传算法每一代都会保留着前面所有代数的最优解，因此最终会以概率1收敛到全局最优。 改进方案(Variants)下面介绍的是对原始的遗传算法进行改进后得到的变种，改进主要有三大方向：编码方式(Chromosome representation), 精英选择(Elitism)和参数自适应（Adaptive）。 编码方式我们前面采用的均是01编码方式，但是实际上根据不同问题可以选择不同的编码方式，如格雷码（Gray Code）、浮点数编码都是可选的编码方式。 二进制编码的缺点是：对于一些连续函数的优化问题，由于其随机性使得其局部搜索能力较差，如对于一些高精度的问题（如上题），当解迫近于最优解后，由于其变异后表现型变化很大，不连续，所以会远离最优解，达不到稳定。 而格雷码能有效地防止这类现象，当一个染色体变异后，它原来的表现现和现在的表现型是连续的。主要优点有： 便于提高遗传算法的局部搜索能力 交叉、变异等遗传操作便于实现 符合最小字符集编码原则 便于利用模式定理对算法进行理论分析 这篇文献 An Improved Genetic Algorithm for Pipe Network Optimization 就适用了格雷码作为其编码方式。 而使用浮点数编码的原因往往是对连续函数优化时二进制编码精度不够。 精英选择精英选择实际上就是在前面收敛性证明中改进遗传算法使得其以概率1收敛到全局最优的改进方案，在这篇文献中Removing the Genetics from the Standard Genetic Algorithm 对其做了详细的描述。 参数自适应前面我们说到了变异的概率和交叉的概率都是人为设定的固定值，但是实际上这个概率在不同的种群下应该会有其对应的最优值。因此这篇文献 Adaptive probabilities of crossover and mutation in genetic algorithms，将两个概率设定为下面的两个公式 $$ P_c=\begin{cases}\frac{k_1(f_{max}-f’)}{f_{max}-f_{avg}}&amp;&amp; {f’ \geq f_{avg}}\\k_2 &amp;&amp; {f’ &lt; f_{avg}}\end{cases}$$ $$ P_m=\begin{cases}\frac{k_3(f_{max}-f’)}{f_{max}-f_{avg}} &amp;&amp; {f’ \geq f_{avg}}\\k_4 &amp;&amp; {f’ &lt; f_{avg}}\end{cases}$$ 上面的公式表明，交叉率和变异率随着个体的适应度在种群平均适应度和最大适应度之间进行线性调整。上面的公式中 $f_{max}$ 表示种群的最大适应度, $f_{avg}$ 表示种群的平均适应度, $f’$ 表示参与交叉的两个个体中较大的适应度。 由上面的公式可知，当适应度越接近最大适应度时．交叉率和变异率越小，好处是降低了最优解被破坏的概率，坏处是当前的最优适应度等于最大适应度时，交叉率和变异率为零。这使得AGA在演化初期并不理想。因为在进化初期的群体中，较优个体几乎处于一种不发生变化的状态，而此时的优良个体不一定是全局最优解，这容易使演化走向局部收敛的可能性增加。 因此这篇文献自适应遗传算法交叉变异算子的改进对上面的公式进行了如下的改进 $$ P_c=\begin{cases}p_{cmin}-\frac{（p_{cmax}-p_{cmin}）(f’-f_{avg})}{f_{max}-f_{avg}}&amp;&amp; {f’ \geq f_{avg}}\\p_{cmax} &amp;&amp; {f’ &lt; f_{avg}}\end{cases}$$ $$ P_m=\begin{cases}p_{mmin}-\frac{（p_{mmax}-p_{mmin}）(f’-f_{avg})}{f_{max}-f_{avg}}&amp;&amp; {f’ \geq f_{avg}}\\p_{mmax} &amp;&amp; {f’ &lt; f_{avg}}\end{cases}$$ $p_{cmin}$, $p_{cmax}$ 分别表示交叉率取值的下限和上限；而$p_{mmin}$, $p_{mmax}$ 分别表示变异率取值的下限和上限,这样就避免了前面的概率出现0的情况。 参考文献这些文献在上面已经在不同部分给出其相应的链接了，这里再次统一列出 Blum C, Roli A. Metaheuristics in combinatorial optimization: Overview and conceptual comparison[J]. ACM Computing Surveys (CSUR), 2003, 35(3): 268-308. Chu P C, Beasley J E. A genetic algorithm for the multidimensional knapsack problem[J]. Journal of heuristics, 1998, 4(1): 63-86. Goldberg, David .Genetic Algorithms in Search, Optimization and Machine Learning[M].MA: Addison-Wesley Professional, 1989:ISBN 978-0201157673 Rudolph G. Convergence analysis of canonical genetic algorithms[J]. IEEE transactions on neural networks, 1994, 5(1): 96-101 Srinivas M, Patnaik L M. Adaptive probabilities of crossover and mutation in genetic algorithms[J]. IEEE Transactions on Systems, Man, and Cybernetics, 1994, 24(4): 656-667. Dandy G C, Simpson A R, Murphy L J. An improved genetic algorithm for pipe network optimization[J]. Water resources research, 1996, 32(2): 449-458. Baluja S, Caruana R. Removing the genetics from the standard genetic algorithm[C].Machine Learning: Proceedings of the Twelfth International Conference. 1995: 38-46. Srinivas M, Patnaik L M. Adaptive probabilities of crossover and mutation in genetic algorithms[J]. IEEE Transactions on Systems, Man, and Cybernetics, 1994, 24(4): 656-667. 邝航宇, 金晶, 苏勇. 自适应遗传算法交叉变异算子的改进[J]. 计算机工程与应用, 2006, 42(12): 93-96.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[年轻的时候，做什么才不会浪费]]></title>
      <url>%2F2017%2F03%2F25%2F%E5%B9%B4%E8%BD%BB%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E5%81%9A%E4%BB%80%E4%B9%88%E6%89%8D%E4%B8%8D%E4%BC%9A%E6%B5%AA%E8%B4%B9%2F</url>
      <content type="text"><![CDATA[文章为转载，作者是孙圈圈,侵删。 最近去各个城市做读者见面会，见到很多年轻人，跟他们交流，无一例外地：焦虑，迷茫，没方向，一无所有，着急…… 但，这些 “月经式”问题，谁年轻的时候没痛过？ 从学校到公司，这条路有多长刚工作的时候，我比现在有个性多了：我老板是区域总监，在她下面的一位大区经理（我非常不喜欢的一个人）让我做一份数据表格。 实际上，每个大区经理都有自己的助理，但他说自己助理太辛苦，让我做。 结果我不服，直接发了一封邮件给他，抄送我老板，把我的岗位说明书发给那位大区经理，跟他说：在我的工作职责里面，没有一条是需要服务你的。 现在想想，当时老板真的挺宽容我的，因为这样一来，她会处于很尴尬的境地，那位大区经理会以为，我的行为是我老板指使的。 反正，换做是我现在，遇到当年的自己，肯定火冒三丈。 那时候，还经常跟同事一起抱怨：公司培训机制不行、学不到东西；老板不民主、一意孤行；上层不体察民情、听不到员工的声音；公司太看重短期利益、不考虑长期发展…… 等到后来，自己做管理者，才知道，一个团队的管理、一个公司的经营，远远不是我作为一个初级员工所想象的那样。 有很多文章，批评年轻员工不负责、不会站在老板角度考虑问题。但要我说，这些都没说到点子上，即：从学校到公司，到底有什么不同？ 最大的区别就是：你给学校付钱，但公司给你付钱。 所以在学校里，老师为你服务，有义务帮助你学习；但在公司，你是为老板服务，有义务帮他解决问题。 对这个概念的模糊不清，是导致我们在职场屡屡受挫的最大障碍。 因为如果不意识到这一点，我们就不会知道：应该站在老板的角度，去了解他需要什么，而不是要他站在我们的角度，来关心我们要什么；不应该像在学校跟老师相处那样，向老板寻求答案，而是要主动学习，帮老板找到答案。 在中国教育体制下浸润了20年、习惯了向老师寻求标准答案的我们，在进入公司之后，能够多快地抛弃这种惯性，从而在没有标准答案的问题面前，去主动学习和思考、摸索解决方案，基本上决定了我们最初5年的职场上升速度。 焦虑感什么时候才会消失科比说他见过洛杉矶凌晨4点的样子，我没见过，但我见过上海凌晨4点的样子，因为那时候我还没睡。 不是因为在工作，而是因为我焦虑，睡不着。 毕业那年，在没有任何实习经历的情况下找工作，所以第一份工作找得不好。 工作三个月后，就发现这个职位几乎不可能有什么发展。 从此就一直想要逃离，但毕业半年，谁会要你呢？于是经常焦虑、自我怀疑，总会想：我这辈子是不是就这样了？是不是就要困在这儿了？ 还好，一直都在关注外部机会，最后幸运地进了咨询公司。但马上又开始焦虑，因为进公司之后1个月，全球经济危机就开始了，公司在全球范围内冻薪，甚至在传有可能会裁员。 如果被裁，那就意味着，我毕业1年，得第三次换工作了。 还好，又幸运地留在了公司。但公司冻薪了两年，不能加薪不能升职。 而且，公司还在那年取消了分析师级别，应届生一进来就是高级分析师了。这意味着，我毕业快3年了，还跟应届生的工资一样、职位也一样。 知道消息的那天，躺在床上焦虑得睡不着：毕业快3年了，事业还停留在起点，还没了男朋友，刚到上海所以也没有好朋友，租房子一年搬了6次家，这样的人生还有什么希望？ 但后来，毕竟还是走出来了，经济危机终究过去了。 人生也终于开挂了，9个月完成别人3年达到的晋升、成为大中华区晋升最快的顾问、几次在1年内薪资翻倍、带着团队拿比赛冠军、转换到我喜欢的咨询业务线、买房子把家人接到身边、买东西开始不关注价格、遇到对的人…… 当年焦虑的那些问题，如今都一一解决了。 其实，现在随便去问一个年轻人，都会发现，我的经历没有任何特别之处，这些问题都是大多数人年轻时候的必经之路。 只是，回头再看年轻时候那些焦虑的经历，我唯一想分享的经验是：在我们应对焦虑的时候，需要学会一次只解决一个问题。 比如，意识到工作没发展，就把一切精力放在寻找转行机会上；经济危机的时候，反正也没法找到工作，就干脆埋头苦干，等到危机一过，就得到了自己想要的；工作顺利之后，才开始考虑感情问题，之前都先放一边；再次遇到天花板的时候，就尽全力转换业务线。 这些问题，几乎没有哪一个是我同一时间花时间去解决的。 没钱、没男/女朋友、没地位、工作不顺、人际关系受挫……这些都是我们大多数人年轻时候躲不开的问题。 但是，当我们把所有问题混为一谈的时候，就会疲于奔命、无从下手，脑子里只有“焦虑”二字，一团浆糊。 只有当我们把问题掰开，逐个分析的时候，才会看到问题的本质，看到我们想要的“解决方案”，逐个击破。 现在，我经常会对那些看职场鸡汤的人嗤之以鼻，但倒回很多年前，那时候的自己，面对诸多问题、急于一下子解决、又没有头绪的时候，不也曾诉诸鸡汤、试图寻求一个简单粗暴的答案、慰藉自己的焦虑情绪吗？ 等我们对鸡汤的粗暴答案嗤之以鼻、开始主动思考的时候，才是我们真正认识世界、开始解决问题的时候。 年轻的时候，做什么才不会浪费线下签售的时候，有读者问我一个问题：我现在还年轻，现在的职业未必是以后要从事的，而且时代变化这么快，现在的工作也随时可能被淘汰。那我现在学的东西，是不是都白费了？我要怎么预测一下，未来10年的情况呢？ 这个问题，几乎没人可以回答。凯文凯利在《失控》里面有句话：所有对未来的长期预测都是错的。 但是，虽然长期无法预测，有些东西是不太会变的。 《人类简史》里面提到，根据史学家的考证，只有智人能够表达出从来没看过、碰过、听过的事物，这种能力让他们得以聚集大批人力，灵活合作，从而战胜比智人更加强大的其它人种。 说白了，就是讲故事的能力。 而我们看今天，人类进化到这个阶段，讲故事的能力依然重要，不管你说服客户、说服老板还是说服投资人，甚至做自媒体，都很需要。 所以，不管行业、职业被颠覆成什么样子，有些核心的底层能力是任何时代、任何职业都用得上的。那么，只要我们掌握这些能力，就能够以不变应万变了。 我们现在做的事情，都未必是将来想做的，也未必是适合自己的，从功利的角度讲，确实有些事情是会白费的。但我们做以下这些事情，是永远不会浪费时间的： 核心能力提升就像上面说的，核心能力是能够让我们以不变应万变的能力。 这些核心能力，除了上面说的讲故事能力之外，还包括：快速学习能力、分析与解决问题能力、创新能力等等。 如果一份工作只能够给你知识和技能，却无法让你提升这些能力，那么就会是一份高风险的工作，因为当某种知识和技能不再稀缺的时候，你就很难有什么立足之本，瞬间回归起点。 而当你的工作能够帮助你提升这些核心能力的时候，即便转换职业，也只是一时下滑，很快就能够反弹上去。 认识自己的优劣势过去给企业做咨询的时候，接触了很多人才研究，这些研究都试图发现：究竟是什么因素，让一些人可以脱颖而出，更容易成功？ 最终会发现，知识和技能这些，都不关键，而能力，有不小的影响，但最终起决定作用的，却是我们的天性：性格特质、动机、价值观这些。 似乎这是个悲观的发现，因为我们在成年之后，除非遭遇重大的人生变故，否则这些因素几乎是不可改变的。 既然如此，我们还需要白费力气去做什么努力呢？ 但实际并非如此，因为这些天性，几乎没有什么好坏之分，比如在我们的社会，似乎外向的人更有优势，因为可以更加快速地跟人熟络起来。 然而，内向的人也有自己的优势，他们很少直接发表自己的看法，更加倾向于深思熟虑，所以往往在思考方面有自己的优势。 由此看来，任何天性，都可以找到适合自己的成功道路。 年轻的时候，我们不了解自己、不满意自己、不想成为自己，跟自己的天性对抗，但没有关系，这些对抗最终会让我们发现自己的特点、边界和意义。 等到了一定年龄，就会知道，不应该再对抗自己的天性，而是发现并顺应天性，找到适合自己的定位，最大化自己的优势，成就自己。 接受无法改变的，改变可以改变的，这才是我们得以向前的高性价比方式。 广结善缘年轻的时候，要成就一件事情，靠自己苦干就行，因为那时候，我们的工作主要是“对事”，比如写好一份报告、解决一个问题、克服一个技术障碍等等。 而随着年龄增长、地位提升，要成就一件事情，靠苦干远远不够，因为我们的工作变得主要是“对人”，比如管理团队、说服客户、拿到投资等等。 所以，越往上发展，你越需要与人交往，越需要更多的资源，越需要他人的帮助。个人英雄主义，往往会成为一个人未来发展的最大天花板。 但人际交往这样的事情，不会是一次性的利益交换，而是基于长久积累的信任。 如果我们能够在自己力所能及的情况下，不计回报地给别人提供一些帮助，你将会在后面几年的人生中，收获意外惊喜。 我自己在几个月前开始创业，现在圈外的课程设计负责人，是我的前同事，一位非常资深的项目经理。 她本身对创业不感兴趣，在做自由顾问，但因为我们课程设计的要求很高，初创阶段又没有足够的钱，所以很难找到适合的人。 她就拿了相当于自己做自由顾问1/10的工资，帮我设计课程、培养我们的课程教练、带团队，丝毫没有犹豫。我几次觉得不好意思，想要多给她一些回报，她跟我说：“等你做大了再说，我现在还不缺钱。” 然后，最近因为我们产品内测，需要更多的课程迅速上线，她甚至推了报酬丰厚的咨询项目，投入了更多时间在圈外。 我从前极少麻烦朋友帮忙，但这段时间，得到了太多人的帮助，感觉自己消耗了前10年积攒的人品。 所以人际交往这个事情，真的是个长期投入，如果在你想用的时候才去培育，是来不及的。 年轻的时候，能多帮别人就多帮，别去计较一时得失，过几年自会发现，“帮助别人”是你在年轻时候做的最有价值的投资。 读完这篇文章，年轻的你就能接受并照做吗？未必。 你们可能会说我是在倚老卖老、讲大道理。然而，这大概就是人生的有趣之处，总要自己痛过，才会愿意领悟。 年轻的时候，有的是资本去试错，也应该去试错，唯有如此，才能够从中吸取教训，通过与现实的碰撞不断探索自己的意义，最终走向心智成熟。 还是那句话，只要还年轻，对未来的恐慌、对成功的渴望、对未知的不安、对想要而不得的不甘……这些弯路，该经历的都一定会经历，该焦虑的也一样都不会少，无处可逃。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[维特比算法]]></title>
      <url>%2F2017%2F03%2F02%2F%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[维特比算法（Viterbi algorithm）是在一个用途非常广的算法，本科学通信的时候已经听过这个算法，最近在看 HMM（Hidden Markov model） 的时候也看到了这个算法。于是决定研究一下这个算法的原理及其具体实现，如果了解动态规划的同学应该很容易了解维特比算法，因为维特比算法的核心就是动态规划。 对于 HMM 而言，其中一个重要的任务就是要找出最有可能产生其观测序列的隐含序列。一般来说，HMM问题可由下面五个元素描述 12345观测序列（observations）：实际观测到的现象序列隐含状态（states）：所有的可能的隐含状态初始概率（start_probability）：每个隐含状态的初始概率转移概率（transition_probability）：从一个隐含状态转移到另一个隐含状态的概率发射概率（emission_probability）：某种隐含状态产生某种观测现象的概率 下面以维基百科上的具体例子来说明 想象一个乡村诊所。村民有着非常理想化的特性，要么健康要么发烧。他们只有问诊所的医生的才能知道是否发烧。 聪明的医生通过询问病人的感觉诊断他们是否发烧。村民只回答他们感觉正常、头晕或冷。假设一个病人每天来到诊所并告诉医生他的感觉。医生相信病人的健康状况如同一个离散马尔可夫链。病人的状态有两种“健康”和“发烧”，但医生不能直接观察到，这意味着状态对他是“隐含”的。每天病人会告诉医生自己有以下几种由他的健康状态决定的感觉的一种：正常、冷或头晕。这些是观察结果。 整个系统为一个隐马尔可夫模型(HMM)。医生知道村民的总体健康状况，还知道发烧和没发烧的病人通常会抱怨什么症状。 换句话说，医生知道隐马尔可夫模型的参数。则这些上面提到的五个元素表示如下 123456789101112131415states = ('Healthy', 'Fever') observations = ('normal', 'cold', 'dizzy') start_probability = &#123;'Healthy': 0.6, 'Fever': 0.4&#125; transition_probability = &#123; 'Healthy' : &#123;'Healthy': 0.7, 'Fever': 0.3&#125;, 'Fever' : &#123;'Healthy': 0.4, 'Fever': 0.6&#125;, &#125; emission_probability = &#123; 'Healthy' : &#123;'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1&#125;, 'Fever' : &#123;'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6&#125;, &#125; 其对应的状态转移图如下所示 现在的问题是假设病人连续三天看医生，医生发现第一天他感觉正常，第二天感觉冷，第三天感觉头晕。 于是医生产生了一个问题：怎样的健康状态序列最能够解释这些观察结果。维特比算法解答了这个问题。 首先直观地看这个问题，在HMM中，一个观测现象后面的对应的各个状态都有一个概率值，我们只需要选择概率值最大的那个状态即可，但是这个概率值是跟前面一个状态有关的（马尔科夫假设），因此不能独立考虑每个观测现象。 为了从时间复杂度方面进行比较，现在将问题一般化：假设观测序列的长度为 m，隐含状态个数为 n。则有下面的隐含状态转移图（下图为了便于表示，将只画出n = 3 的图）。 假如采用穷举法，穷举出所有可能的状态序列再比较他们的概率值，则时间复杂度是 $O(n^m)$, 显然这样的时间复杂度是无法接受的，而通过维特比算法能把时间复杂度降到 $O(m*n^2)$ 从动态规划的问题去考虑这个问题，根据上图的定义，记 last_state 为上一个观测现象对应的各个隐含状态的概率，curr_state 为现在的观测现象对应的各个隐含状态的概率。则求解curr_state实际上只依赖于last_state。而他们的依赖关系可通过下面的 python 代码表示出来 12345for cs in states: curr_state[cs] = max(last_state[ls] * transition_probability[ls][cs] * emission_probability[cs][observation] for ls in states) 计算过程利用了转移概率 transition_probability 和发射概率 emission_probability，选出那个最有可能产生当前状态 cs 的上一状态 ls。 除了上面的计算，同时要为每个隐含状态维护一个路径 path， path[s] 表示到达状态 s 前的最优状态序列。通过前面的计算选出那个最有可能产生当前状态 cs 的上一状态 ls后，往path[cs] 中插入 ls 。则依照这种方法遍历完所有的观测序列后，只需要选择 curr_state 中概率值最大的那个 state 作为最终的隐含状态，同时从 path 中取出 path[state] 作为该最终隐含状态前面的状态序列。 从上面的分析可知，观测序列只需要遍历一遍，时间复杂度为 $O(m)$，而每次要计算当前各个状态最可能的前一状态，时间复杂度为 $O(n^2)$,因此总体的时间复杂度为 $O(m*n^2)$. 假如在 NLP 中应用 HMM，则将词序列看做是观测到的现象，而词性、标签等信息看做是隐含状态，那么就可以通过维特比算法求解其隐含状态序列，而这也是 HMM 在分词，词性标注，命名实体识别中的应用。其关键往往是找出上面提到的初始概率（start_probability）、转移概率（transition_probability）、发射概率（emission_probability）。 而在通信领域中，假如将收到的编码信息看作是观测序列，对应的解码信息为隐含状态，那么通过维特比算法也能够找出概率最大的解码信息。 需要注意的是维特比算法适用于多步骤多选择的最优问题，类似于下面的网络，《数学之美》中将其叫做“篱笆网络(Lattice)”。每一步都有多个选择，并且保留了前面一步各个选择的最优解，通过回溯的方法找到最优选择路径。 这里要强调的是 viterbi 算法可以用于解决 HMM 问题，但是也可以用于解决其他符合上面描述的问题。 最后，上文中的完整的代码见这里 参考：维特比算法]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习基石--学习的可行性]]></title>
      <url>%2F2017%2F02%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3--%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%2F</url>
      <content type="text"><![CDATA[本文是《机器学习基石》第四讲 Feasibility of Learning 的课程笔记。通过 Hoeffding 不等式，引出了机器学习中学习的可行性。 刚刚接触机器学习的时候，往往会被各种机器学习算法搞得昏头涨脑，却往往忽略了一个问题：那就是机器学习中的“学习”二字到底指的是什么，或者说机器为什么能够学习，到底学到了什么东西？ 要回答这个问题，首先要确认一个大前提：数据集（包括训练集和测试集）是从相同分布中产生的，也就是说产生数据的环境应该是一致的，否则假如训练集与数据集的产生方式不一样，那么从训练集中是不可能学习到训练集中相关的知识的 有了上面这个大前提，从概率论的角度来讲，机器学习学的就是这个概率分布，；或者说是模式识别中的模式（pattern）；然后用学习到的概率分布去预测未见过但是也是在这个概率分布下产生的样本，这样一来，便称机器能够“学习”了。 下面的内容便是将这一过程通过数学来严谨化 Hoeffding 不等式为了引入 Hoeffding 不等式，首先来看一下概率论中一个简单的例子：假如一个罐子中有绿色和橙色两种弹珠，现在想知道罐子中橙色弹珠的比例，该怎么做？ 最直观的方法就是将罐子中所有的弹珠分类并计数，然后计算橙色弹珠的比例。但是当罐子中的弹珠数目变得很大的时候，在实际中显然是无法将所有弹珠都数一遍。 这时便需要进行抽样并从抽出的样本（sample）中估计橙色弹珠的比例，但是抽样一定会带来一定的误差的，而且直观上来看，抽样的样本数目越多，误差越小。而 Hoeffding 不等式就是描述这个误差跟抽样数目的关系，假如橙色弹珠的真实比例为 $\mu$ , 而从样本中估计出的比例为 $\nu$， 样本大小为 $N$, 则对应的 Hoeffding 不等式如下 $$p(|\nu - \mu| \gt \epsilon) \le 2\exp(-2\epsilon^2N)$$ 上式中的 $\epsilon$ 表示允许的误差范围 从 Hoeffding 不等式到机器学习假如将上面的罐子中的一个弹珠抽象为机器学习中的一个样本，考虑一个二分类问题，绿色弹珠表示样本标签与我们的模型 $h$ 预测出的标签一致，而橙色弹珠则表示样本标签与预测标签不一致。则橙色弹珠的比例就是模型 $h$ 的错误率。同时将模型 $h$ 在全部弹珠中的错误率记为 $E_{out}(h)$, 而在样本中的错误率记为 $E_{in}(h)$，则根据 Hoeffding 不等式有 $$p(|E_{in}(h) - E_{out}(h)| \gt \epsilon) \le 2\exp(-2\epsilon^2N)$$ 也就是说，训练样本的数目越大，$E_{in}(h)$ 和 $E_{out}(h)$， 也就是训练误差和泛化误差越接近。 这一等式实际上代表了 PAC (probaly approximately correct) 学习理论中的 probably 部分，PAC 理论简单描述如下(摘自 Wikipedia) In this framework, the learner receives samples and must select a generalization function (called the hypothesis) from a certain class of possible functions. The goal is that, with high probability (the “probably” part), the selected function will have low generalization error (the “approximately correct” part). 上面的 Hoeffding 不等式只是说明了训练误差和泛化误差可以很接近，但是这一接近必须要在训练误差也就是 $E_{in}(h)$ 很小的情况下才有意义，否则大的训练误差就有大的范化误差，而大的范化误差的模型实际上是没有意义的。而小的训练误差对应到 PAC 中的 AC(approximately correct) 部分。 从一个 hyposthesis 到多个 hypothesis上面的 Hoeffding 不等式描述的是一个 hypothesis 也就是一个 $h$ 的情况，但是实际中往往有多个 hypothesis 可选，这个时候的 Hoeffding 不等式又会变成怎样？ 首先回顾一下单个 $h$ 的 Hoeffding 不等式，它告诉我们下面这个事情发生的概率很大：$h$ 在抽取的样本上得到的样本误差（也就是训练误差）跟 $h$ 的总体误差（也就是泛化误差）很接近。 从另一个角度来讲，也就是说还是有很小的概率抽出一些样本，使得 $h$ 在样本上得到的误差与其在总体上得到的误差相差很大（实际上就是抽出的样本不能很好反映总体），讲义中将这部分的 sample 称为 bad data，就是使得 $h$ 的 $E_{in}(h)$很小，$E_{out}(h)$ 很大的样本。如下图所示，如果进行多次抽样，那么肯定有一些样本会导致 $E_{in}(h)$和 $E_{out}(h)$ 的差距较大。 注：这里 $E_{in}(h)$ 很小，$E_{out}(h)$ 很大其实已经是我们常听到的过拟合现象，影响过拟合的因素有很多，而抽样的数据的分布是否能够代表整体数据的分布则是其中一个因素。下面是一个简单的例子：对于一个高斯分布产生的数据，如果抽样数据是图中的黑色点，那么拟合出来的曲线可能是图中的黑线，也就是说假如抽样数据的分布如果跟原始数据分布不一致，我们的模型拟合了抽样的数据，对于原始数据而言，自然没有预测能力，也就是$E_{in}(h)$ 很小，$E_{out}(h)$ 很大，可以说是过拟合了抽样的数据。 回到讨论的话题，如果对于 $M$ 个hypothesis 呢？上图可以改为如下形式 在上图中，由于每个 hypothesis 都不同，因此对各个 hypothesis 而言其 bad data 也不同，只有当样本对各个 hypothesis 而言都不是 bad 的时候，才不会泛化误差和训练误差差距很大的情况。在有 $M$ 个 hypothesis 的时候，用 Hoeffding 不等式表示选择了 bad data 的概率为 上面的不等式表明，对于有限多个 hypothesis 而言，$E_{in}(h) \approx E_{out}(h)$ 还是 PAC 的，只是两者误差的 upper bound 变大了，但是数据量 $N$ 的增大能够抵消这一影响。 在上面的前提下，在有多个 hypothesis 的情况下，只需要选择 $E_{in}(h)$ 小的，就能拿保证 $E_{out}(h)$ 也是小的，也就是学习是可行的。 小结本文主要是通过 Hoeffding 不等式证明了当模型的所有 hypothesis 的个数 $M$ 为有限个时，样本数目 $N$ 足够大时，就能够保证泛化误差 $E_{out}(h)$ 和训练误差 $E_{in}(h)$ 很接近。 这时候只要找到一个 hypothesis 使得 $E_{in}(h)$ 很小，那么 $E_{out}(h)$ 也会很小，从而达到学习的目的。 当然有一个大前提就是训练样本和测试样本必须要在同一分布下产生，否则学习无从谈起。 上面的内容可通过下图进行描述 但是还有一个问题，就是实际中某个模型空间里的 hypothesis 往往是无限多个的，这种情况下又该如何通过数学描述？这部分内容将在后面讲述。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习基石--机器学习的分类]]></title>
      <url>%2F2017%2F02%2F26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3--%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%88%86%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[本文是《机器学习基石》第三讲 Types of Learning 课程的笔记。主要概括性介绍了机器学习的几个分类标准及其具体分类。 根据输出来分根据模型输出来对机器学习分类是最常见的分类方法，往往可以分为回归（regression）和分类（classification）两大问题，而根据类比数量的不同，分类问题又往往又有二分类和多分类两种。 除了分类和回归，讲义中还提到了一种 Structured Learning,这一类型的输出的信息之间有结构化的信息。如输入 “I love ML”, 输出该句子中各个词对应的词性（如PVN，PVP，NVN等），可以粗略将其看作是一个多分类问题，只是这个多分类问题的类别非常多且类比没有明确的定义。用到这一类型学习方法的例子还包括语音识别（语音-&gt;句子）等 根据样本来分根据给出的样本是否有标记对机器学习进行分类也是一种常见的分类方法，可分为有监督学习（supervised），无监督学习（unsupervised）和半监督学习（semi-supervised）。 有监督学习就是除了给出样本的属性 $x$ 外，还给出了样本的标记 $y$,这个标记可以是样本的分类等。常见的分类、回归等一般都是有监督学习。 而无监督学习则是只给出给出样本的属性 $x$，让后要求找出这些样本内在属性及联系。其代表的应用是聚类，如给出若干无标记的文章，根据其主题将其聚成不同的类，最常见的聚类方法是K-Means，但是解决文本聚类问题一般通过主题模型，常见的有LSA，pLSA，LDA，HDP等；除此之外，无监督学习还应用到密度估计（density estimation）中，如根据交通事故的发生地点做密度估计，从而得到危险的地段，一般解决这类问题可通过混合高斯模型等；无监督学习还可应用在异常点检测（outlier detection）中，例如从海量的用户日志中找到某个可疑的用户操作，解决这类问题的方法也有很多，比如通过PCA映射到低维度后通过可视化来找。 无监督学习解决了有监督学习中需要获取大量标记样本带来的困难，而半监督学习则是介于两者之间的一种方法。半监督学习主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。如 active learning 就是半监督学习的一种，其思想就是让机器标注数据然后对于有疑问的标记进行“提问”。 除了上面提到的三种的学习方法，讲义还提到了另外一种不那么明显依赖于样本的学习方法 Reinforcement Learning：强化学习(增强学习)，这种学习方法通过奖励或惩罚来训练当前的算法。常见的应用有机器人以及各种 AI Game（如AlphaGo，Fine Art等） 根据训练方式来分这里根据训练方式主要是指训练时数据的输入方式，根据数据是否一次性送入到模型中训练将其分为 batch learning 和 online learning。 batch learning 指数据整批输进去，训练出一个模型用于预测等。 而 online learning 指每次有新样本的时候就用来训练更新 hypothesis，常见的比如说有垃圾邮件分类系统，这里有两点需要注意： 一是这种方法往往是依赖于训练算法的，如 SGD 等就适用在 online learning 中，因为其每次重新训练只需要依靠新的样本即可，而其他一些算法如果要加入新的数据就需要将所有的数据重新进行训练，这样的算法如果用在 online learning 中的代价就太大了。 二是虽然说每次有新的样本就训练更新 hypothesis，但是也不是来一个就更新一下，这样的训练成本也很高，实际中往往是等样本数积累到一定数量的时候才对这一批进行一个训练和更新。就像 gradient descent 中的 mini-batch。 根据输入来分根据输入的样本的特征来分也可以分为下面三类（虽然这中分类方法并不常见）：concrete features，raw feaures 和 abstract features。 concrete features 指输入的样本已经明确给出了其各种特征，如信用卡例子中顾客的各项资料等。 raw feaures 一般指图像或音频中的图像或声波，这些信息是原始的信号，需要进行一些转换才能使用。 abstract features 并没有一个严格定义，原讲义给出了KDDCup 2011 的例子： given previous (userid, itemid, rating) tuples, predict the rating thatsome userid would give to itemid 这种按照输入样本的 features 进行分类的方法在实际中并不常用，因为输入的样本往往是各种 features交杂在一起的，不同问题需要与其相应的 features 才能得到好的效果，features 对结果的影响比较大。因此机器学习中也产生了 feature engineering 一说。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习基石--PLA]]></title>
      <url>%2F2017%2F02%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3--PLA%2F</url>
      <content type="text"><![CDATA[本文是《机器学习基石》第二讲 Learning to Answer Yes/No 课程的笔记。主要介绍了机器学习的基本概念，以及感知机及其训练算法 PLA。 机器学习的基本概念该讲以根据客户的特征来决定是否给客户发放信用卡的例子引出了机器学习要解决的问题：对于我们的问题，存在着一个未知的理想目标函数 $f$ 能够满足我们的决策需求(在该例子中就是根据给定用户的特征，输出是否给用户发放信用卡)，但是这个函数是未知的，我们只能从观测到的数据集 $D$ 中通过算法 $A$ 提取出一个近似的函数 $g$ 来逼近理想的目标函数 $f$。而当选定了算法 $A$ 后，选取 $g$ 的集合 $H$ 实际上也确定了(例如目标函数是线性或非线性的)。 下图展示了上面提到的过程 从上面概括了关于机器学习中的基本过程。但是在实际中通过机器学习解决问题的过程往往是的论述可知，机器学习中首先需要确定目前的一个问题，然后根据问题提出假设并依照假设去搜集数据，然后对数据进行特征提取、转换等，接着尝试通过不同算法去建模并验证。在这门课程中往往更关心数据特征的提取以及不同算法的研究，这固然很重要，但是实际中确认问题以及搜集相应的数据也是一个非常重要的步骤，不可忽视。 下面要介绍这门课程的第一个算法： 感知机感知机是神经网络的基础，与线性回归（Linear Regression），逻辑回归（Logistics Regression）等模型也非常类似，是一种非常典型的线性模型。 原始的感知机算法用于解决二分类问题，其思想如下：假设样本有 $d$ 个特征，但是每个特征的重要性不一样，因此各个特征的权重也不一样，对其进行加权后得到的总和假如大于某个阈值则认为归为其中一类，反之归为另一类。如在信用卡的例子中，通过感知机有如下的结果 将上面的化为常数项，即可将得到更一般的表达形式如下 上面的 $w$ 和 $x$ 均是一个列向量。 PLA上面只是说明了感知机这一算法的基本模型，但是感知机还要通过学习才能对样本进行正确的分类，这个学习的过程就是我们下面要讲的 PLA(Perceptron Learning Algorithm)。PLA的过程如下 1）随机初始化参数 $w$2）利用参数 $w$ 预测每个样本点的值并与其实际的值比较，对于分类错误的样本点$(x_n, y_n)$，利用公式 $w = w + y_nx_n$ 更新参数 $w$ 的值3）重复上面的过程直到所有的样本点都能够被参数 $w$ 正确预测。 对于某个被预测错误的样本点，参数 $w$ 更新的过程如下所示 注意上面的算法的前提是所有的样本点都必须线性可分，假如样本点线性不可分，那么PLA按照上面的规则会陷入死循环中。如下是线性可分与线性不可分的例子) 收敛性上面提到只有当所有的样本均为线性可分时，PLA才能将所有的样本点正确分类后再停下了，但是这仅仅是定性的说明而已，并没有严格的数学正面来支撑其收敛性，下面要讲的便是通过数学证明来说明 PLA 算法的收敛性。 建议中通过下面两页PPT来说明PLA的收敛性 上面讲的是随着参数 $w$ 的更新，$w_f^Tw_{t+1}$ 的值越来越大，也就是两者越来越相似（衡量两个向量相似性的一种方法就是考虑他们的内积，值越大，代表两者约接近，但是这里还没对向量归一化，所以证明并不严格，但是已经说明了两者具有这个趋势，下面是更严格的过程） 上面似乎只是说明了经过 T 次的纠错，$w_t$ 的值会限制在一个范围内，但是并没有给出最终结论 $$\frac{w_f}{||w_f|| }\frac{w_T}{||w_T||} \ge \sqrt{T} * constant$$的证明过程，因此这里对其推导过程进行描述。(注：这里的 $w_f$ 是不变的，因此 $w_f$ 与 $w_f^T$ 是一样的) 假设经过了 T 次纠错，那由第一张PPT可知$$w_f^Tw_T \ge w_f^Tw_{T-1} + \min_n y_nw_f^Tx_n \ge T\min_n y_nw_f^Tx_n$$ 而由第二张PPT可知 $$||w_T||^2 \le ||w_{T-1}||^2 + \max_n||x_n||^2 \le T\max_n||x_n||^2\||w_T|| \le \sqrt{T} \max_n||x_n||$$ 综合上面的两条式子有 $$\frac{w_f^T}{||w_f^T||}\frac{w_T}{||w_T||} \ge \frac{T\min_n y_n^Tw_f^Tx_n}{||w_f^T||\sqrt{T} \max_n||x_n||} = \sqrt{T} \frac{\min_n y_n\frac{w_f^T}{||w_f^T||}x_n}{\max_n||x_n||} = \sqrt{T} * constant$$ 因此上面的命题得证。至此，已经可知道犯错误的次数 T 是受到某个上限的约束的。下面会给出这个具体的上限是多少。 又因为 $$1 \ge \frac{w_f^T}{||w_f^T||}\frac{w_T}{||w_T||} \ge \sqrt{T} * constant\\\frac{1}{constant^2} \ge T$$ 即犯错的次数的上限为 $\frac{1}{constant^2} $, 假设令$$ \max_n||x||^2 = R^2, \rho = \min_n y_n\frac{w_f^T}{||w_f^T||}x_n$$则有$$T \le \frac{R^2}{\rho^2}$$ 这也说明了PLA会在有限步内收敛，而这也是后面的练习题里面的答案。 优缺点及改进PLA 的优点和缺点都非常明显，其中优点是简单，易于实现，但是缺点是假设了数据是线性可分的，然而事先并无法知道数据是否线性可分的。正如上面提到的一样，假如将PLA 用在线性不可分的数据中时，会导致PLA永远都无法对样本进行正确分类从而陷入到死循环中。 为了避免上面的情况，将 PLA 的条件放宽一点，不再要求所有的样本都能正确地分开，而是要求犯错的的样本尽可能的少，即将问题变为了$$arg\min_w \sum_{n=1}^{N} 1\lbrace y_n \ne sign(w^Tx_n) \rbrace$$ 这个最优化问题是个 NP-hard 问题，无法求得其最优解，因此只能求尽可能接近其最优解的近似解。讲义中提出的一种求解其近似解的算法 Pocket Algorithm。其思想就是每次保留当前最好的 $w$, 当遇到错误的样本点对 $w$ 进行修正后，比较修正后的$w$ 与原来最好的 $w$ 在整个样本点上的总体效果再决定保留哪一个，重复迭代足够多的次数后返回当前得到的最好的 $w$。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[概率论与数理统计知识整理(6)-参数估计]]></title>
      <url>%2F2017%2F02%2F18%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(6)-%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%2F</url>
      <content type="text"><![CDATA[在数理统计中，常常需要通过样本来估计总体的参数，估计可划分为两大类：点估计和区间估计。点估计就是估计总体中某个参数的值，而区间估计是估计总体的某个参数落在某个区间的概率大小。本文主要讲述点估计中的矩估计法和最大似然估计法，以及针对服从正态分布的期望和方差进行区间估计。 点估计点估计一般解决的问题是总体 $X$ 的分布函数 $F(X,\theta)$ 形式为已知，但是 $\theta$ 参数未知。点估计的目的就是通过样本 $X_1,X_2,…X_n$ 构造一个适当的统计量 $\theta’(X_1,X_2,…X_n)$，用于作为未知参数 $\theta$ 的近似值。由于 $\theta’$ 是样本的函数，因此对于不同的样本，$\theta’$ 的值一般不同。 点估计中一般用到的方法包括矩估计法和最大似然估计法。 矩估计法矩估计法的核心思想是样本矩总是依概率收敛于相应的总体矩，因此可通过样本矩作为相应的总体矩的估计量，进而根据总体矩与待估参数的关系求出待估参数。 矩估计法的一般描述如下：设 $X$ 为连续型随机变量，其概率密度函数为 $f(x;\theta_1, \theta_2,..\theta_k)$；离散型随机变量，其分布律为$P(X=x) = p(x; \theta_1, \theta_2,..\theta_k)$；则总体的 $k$ 阶矩分别为$$\mu_k = E(X^k) = \int_{-\infty}^{\infty} x^kf(x;\theta_1, \theta_2…\theta_k) dx$$$$\mu_k = E(X^k) = \sum_{x \in R_x} x^kp(x;\theta_1, \theta_2….\theta_k)$$ 上式中的 $R_x$ 是 $X$ 可能取值的范围；上面是总体的 k 阶矩的定义，但是实际估计时，往往到只需要使用其一阶矩和二阶矩，也就是 $E(X)$ 和 $E(X^2)$。 而样本 $X_1, X_2…X_n$ 的 $k$ 阶矩的定义为 $$A_k = \frac{1}{n} \sum_{i=1}^{n}X_i^k$$ 由于总体的 k 阶矩往往是未知参数 $\theta$ 的函数，因此常常先用总体的 k 阶 $\mu_k$ 矩将参数 $\theta$ 表示出来，然后用样本矩 $A_k$ 代替 $\mu_k$,进而得出估计的 $\theta$ 的值。下面是一个简单的例子 最大似然估计法最大似然估计的思想是既然当前取得了这组样本，那么有理由相信已取得的样本出现的概率是很大的。因此通过极大化这组样本的联合概率来估计未知参数的值。 离散型总体单总体为离散型的时候，设当前样本为 $X_1,X_2,…X_n$， 则其联合概率为 $\prod_{i=1}^{n} p(x_i;\theta)$, 其中 $x_i$ 是 $X_i$ 相应的观测值，则上面的联合概率实际上是参数 $\theta$ 的函数，记为$$L(\theta) = \prod_{i=1}^{n} p(x_i;\theta)$$上面的 $L(\theta)$ 被称为样本的似然函数。 选择 $\theta$ 的值使得 $L(\theta)$ 最大便是最大似然估计做的事情。一般通过对 似然函数求导便可求得其最大值对应的 $\theta$。如下是一个简单的例子 上面最后求解的结果是 $p’ = \overline x$。同时也注意到求解似然函数最大化时会先对似然函数取 $log$ , 目的是将连乘变为连加，方便运算，同时这种方法也被称为对数极大似然估计。 连续型总体若总体是连续型，设其概率密度函数为 $f(x,\theta)$，则当前样本 $X_1,X_2,…X_n$ 的联合概率密度函数为$$\prod_{i=1}^{n}f(x_i;\theta)$$其中 $x_1,x_2,…x_n$ 是相应于样本的一个样本值，则随机点落在 （$x_1,x_2,…x_n$）的领域（边长为 $dx_1, dx_2,…dx_n$的n维立方体）内的概率近似为$$\prod_{i=1}^{n}f(x_i;\theta)dx_i$$ 同样我们要让上式取到最大，但是因子 $\prod_{i=1}^{n}dx_i$ 不随 $\theta$ 改变，因此只需考虑函数$ L(\theta) = \prod_{i=1}^{n}f(x_i;\theta)$最大即可，这里 $L(\theta)$ 被称为似然函数，极大化也是通过求导来解决。 下面是一个连续型总体进行极大似然估计的例子 评选标准对于同一参数，不同的估计方法求出的估计量可能不一样，那么如何判断不同的估计量之间的优劣，无偏性，有效性和相合性是常用的三个指标。 无偏性无偏性指的是从样本中得到的估计量 $\theta’$ 的期望与总体的参数 $\theta$ 相等，也就是 $$E(\theta’) = \theta$$此时称 $\theta’$ 是 $\theta$ 的无偏估计量。无偏估计量的意义是对于某些样本值，这一估计量得到的估计值比真实值要打，而对于另外一些样本则偏小，反复将这一估计量使用多次，就平均来说其偏差为零。 有效性当两个估计量 $\theta_1’, \theta_2’$ 均是无偏估计量时，就要通过比较他们的有效性来决定选取哪个估计量。有效性指的是在样本容量 $n$ 相同的情况下，假如 $\theta_1’$ 的观察值较 $\theta_2’$ 的值更密集在真值 $\theta$ 附近，那么认为$\theta_1’$ 比 $\theta_2’$ 更为理想。 实际上，上面比较的就是两个估计量的方差大小，方差越小，则越有效，因此当两个总体的样本数相同的时候，若 $D(\theta_1’) &lt; D(\theta_2’)$ 时， 就称 $\theta_1’$ 比 $\theta_2’$ 更有效。 相合性当样本数目 $n \rightarrow \infty$ 时，估计量 $\theta’(X_1，X_2…X_n)$ 依概率收敛于真正的 $\theta$ ,则称 $\theta’$ 为 $\theta$ 的相合估计量。即有以下式子成立$$ \lim_{n \rightarrow \infty}P(|\theta’ - \theta| &lt; \epsilon) = 1$$ 相合性是一个估计量的基本要求，如果估计量没有相合性，那么无论样本数量 n 取多大，这些估计量都无法准确估计正确参数，都是不可取的。 区间估计对于总体中的未知参数，我们的估计总是存在着一定的误差的，如何去衡量这个误差是一个需要考虑的事情。同时，除了上面的点估计，在实际中我们往往还希望估计出参数的一个范围，同时参数落在这个范围的概率，或者是说可信程度。 估计参数落在某个范围以及落在这个范围的可信程度就是区间估计干的事情。 其严格定义如下 设总体的分布中存在一个未知参数 $\theta$, 对于给定的值 $\alpha(0 &lt; \alpha &lt;1)$, 若通过样本 $X_1,X_2,X_3…X_n$ 估计的两个统计量 $\theta’_1$ 和 $\theta’_2$满足下面不等式时$$P(\theta’_1 &lt; \theta &lt; \theta’_2) \ge 1 - \alpha$$则称区间 $(\theta’_1, \theta’_2)$ 是参数 $\theta$ 置信水平为 $1-\alpha$ 的置信区间, $\theta’_1, \theta’_2$ 分别称为置信下限和置信上限。 上面式子的含义是若反复抽样多次（每次得到的样本的容量相等），每个样本值确定一个区间$(\theta’_1, \theta’_2)$，这个区间要么包含 $\theta$ 的真值，要么不包含 $\theta$ 的真值,在这么多的区间中，包含 $\theta$ 真值的约占 $1-\alpha$. 正态分布均值与方差的区间估计由于正态分布的普遍性，下面主要讲述对正态分布的期望和方差进行区间估计的方法，而这里会用到我们前面讲到的统计量的三大分布： $\chi^2$ 分布， $t$ 分布， $F$ 分布，以及对其拓展的一些定理，具体的定理及其证明可参考这篇文章。 下面会讲述单个正态分布的期望和方差的估计，以及两个正态分布的期望差和方差比的估计。 单个正态分布下面的关于单个正态分布的讨论都是基于以下假设：给定置信水平为 $1-\alpha$, 设 $X_1,X_2,X_3…X_n$ 为总体 $N(\mu, \sigma^2)$ 的样本，$\overline X，S^2$ 分别是样本的期望和方差。 估计期望 $\mu$ 的置信区间通过样本 $X_1,X_2,X_3…X_n$ 估计总体 $N(\mu, \sigma^2)$ 的期望 $\mu$ 时可以分为两种情况： 总体的方差 $\sigma^2$ 已知 总体的方差 $\sigma^2$ 未知 总体的方差 $\sigma^2$ 已知若已知总体的方差，则因为 $\overline X \sim N(\mu , \sigma^2/n)$, 即$\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1)$, 下面都会这样不加证明给出这些统计量服从的分布，具体的证明参考这篇文章。 按照标准正态分布的上 $\alpha$ 分位点的定义有$$P(|\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} | &lt; z_{\alpha/2}) = 1 - \alpha$$ 从概率密度函数上直观看为： 进一步化简有$$P(\overline X - \frac{\sigma}{\sqrt{n}}z_{\alpha/2} &lt; \mu &lt; \overline X + \frac{\sigma}{\sqrt{n}}z_{\alpha/2}) = 1 - \alpha$$ 给定 $\alpha, z_{\alpha/2}$ 的值可以通过查表获得。这样便得到了期望 $\mu$ 的一个估计区间为 $(\overline X - \frac{\sigma}{\sqrt{n}}z_{\alpha/2}, \overline X + \frac{\sigma}{\sqrt{n}}z_{\alpha/2})$, 其置信度为 $1 - \alpha$。注意置信水平为 $1 - \alpha$ 的置信区间并不是唯一的，假如说给定 $\alpha = 0.05$, 则上面的式子可写为$$P(\overline X - \frac{\sigma}{\sqrt{n}}z_{0.025} &lt; \mu &lt; \overline X + \frac{\sigma}{\sqrt{n}}z_{0.025}) = 1 - \alpha$$ 同时也可写为$$P(\overline X - \frac{\sigma}{\sqrt{n}}z_{0.04} &lt; \mu &lt; \overline X + \frac{\sigma}{\sqrt{n}}z_{0.01}) = 1 - \alpha$$ 但是写成不对称的形式计算出来的区间长度要更长，显然，置信度相同的情况下，置信区间肯定是越小越好，所以对于正态分布的分位点往往选择对称形式。 下面的求解方法与这方法类似，只是构造的统计量不同，因而服从的分布也不同。 总体的方差 $\sigma^2$ 未知 当总体方差未知时，就无法利用上面标准正态分布。但是回忆 $t$ 分布的作用及其定理，可知 $$\frac{\overline X - \mu}{S/\sqrt{n}} \sim t(n-1)$$ 同样按照 $t$ 分布的上 $\alpha$ 分位点的定义有$$P(|\frac{\overline X - \mu}{S/\sqrt{n}}| &lt; t_{\alpha/2}(n-1)) = 1 - \alpha$$ 其对应的概率密度函数如下所示 进一步化简可得$$P(\overline X - \frac{S}{\sqrt{n}}t_{\alpha/2}(n - 1) &lt; \mu &lt; \overline X + \frac{S}{\sqrt{n}}t_{\alpha/2}(n - 1)) = 1 - \alpha$$ 则期望 $\mu$ 的一个置信水平为 $1-\alpha$ 的置信区间为$$(\overline X - \frac{S}{\sqrt{n}}t_{\alpha/2}(n - 1), \overline X + \frac{S}{\sqrt{n}}t_{\alpha/2}(n - 1))$$ 估计方差 $\sigma^2$ 的置信区间估计方差 $\sigma^2$ 的置信区间也可分为两种情况 总体的期望 $\mu$ 已知 总体的期望 $\mu$ 未知 总体的期望 $\mu$ 已知 当期望 $\mu$ 已知时，求解方差 $\sigma^2$ 的置信区间的方法跟上面已知方差 $\sigma^2$ 求解期望 $\mu$ 的一样，都是利用 $\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1)$，然后写出对应未知量的区间，这里就不详细讲述已知 $\mu$ 求解方差 $\sigma^2$ 的详细过程了。 总体的期望 $\mu$ 未知 当期望 $\mu$ 未知时，求解方差 $\sigma^2$ 的区间估计就再也不能利用上面的 $\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1)$。结合 $\chi^2$ 分布的特性及其推导的定理可知 $$\frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)$$ 同样按照 $\chi^2$ 分布的 $\alpha$ 分位点的定义有 $$P( \chi^2_{1 - \alpha/2}(n-1) &lt; \frac{(n-1)S^2}{\sigma^2} &lt; \chi^2_{\alpha/2}(n-1)) = 1 - \alpha$$ 注意这里不能用绝对值了，原因是 $\chi^2$ 分布的概率密度函数不像标准正态分布或 $t$ 分布那样是对称的。其对应的概率密度函数如下所示 进一步化简可得 $$P(\frac{(n-1)S^2}{\chi^2_{\alpha/2}(n-1)} &lt; \sigma^2 &lt; \frac{(n-1)S^2}{\chi^2_{1 - \alpha/2}(n-1)}) = 1 - \alpha$$ 即给定样本，总体期望 $\mu$ 未知的时候，总体方差 $\sigma^2$ 的一个置信水平为 $1-\alpha$ 的置信区间为 $$(\frac{(n-1)S^2}{\chi^2_{\alpha/2}(n-1)}, \frac{(n-1)S^2}{\chi^2_{1 - \alpha/2}(n-1)})$$ 实际上， $\chi^2$ 分布的一个作用就是在正态总体分布中期望未知时估计其方差的置信区间。 两个正态分布下面讲述两个正态分布的期望差值的区间估计以及方差比的估计。考虑以下问题：已知产品的某一质量指标服从正态分布，但由于原料、操作人员不同，或工艺过程的改变等因素，引起总体均值、方差有所变化。我们需要知道这些变化有多大，就需要考虑两个正态分布均值差或方差比的估计问题。 下面的讨论都是假设给定了置信水平为 $1- \alpha$, 并设 $X_1, X_2,….X_n$ 是来自第一个总体 $N_1(\mu_1, \sigma_1^2)$ 的样本，$Y_1, Y_2,….Y_n$ 是来自第二个总体 $N_2(\mu_2, \sigma_2^2)$ 的样本，并假设 $\overline X, \overline Y$ 是第一、第二个样本的均值，$S_1^2, S_2^2$ 是第一、第二个样本的方差。 估计 $\mu_1 - \mu_2$ 的置信区间估计 $\mu_1 - \mu_2$ 的置信区间时也可以分为两种情况 总体的方差 $\sigma_1^2, \sigma_2^2$ 已知 总体的方差 $\sigma_1^2, \sigma_2^2$ 未知，但是知道 $\sigma_1^2 = \sigma_2^2 = \sigma^2$（$\sigma$未知） 总体的方差 $\sigma_1^2, \sigma_2^2$ 已知由 $\overline X \sim N(\mu_1, \sigma_1^2/n_1), \overline Y \sim N(\mu_2, \sigma_2^2/n_2)$ 可知$$\overline X - \overline Y \sim N(\mu_1 - \mu_2, \sigma_1^2/n_1 + \sigma_2^2/n_2)\ \frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}} \sim N(0, 1)$$ 与上面相同，按照标准正态分布的上 $\alpha$ 分位点的定义有$$P(|\frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}} | &lt; z_{\alpha/2}) = 1 - \alpha$$ 同样可解得 $\mu_1 - \mu_2$ 置信度为 $1-\alpha$ 的区间。 总体的方差 $\sigma_1^2, \sigma_2^2$ 未知，但 $\sigma_1^2 = \sigma_2^2 = \sigma^2$（$\sigma$未知） 根据 $t$ 分布的作用及其推导的定理可知 $$\frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{S_w\sqrt{1/n_1+1/n_2}} \sim t(n_1+n_2-2)$$ 其中 $S_w = \sqrt{\frac{(n_1 -1)S_1^2+(n_2 -1)S_2^2}{n_1+n_2-2}}$ 同样根据 $t$ 分布的上 $\alpha$ 分位点的定义有$$P(|\frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{S_w\sqrt{1/n_1+1/n_2}}| &lt; t_{\alpha/2}(n_1+n_2-2)) = 1 - \alpha$$ 通过查表同样可以求出 $\mu_1 - \mu_2$ 置信度为 $1-\alpha$ 的区间，结合上面 $t$ 分布在单个正态总体分布参数估计的问题可知， $t$ 分布专门用于解决正态分布中方差未知时估计其期望的问题。 估计 $\sigma_1^2 / \sigma_2^2$ 的置信区间估计 $\sigma_1^2 / \sigma_2^2$ 的置信区间同样可以分为两种情况 总体期望 $\mu_1, \mu_2$ 已知 总体期望 $\mu_1, \mu_2$ 未知 总体期望 $\mu_1, \mu_2$ 已知时可以先通过标准正态分布求出 $\sigma_1^2, \sigma_2^2$ 各自的范围, 然后求解 $\sigma_1^2 / \sigma_2^2$ 的范围。下面主要讲总体期望 $\mu_1, \mu_2$ 未知时，如何估计 $\sigma_1^2 / \sigma_2^2$ 的范围。 由 $F$ 分布的定义以及推导的定理可知 $$\frac{S_1^2/S_2^2}{\sigma_1^2/ \sigma_2^2} \sim F(n_1 - 1, n_2 - 1)$$ 根据 $F$ 分布的 $\alpha$ 分位点的定义有 $$P( F_{1 - \alpha/2}(n_1 - 1, n_2 - 1) &lt; \frac{S_1^2/S_2^2}{\sigma_1^2/ \sigma_2^2} &lt; F_{\alpha/2}(n_1 - 1, n_2 - 1)) = 1 - \alpha$$ 化简可得 $$P( \frac{S_1^2}{S_2^2}\frac{1}{F_{\alpha/2}(n_1 - 1, n_2 - 1)} &lt; \frac{\sigma_1^2}{ \sigma_2^2} &lt; \frac{S_1^2}{S_2^2}\frac{1}{F_{1 - \alpha/2}(n_1 - 1, n_2 - 1)}) = 1 - \alpha$$ 即 $\sigma_1^2 / \sigma_2^2$ 一个置信度为 $1-\alpha$ 的置信区间为$$(\frac{S_1^2}{S_2^2}\frac{1}{F_{\alpha/2}(n_1 - 1, n_2 - 1)} &lt; \frac{\sigma_1^2}{ \sigma_2^2}, \frac{S_1^2}{S_2^2}\frac{1}{F_{1 - \alpha/2}(n_1 - 1, n_2 - 1)})$$ 小结在上面对正态分布总体进行参数估计中，用到了数理统计中的三大分布： $\chi^2$分布， $t$分布和$F$分布， 其中 $\chi^2$ 分布主要解决总体期望未知时估计其方差的问题， $t$ 分布主要解决总体方差未知时估计其期望的问题，$F$ 主要解决期望未知时两个正态分布的方差比值问题。 单侧置信区间上面均是讨论未知参数 $\theta$ 的双侧置信区间，但是在实际问题中，往往考虑的只是一个上限或下限，比如说设备、原件的寿命我们关心的是平均寿命 $\theta$ 的下限。这就引出了单侧置信区间的概念。单侧置信区间跟双侧置信区间的概念非常类似。 总体的参数 $\theta$ 未知, 对于给定的 $\alpha$ ,若由样本 $X_1, X_2..X_n$ 确定的统计量 $\theta’$满足$$P(\theta &gt; \theta’) = 1 - \alpha$$则称 $(\theta’, \infty)$ 是参数 $\theta$ 的置信水平为 $1 - \alpha$ 的单侧置信区间，而 $\theta’$ 是单侧置信下限，将 $\theta &gt; \theta’$ 变为 $\theta &lt; \theta’$ 后，相应地变为单侧置信上限。 单侧置信区间的计算方法与上面提到的双侧置信区间的计算方法已知，都是根据给定的 $\alpha$ 值和统计量服从的分布去查表，找到相应的分位点后带入不等式求解目标估计量的范围即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[最优化计算课程总结]]></title>
      <url>%2F2017%2F02%2F01%2F%E6%9C%80%E4%BC%98%E5%8C%96%E8%AE%A1%E7%AE%97%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93%2F</url>
      <content type="text"><![CDATA[本文主要是最优化计算这门课程的课程总结，参考的教材为《最优化计算》，主要讲述的内容是函数优化，相对于函数优化的另外一种优化是组合优化，两者的主要区别是前者的可行解是连续的，后者的可行解是离散的，或者说前者的可行解是无限的，而后者是有限的。 最优化要解决的问题非常直观，就是给定若干个由若干个变量组成的目标函数，然后使得变量取某一组值时目标函数值最大或最小，这时候变量的取值便称为最优解；有时候变量还有一定的约束，如要满足某些等式或不等式，这时候的约束称为有约束优化，以区别于前面的无约束优化。 可以说，最优化是一门纯数学的课程，但是现实世界中，很多问题都可以通过建模然后将问题最终转化为求解一个最优化问题，比如说在机器学习中，很多算法往往有个目标函数，这个目标函数可以用与描述预测结果与实际结果的误差，这时候就要最小化这个目标函数(回归，SVM等)；而最大化的问题通过改变符号也转为最小化问题。因此，最优化在实际中有广泛应用。 根据目标函数的形式、数量以及是否有约束条件可以将优化问题分为多种类型。本文主要讲述的是单目标规划，并且将单目标规划进一步分为括线性和非线性，有约束和无约束。 在讲述具体的优化算法前，先介绍最优化中常用的概念。 最优化的基本模型为 $$min\;f(x)\\s.t\quad c(x) \ge 0\\$$ 其中 $x = (x_1,x_2,x_3,…x_n)$ 是一个变量组成的向量，也就是包含了若干变量，$c(x)$则是对各个变量约束的等式和不等式。 有了基本模型，下面介绍在最优化中的常用概念。 目标函数：就是$f(x)$ 决策变量：目标函数$f(x)$中的所有变量 约束条件：$c(x)$中包含的所有的等式和不等式 可行域：约束条件在空间围成的区域 可行解：可行域中的每个点都是原问题的一个可行点 最优解：能够使目标函数达到最大或最小的可行解 凸集：集合的一种，用于描述可行域，满足以下性质 令 $K$ 为集合，$\forall x_1,x_2 \in K$,若 $\alpha x_1 + (1-\alpha)x_2 \in K$,其中$\alpha \in [0,1])$,则 $K$ 为凸集 凸集直观的图像表示如下 线性规划数学模型与基本概念线性规划就是目标函数和约束条件都是线性的最优化问题，且一般都是带约束的，线性规划的一般模型如下所示 为了模型的一致性，通常会将以上的模型化为标准型，标准型要求1）目标函数求最小值2）约束条件全为等式3）所有的$x$有 $x \ge 0$ 化为标准型中的关键点是将约束条件中的不等式变为等式1）对于约束条件为 $\le$ 的情况，在 $\le$ 左边添加一个松弛变量(非负)。2）对于约束条件为 $\ge$ 的情况，在 $\ge$ 左边减去一个剩余变量(非负)注意：松弛变量、剩余变量在目标函数中的价值系数为0。 如下为一个简单的例子 对于化为标准型的线性规划模型中约束条件的 $m × n$ 系数矩阵$A$，从$A$中取大小为$m × m$的子矩阵$B$，若$Rank(B) = m$(即B为满秩矩阵)，则称$B$为线性规划问题的一个基矩阵。取$B = (A_1,A_2,···,A_m)$ ，其中$A_j = (a_{1j},a_{2j},···,a_{mj})^T$ 则称$x_1,x_2,···,x_m$为基变量，其它为非基变量。如下所示 令所有的非基变量为0，则得到的解称为基本解，可知基本解的个数 $\le C_n^m$。但是基本解不一定满足约束条件，满足约束条件的基本解称为基本可行解，而基本可行解对应的基变量称为可行基。 基本解，基本可行解，可行解，非可行解的关系如下图所示 关于线性规划有三条重要定理 定理1 若线性规划存在可行域，则可行域为凸集定理2 线性规划的基本可行解对应于可行域的顶点定理3 若线性规划有最优解，则一定存在基本可行解为最优解。 上面直观解释和定理都是为了说明这个事实：如果线性规划的最优解存在，那最优解一定在可行域的顶点上。下面要介绍的单纯形法就是利用这个性质。 单纯形法单纯形法是解决线性规划的经典方法。其基本思想是先从可行域的一个顶点出发，然后从当前顶点沿着可行域(可行域是一个凸多面体)的边找下一个使得目标函数更小的顶点，假如找得到就移动到更优的顶点，找不到就说明当前顶点是线性规划的最优解。 因此，单纯形法的主要步骤为1) 确定初始基本可行解2) 判别当前基本可行解是否是最优解3) 从一个基本可行解转换到相邻且改善了的基本可行解 在实际运算的时候，一般通过单纯形表实现。其求解过程如下所示 对于下面已经化为标准型的线性规划问题 其单纯形表为 表中的基指的是基变量，最后一行 $\sigma_j$ 是检验数，用于检验当前的基本可行解是否为最优解。 对应于单纯形表，求解步骤如下（1）确定初始基本可行解。也就是列出如上图所示的初始的单纯形表，将线性规划化为标准型后,通过矩阵的初等行变换,可以使系数矩阵A中包含一个单位阵，而这个单位阵对应的基变量即可作为初始基本可行解。（2）判别当前基本可行解是否是最优解。通过最后一行的检验数 $\sigma_j$ 判断，假如所有非基变量的检验数 $\sigma_j \ge 0$，则基变量为最优解，计算结束；假如存在$\sigma_k \lt 0$,且$A_k \le 0$,则问题为无界解，计算结束；否则转第（3）步（3）从一个基本可行解转换到相邻且改善了的基本可行解。这一步要确定一个入基变量和一个出基变量，入基变量就是从非基变量中选择的一个变量，然后将其变为基变量，出基变量就是从基变量中选择一个变量，然后将其变为非基变量。实际上就是将这两个变量的位置(基或非基)互换，对应于从可行域的一个顶点走到另外一个顶点。 入基变量的确定:从所有的检验数中找出最小的 $\sigma_k$,对应的$x_k$为入基变量。出基变量的确定：通过下式确定的 $l$所对应的的 $x_l$作为出基变量 $$\min_{1 \le i \le m} \lbrace \frac{b_i}{a_{ik}} | a_{ik} &gt; 0 \rbrace$$ 上式中的 $a_{ik}$ 为入基变量对应系数举证 A 的第k列 找到入基变量$x_k$和出基变量$x_l$后，用入基变量替换出基变量，通过初等行变换使得$x_k$对应系数矩阵A的一列成为原单位矩阵中$x_l$对应的那列，其他的值做相应的计算(见下图)，画出的新的单纯形表如下所示： （4）重复步骤（2）和（3），直到找到最优解 注意除了单纯形法以外，解决线性规划还有另外一类方法：内点法。这里不详细展开，具体可以参考这篇论文 Interior Point Methods and Linear Programming。 对偶理论在求解一个规划问题（不限于线性规划）的时候，我们常常需要知道这个问题有没有可行解（有时候约束条件很复杂，不要说最优解，找到可行解都很难），或者是估计一下目前的解离最优解还有多远（大型问题多用迭代解法，如果能大致估计当前的解的质量，就对算法什么时候运行结束有一个大致的把握，如果得到了可接受的近似解也可以提前停止），以及判断原问题的最优解是否无界（万一出现这种情况迭代就停不下来了）。 而对偶问题就是回答这些问题的利器：弱对偶定理给原问题的最优解定了一个界，强对偶定理给出了原问题最优解的一个判定条件。同时，还有很多别的优良性质：例如可以化难为易（把难以求解的约束条件扔到目标函数的位置上去），如果问题的形式合适(变量少，约束多)还可以通过把约束变量和对偶变量互换来把大规模问题转换成小规模问题。实际上，很多凸优化问题都是通过解对偶问题来求解的，线性规划只是其中一个特例而已。 一般地，对于原问题$$min\;z = c^Tx \quad \\s.t.\,Ax \ge b(x \ge 0)\tag{1}$$ 其对偶问题为$$max\;w = b^Ty \quad \\s.t.\,A^Ty \le c(y \ge 0)\tag{2}$$ 根据原问题写出其对偶问题时要注意约束条件和变量的符号变化情况，其变换规则如下 max 问题第 i 个约束取“≥”，则min问题第 i 个变量 ≤ 0 min 问题第 i 个约束取“≤”，则max问题第 i 个变量 ≤ 0 原问题第 i 个约束取等式，对偶问题第 i 个变量无约束 max 问题第 j 个变量 ≤ 0 ,则min问题第j个约束取“≤” min 问题第 j 个变量 ≤ 0 ，则max问题第j个约束取“≥” 原问题第j个变量无约束，对偶问题第j个约束取等式 以上规则是具体的变换规则，实际上其变换规律就是假如原问题不符合以上的给出的(1)或(2)的标准形状，那么原问题的第 i 个不符合要求的约束，对应的对偶问题的第i个变量要 ≤ 0 ；同样假如原问题的第 i 个变量不符合要求(就是 $x_i$≤ 0)，对应的对偶问题的第i个约束要改变符号。 对偶定理包含了一系列的定理，其中主要有弱对偶定理，强对偶定理，最优性定理，互补松弛定理。通过这些定理可以将原来难以解决的原问题通过引入对偶理论从而得以解决，各定理的具体内容如下: 弱对偶定理 max 问题任一可行解的目标值为min问题目标值的一个下界；min 问题任一可行解的目标值为max问题目标值的一个上界 强对偶定理 若原问题有最优解，那么对偶问题也有最优解，且两个问题最优解的目标函数值相等。 无界性 若原问题(对偶问题)为无界解，则对偶问题(原问题)为无可行解。 需要注意的是，无界性的逆不存在。若原(对偶)问题为无可行解，对偶(原问题)问题或为无界解，或为无可行解。 最优性 若 $\overline x$ 和 $\overline y$ 分别为原问题和对偶问题的可行解，那么原问题和对偶问题都有最优解，且当 $c^T\overline x=b^T\overline y$时，$\overline x$ 和 $\overline y$分别为原问题和对偶问题的最优解。 互补松弛定理 若 $\overline x$ 和 $\overline y$ 分别为原问题和对偶问题的可行解，则它们分别是原问题和对偶问题的最优解的充要条件是$\overline x^T(A^T\overline y-c)=0$和$\overline y^T(A\overline x-b)=0$ 通过互补松弛定理，给出原问题(对偶问题)的最优解，便可求得其对偶问题(原问题)的最优解。 对应于单纯形法有对偶单纯形法，其解法与单纯形的一样，只是找出基变量和入基变量的方法不一样。 灵敏度分析在许多实际问题中，数据模型的数据未知，需要根据实际情况进行测量、估计和预测，因此这些数据不是十分精确，数据的略微的变化可能会引起问题解的显著变化。所谓灵敏度分析就是研究输入数据的扰动对LP最优解的影响，或者说是LP最优解对参数变化、约束条件增减、决策变量增减的Robust(稳健性)。 灵敏度分析主要就是考虑问题 $$min\;z = c^Tx \quad \\s.t.\,Ax \ge b(x \ge 0)\tag{1}$$ 中，参数 c,b,A 的变化是否会引起最优解的变化。 $c$ 的变化可分为两种：$c$为非基变量的价值系数和c为基变量的价值系数1.非基变量价值系数 $c_k$的变化假设 $\overline{c_k} = c_k + \Delta c_k$, 则其在单纯形法中的检验数变为$\overline{\sigma_k} = \sigma_k + \Delta c_k $,只需要让$\overline{\sigma_k} \ge 0$ 即 $\sigma_k \ge -\Delta c_k $即可保证最优解不变 2.基变量价值系数 $c_b$的变化假设 $\overline{c_b} = c_b + \Delta c_b$, 则其在单纯形法中的检验数变为$\overline{\sigma_b} = \sigma_b - (0,0,0,..\Delta c_b,…0,0,0)B^{-1}N $,其中$(0,0,0,..\Delta c_b,…0,0,0)$为基变量的价值系数的变化量组成的向量，$B^{-1}N $为单纯形表中非基变量对应的系数列组成的矩阵，同样只需要让$\overline{\sigma_b} \ge 0$ 即可保证最优解不变. b变化的时候需要保证 $\overline{b} = B^{-1}(b+\Delta b) \ge 0$,否则需要将$\overline{b}$ 作为新的b的值代入到原来的单纯形表中，让后通过对偶单纯形法进行求解。对偶单纯形法的步骤与原始单纯形法的步骤非常相似，只是选择出基变量和入基变量的方法不同。出基变量选择为 $b_k = min\;\lbrace b_i,i=1,2,..m\rbrace$所对应的 $x_k$,入基变量选择为下面公式对应的$x_l$, $$\frac{\sigma_l}{a_{kl}} = max\;[\frac{\sigma_j}{a_{kj}}|a_{kj} \lt 0,j=1,2,…n]$$ 非线性规划数学模型与基本概念目标函数或约束函数至少有一个不是决策变量的线性函数。即 $$min f(x)\\s.t\quad h_i(x) = 0(i=1,2,…,m)\\\quad\quad g_j(x) \ge 0(j=1,2,…,l)\\$$其中，$f(x),h_i(x),g_j(x)$ 中至少有一个是非线性函数。 梯度与海塞矩阵梯度和海塞矩阵是在非线性规划中用得较多的概念，其定义如下： 梯度可微函数$f(x)$的梯度，记为$\nabla f(x)$,它是以 $f (x)$ 对 $x$ 的偏导数为元素的n维向量,如下所示$$\nabla f(x) = (\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},….,\frac{\partial f(x)}{\partial x_n})$$而在某一点的梯度就是将这一点的值代入到上式中，如在点$x_0$上的梯度为$$\nabla f(x_0) = (\frac{\partial f(x_0)}{\partial x_1},\frac{\partial f(x_0)}{\partial x_2},….,\frac{\partial f(x_0)}{\partial x_n})$$ 对于一元函数，其梯度就是其一阶导数。 对于任何函数$f(x)$，假如$\overline x$是$f(x)$的局部极小点且$f(x)$在$\overline x$处可微，那么必有$\nabla f(\overline x) =0$ 海塞矩阵海塞矩阵的定义与梯度类似，但是求的是二阶偏导，并且结果是一个矩阵，如下所示 $$\nabla^2 f(x) =\begin{pmatrix}\frac{\partial^2 f(x)}{\partial^2 x_1} &amp; \frac{\partial^2 f(x)}{\partial x_1\partial x_2} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial x_1\partial x_n}\\\frac{\partial^2 f(x)}{\partial x_2\partial x_1} &amp; \frac{\partial^2 f(x)}{\partial^2 x_2} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial x_2\partial x_n}\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\frac{\partial^2 f(x)}{\partial x_n\partial x_1}&amp; \frac{\partial^2 f(x)}{\partial x_n\partial x_2} &amp; \cdots &amp; \frac{\partial^2 f(x)}{\partial^2 x_n}\\\end{pmatrix}$$ 对于任何函数$f(x)$，假如$f(x)$在$\overline x$处有二阶连续偏导，若$\nabla f(\overline x) =0$且海塞矩阵$\nabla^2 f(\overline x)$正定，则 $\overline x$ 为严格局部最小点。 凸函数凸函数的定义如下 设$f(x)$为定义在n维欧氏空间中某个凸集S上的函数，若对于任何实数$\alpha(0&lt;\alpha&lt;1)$以及S中的任意不同两点$x^{(1)}$和$x^{(2)}$，均有$$f(\alpha x^{(1)}+ (1-\alpha)x^{(2)}) \le \alpha f(x^{(1)}) + (1-\alpha)f(x^{(2)})$$则称$f(x)$为定义在凸集 S 上的凸函数。假如上面不等式中的 $\le$ 改为 $\lt$， 则称其为严格凸函数。 凹函数的定义类似，只需要把上式的不等号方向改变即可。图像直观表示两者如下所示 凸函数的一个重要的性质是其局部极小值点为全局极小值点。 根据凸函数的定义来判断一个函数是否为凸函数往往比较困难，这里分别通过一阶条件和二阶条件判断凸函数。 一阶条件 设 $f(x)$ 在凸集 S上有一阶连续偏导数，则 $f(x)$ 为S上的凸函数的充要条件为：对于 任意不同两点$x^{(1)}$和$x^{(2)}$，均有$$f(x^{(2)}) \ge f(x^{(1)}) + \nabla f(x^{(1)})^T(x^{(2)} - x^{(1)})$$ 二阶条件 设 $f(x)$ 在凸集 S上有二阶连续偏导数，则$f(x)$ 为S上的凸函数的充要条件为：$f(x)$ 的海塞矩阵 $\nabla^2 f(x)$在S上处处半正定(为凹函数的充要条件为处处半负定)。注意：假如海塞矩阵 $\nabla^2 f(x)$在S上处处正定，则$f(x)$为严格凸函数，但是反过来不成立。 关于正定、半正定、负定的定义及判断方法如下所示 而顺序主子式的定义如下 通过海塞矩阵判断凸函数例子如下 凸规划凸规划指可行域为凸集，目标函数为凸函数的规划问题。其具体形式如下 注意上面的$g_j(X)$为凹函数，这样围起来的可行域才为凸集。 因为凸函数和凸集的性质，凸规划有一条重要的性质：凸规划的任一局部极小点为全局极小点。 一维搜索一维搜索就是目标变量只有一个的时候的最优化问题，又称为单变量函数寻优法。 求解这类问题一般有两种方法，一类是区间收缩法（如黄金分割法），一类是函数逼近法（如三点二次插值法、牛顿法）。下面分别介绍 单谷函数定义：如果函数$f(x)$在区间 [a,b] 上只有一个极小值点, 则称$f(x)$为 [a, b] 上的单谷函数。 单谷函数具有一个重要的消去性质 设$f(x)$是区间 [a,b] 上的一个单谷函数，$\overline x \in$ [a,b] 是其极小点， $x_1$ 和$x_2$是 [a, b] 上的任意两点，且$a&lt;x_1 &lt;x_2&lt;b$，那么比较$f(x_1)$与$f(x_2)$的值后，可得出如下结论：1)若$f(x_1)≥f(x_2),\overline x\in [x_1,b]$2)若$f(x_1) &lt; f(x_2),\overline x \in [a,x2]$两种情况如下图所示 外推内插法在一维搜索中的区间收缩法中需要用到单谷区间，而寻找单谷区间的方法就是下面要介绍的外推内插法。 其思路为从某个初始点出发，沿函数值下降的方向前进，直至发现函数值上升为止。而由两边高，中间低的三点，可确定极小点所在的初始区间。 进退算法1.选定初始点a 和步长h; 2.计算并比较$f(a)$和$f(a+h)$；有前进和后退两种情况：1)前进算法：若$f(a) \ge f(a+h)$则步长加倍，计算$f(a+3h)$。若$f(a+h) \le f(a+3h)$，$a_1=a, a_2=a+3h$, 停止运算；否则将步长加倍，并重复上述运算。2)后退算法：若$f(a) \lt f(a+h)$则步长改为 $-h$，计算$f(a-h)$。若$f(a-h) \ge f(a)$，$a_1=a-h, a_2=a+h$, 停止运算；否则将步长加倍，继续后退。 3.得到的满足“中间小两头大”的三点已经可以作为单谷区间，但是当这个区间太大的时候，也可以进行缩短，缩短的方法如下： 假如得到的三点 $a&lt;b&lt;c$，在 $b，c$ 之间内插一点$\overline b = (b+c)/2$。这样得到四个点：$a ，b，\overline b，c$。比较这4个点的函数值，令其中函数值最小的点为 $x_2$ , $x_2$ 的左右邻点为 $x_1$ 和 $x_3$，至此得到了更小的极值点存在区间 [$x_1,x_3$]，且 $x_1、x_2、x_3$ 三点，满足“两头大中间小”的条件。依照此方法可以划出更小的单谷区间。 黄金分割法黄金分割法的思想是：反复使用单谷函数的消去性质，不断缩小包含极小点的搜索区间，直到满足精度为止。 该方法的优点是需计算函数值，通用性强。 三点二次插值法三点二次插值法的思想是：形式复杂的函数进行数学运算时不方便，因此找一个近似的、解析性能好、便于计算的简单函数来代替，用近似函数的极小点作为原函数极小点的近似，常用于近似的简单函数是二次函数。 三点二次插值多项式近似法（抛物线法）的基本原理为：设目标函数 $f(x)$ 在三点 $x_1 &lt; x_2 &lt;x_3$ 上的函数值分别为$f_1,f_2,f_3$,假设相应的二次插值多项式为$P_2(x)=c＋bx + ax^2$,令$P_2(x)$ 和 $f(x)$ 在三点上的函数值相等，进而求出$P_2(x)$中三个未知参数 $a、b、c$ 的值以及$P_2(x)$的稳定点$\overline x = -\frac{b}{2a}$。而这个方法的命名也源于其原理，通过三个点用二次函数去逼近原函数。其图像直观表示如下 需要注意的是选取的三个点要有一定的要求，若任意取这三个点，则求出的$\overline x$ 可能位于给定区间之外或误差太大，见下图 因此，最初的三个点$x_1&lt;x_2&lt;x_3$ 应构成一个两边高，中间低的极小化框架。 在完成一次计算后，得到近似的$\overline x$,要进行搜索区间的收缩，然后在新区间中重新构造三点组成的“极小化框架” 。构造的方法为比较$f(\overline x),f(x_2)$，以函数值较小的点为中间点，加上左右两点。 最后终止准则可以采用目标函数值的相对误差或绝对误差来判断。 牛顿法牛顿法是一种函数逼近法，其基本思想是在极小点附近用二阶泰勒多项式近似代替目标函数，求解二阶泰勒多项式的极小点作为目标函数的极小点。牛顿法在数值分析中是用于求解方程的根，而求解函数极值点等价于求其导函数为0时的根(假如函数可微)。 将$f(x)$在点$x_k$处进行泰勒展开，取前三项有$$f(x) \approx \varphi(x) = f(x_k)+f^{‘}(x_k)(x-x_k)+\frac{1}{2}f^{“}(x_k)(x-x_k)^2$$ 求$\varphi^{‘}(x) = 0$ 的根，可得$$x_{k+1} = x_k - \frac{f^{‘}(x_k)}{f^{“}(x_k)}$$ 通过上面公式进行迭代直至 $|f^{‘}(x_k)| \lt \epsilon$ ($\epsilon$为很小的正数)。 无约束非线性规划前面介绍的一维搜索虽然也属于无约束非线性规划，只是仅有一个约束变量。但是由于实际问题中变量的个数往往不止一个，因此多个变量的无约束非线性规划在实际中使用更为广泛。下面主要介绍多变量无约束非线性规划的解决方法。无约束问题的最优化方大致分为两类： 1. 直接法：求解过程中只用到目标函数值，无须计算导数。如变量轮换(坐标轮换)，模式搜索等。2. 解析法：用函数的解析性（一阶、二阶导数），即在计算过程中需要计算目标函数的导数。如：梯度法、共扼梯度法、牛顿法等 一般来说，解析法的收敛速率较高，直接法的可靠性较高。 直接法坐标(变量)轮换法坐标轮换法属于直接法，既可以用于无约束优化问题的求解，又可以经过适当处理用于约束优化问题求解。 坐标轮换法是每次搜索只允许一个变量变化，其余变量保持不变，即沿坐标方向轮流进行搜索的寻优方法。它把多变量的优化问题轮流地转化成单变量（其余变量视为常量）的优化问题，因此又称这种方法为变量轮换法。此种方法只需目标函数的数值信息而不需要目标函数的导数。 如下图为只有两个变量时进行坐标轮换的搜索过程，可以看到，每一个的移动都只在一个方向上改变 判断其收敛(终止)的可采用点距准则或函数值准则，当点距或函数值只差小于指定的值时则收敛。其中采用的点应该是一轮迭代的始点和终点，而不是某搜索方向的前后迭代点。 其流程图如下所示 坐标轮换法程序简单，易于掌握。但是计算效率比较低，尤其是当优化问题的维数较高时更为严重。一般把此种方法应用于维数小于10的低维优化问题。 模式搜索法模式搜索法的思想是算法从初始基点开始，交替实施两种搜索:轴向搜索和模式搜索。轴向搜索一次沿着n个坐标轴方向进行，用来确定新的迭代点和有利于函数值下降的方向。模式搜索则沿着相邻两个迭代点的连线方向进行，试图使函数值下降得更快。 其搜索过程与坐标轮换法类似，其中轴向搜索其实就是进行了一轮的坐标搜索，与坐标轮换法不同点在于在进行了一轮坐标搜索后会进行模式搜索。如下所示模式搜索法的具体过程为 通过图像直观表示如下 可变单纯形法可变单纯形法也称可变多面体搜索法，是一种传统的处理无约束最优化问题的直接算法. 首先在n欧氏空间中构造一个包含n+1个顶点的凸多面体，求出各顶点的函数值，并确定其中的最大值、次大值和最小值，然后通过反射、扩张、内缩、缩边等策略求出一个较好解，用之取代最大(差)点，从而构成新的多面体，如此多次迭代则可逼近一个性能较好的极小点。 算法简单、计算量小、优化快速，且不要求函数可导，因而适用范围较广。但它对初始解依赖性较强，容易陷入局部极小点，而且优化效果随函数维数的增加明显下降。 Lagarias(1998)研究了可变单纯形法求解低维函数时的收敛特性，但结论难以推广到高维问题，也即单一可变单纯形法难以保证对高维复杂函数具有较好的优化效果。 解析法解析法是利用了函数的导数信息的一类方法，其主要思想是通过导数信息找到函数下降的方向，让后沿着这个方向往下走直到走到最小值。 最速下降法最速下降法利用了函数在某点上的负梯度方向是函数在该店下降最快的方向这一结论。其迭代公式为 $$x^{(k+1)} = x^{(k)} + \alpha^{(k)}d^{(k)}$$ 其中$d^{(k)} = - \nabla f(x^{(k)})$ 为下降方向, $\alpha^{(k)}$ 为步长，其求解方法是对$\alpha^{(k)}$进行一维搜索(因为此时$x^{(k)},d^{(k)}$已知)，即 $$f(x^{(k)} + \alpha^{(k)}d^{(k)}) =min\; f(x^{(k)} + \alpha d^{(k)}) = min\;\varphi(\alpha)$$ 令$\varphi^{‘}(\alpha) = 0$,求出的 $\alpha$ 的值即为步长。 其收敛的判断准则是梯度足够小，即其二阶范数$||\nabla f(x^{(k)})|| \lt \epsilon$ 在最速下降法中相邻的两个迭代点的梯度是彼此正交的。也即在梯度的迭代过程中，相邻的搜索方向相互垂直。 因此最速下降法向极小点的逼近路径是锯齿形路线，越接近极小点，锯齿越细，前进速度越慢。这是因为梯度是函数的局部性质，从局部上看，在该点附近函数的下降最快，但从总体上看则走了许多弯路，因此函数值的下降并不快。其示意图如下所示 牛顿法牛顿法跟最速下降法的思想是一样的，都是找一个能够使函数值下降的方向前进，只是最速下降法找的方向是负梯度方向，而牛顿法找的是牛顿方向。根据每次迭代的步长是否固定，可以将牛顿法分为原始牛顿法和阻尼牛顿法两种，实际中应用较多的是阻尼牛顿法。 原始牛顿法原始牛顿法的思想在一维搜索中已经提到,只是一维搜索中是只有一个 $x$变量，而这里处理的是多个$x$变量，相应地用梯度和海塞矩阵代替原来的一阶导数和二阶导数。 其思想就是在第 k 次迭代的迭代点 $x^{(k)}$ 邻域内，通过泰勒展开用一个二次函数去近似代替原目标函数$f(x)$，然后求出该二次函数的极小点作为对原目标函数求优的下一个迭代点，依次类推，通过多次重复迭代，使迭代点逐步逼近原目标函数的极小点。 其主要步骤为 设目标函数$f(x)$具有连续的一、二阶导数，在$x^{(k)}$点邻域内取$f(x)$的二次泰勒多项式作近似式，对于只有一个变量$x$时为 $$f(x) \approx \varphi(x) = f(x_k)+f^{‘}(x_k)(x-x_k)+\frac{1}{2}f^{‘’}(x_k)(x-x_k)^2$$ 而有多个变量 $x$ 时，有 $$f(x) \approx \varphi(x) = f(x^{(k)})+\nabla f(x^{(k)})^T\Delta x+\frac{1}{2}\Delta x^TH_k\Delta x$$ 令 $x^{(k+1)}$ 为函数的极小点,则应有 $\nabla \varphi(x^{(k+1)}) = 0$ 令 $ \nabla \varphi(x)= \nabla f(x^{(k)})+H_k\Delta x=0$,且 $\Delta x = x^{(k+1)}-x^{(k)}$ 则有 $x^{(k+1)} = x^{(k)}-H_k^{-1}\nabla f(x^{(k)})$，而 $-H_k^{-1}\nabla f(x^{(k)})$ 则为点 $x^{(k)}$ 处的牛顿方向，也就是该点的下降方向。通过该公式进行迭代直至该点的梯度收敛，即其梯度的二阶范数$||\nabla f(x^{(k)})|| \lt \epsilon$。 牛顿法是具有二次收敛性的算法。若用原始牛顿法求某二次目标函数的最优解，则构造的逼近函数与原目标函数是完全相同的二次式，其等值线完全重合，故从任一点出发，一定可以一次达到目标函数的极小点。 其优点是：对于二次正定函数，迭代一次即可以得到最优解，对于非二次函数，若函数二次性较强或迭代点已经进入最优点的较小邻域，则收敛速度也很快。 其缺点是：由于迭代点的位置是按照极值条件确定的，并未沿函数值下降方向搜索，因此，对于非二次函数，有时会使函数值上升，即 $f(x_{k+1}) &gt; f(x_k)$，而使计算失败。 阻尼牛顿法阻尼牛顿法对原始牛顿法进行了改进,每次迭代加入了步长 $\alpha^{(k)}$，即将迭代公式从 $x^{(k+1)} = x^{(k)}-H_k^{-1}\nabla f(x^{(k)})$ 变为了 $x^{(k+1)} = x^{(k)}-\alpha^{(k)}H_k^{-1}\nabla f(x^{(k)})$ 最优步长 $\alpha^{(k)}$ 也称为阻尼因子，其求解方法也类似于最速下降中通过一维搜索得到的最优步长。 其优点是：由于阻尼牛顿法每次迭代都在牛顿方向进行一维搜索，避免了迭代后函数值上升的现象，从而保持了牛顿法的二次收敛性，而对初始点的选择没有苛刻的要求。 缺点是：1）对目标函数要求苛刻，要求函数具有连续的一、二阶导数；为保证函数的稳定下降，海赛矩阵必须正定；为求逆阵要求海赛矩阵非奇异。2）计算复杂且计算量大，存储量大 拟牛顿法(变尺度法)从前面介绍的最速下降法和牛顿法可知，梯度法的搜索方向只需计算函数的一阶偏导数，计算量小，当迭代点远离最优点时，函数值下降很快，但当迭代点接近最优点时收敛速度极慢。牛顿法的搜索方向不仅需要计算一阶偏导数，而且要计算二阶偏导数及其逆阵，计算量很大，但牛顿法具有二次收敛性，当迭代点接近最优点时，收敛速度很快。 若迭代过程先用梯度法，后用牛顿法并避开牛顿法的海赛矩阵的逆矩阵的烦琐计算，则可以得到一种较好的优化方法，这就是“拟牛顿法”产生的基本构想。为此，综合梯度法和牛顿法的优点，提出拟牛顿法。 拟牛顿法的迭代公式与最速下降和阻尼牛顿法类似， $x^{(k+1)} = x^{(k)}-\alpha^{(k)}A_k\nabla f(x^{(k)})$ 其中$A_k$为构造的构造的 n×n 阶对称矩阵，拟牛顿方向即为$-A_k\nabla f(x^{(k)})$。 当$A_k = I$时，上式为最速下降法的迭代公式当$A_k = H_k^{-1}$时，上式为阻尼牛顿法的迭代公式 拟牛顿法原来使通过DFP法构造 $A_k$，构造过程中避开了二阶导数的计算，因此收敛速度也比较快；但是DFP算法由于舍入误差和一维搜索的不精确，有可能导致$A_k$奇异，而使数值稳定性方面不够理想。所以1970年提出更稳定的算法，称为BFGS算法。这里不详细展开讨论这两种算法。 共轭梯度法共轭梯度法的搜索方向采用梯度法基础上的共轭方向，如图所示， 目标函数 $f(x)$ 在迭代点 $x^{(k+1)}$ 处的负梯度为$-\nabla f(x^{(k+1)})$，该方向与前一搜索方向 $S^k$ 互为正交，在此基础上构造一种具有较高收敛速度的算法，该算法的搜索方向要满足以下两个条件： 1）以 $x^{(k+1)}$ 点出发的搜索方向 $S^{k+1}$ 是$-\nabla f(x^{(k+1)})$ 与 $S^k$ 的线性组合。即$$ S^{k+1} = -\nabla f(x^{(k+1)}) + \beta_kS^k$$ $$\beta_k = (\frac{||\nabla f(x^{(k+1)})||}{||\nabla f(x^{(k)})||})^2$$ 2）$[S^{k+1}]^TGS^k=0$ 除了计算下降方向方法不同，其迭代公式与前面的方法类似,为 $x^{(k+1)} = x^{(k)}+\alpha^{(k)}S^{(k)}$ 收敛的判断也是判断梯度的二阶范数 $||\nabla f(x^{(k+1)})|| \lt \epsilon$ 是否成立即可 共轭梯度法属于解析法，其算法需求一阶导数，所用公式及算法简单，所需存储量少该方法以正定二次函数的共轭方向理论为基础，对二次型函数可以经过有限步达到极小点，所以具有二次收敛性。但是对于非二次型函数，以及在实际计算中由于计算机舍入误差的影响，虽然经过 n 次迭代，仍不能达到极小点，则通常以重置负梯度方向开始，搜索直至达到预定精度，其收敛速度也是较快的。 方法对比为了比较各种优化方法的特性，必须建立合理的评价准则。 无约束优化方法的评价准则主要包括以下几个方面 1、可靠性。即在合理的精度要求下，在一定允许时间内能解出各种不同类型问题的成功率。能够解出的问题越多，则算法的可靠性越好2、有效性。即算法的解题效率。它有两个衡量标准。其一是对同一题目，在相同精度和初始条件下，比较机时多少。其二是在相同精度下，计算同一题目所需要的函数的计算次数。3、简便性。一方面指实现该算法的准备工作量的大小。另一方面指算法占用存储单元的数量。 各个算法的性能对比如下：可靠性：牛顿法较差，因为它对目标函数要求太高，解题成功率较低。有效性：坐标变换法和梯度法的计算效率较低，因为它们从理论上不具有二次收敛性。简便性：牛顿法和拟牛顿法的程序编制较复杂，牛顿法还占用较多的存储单元。 在选用无约束优化方法时，一方面要考虑优化方法的特点，另一方面要考虑目标函数的情况。1、一般而言，对于维数较低或者很难求得导数的目标函数，使用坐标轮换法较合适。2、对于二次性较强的目标函数，使用牛顿法效果好。3、对于一阶偏导数易求的目标函数，使用梯度法可使程序编制简单，但精度不宜过高4、综合而言，共轭梯度法和DFP法具有较好的性能。 约束非线性规划约束条件下的非线性规划模型如下所示 $$min\;f(x)\\s.t\quad h_i(x) = 0(i=1,2,…,m)\\\quad\quad g_j(x) \ge 0(j=1,2,…,l)\\$$ 在约束非线性规划中有几个比较重要的概念 起作用约束和不起作用约束 对于约束条件 $g_j(x) \ge 0(j=1,2,…,l)$,满足它有两种可能：其一是$g_j(x) \gt 0$, 这时, $x$ 不是处于由这一约束条件形成的可行域的边界上，因此当点不论沿什么方向稍微离开时,都不会违背这一约束条件,这样的约束就称为不起作用约束，即它对 的微小扰动不起限制作用,也就是约束中的所有等式约束均是起作用约束，包括 所有的$h(x)$ 和 $g(x)$ 中取等号的约束。注意，不起作用约束并不是无效约束！ 有效集(积极集)有效集定义为不等式约束中符号为等号的那些约束条件的下标，如对于上面的带约束的非线性规划模型，其有效集为 $I(x) = \lbrace j|g_j(x)=0, 1 \le j \le l \rbrace$ KT条件在不同的资料中，KT(Kuhn-Tucker) 条件也会被称为 KKT(Karush-Kuhn-Tucker)条件。原因是这个理论是 Karush(1939年) 以及 Kuhn和Tucker(1951) 先后独立发表出来的。而且是在Kuhn和Tucker发表之后才逐渐受到重视,所有很多教材都会将这一条件称为KT条件，我们这里只需要知道这KT跟KKT是一样的东西就可以了。 KT条件是非线性规划领域中最重要的理论成果之一，其重要的意义在于它是确定某点是最优点的一阶必要条件，只要是最优点就必须满足这个条件。但一般来说它不是充分条件，即满足KT条件的点并不一定是最优点。但是对于凸规划，KT条件是最优点存在的充要条件。也就是说在凸规划中通过KT条件可以找到最优解。 对于非线性规划模型$$min\;f(x)\\s.t\quad h_i(x) = 0(i=1,2,…,m)\\\quad\quad g_j(x) \ge 0(j=1,2,…,l)\\$$ 其KT条件为$$\nabla f(x) - \sum_{i=1}^mr_i\nabla h_i(x) - \sum_{j=1}^l\lambda_j\nabla g_j(x) = 0\\\lambda_j g_j(x) = 0\quad (j=1,2,3…l)\\\lambda_j \ge 0 \quad (j=1,2,3…l)\\$$ 求解上面的KT条件组成的方程组得到的 $x$ 值就是KT点，也就是最优点(对于凸规划而言)。另外,下式 $f(x) - \sum_{i=1}^mr_i h_i(x) - \sum_{j=1}^l\lambda_j g_j(x) $ 通常被称为上面非线性规划问题的拉格朗日函数，对应的 $(r_1,r_2,…r_m)$ 和 $(\lambda_1,\lambda_2,…\lambda_l)$ 称为拉格朗日乘子。 罚函数法罚函数法的思想是借助惩罚函数将约束问题装化为无约束问题进行求解，根据惩罚函数的不同，罚函数法又分为外点法，内点法和乘子法。 外点法外点法可以用来解决只有等式约束、只有不等式约束或同时含有等式和不等式约束问题。 先考虑只有等式约束的问题如下$$min\;f(x)\\s.t\quad h_i(x) = 0(i=1,2,…,m)\\$$ 则可以将上面的带约束的问题等价于下面的无约束问题 $$min\;p(x,M) = f(x) + M\sum_i^m [h_i(x)]^2$$ 其中 M 是充分大的正数(求解时一般让其趋于无穷大),被称为罚因子，而 $p(x,M)$ 称为惩罚函数，$M\sum_i^m [h_i(x)]^2$ 为惩罚项，其作用就是当 $x$ 不满足任一等式约束 $h(x) = 0$ 时，罚项就会变得很大从而使解不能满足最小，也就是惩罚了这个解。 同样对于不等式约束问题$$min\;f(x)\\s.t\quad g_j(x) \ge 0(j=1,2,…,l)\\$$ 构造的惩罚函数为 $$p(x,M) = f(x) + M\sum_j^l [min(0, g_j(x))]^2$$ 其含义也是当 $x$ 满足不等式约束条件时，罚项为0，不满足是罚项就会变得很大，从而使当前的 $x$ 不忙足目标函数值最小。 对于同时含有等式约束和不等式约束的规划问题，只要将上面的等式约束和不等式约束中的罚项加在一起即可构造惩罚函数：$$p(x,M) = f(x) + M( \sum_i^m [h_i(x)]^2 + \sum_j^l [min(0, g_j(x))]^2)$$ 求解时只需要用无约束问题的求解方法求解惩罚函数的最小点即可，如下为一个求解的例子 让 $M_k \rightarrow \infty$ 即可得到最优解为 $x =(2,1)^T$。 但是假如让M逐步变化，可以得到以下表格 从上面的表格可知，当 $M_k$ 从$1 \rightarrow \infty$ 过程中，罚函数的一系列无约束极小点是从可行域的外部趋近最优解的，因此，这也是外点法名称的来历。 内点法这里讲述的内点法只考虑不等式约束的问题，对于问题 $$min\;f(x)\\s.t\quad g_j(x) \ge 0(j=1,2,…,l)\\$$ 其严格内点集合(又称可行域内部)定义为 $H = \lbrace g_j(x) &gt; 0 |j=1,2,…,l\rbrace$ 内点法就是通过在严格内点集合中进行迭代得到最优解，这也是内点法这一说法的来历，需要注意的是内点法得到的最优解并不一定是全局最优解，因为内点法只是在严格内点中迭代，而全局最优解有可能落在边界上。但是对于最优解不落在边界的问题，内点法能够得到最优解，并且对于那些最优解落在边界上的问题，内点法也能够获得较好的近似解。 对于上面的模型可以构造障碍函数 $$p(x,r_k) = f(x) + r_k \sum_j^l \frac{1}{g_j(x)}$$ 或 $$p(x,r_k) = f(x) - r_k \sum_j^l ln(g_j(x))$$ 求解障碍函数的最小值就相当于求解原来的带约束问题；障碍函数类似于外点法中的惩罚函数，其中 $r_k \sum_j^l \frac{1}{g_j(x)}$ 或 $- r_k \sum_j^l ln(g_j(x))$ 被称为障碍项，$r_k$ 称为障碍因子。 障碍函数的作用是惩罚靠近可行域边界的 $x$ 点，即那些使 $g(x) = 0$ 的点，当 $x$ 靠近这些边界的时候，障碍项会变得很大，从而使得其不满足障碍函数最小。 其求解的一个例子如下 乘子法乘子法类似于上面提到的外点法和内点法，也是通过引入乘子罚函数使约束问题变为无约束问题。但是与前面不同的地方是乘子罚函数是在罚函数的基础上增加了拉格朗日乘子项，从而称为増广拉格朗日函数。这里只讨论等式约束的情况。 对于问题$$min\;f(x)\\s.t\quad h_i(x) = 0(i=1,2,…,m)\\$$ 定义增广拉格朗日函数(乘子罚函数)为 $$\varphi (x,\lambda, M) = f(x) - \sum_{i=0}^m \lambda_i h_i(x) + \frac{M}{2} \sum_{i=1}^m [h_i(x)]^2$$ 其中，$\overrightarrow \lambda = (\lambda_1,\lambda_2,…\lambda_m)^T$ 为拉格朗日乘子向量。则原问题的求解转为了求解增广拉格朗日函数的极小点。 乘子罚函数 $\varphi (x,\lambda, M)$ 与普通拉格朗日函数的区别是增加了罚项 $\frac{M}{2} \sum_{i=1}^m [h_i(x)]^2$, 与罚函数的区别是增加了乘子项$ - \sum_{i=0}^m \lambda_i h_i(x)$. 假如知道拉格朗日乘子 $\overrightarrow \lambda$, 再给定一个足够大的罚因子M(M不必趋于无穷大)，就可以通过极小化 $\varphi (x,\lambda, M)$ 得到问题的局部最优解。但是由于 $\overrightarrow \lambda$ 事先无法知道，所以给定一个足够大的 $M$ 和初始的估计量 $\overrightarrow {\lambda^{(1)}}$，每次通过下面的公式对 $\overrightarrow \lambda$ 进行修正。 $$ \overrightarrow {\lambda^{(k+1)}} = \overrightarrow {\lambda^{(k)}} - Mh_i(x^{(k)})$$ 上式中的 $x^{(k)}$ 是第 k 次迭代拉格朗日乘子为 $\overrightarrow {\lambda^{(k)}}$ 时得到的极小点。通过上式进行迭代直至 $\overrightarrow \lambda$ 收敛。 在乘子法中，罚因子 $M$ 不必趋于无穷大，只要足够大，就可以通过极小化乘子罚函数，得到原来约束问题的局部最优解，而这避免了罚函数法中的病态问题。实践证明，乘子法优于罚函数法,使用范围比罚函数要广。 下面为一个求解的简单例子 得到上面的等式后假设 $\overrightarrow {\lambda^{(k)}}$ 的收敛值为 $\alpha$,则有 $\alpha = \frac{7}{23} \alpha + \frac{28}{23}$ 二次规划二次规划是特殊的非线性规划，形式简单，既可以使用求耳机非线性规划的一般方法，又可以使用特定的解法。在实际中有广泛应用，如支持向量机（SVM）本质上就是一个二次规划问题。 二次规划问题的一般模型也如下$$min\;f(x)\\s.t\quad h_i(x) = 0(i=1,2,…,m)\\\quad\quad g_j(x) \ge 0(j=1,2,…,l)\\$$ 但是要求 $f(x)$ 是二次函数，而 $h_i(x)$ 和 $g_j(x)$ 是线性函数。因此可以展开写成如下的形式： $$min\;f(x) = \frac{1}{2}x^TGx + r^Tx\\s.t\quad A_i^Tx - b_i = 0(i=1,2,…,m)\\\quad\quad\quad A_i^Tx-b_i \ge 0(i=m+1,…,m+l)\\$$ 其中 G 为 $n × n$ 阶对称矩阵( $n$ 为未知变量个数)，$r,A_i$ 为n 位列向量，$b_i$ 为实数。若矩阵 G 为(正定)半正定矩阵，那么将问题称为严格)凸二次规划。前面提到，对于凸规划，$\overline x$ 为全局极小点的充要条件是该点满足如下的KT条件。 $$G\overline x + r - \sum_{i=1}^{m+l}\lambda_iA=0\\\lambda_i (A_i^Tx-b_i) = 0(i=m+1,…,m+l)\\\lambda_i \ge 0(i=m+1,…,m+l)$$ 通过求解KT条件组成的方程组，能够解出二次规划的最优化问题，这只是求解非线性规划的一般方法，还有一些专门用来求解二次规划的方法，对于只含有等式约束的二次规划问题可采用消去法，而同时含有等式约束和不等式约束的解决方法是有效集法，这是更一般的解决方法，下面主要介绍有效集法的原理和过程。 有效集法又称积极集法，其基本思想是通过求解有限个等式约束二次规划问题来求解一般约束下的二次规划问题。从直观上理解，不起作用的约束在解的附近不起任何作用，可以去掉不考虑，而起作用(积极)的不等式约束由于在解处等于0，故可以用等式约束来代替不等式约束。 有效集方法中的有效集指的是约束中取等号的那些约束条件，对于上面的问题，定义 $I(x) = \lbrace i| A_i^Tx =b_i, m+1 \le i \le m+l\rbrace $ 则有效集为 $E = \lbrace 1,2,…m\rbrace \bigcup I(x)$ 有效集方法就是将其转化为以下问题进行求解，并对得到的解进行讨论$$min\;f(x) = \frac{1}{2}x^TGx + r^Tx\\s.t\quad A_i^Tx = b_i (i \in E)\\$$ 其具体计算步骤如下]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Programming Collective Intelligence》读书笔记(3)--聚类]]></title>
      <url>%2F2017%2F01%2F25%2F%E3%80%8AProgramming%20Collective%20Intelligence%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0(3)--%E8%81%9A%E7%B1%BB%2F</url>
      <content type="text"><![CDATA[《Programming Collective Intelligence》（中文名为《集体智慧编程》），是一本关于数据挖掘的书籍，每一章都会通过一个实际的例子来讲述某个机器学习算法，同时会涉及到数据的采集和处理等，是一本实践性很强的书籍。 本文是本书的第三章 Discovering Groups的读书笔记 ,主要介绍了对文本进行聚类以及对聚类后的结果进行可视化。 聚类就是将相似的数据聚合在一起，形成一类，与分类比较相似，但是又有不同。一般来说聚类没有明确的类别数目，而分类根据具体的问题在一开始就确定了所有分类的可能，而且一般用于聚类的数据不需要标签, 属于无监督算法，而用于分类的数据则需要，属于有监督的算法。常见的聚类算法有KMeans，SOM（Self-Organized Feature Map）等，常见的分类算法则更多，如逻辑回归，朴素贝叶斯，SVM等。 本文主要讲逐层聚类(Hierarchical Clustering)和KMeans聚类这两个聚类算法,依然会通过一个例子来阐述算法的具体实现。文中所有的数据和代码可从这里获取。 获取数据本文使用的数据为博客文本数据，通过分析博客文本之间的相似性，从而将相似的博客聚类在一起。 通过博客的 RSS 源可以获取博客所有的文本数据，python中提供了 feedparser 这个第三方 package 来实现这个功能，如下代码实现的功能就是获取一个博客的 RSS 中的所有文章的summary 并进行分词统计，注意本文使用的均是英文文本。 12345678910111213141516171819202122232425import reimport feedparserfrom collections import defaultdictdef parse_rss(target_url): rss = feedparser.parse(target_url) word_count = defaultdict(int) # traverse all passages of the blog for entry in rss.entries: if 'summary' in entry: summary = entry.summary # su else: summary = entry.description words = extract_words(entry.title+' '+summary) for word in words: word_count[word] += 1 return rss.feed.get('title', 'empty title'), word_count # title can be empty sometimesdef extract_words(content): # remove tag in the form of &lt;XXXX&gt; txt = re.compile(r'&lt;[^&gt;]+&gt;').sub('',content) # split words by all non-alpha characters words = re.compile(r'[^A-Z^a-z]+').split(content) # turn all words into lowercase return [word.lower() for word in words if word != ''] 上面的 parse_rss 函数实现的功能就是从 RSS 的url中提取出所有文本的summary（或description），然后通过extract_words函数剔除html标签并分词，进而统计出该 RSS 源中各个词语所出现的次数。 由于聚类需要多个博客的数据，本文使用了原书提供的100个博客的RSS作为原始数据，并通过原始数据构造一个 blog-word 矩阵，行表示博客，列表示各个具体的词语，矩阵中的值表示某个词语在某个博客中出现的总次数。由于词语数目过多，因此这里会限制出现在矩阵的列的词语必须要在原始数据中出现的频率在一定的百分比。得到 blog-word 矩阵需要一定的运算量，因此将其写入到文件中进行持久化，方便下次的读取。下面就是实现上面功能的代码。 12345678910111213141516171819202122232425262728293031323334353637def get_content_from_feedlist(feed_list, data_file): word_appear_count = defaultdict(int) # count thow many blogs does a word appear in blog_word_count = &#123;&#125; # words of each blog empty_title_count = 0 for rss_url in file(feed_list): title, wc = parse_rss(rss_url.strip()) if title == 'empty title': # cannot get title of some rss empty_title_count += 1 title = title+' %s'%empty_title_count blog_word_count[title] = wc for word, count in wc.items(): word_appear_count[word] += 1 # caculate the appearing percentage of each word # record those words that appear within maximum and minimum percentage minimum, maximum = 0.1, 0.5 word_list = [] total_blog = len(blog_word_count) for word, count in word_appear_count.items(): if minimum &lt;= count*1.0/total_blog &lt;= maximum: word_list.append(word) # write data into data_file with io.open(data_file, mode = 'w', encoding = 'utf8') as wf: wf.write('Blog'.decode('utf8')) for word in word_list: wf.write(('\t%s'%word).decode('utf8')) wf.write('\n'.decode('utf8')) # words of each blog for blog_title, blog_words in blog_word_count.items(): wf.write(blog_title.decode('utf8')) for word in word_list: if word in blog_words: wf.write(('\t%s'%blog_words[word]).decode('utf8')) else: wf.write(('\t'+'0').decode('utf8')) wf.write('\n'.decode('utf8')) 上面的 get_content_from_feedlist中通过获取了 feed_list(每行一个rss源的url)中所有博客的文本，并将那些出现频率在 0.1 到 0.5 间的词语作为 blog-word 矩阵的列，出现频率的计算方法为 词语出现的博客数/总的博客数。然后将blog-word 矩阵写入到 data_file 文件中。至此完成了获取数据的步骤。 聚类下面将讲述两种方法对上面获取的博客数据进行聚类：逐层聚类（Hierarchical Clustering） 和 KMeans聚类（KMeans Clustering）。 逐层聚类（Hierarchical Clustering）逐层聚类的思想很简单，每次将距离最近的两个cluster进行聚类，组成一个新的类，然后重复此过程直到只剩下一个最大的cluster。整个过程如下图所示： 这种聚类方法的具体过程如同构造一棵树，其中每个叶子节点表示一个单一的实例，而其他节点表示由多个实例聚成的cluster。 算法的思路比较简单，但是有个关键问题就是如何判断cluster 与 cluster之间的距离。这里采用皮尔逊系数，皮尔逊系数在这篇文章中有比较详细的描述，这里就不详细展开了，实际上皮尔逊系数就是概率论中常用的相关系数，用于表示两者的相关性，范围在 [-1, 1]之间，其中正值表示正相关，负值表示负相关，且绝对值越大，相关性越强，这里的距离越短，表示两个的正相关性越强，因此采用 1-皮尔逊系数 作为距离的值。实现的代码如下所示： 1234567891011121314def pearson(v1,v2): # Simple sums sum1=sum(v1) sum2=sum(v2) # Sums of the squares sum1Sq=sum([pow(v,2) for v in v1]) sum2Sq=sum([pow(v,2) for v in v2]) # Sum of the products pSum=sum([v1[i]*v2[i] for i in xrange(len(v1))]) # Calculate r (Pearson score) num=pSum-(sum1*sum2/len(v1)) den=sqrt((sum1Sq-pow(sum1,2)/len(v1))*(sum2Sq-pow(sum2,2)/len(v1))) if den==0: return 0 return 1.0-num/den 另外一个问题就是如何在 python 中表示一个cluster，从上面的算法过程可知，一个cluster包括了它的两个子cluster（最开始的单实例没有），cluster的中心值等，在 python 并没有符合这种要求的数据结构，因此可以创建一个python类这个cluster。 创建的cluster类如下所示：123456789class hcluster: """describe a cluster as a node in a tree""" def __init__(self, id, vector, distance=0, left = None, right = None): self.id = id self.vector = vector self.distance = distance self.left = left self.right = right 各个变量及其含义如下所示 变量 含义 id cluster的唯一标示号 vector cluster的值，即cluster的中心点 distance 组成cluster的两个子clusters的距离，叶子节点为0 left，right 组成cluster的两个子clusters，叶子节点为None 通过各个 cluster 的唯一标示号 id 可以方便进行 cluster 的合并和删除，而存储 distance 以及子clusters的便于后面对聚类结果的可视化。下面是逐层聚类的过程的代码 12345678910111213141516171819202122232425262728293031323334def hierarchicalClustering(blog_data, distance = pearson): # initi clusters, each node is a cluster clusters = [hcluster(id = i, vector = blog_data[i]) for i in xrange(len(blog_data))] # use negativ number to represent cluster with more than one node clust_id = -1 # use distance to store caculated results distances = &#123;&#125; while len(clusters) &gt; 1: similar_pairs = (0,1) closest_distance = distance(clusters[0].vector, clusters[1].vector) for i in xrange(len(clusters)): for j in xrange(i+1, len(clusters)): if (clusters[i].id, clusters[j].id) not in distances: distances[(clusters[i].id, clusters[j].id)] = distance(clusters[i].vector, clusters[j].vector) d = distances[(clusters[i].id, clusters[j].id)] if closest_distance &gt; d: closest_distance = d similar_pairs = (i, j) merged_vector = [(clusters[similar_pairs[0]].vector[i] + clusters[similar_pairs[1]].vector[i])/2.0 for i in xrange(len(clusters[similar_pairs[0]].vector))] new_cluster = hcluster(id = clust_id, vector = merged_vector, distance = closest_distance, left = clusters[similar_pairs[0]], right = clusters[similar_pairs[1]]) # must delete elements from higher index to lower index del clusters[similar_pairs[1]] del clusters[similar_pairs[0]] clusters.append(new_cluster) clust_id -= 1 return clusters[0] 上面的代码重复 合并两个最邻近cluster为新cluster-&gt;调整新cluster的位置的操作，直到只剩下一个cluster并将该cluster返回。 KMeans 聚类KMeans 聚类的思想是一开始就确认了最终需要聚类成 k 个 clusters，然后在训练数据的范围内随机选择 k 个初始点作为初始 k 个 cluster 的中心，并将每个实例聚类到离其最近的一个cluster，所有的点都分到离其最近的cluster后，根据各个cluster中的点重新调整这个cluster的中心。重复这个过程直到每个cluster中的点不变为止。如下图为KMeans的过程 这里度量距离的方式仍采用皮尔逊系数，下面是实现以上功能的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142def kMeans(blog_data, distance = pearson, k = 5): m, n = len(blog_data), len(blog_data[0]) max_value = [0 for i in xrange(n)] min_value = [0 for i in xrange(n)] for i in xrange(m): for j in xrange(n): max_value[j] = max(max_value[j], blog_data[i][j]) min_value[j] = min(min_value[j], blog_data[i][j]) # initial random clusters clusters = [] for i in xrange(k): clusters.append([min_value[j] + random.random()*(max_value[j] - min_value[j]) for j in xrange(n)]) count = 0 previous_cluster_nodes = None while True: count += 1 print 'iteration count %s'%count curr_cluster_nodes = [[] for i in xrange(k)] for i in xrange(m): closest_distance = distance(blog_data[i], clusters[0]) cluster = 0 for j in xrange(1, k): d = distance(blog_data[i], clusters[j]) if closest_distance &gt; d: closest_distance = d cluster = j curr_cluster_nodes[cluster].append(i) if curr_cluster_nodes == previous_cluster_nodes: break previous_cluster_nodes = curr_cluster_nodes # modify the core of each cluster for i in xrange(k): tmp = [0 for _ in xrange(n)] for node in curr_cluster_nodes[i]: for j in xrange(n): tmp[j] += blog_data[node][j] clusters[i] = [float(tmp[j])/len(curr_cluster_nodes) for j in xrange(n)] return clusters, curr_cluster_nodes 上面的代码最终返回k个cluster的中心点的值，以及各个cluster中所包含的点（实例）。需要注意的是由于每次选择的初始点是随机的，因此每次运行所获得的结果不一定相同。 可视化下面介绍如何将上面两种聚类算法得到的结果进行可视化，可视化通过python中的 python image library(PIL) 实现。 逐层聚类结果可视化从上面的算法描述可知，逐层距离每次将两个cluster聚合在一起，从构造过程来看，实际上最终是构造了一颗二叉树，因此这里会以二叉树的形式将聚类的结果可视化。由于二叉树叶子节点过多，为了便于展示，将二叉树横着放，如下图就是二叉树的放置方式 要将这课二叉树可视化，首先需要知道这这棵树以上图放置时的高度和宽度(深度)，然后根据图片的大小进行相应的缩放。 回想上面在逐层聚类中定义的下面的类表示一个cluster12345678class hcluster: """describe a cluster as a node in a tree""" def __init__(self, id, vector, distance=0, left = None, right = None): self.id = id self.vector = vector self.distance = distance self.left = left self.right = right 通过 self.left 和 self.right 属性可以访问当前cluster的两个子cluster，而 self.distance 则表示两个子cluster的距离的大小，该属性在图中表现为当前cluster到两个子cluster的线段的长短，假如self.distance的值越大，那么这个cluster到两个子cluster的线段也越长。因此可以通过distance的值来获取二叉树的最长深度来作为图片的宽度。获取二叉树深度代码如下所示 1234def get_depth(cluster): # The distance of an endpoint is 0.0 if cluster.left==None and cluster.right==None: return 0 return max(get_depth(cluster.left),get_depth(cluster.right))+cluster.distance 通过递归的方式将两棵字数中最长的深度加上当前节点到两棵子树的长度，便是以当前节点为根节点的树的深度。 同样地，树的高度也可以通过递归方式获取，由上面的二叉树的放置方法可知，整棵树的高度就是其两棵子树的高度之和。下面便是获取高度的代码 123def get_height(cluster): if cluster.left==None and cluster.right==None: return 1 return get_height(cluster.left)+get_height(cluster.right) 整个二叉树中有两种节点：一种是叶子节点，表示一个单一实例，另外的非叶子结点的表示有多个叶子节点组成的cluster。对于叶子节点，只需要在图片上显示其内容即可，而对于非叶子节点则需要画出其两棵子树的分支，通过 self.id 属性可以区分节点是否为叶子节点。下面便是具体的实现代码,（注意这里需要用到PIL了，关于PIL的具体用法可参考这里） 123456789101112131415161718192021222324from PIL import Image,ImageDrawdef draw_node(draw,cluster,x,y,scaling,blog_names): if cluster.id &lt; 0: h1=get_height(cluster.left)*20 h2=get_height(cluster.right)*20 top=y-(h1+h2)/2 bottom=y+(h1+h2)/2 # Line length ll=cluster.distance*scaling # Vertical line from this cluster to children draw.line((x,top+h1/2,x,bottom-h2/2),fill=(255,0,0)) # Horizontal line to left item draw.line((x,top+h1/2,x+ll,top+h1/2),fill=(255,0,0)) # Horizontal line to right item draw.line((x,bottom-h2/2,x+ll,bottom-h2/2),fill=(255,0,0)) # Call the function to draw the left and right nodes draw_node(draw,cluster.left,x+ll,top+h1/2,scaling,blog_names) draw_node(draw,cluster.right,x+ll,bottom-h2/2,scaling,blog_names) else: # If this is an endpoint, draw the item label draw.text((x+5,y-7),blog_names[cluster.id],(0,0,0)) 上面的 scaling 参数是根据树的深度和图片的宽度的比例得到的缩放因子，在开始画图前需要通过树的深度和图片预定义的宽度获取。下面的代码便是在作图前的需要准备的参数以及调用上面定义好的函数进行作图的过程。123456789101112131415161718def draw_cluster(cluster, blog_names, jpeg_path): # height and width h=get_height(cluster)*20 w=1200 depth=get_depth(cluster) # width is fixed, so scale distances accordingly scaling=float(w-150)/depth # Create a new image with a white background img=Image.new('RGB',(w,h),(255,255,255)) draw=ImageDraw.Draw(img) draw.line((0,h/2,10,h/2),fill=(255,0,0)) # Draw the first node draw_node(draw,cluster,10,h/2,scaling,blog_names) img.save(jpeg_path,'JPEG') 上面完整的代码可从这里获取 作图的结果如下所示： KMeans聚类结果可视化原书并没有给出KMeans聚类结果的可视化操作，只是提供了一种多维缩放(Multidimensional scaling, MDS)的技术, 用于将高维的数据转换为二维，同时保留数据间的距离关系，这样便可通过图片对其进行可视化。实际上， MDS 与 PCA 都是一种基于线性变换而进行降维的方法。 MDS的思想是通过点的原始距离矩阵 $D$, 计算出变换到新的维度空间中的点的距离矩阵 $B$, 然后对 $B$ 做特征值分解，选取前 $n$ 个特征值($n$ 为所变换到的维度空间的维度值)及其对应的特征向量矩阵，便可得到新的维度空间中各点的坐标。具体的推导过程可以参考周志华的机器学习中第十章的内容或这篇博客，这里不详细展开论述了。 但是该书给出的方案并不是上面讲述的方法，而是先在二维空间中随机初始化 $m$ 个点作为m个实例的初始点，然后根据其在二维空间的距离与高维空间中实际的距离的误差来调整点的位置，是一种迭代的方法。其具体实现如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def scale_dowm(blog_data,distance=pearson,rate=0.01): n=len(blog_data) # The real distances between every pair of items real_list=[[distance(blog_data[i],blog_data[j]) for j in xrange(n)] for i in xrange(n)] # Randomly initialize the starting points of the locations in 2D loc=[[random.random(), random.random()] for i in xrange(n)] fake_list=[[0.0 for j in xrange(n)] for i in xrange(n)] lasterror=None for m in range(0,1000): # Find projected distances for i in range(n): for j in range(n): fake_list[i][j]=sqrt(sum([pow(loc[i][x]-loc[j][x],2) for x in xrange(len(loc[i]))])) # Move points grad=[[0.0,0.0] for i in range(n)] totalerror=0 for k in range(n): for j in range(n): if j==k or real_list[j][k] == 0: continue # acoid the case when real_list[j][k] == 0.0 # The error is percent difference between the distances error_term=(fake_list[j][k]-real_list[j][k])/real_list[j][k] # Each point needs to be moved away from or towards the other # point in proportion to how much error it has grad[k][0] += ((loc[k][0]-loc[j][0])/fake_list[j][k])*error_term grad[k][14] += ((loc[k][15]-loc[j][16])/fake_list[j][k])*error_term # Keep track of the total error totalerror+=abs(error_term) # print 'curr error &#123;0&#125;'.format(totalerror) # If the answer got worse by moving the points, we are done if lasterror and lasterror&lt;totalerror: break lasterror=totalerror # Move each of the points by the learning rate times the gradient for k in range(n): loc[k][0] -= rate*grad[k][0] loc[k][17] -= rate*grad[k][18] return loc 上面的scale_dowm函数传入原始的博客数据 blog_data,然后进行迭代计算，最终返回这些数据在二维空间中对应的向量。每次的迭代时，根据每个点与其他各个点的距离误差调整该点的距离，并且计算出一个总体误差，当本次调整后的误差比上一次要大或者迭代次数达到最大，就跳出循环。 上面返回了各个高纬向量在二维空间中的坐标，因此通过PIL 可以很自然地作出图。原书讲到这里就结束了，但是结合我们KMeans聚类的结果可知，每个聚类中心的向量长度与博客数据的长度一样，因此可以将KMeans聚类得到的聚类中心一并传入到scale_dowm函数中，然后得到聚类中心在二维空间中的坐标，然后以其为中心，连线到其聚类中的各个点。 作图的实现的代码如下所示：12345678910def draw_clusters(blog_data, clusters, cluster_nodes, blog_names, jpeg_path = 'Clustering_data/mds2d.jpg'): img=Image.new('RGB',(2000,2000),(255,255,255)) draw=ImageDraw.Draw(img) for i in xrange(len(clusters)): for node in cluster_nodes[i]: c_x,c_y = (clusters[i][0] + 0.5)*1000, (clusters[i][19] + 0.5)*1000 x, y =(blog_data[node][0]+0.5)*1000, (blog_data[node][20]+0.5)*1000 draw.line((c_x, c_y, x, y),fill=(255,0,0)) draw.text((x,y),blog_names[node],(0,0,0)) img.save(jpeg_path ,'JPEG') 完整的代码可参见 这里 当类别数为3时，上面可视化得到的结果为： 除了博客数据，原书还用了 Zebo 网站上的数据进行了聚类，但是采用的聚类算法仍是我们上面提到的两个聚类算法。只是采用度量距离的标准不同，上面博客数据采用的是皮尔逊系数，而从Zebo 网站上获取的数据的值仅仅是0和1，不宜采用皮尔逊系数，而是采用了 Tanimoto coefficient, 该系数用于表示两者的重合程度。对于两个长度为 $m$ ,值为0或1的向量 $A,B$，其Tanimoto coefficient计算公式如下： $$\frac{\sum_{i=1}^{m}a_ib_i}{\sum_{i=1}^{m}(a_i + b_i + a_ib_i)}$$ 计算出来的值得范围为 [ 0.0, 1.0 ], 且值越大，表示两者相似性越强。下面是求解Tanimoto coefficient 的代码，注意为了用 Tanimoto coefficient 表示距离，最后返回的是 1 - Tanimoto coefficient，表示距离值越小，两者越相似。 1234567def tanamoto(v1,v2): c1,c2,shr=0,0,0 for i in range(len(v1)): if v1[i]!=0: c1+=1 # in v1 if v2[i]!=0: c2+=1 # in v2 if v1[i]!=0 and v2[i]!=0: shr+=1 # in both return 1.0-(float(shr)/(c1+c2-shr))]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用sklearn优雅地进行数据挖掘]]></title>
      <url>%2F2017%2F01%2F16%2F%E4%BD%BF%E7%94%A8sklearn%E4%BC%98%E9%9B%85%E5%9C%B0%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F</url>
      <content type="text"><![CDATA[本文转载自 http://www.cnblogs.com/jasonfreak/p/5448462.html， 主要讲述如何通过 sklearn 完成数据挖掘中的一个完整的流程。 数据挖掘的步骤数据挖掘通常包括数据采集，数据分析，特征工程，训练模型，模型评估等步骤。使用sklearn工具可以方便地进行特征工程和模型训练工作，在《使用sklearn做单机特征工程》中，我们最后留下了一些疑问：特征处理类都有三个方法 fit、transform和fit_transform，而 fit 方法居然和模型训练方法 fit 同名（不光同名，参数列表都一样），这难道都是巧合？ 显然，这不是巧合，这正是sklearn的设计风格。我们能够更加优雅地使用sklearn进行特征工程和模型训练工作。此时，不妨从一个基本的数据挖掘场景入手： 我们使用 sklearn 进行虚线框内的工作（sklearn也可以进行文本特征提取）。通过分析 sklearn 源码，我们可以看到除训练，预测和评估以外，处理其他工作的类都实现了3个方法：fit、transform和fit_transform。从命名中可以看到，fit_transform方法是先调用fit然后调用transform，我们只需要关注 fit 方法和 transform 方法即可。 transform 方法主要用来对特征进行转换。从可利用信息的角度来说，转换分为无信息转换和有信息转换。无信息转换是指不利用任何其他信息进行转换，比如指数、对数函数转换等。有信息转换从是否利用目标值向量又可分为无监督转换和有监督转换。无监督转换指只利用特征的统计信息的转换，统计信息包括均值、标准差、边界等等，比如标准化、PCA法降维等。有监督转换指既利用了特征信息又利用了目标值信息的转换，比如通过模型选择特征、LDA法降维等。通过总结常用的转换类，我们得到下表： 包 类 参数列表 类别 fit方法有用 说明 sklearn.preprocessing StandardScaler 特征 无监督 Y 标准化 sklearn.preprocessing MinMaxScaler 特征 无监督 Y 区间缩放 sklearn.preprocessing Binarizer 特征 无信息 N 定量特征二值化 sklearn.preprocessing OneHotEncoder 特征 无监督 Y 定性特征编码 sklearn.preprocessing Imputer 特征 无监督 Y 缺失值计算 sklearn.preprocessing PolynomialFeatures 特征 无信息 N 多项式变换（fit方法仅仅生成了多项式的表达式） sklearn.preprocessing FunctionTransformer 特征 无信息 N 自定义函数变换（自定义函数在transform方法中调用） sklearn.feature_selection VarianceThreshold 特征 无监督 Y 方差选择法 sklearn.feature_selection SelectKBest 特征/特征+目标值 无监督/有监督 Y 自定义特征评分选择法 sklearn.feature_selection SelectKBest+chi2 特征+目标值 有监督 Y 卡方检验选择法 sklearn.feature_selection RFE 特征+目标值 有监督 Y 递归特征消除法 sklearn.feature_selection SelectFromModel 特征+目标值 有监督 Y 自定义模型训练选择法 sklearn.decomposition PCA 特征 无监督 Y PCA降维 sklearn.lda LDA 特征+目标值 有监督 Y LDA降维 不难看到，只有有信息的转换类的fit方法才实际有用，显然fit方法的主要工作是获取特征信息和目标值信息，在这点上，fit方法和模型训练时的fit方法就能够联系在一起了：都是通过分析特征和目标值，提取有价值的信息，对于转换类来说是某些统计量，对于模型来说可能是特征的权值系数等。另外，只有有监督的转换类的fit和transform方法才需要特征和目标值两个参数。fit方法无用不代表其没实现，而是除合法性校验以外，其并没有对特征和目标值进行任何处理，Normalizer的fit方法实现如下： 1234567def fit(self, X, y=None): """Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines. """ X = check_array(X, accept_sparse='csr') return self 基于这些特征处理工作都有共同的方法，那么试想可不可以将他们组合在一起？在本文假设的场景中，我们可以看到这些工作的组合形式有两种：流水线式和并行式。基于流水线组合的工作需要依次进行，前一个工作的输出是后一个工作的输入；基于并行式的工作可以同时进行，其使用同样的输入，所有工作完成后将各自的输出合并之后输出。sklearn提供了包pipeline来完成流水线式和并行式的工作。 数据初貌在此，我们仍然使用IRIS数据集来进行说明。为了适应提出的场景，对原数据集需要稍微加工： 1234567891011from numpy import hstack, vstack, array, median, nanfrom numpy.random import choicefrom sklearn.datasets import load_iris#特征矩阵加工#使用vstack增加一行含缺失值的样本(nan, nan, nan, nan)#使用hstack增加一列表示花的颜色（0-白、1-黄、2-红），花的颜色是随机的，意味着颜色并不影响花的分类iris.data = hstack((choice([0, 1, 2], size=iris.data.shape[0]+1).reshape(-1,1), vstack((iris.data, array([nan, nan, nan, nan]).reshape(1,-1)))))#目标值向量加工#增加一个目标值，对应含缺失值的样本，值为众数iris.target = hstack((iris.target, array([median(iris.target)]))) 关键技术并行处理，流水线处理，自动化调参，持久化是使用sklearn优雅地进行数据挖掘的核心。并行处理和流水线处理将多个特征处理工作，甚至包括模型训练工作组合成一个工作（从代码的角度来说，即将多个对象组合成了一个对象）。在组合的前提下，自动化调参技术帮我们省去了人工调参的反锁。训练好的模型是贮存在内存中的数据，持久化能够将这些数据保存在文件系统中，之后使用时无需再进行训练，直接从文件系统中加载即可。 并行技术并行处理使得多个特征处理工作能够并行地进行。根据对特征矩阵的读取方式不同，可分为整体并行处理和部分并行处理。整体并行处理，即并行处理的每个工作的输入都是特征矩阵的整体；部分并行处理，即可定义每个工作需要输入的特征矩阵的列。 整体并行处理pipeline包提供了FeatureUnion类来进行整体并行处理：12345678910111213from numpy import log1pfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizerfrom sklearn.pipeline import FeatureUnion#新建将整体特征矩阵进行对数函数转换的对象step2_1 = ('ToLog', FunctionTransformer(log1p))#新建将整体特征矩阵进行二值化类的对象step2_2 = ('ToBinary', Binarizer())#新建整体并行处理对象#该对象也有fit和transform方法，fit和transform方法均是并行地调用需要并行处理的对象的fit和transform方法#参数transformer_list为需要并行处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象step2 = ('FeatureUnion', FeatureUnion(transformer_list=[step2_1, step2_2, step2_3])) 部分并行处理整体并行处理有其缺陷，在一些场景下，我们只需要对特征矩阵的某些列进行转换，而不是所有列。pipeline并没有提供相应的类（仅OneHotEncoder类实现了该功能），需要我们在FeatureUnion的基础上进行优化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from sklearn.pipeline import FeatureUnion, _fit_one_transformer, _fit_transform_one, _transform_one from sklearn.externals.joblib import Parallel, delayedfrom scipy import sparseimport numpy as np#部分并行处理，继承FeatureUnionclass FeatureUnionExt(FeatureUnion): #相比FeatureUnion，多了idx_list参数，其表示每个并行工作需要读取的特征矩阵的列 def __init__(self, transformer_list, idx_list, n_jobs=1, transformer_weights=None): self.idx_list = idx_list FeatureUnion.__init__(self, transformer_list=map(lambda trans:(trans[0], trans[1]), transformer_list), n_jobs=n_jobs, transformer_weights=transformer_weights) #由于只部分读取特征矩阵，方法fit需要重构 def fit(self, X, y=None): transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list) transformers = Parallel(n_jobs=self.n_jobs)( #从特征矩阵中提取部分输入fit方法 delayed(_fit_one_transformer)(trans, X[:,idx], y) for name, trans, idx in transformer_idx_list) self._update_transformer_list(transformers) return self #由于只部分读取特征矩阵，方法fit_transform需要重构 def fit_transform(self, X, y=None, **fit_params): transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list) result = Parallel(n_jobs=self.n_jobs)( #从特征矩阵中提取部分输入fit_transform方法 delayed(_fit_transform_one)(trans, name, X[:,idx], y, self.transformer_weights, **fit_params) for name, trans, idx in transformer_idx_list) Xs, transformers = zip(*result) self._update_transformer_list(transformers) if any(sparse.issparse(f) for f in Xs): Xs = sparse.hstack(Xs).tocsr() else: Xs = np.hstack(Xs) return Xs #由于只部分读取特征矩阵，方法transform需要重构 def transform(self, X): transformer_idx_list = map(lambda trans, idx:(trans[0], trans[1], idx), self.transformer_list, self.idx_list) Xs = Parallel(n_jobs=self.n_jobs)( #从特征矩阵中提取部分输入transform方法 delayed(_transform_one)(trans, name, X[:,idx], self.transformer_weights) for name, trans, idx in transformer_idx_list) if any(sparse.issparse(f) for f in Xs): Xs = sparse.hstack(Xs).tocsr() else: Xs = np.hstack(Xs) return Xs 在本文提出的场景中，我们对特征矩阵的第1列（花的颜色）进行定性特征编码，对第2、3、4列进行对数函数转换，对第5列进行定量特征二值化处理。使用FeatureUnionExt类进行部分并行处理的代码如下：123456789101112131415from numpy import log1pfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizer#新建将部分特征矩阵进行定性特征编码的对象step2_1 = ('OneHotEncoder', OneHotEncoder(sparse=False))#新建将部分特征矩阵进行对数函数转换的对象step2_2 = ('ToLog', FunctionTransformer(log1p))#新建将部分特征矩阵进行二值化类的对象step2_3 = ('ToBinary', Binarizer())#新建部分并行处理对象#参数transformer_list为需要并行处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象#参数idx_list为相应的需要读取的特征矩阵的列step2 = ('FeatureUnionExt', FeatureUnionExt(transformer_list=[step2_1, step2_2, step2_3], idx_list=[[0], [1, 2, 3], [4]])) 流水线处理pipeline包提供了Pipeline类来进行流水线处理。流水线上除最后一个工作以外，其他都要执行fit_transform方法，且上一个工作输出作为下一个工作的输入。最后一个工作必须实现fit方法，输入为上一个工作的输出；但是不限定一定有transform方法，因为流水线的最后一个工作可能是训练！ 根据本文提出的场景，结合并行处理，构建完整的流水线的代码如下：123456789101112131415161718192021222324252627282930313233from numpy import log1pfrom sklearn.preprocessing import Imputerfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.preprocessing import Binarizerfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2from sklearn.decomposition import PCAfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import Pipeline#新建计算缺失值的对象step1 = ('Imputer', Imputer())#新建将部分特征矩阵进行定性特征编码的对象step2_1 = ('OneHotEncoder', OneHotEncoder(sparse=False))#新建将部分特征矩阵进行对数函数转换的对象step2_2 = ('ToLog', FunctionTransformer(log1p))#新建将部分特征矩阵进行二值化类的对象step2_3 = ('ToBinary', Binarizer())#新建部分并行处理对象，返回值为每个并行工作的输出的合并step2 = ('FeatureUnionExt', FeatureUnionExt(transformer_list=[step2_1, step2_2, step2_3], idx_list=[[0], [1, 2, 3], [4]]))#新建无量纲化对象step3 = ('MinMaxScaler', MinMaxScaler())#新建卡方校验选择特征的对象step4 = ('SelectKBest', SelectKBest(chi2, k=3))#新建PCA降维的对象step5 = ('PCA', PCA(n_components=2))#新建逻辑回归的对象，其为待训练的模型作为流水线的最后一步step6 = ('LogisticRegression', LogisticRegression(penalty='l2'))#新建流水线处理对象#参数steps为需要流水线处理的对象列表，该列表为二元组列表，第一元为对象的名称，第二元为对象pipeline = Pipeline(steps=[step1, step2, step3, step4, step5, step6]) 自动化调参网格搜索为自动化调参的常见技术之一，grid_search包提供了自动化调参的工具，包括GridSearchCV类。对组合好的对象进行训练以及调参的代码如下：12345678from sklearn.grid_search import GridSearchCV#新建网格搜索对象#第一参数为待训练的模型 #param_grid为待调参数组成的网格，字典格式，键为参数名称（格式“对象名称__子对象名称__参数名称”），值为可取的参数值列表 grid_search = GridSearchCV(pipeline, param_grid=&#123;'FeatureUnionExt__ToBinary__threshold':[1.0, 2.0, 3.0, 4.0], 'LogisticRegression__C':[0.1, 0.2, 0.4, 0.8]&#125;)#训练以及调参grid_search.fit(iris.data, iris.target) 持久化externals.joblib包提供了dump和load方法来持久化和加载内存数据：1234567#持久化数据#第一个参数为内存中的对象#第二个参数为保存在文件系统中的名称#第三个参数为压缩级别，0为不压缩，3为合适的压缩级别dump(grid_search, 'grid_search.dmp', compress=3)#从文件系统中加载数据到内存中grid_search = load('grid_search.dmp') 小结 包 类或方法 说明 sklearn.pipeline Pipeline 流水线处理 sklearn.pipeline FeatureUnion 并行处理 sklearn.grid_search GridSearchCV 网格搜索调参 externals.joblib dump 数据持久化 externals.joblib load 从文件系统中加载数据至内存 组合和持久化都会涉及pickle技术，在sklearn的技术文档中有说明，将lambda定义的函数作为FunctionTransformer的自定义转换函数将不能pickle化。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[先验概率，后验概率，共轭分布与共轭先验]]></title>
      <url>%2F2017%2F01%2F08%2F%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%8C%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%EF%BC%8C%E5%85%B1%E8%BD%AD%E5%88%86%E5%B8%83%E4%B8%8E%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C%2F</url>
      <content type="text"><![CDATA[本文主要讲述先验概率，后验概率，共轭分布和共轭先验这几个概念。 众所周知，概率论中有两大学派：频率学派和贝叶斯学派。先验概率，后验概率，共轭分布和共轭先验是贝叶斯学派中的几个概念。原因是贝叶斯学派认为分布存在先验分布和后验分布的不同，而频率学派则认为一个事件的概率只有一个。 下面先以一个直观的例子来说明先验概率和后验概率的概念 比如说，你来到一个山洞,这个山洞里可能有熊也可能没有熊, 记你觉得山洞有熊的为事件 $Y$. 然后,你也许听到山洞里传来熊的吼声, 记听到熊吼声为事件 $X$. 你一开始认为山洞有熊的概率是 $P(Y)$; 听到熊的吼声之后,你认为有熊的概率是 $P(Y|X)$。在这里，$P(Y)$就是先验概率,$P(Y|X)$是后验概率. 回到概率论中一个经典的例子:抛硬币。抛硬币时抛出正面的概率为多大？假如事前关于这枚硬币没有任何额外信息，那么一般都会认为是 1/2，这时候的 1/2 就是正面朝上的先验概率 。但是在经过一系列实验确认后再得到的正面朝上的概率很可能就不是1/2了(受到到硬币的质地，重量分布等因素的影响)，这个概率便是后验概率。 简单理解就是在事件发生之前，根据以往的经验推测的与该事件相关的概率就是先验概率，而在事件(试验)真正发生后，通过事件(试验)的结果可以修正先验概率，从而得到后验概率。 那么对于抛硬币这个事件来说，抛出正面硬币的概率就应该是一个概率的概率，也就是说它的结果不是一个单一的值 1/2，而是一个概率分布，可能有很高的概率是1/2，但是也有一定的概率是100%（比如抛100次结果还真都100次都是正面）。那么在这里这个概率的分布用函数来表示就是一个似然函数，所以似然函数也被称为“分布的分布”。用公式来表示就是： 后验概率（posterior probability）∝ 似然函数（likelyhood function）*先验概率（prior probability） 即： $P(X|D) ∝ P(μ|D)*P(X)$ 这里 $D$ 表示一组观测实验(比如我扔了五次硬币得到5次正反面的结果)，$X$ 表示随机变量（在这里是硬币的正反面），表示随机函数里面的参数（在这里就是硬币掷为正面的概率）。 注意这里是正比于而不是等于，这个是理解似然函数的一个关键，右侧直接的乘积其实是不满足概率分布归一化的条件的（就是右侧的积分最后不会等于1）那么这个正比符号怎样才能变成等号呢？其实只要再除以一个系数进行归一化就可以了： $P(X|D) =P(μ|D)*P(X)/P(D)$ 这个归一化的系数是怎么来的呢？让我们回忆一下贝叶斯公式： $P(X|D)*P(D)=P(XD)$ 而$P(μ|D)=P(D|X)$(似然函数在计算时的做法就是将D的观察结果代入P(X)的分布式子中去得到的) 于是 $(P(μ|D)/P(D))*P(X)=P(D|X)*P(X)/P(D)=P(XD)/P(D)=P(X|D)$ 似然函数的形式是依赖于观测值的，它在贝叶斯学派与频率学派都有很大的作用，不过在两家的用法并不相同。 频率学派认为每个事件的概率是一个客观存在的常数值，只是我们不知道而已。比如抛硬币，在实验估计之前我们不知道它是多少，频率学派也不会管之前大家说抛硬币出现正面的概率是1/2还是多少，所谓“眼见为实，耳听为虚”，他们的最终结论只和在实验中观测到的数据有关系。但是它肯定是一个确定的常数，然后我们通过观察实验，获得一组样本值 $D$，再将这组样本值代入似然函数 $P(D|X)$ ，求解使得似然函数最大的值就是估计出来的（当然由于实验的结果不同，这个估计出来的也很可能不是1/2，实验不同得到的结果也不同，但是根据大数定律，理论上实验次数足够多以后，求出来的是会越来越接近真实的概率的）。也就是说频率学派认为答案只有一个，我们不断地通过各种估计法来猜测这个值。 而贝叶斯学派并不会完全拒绝大家之前所说的“硬币扔出正面的概率是1/2”的说法，只是贝叶斯学派认为最终硬币扔出正面反面的概率并不是一个常数值，不是一个有唯一答案的真理，这个值本身应该也是一个随机变量，是在不断变化的一个数值，如何得到这个值，贝叶斯学派认为也需要通过实验在“硬币扔出正面的概率是1/2”的说法（先验概率）的基础上通过实验数据（似然函数）不断去预估这个扔出正面概率的实际分布（后验分布）。 举个例子：假如我扔了5次硬币，先出现了3次正面，后出现了两次反面，那么这时的似然函数就应该是 $P(μ|D)=P(D|X)=L(μ)=μ*μ*μ*（1-μ）*（1-μ）$ ($\mu$ 是硬币抛正面的概率，在似然函数里就相当于概率分布函数里的随机变量一样变成一个随机变化的值了） 如果用我们以前统计课本上的频率学派的最大似然估计法，对$L(μ)$求导求最大值，得到 $μ=3/5$， 那么得出结论就是最后抛硬币为正面的概率就是 3/5，当然还要附上一个参数估计的置信度，表示这个结论自然不是100%准确的 但是如果采用贝叶斯学派的后验概率$P(X|D) = P(D|X)*P(X)/P(D)=L(μ)*P(X)/P(D)$， 其中 $P(D)$ 可以简单地由古典概型算出来：$P(D)=1/=1/32=0.03125$。如果 $μ$ 取了 3/5，代入上式那么抛硬币为正面的概率就是 0.55296，而不是1/2，当然贝叶斯学派最终得到的后验概率是一个随 $μ$ 变化的分布，只不过在这种情况这个分布取到 0.55296 这个值的概率最大而已 清楚似然函数、先验概率、后验概率的几个贝叶斯学派的基本概念，要明白共轭分布和共轭先验就很简单了，所谓共轭分布就是先验概率和后验概率具有一样函数形式的分布形式，举个例子就是假如先验分布函数是形如 $C_1\mu^a (1-\mu)^b$ 的形式（比如二项分布就是这种形式）而后验分布是 $C_2\mu^m (1-\mu)^n$ 这样的形式，两者只是具体参数值不同，或者先验分布和后验分布都是高斯分布等等的情形就可为认为先验分布和后验分布具有同样的形式。 这种形式不变的好处是，我们能够在先验分布中赋予参数很明确的物理意义，这个物理意义可以延续到后验分布中进行解释，同时从先验变换到后验的过程中从数据中补充的知识也容易有物理解释。同时能够后验分布和先验分布共轭的情况下是可以大大简化计算量。 那么共轭先验又是什么概念呢？因为在现实建模问题中，往往我们先得到和固定的反而是似然函数（其实也很好理解，客观的实验观察数据才是第一手最solid的材料），这时先验函数（可以理解为先验知识或者是对后验分布的一种假设和猜测）是可以选择的。这时如果我选的先验分布最后乘上这个似然函数，使得后验分布与先验分布共轭，那么我们就称这个先验函数为似然函数的共轭先验。基于上面说到的共轭分布的好处，往往选择先验函数时都会让先验概率分布和后验概率分布共轭。 共轭分布与共轭先验条件概率和后验概率有什么不同？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用sklearn做单机特征工程]]></title>
      <url>%2F2017%2F01%2F03%2F%E4%BD%BF%E7%94%A8sklearn%E5%81%9A%E5%8D%95%E6%9C%BA%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[本文转载自 http://www.cnblogs.com/jasonfreak/p/5448385.html ,在某些部分会拓展补充一些内容，全文主要讲述有关特征工程中通常使用的方法以及在sklearn中的相关实现。 特征工程是什么有这么一句话在业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。通过总结和归纳，人们认为特征工程包括以下方面： 特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。首次接触到sklearn，通常会被其丰富且方便的算法模型库吸引，但是这里介绍的特征处理库也十分强大！ 本文中使用sklearn中的IRIS（鸢尾花）数据集来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris Virginica（维吉尼亚鸢尾））。导入IRIS数据集的代码如下：12345678910from sklearn.datasets import load_iris#导入IRIS数据集iris = load_iris()#特征矩阵iris.data#目标向量iris.target 数据预处理通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题： 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。 信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。 存在缺失值：缺失值需要补充。 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。 我们使用sklearn中的 preproccessing 库来进行数据预处理，可以覆盖以上问题的解决方案。 无量纲化 无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和归一化。 标准化一般指的是0均值标准化(zero-mean normalization)也叫z-score标准化。z-score标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布，即均值为0，标准差为1。 归一化一般指的是区间缩放法，利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。 标准化0均值标准化需要计算特征的均值和标准差，公式表达为：$$x’ = \frac{x - \mu}{\sigma}$$公式中的 $\mu$ 为原始数据的期望， $\sigma$为原始数据的标准差。 使用 preproccessing 库的 StandardScaler 类对数据进行标准化的代码如下： 1234from sklearn.preprocessing import StandardScaler#标准化，返回值为标准化后的数据StandardScaler().fit_transform(iris.data) 标准化的原理比较复杂，它表示的是原始值与均值之间差多少个标准差，是一个相对值，所以也有去除量纲的功效。同时，它还带来两个附加的好处：均值为0，标准差为1。关于这两个属性的具体好处参考 http://www.zhaokv.com/2016/01/normalization-and-standardization.html 如下 均值为0有什么好处呢？它可以使数据以0为中心左右分布（这不是废话嘛），而数据以0为中心左右分布会带来很多便利。比如在去中心化的数据上做SVD分解等价于在原始数据上做PCA；机器学习中很多函数如Sigmoid、Tanh、Softmax等都以0为中心左右分布（不一定对称）。 标准差为1有什么好处呢？这个更复杂一些。对于$x_i, x_{i’}$两点间距离，往往表示为$$D(x_i,x_{i’})=\sum\limits_{j=1}^pw_j\cdot d_j(x_{ij},x_{i’j})(\sum\limits_{j=1}^pw_j=1)$$其中 $d_j(x_{ij},x_{i’j})$是属性 $j$ 两个点之间的距离, $w_j$ 是该属性间距离在总距离中的权重注意设 $w_j=1,\forall j$并不能实现每个属性对最后的结果贡献度相同。对于给定的数据集，所有点对间距离的平均值是个定值，即$$\bar{D}=\frac{1}{N^2}\sum\limits_{i=1}^N\sum\limits_{i’=1}^ND(x_i,x_{i’})=\sum\limits_{j=1}^pw_j\cdot \bar{d}_j$$是个常数，其中 $\bar{d}_j=\frac{1}{N^2}\sum\limits_{i=1}^N\sum\limits_{i’=1}^Nd_j(x_{ij}, x_{x’j})$, 可见第 $j$ 个变量对最终整体平均距离的影响是 $w_j\cdot \bar{d}_j$,所以设 $w_j\sim 1/\bar{d}_j$ 可以使所有属性对全体数据集平均距离的贡献相同。现在设 $d_j$ 为欧氏距离的平方，它是最常用的距离衡量方法之一，则有$$\bar{d_j}=\frac{1}{N^2}\sum\limits_{i=1}^N\sum\limits_{i’=1}^N(x_{ij}-x_{i’j})^2=2\cdot var_j$$其中 $var_j$ 是 $Var(X_j)$ 样本估计，也就是说每个变量的重要程度正比于这个变量在这个数据集上的方差。如果我们让每一维变量的标准差都为1（即方差都为1），每维变量在计算距离的时候重要程度相同。 因此在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，往往采用标准化方法。 归一化区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：$$x’ = \frac{x - Min}{Max - Min}$$ 使用 preproccessing 库的 MinMaxScaler 类对数据进行区间缩放的代码如下： 1234from sklearn.preprocessing import MinMaxScaler#区间缩放，返回值为缩放到[0, 1]区间的数据MinMaxScaler().fit_transform(iris.data) 归一化的依据非常简单，不同变量往往量纲不同，归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。比如两个人体重差10KG，身高差0.02M，在衡量两个人的差别时体重的差距会把身高的差距完全掩盖，归一化之后就不会有这样的问题。 除此之外，归一化后能够加快梯度下降求最优解的速度，如下图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征 $X1$ 和 $X2$ 的区间相差非常大，$X1$ 区间是[0,2000]，$X2$区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。 因此。在涉及到计算点与点之间的距离时，使用归一化或标准化都会对最后的结果有所提升，甚至会有质的区别。那在归一化与标准化之间应该如何选择呢？根据上面论述我们看到，如果把所有维度的变量一视同仁，在最后计算距离中发挥相同的作用应该选择标准化，如果想保留原始数据中由标准差所反映的潜在权重关系应该选择归一化。另外，标准化更适合现代嘈杂大数据场景。 对定量特征二值化定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。使用 preproccessing 库的 Binarizer 类对数据进行二值化的代码如下： 1234from sklearn.preprocessing import Binarizer#二值化，阈值设置为3，返回值为二值化后的数据Binarizer(threshold=3).fit_transform(iris.data) 对定性特征哑编码由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用 preproccessing 库的 OneHotEncoder 类对数据进行哑编码的代码如下： 1234from sklearn.preprocessing import OneHotEncoder#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据OneHotEncoder().fit_transform(iris.target.reshape((-1,1))) 缺失值计算由于IRIS数据集没有缺失值，故对数据集新增一个样本，4个特征均赋值为NaN，表示数据缺失。使用 preproccessing 库的 Imputer 类对数据进行缺失值计算的代码如下： 1234567from numpy import vstack, array, nanfrom sklearn.preprocessing import Imputer#缺失值计算，返回值为计算缺失值后的数据#参数missing_value为缺失值的表示形式，默认为NaN#参数strategy为缺失值填充方式，默认为mean（均值）Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))) 数据变换数据变换就是通过组合或变换方式将原来的特征转换为新的特征，常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。 例如，输入的一个二维特征为 $[a, b]$,则度为2的多项式特征为 $ [1, a, b, a^2, ab, b^2]$ 使用 preproccessing 库的 PolynomialFeatures 类对数据进行多项式转换的代码如下： 12345from sklearn.preprocessing import PolynomialFeatures#多项式转换#参数degree为度，默认值为2PolynomialFeatures().fit_transform(iris.data) 基于单变元函数的数据变换可以使用一个统一的方式完成，使用 preproccessing 库的 FunctionTransformer 对数据进行对数函数转换的代码如下： 123456from numpy import log1pfrom sklearn.preprocessing import FunctionTransformer#自定义转换函数为对数函数的数据变换#第一个参数是单变元函数FunctionTransformer(log1p).fit_transform(iris.data) 小结 类 功能 说明 StandardScaler 无量纲化 标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布 MinMaxScaler 无量纲化 区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上 Normalizer 归一化 基于特征矩阵的行，将样本向量转换为“单位向量” Binarizer 二值化 基于给定阈值，将定量特征按阈值划分 OneHotEncoder 哑编码 将定性数据编码为定量数据 Imputer 缺失值计算 计算缺失值，缺失值可填充为均值等 PolynomialFeatures 多项式数据转换 多项式数据转换 FunctionTransformer 自定义单元数据转换 使用单变元的函数来转换数据 特征选择当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 我们使用sklearn中的feature_selection库来进行特征选择。 Filter方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用 feature_selection 库的 VarianceThreshold 类来选择特征的代码如下：12345from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(iris.data) 相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用 feature_selection 库的 SelectKBest 类结合相关系数来选择特征的代码如下： 1234567from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) 卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。具体的意义可参考这篇文章卡方检验原理及应用。用 feature_selection 库的 SelectKBest 类结合卡方检验来选择特征的代码如下 12345from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target) 互信息法经典的互信息也是变量间相互依赖性的量度，互信息计算公式如下：$$I(X,Y) = \sum_{x \in X}\sum_{y \in Y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}$$为了处理定量数据，最大信息系数法被提出，使用 feature_selection 库的 SelectKBest 类结合最大信息系数法来选择特征的代码如下： 1234567891011from sklearn.feature_selection import SelectKBestfrom minepy import MINE#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5def mic(x, y): m = MINE() m.compute_score(x, y) return (m.mic(), 0.5)#选择K个最好的特征，返回特征选择后的数据SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) Wrapper递归特征消除法递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用feature_selection库的RFE类来选择特征的代码如下：1234567from sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegression#递归特征消除法，返回特征选择后的数据#参数estimator为基模型#参数n_features_to_select为选择的特征个数RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target) Embedded基于惩罚项的特征选择法使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下 12345from sklearn.feature_selection import SelectFromModelfrom sklearn.linear_model import LogisticRegression#带L1惩罚项的逻辑回归作为基模型的特征选择SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target) L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值，故需要构建一个新的逻辑回归模型： 12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.linear_model import LogisticRegressionclass LR(LogisticRegression): def __init__(self, threshold=0.01, dual=False, tol=1e-4, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1): #权值相近的阈值 self.threshold = threshold LogisticRegression.__init__(self, penalty='l1', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight=class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) #使用同样的参数创建L2逻辑回归 self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, class_weight = class_weight, random_state=random_state, solver=solver, max_iter=max_iter, multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs) def fit(self, X, y, sample_weight=None): #训练L1逻辑回归 super(LR, self).fit(X, y, sample_weight=sample_weight) self.coef_old_ = self.coef_.copy() #训练L2逻辑回归 self.l2.fit(X, y, sample_weight=sample_weight) cntOfRow, cntOfCol = self.coef_.shape #权值系数矩阵的行数对应目标值的种类数目 for i in range(cntOfRow): for j in range(cntOfCol): coef = self.coef_[i][j] #L1逻辑回归的权值系数不为0 if coef != 0: idx = [j] #对应在L2逻辑回归中的权值系数 coef1 = self.l2.coef_[i][j] for k in range(cntOfCol): coef2 = self.l2.coef_[i][k] #在L2逻辑回归中，权值系数之差小于设定的阈值，且在L1中对应的权值为0 if abs(coef1-coef2) &lt; self.threshold and j != k and self.coef_[i][k] == 0: idx.append(k) #计算这一类特征的权值系数均值 mean = coef / len(idx) self.coef_[i][idx] = mean return self 使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下： 12345from sklearn.feature_selection import SelectFromModel#带L1和L2惩罚项的逻辑回归作为基模型的特征选择#参数threshold为权值系数之差的阈值SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target) 基于树模型的特征选择法树模型中GBDT也可用来作为基模型进行特征选择，使用 feature_selection 库的 SelectFromModel 类结合 GBDT 模型，来选择特征的代码如下： 12345from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import GradientBoostingClassifier#GBDT作为基模型的特征选择SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target) 小结 类 所属方式 说明 VarianceThreshold Filter 方差选择法 SelectKBest Filter 可选关联系数、卡方校验、最大信息系数作为得分计算的方法 RFE Wrapper 递归地训练基模型，将权值系数较小的特征从特征集合中消除 SelectFromModel Embedded 训练基模型，选择权值系数较高的特征 降维当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。 主成分分析法（PCA）使用 decomposition 库的PCA类选择特征的代码如下：12345from sklearn.decomposition import PCA#主成分分析法，返回降维后的数据#参数n_components为主成分数目PCA(n_components=2).fit_transform(iris.data) 线性判别分析法（LDA）使用lda库的LDA类选择特征的代码如下：12345from sklearn.lda import LDA#线性判别分析法，返回降维后的数据#参数n_components为降维后的维数LDA(n_components=2).fit_transform(iris.data, iris.target) 小结 库 类 说明 decomposition PCA 主成分分析法 lda LDA 线性判别分析法 总结再让我们回归一下本文开始的特征工程的思维导图，我们可以使用sklearn完成几乎所有特征处理的工作，而且不管是数据预处理，还是特征选择，抑或降维，它们都是通过某个类的方法fit_transform完成的，fit_transform要不只带一个参数：特征矩阵，要不带两个参数：特征矩阵加目标向量。这些难道都是巧合吗？还是故意设计成这样？方法fit_transform中有fit这一单词，它和训练模型的fit方法有关联吗？接下来，在《使用sklearn优雅地进行数据挖掘》中将会阐述其中的奥妙！ 参考：归一化与标准化研究｜数据预处理｜归一化 （标准化）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算机网络课程总结--RIP与OSPF]]></title>
      <url>%2F2016%2F12%2F27%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--RIP%E4%B8%8EOSPF%2F</url>
      <content type="text"><![CDATA[本文主要讲述内部网关协议中两个著名的协议RIP和OSPF。 路由器的基本功能是路由(Routing)和转发(Forwarding)。其中路由指的是通过路由选择协议将路由信息注入到路由表中，转发指的是依据路由表和分组携带的信息将分组从入口转到正确的出口的过程。 分组转发的技术有以下几种 Source routing（源路由）：分组携带路径 Table of virtual circuits（虚电路）：穿越网络建立链接及状态，使用链接转发分组 Table of global addresses (IP，数据报交换)：路由器维持到目的地的下一条，分组需要携带目的地地址 本文主要涉及到的是第三种。 静态路由配置命令 Ip route prefix mask {address|interface} [distance] 使用地址(address)和使用接口(interface)的差别 使用接口，路由器知道从哪里转发出去，更高效 使用地址需要第二次查找，以确定转发接口 距离(distance)的特定 直连网络的管理距离为0 静态路由的管理距离为1 值越小，优先级越高 到达某个网络出现多条路由的情况下，可以依据管理距离进行选择 还可以通过管理距离配置备份路由（也称为浮动静态路由），如下图所示 动态路由动态路由就是通过协议动态地学习到路由信息，据其作用域的不同，又可分为内部网管协议(IGP)和边界网络协议(BGP)；内部网关协议有RIP，OSPF，IGRP，EIGRP等，其在网络中的位置入下 这里主要讲述内部网络协议中的RIP和OSPF。 RIPRIP是典型DV(Distance Vector,距离矢量)协议，其工作原理如下： 每个路由器维护两个向量$D_i$和$S_i$来表示该点到网上所有节点的路径距离及其下一个节点 相邻路由器之间交换路径信息 各节点根据路径信息更新路由表 向量$D_i$和$S_i$的含义如下:$d_{i1}$：从节点i 到节点1 的时延向量$S_{i1}$：从节点i到节点1的一条最小时延路径上的下一个节点n ：网络中的节点数 RIP使用跳数作为路由选择的度量，当到达目的网络的跳数超过15跳，数据包将被丢掉。RIP路由更新广播默认周期为30秒。 下图是RIP的一个简单例子 RIP在ipv4中的协议版本有RIPv1和RIPv2,其中 RIPv1 只支持传输自然网段的地址(也就是ABC三类网络)，而 RIPv2 修复了这个缺陷,增加了一个子网掩码的段，支持所有长度的子网掩码，也支持 CIDR 和 VLSM。 路由环路RIP协议可能会导致路由环路，下图为一个路由环路的例子，图中初始状态是正常状态，但是当40.0.0.0的网络挂掉后，C 不会修改到网络 40.0.0.0 的距离不可达，而是相信B传过来的关于 40.0.0.0 网路的距离为 1 的消息，从而导致错误信息不断在网络中传输，每个路由器到 40.0.0.0 的距离不断增大，直到距离超过15才将网络40.0.0.0 标记为不可达。 虽然上面的例子中距离增大到16的时候会将网络 40.0.0.0 标记为不可达，但是这样的话收敛的速度会非常慢。为了提高收敛的速度，常常会有以下方法 水平分割（Split Horizon） 毒性逆转（Poison Reverse） 抑制定时器（Hold-Down Timers） 触发更新（Triggered Updates） 水平分割（Split Horizon）分析上面的例子中路径环产生的原因，就是B向C提供了一条过时的、错误的路由信息。 但是分析可知，B必须经由C方可到达网络40.0.0.0，所以B不可能向C提供任何有价值的路由信息。因此可以修改B对C提供的路由，禁止B向C提供关于此信宿的路由信息，具体操作就是B告诉C一条在正常情况下不真实的消息：网络40.0.0.0不可达（距离为无穷大，实际中记为16即可）。 通过水平分割可以使得上面的情况的收敛速度加快，如下所示 毒性逆转（Poison Reverse）分析上面的例子，当网络40.0.0.0 挂掉的时候，路由器C并没有采取任何措施，而是利用了B的信息更新。 而毒性逆转的方法就是指当C 发现网络40.0.0.0发生故障时，主动将到达信宿的距离改为无穷大（实际中改为16即可），收敛过程如下图所示 抑制定时器（Hold-Down Timers）当C发现网络40.0.0.0发生故障时，启动抑制计时器。 在抑制计时期间内，有三种可能 如果网络状态转变，即down→up，则关闭计时器，保留原有路由信息 如果收到来自B的关于信宿的路由信息，且路径比原有路径短，则关闭计时器，更新路由信息 如果无上述两种情况发生，计时器到时，更新路由为信宿不可达。 触发更新（Triggered Updates）当C发现网络40.0.0.0发生故障时，不等下一刷新周期到来，立刻更改路由为“信宿不可达”，引起全网的连锁反映，迅速刷新 类似于RIP这种DV类协议的优点是算法简单，但是缺点有： 交换的路径信息量大 路径信息传播慢，使得路径信息可能不一致。 收敛速度慢，存在无穷计算问题。 不适合大型网络 RIPngRIPng 是IPV6 中使用的RIP协议，保留了原来 RIP 协议的一些特性，也增加了一些新的特点。 RIPng有以下特点 UDP端口号：使用521端口收、发报文（RIPv2用520端口） 组播地址：FF02::9作为链路本地范围的路由器组播地址 源地址：使用链路本地地址FE80::/10作为源地址发送RIPng路由信息报文 下一跳地址：使用128位的IPv6地址 ’RIPng 和 RIPv2的报文对比如下其中RIPng的一个报文种可包含多条路由信息，而RIPv2一个报文只含有一条路由信息。 RIPng的报文格式为 RIPv2 的报文格式为 RIPng中有Request和Response两种报文，其作用分别如下Request报文：当路由器启动或更新时，发该类报文（组播），用于请求其他路由器的路由信息Response报文：对Request的回应，通常包含全部的路由信息。除了收到Request的时候会发送，路由器还会周期性地发送。 配置RIP的配置的主要工作就是启动RIP进程并声明路由器所连接的网络。如下图所示 默认是RIPv1，如果要配置RIPv2,需要version 2的命令，如下图所示 RIPng的配置过程入下 OSPFOSPF时典型的LS(Link State,链路状态)协议，其思想是通过邻居的LSA(Link-state advertisement, 链路转态通告)构建整个网络的拓扑并构建最小生成树，然后通过dijkstra 计算最短路径，并根据最短路径修改路由表。 OSPF有以下特点 无路由自环 支持VLSM、CIDR 使用带宽作为度量值（108/BW） 收敛速度快 通过分区实现高效的网络管理 支持帧中继，X2.5, Ethernet, ppp等网络 可以在大型网络中使用 OSPF协议的一些基本概念如下; 协议号=89：IP头中代表OSPF报文的协议号是89 RouterID：一个32位的无符号整数，是一台路由器的唯一标识，在整个自治系统内唯一 路由器间的关系：邻居(Neighbor)、邻接(Adjacent)、未知(Unknown) TTL=1：通常OSPF报文不转发，只被传递一条，即在IP报头的TTL值被设为1， DR(Designated Router)和BDR(Backup Designated Router): DR是由所有路由器选举出来的，目的是减少路由器间同步的次数，所有路由器只需要和选举出来的DR进行同步即可，原理如下图所示，BDR是DR的备份路由器。 这里需要注意的是，DR是路由器选出来的，选举规则根据其priority值的大小，若priority相同则比较router id；DR一旦当选，除非路由器故障，否则不会更换；DR选出的同时，也选出BDR，DR故障后，由BDR接替DR成为新的DR 报文类型OSPF的分组有五种，分别如下所示 OSPF报文类型 作用 Type = 1, Hello报文 建立和维护连接，报文中包含自己的RouterID和区域ID Type = 2，DD(数据库描述)报文 描述一个OSPF路由器的链路状态数据库内容，传输LSA摘要 Type = 3，LSR(链路状态请求)报文 请求对方发送自己没有的LSA Type = 4，LSU(链路状态更新)报文 LSR 的应答，可回传多条LSA Type = 3，LSAck(链路状态确认)报文 确认收到LSA 建立毗邻关系OSPF运行的步骤为 建立路由器毗邻关系 选举DR和BDR 发现路由 选择最佳路由 维护路由信息 OSPF中，一个路由器的的状态可能为 Down Init（初始） Two-way（双向） ExStart（准启动） Exchange（交换） Loading（加载） Full adjacency（全毗邻） 结合路由器的状态，OSPF建立毗邻关系的步骤如下 1）RT1，RT2在某个接口激活了OSPF后，都会开始在这个接口上去发组播的Hello报文，目的是发现OSPF邻居，此时双方都处于Down状态。 2）当RT2收到RT1发来的Hello包（Neighbors Seen 为空），此时RT2的状态变为init，然后将RT1的Router-ID存储放在Hello报文中(Neighbors Seen = RT1)发送出去，当RT1收到这个hello报文并从中找到自己的Router-ID，RT1会认为与RT2已经完成了双边关系的建立，此时RT1的状态变为Two-way，而RT1会发送Neighbors Seen = RT2的hello包让RT2的状态也变为Two-way。 3）接下来RT1和RT2会进入ExStart状态并开始进行Master、Slave的协商。协商M/S的目的是为了决定在后续的LSA交互中，谁来决定DD（Database Description）报文的序列号（Sequence Number），而Router-ID大的那个OSPF路由器的接口将会成为Master（注意这里的Master不是DR）.协商过程通过DD报文实现，有三个关键的字段：I、M、MS，其含义如下 字段 含义 I(Init) 如果是第一个DD报文则置1，其它的均置0 M(More) 如果是最后一个DD报文则置0，否则均置1 M/S 设置进行DD报文双方的主从关系，如果本端是Master角色，则置1，否则置0 Sequence Number 指定所发送的DD报文序列号。主从双方利用序列号来确保DD报文传输的可靠性和完整性 4）确认了M/S关系后，两个路由器就进入了 Exchange 状态，主路由器首先开始和从路由器共享链路状态信息。如果将链路状态数据库比喻成一本书，那么DD报文相当于这本书的目录，通过DD报文，可以发现自己所没有的信息。 5）当所有的DD报文传输完后，假如从路由器通过DD报文发现了自己所没有的信息后，会发送LSR报文给主路由器，随后主路由器会发送LSU报文给从路由器。从路由器将该信息合并到它的本地链路状态数据库中。从路由器会回应一个LSAck包给主路由器。此时两者处于loading状态。 6）两者的链路数据库一致，达到了full状态。 其状态转移图如下所示，图中的稳态有三种(Down,Two-way,Full) 其他概念克服路由自环OSPF能够克服路由自环的原因有以下几个 每一条LSA都标记了生成者（用生成该LSA的路由器的RouterID标记），其他路由器只负责传输，这样不会在传输的过程中发生对该信息的改变和错误理解。 路由计算的算法是SPF，计算的结果是一棵树，路由是树上的叶子节点，从根节点到叶子节点是单向不可回复的路径。 区域则通过规定骨干区域避免 大型网络中存在的问题及对策 在大型网络中，OSPF存在着以下问题 链路状态数据库(LSDB)非常庞大，占用大量存储空间 计算最小生成树耗时增加，CPU负担很重,一点变化都会引发从头重新计算 网络拓扑结构经常发生变化，网络经常处于“动荡”之中 针对该问题，常用的解决方法是对OSPF划分区域 OSPF路由器的类型根据位置不同，OSPF路由器可以被被划分为不同类型 内部路由器 — 路由器所有接口都在一个区 主干路由器 — 所有接口都在主干区域的路由器 区域边界路由器(ABR) —路由器接口分属不同区域(Area) 自治域边界路由器 (ASBR) — 路由器至少有一个接口不属于本自治域(AS). OSPFv3OSPFv3保留了OSPFv2的基本机制 网络类型和接口类型 邻居发现和邻接（毗邻）建立机制 接口状态机和邻居状态机 基于LSDB计算路由 LSA老化更新机制 泛洪机制(Flooding mechanism) 共五种协议报文: Hello, DD, LSR, LSU, LSAck 但是 OSPFv3 在 OSPFv2 的基础上做出的变化为： 1）基于链路运行 在 OSPFv2 中，协议的运行是基于子网的，邻居之间形成邻接关系的条件之一就是两端的IP地址属于同一网段而且掩码相同。 而在 OSPFv3 中，协议基于链路运行，与具体的IPv6地址、前缀分离开来，即使同一链路上的不同节点具有不同的IPv6地址时，协议也可以正常运行。 2）取消了编址语义 在OSPFv2中，协议分组和LSA中的许多字段都是来自于网络上的某个IP地址，或掩码，或某个IP子网号。严重依赖IPv4。 在OSPFv3中，取消了上述编址性语义，而只保留协议运行必须的核心内容。ID依然保留32位，但只是一个编号，不再包含地址信息。 3）链路本地地址的使用 在OSPFv2中，每一个运行OSPF的接口都必须有一个全局的IPv4地址，协议的运行和路由的计算都依赖于它。 在IPv6中，每个接口都会分配本地链路地址（link-local address），OSPFv3 使用了这个本地链路地址作为协议分组发送的源地址（虚连接除外），而且使用它作为路由的下一跳。 这样可以节省大量的全局地址，同时可以说协议的运行独立于IPv6，可以方便的扩展用于多种协议的路由 4）使用专门的LSA来发布路由前缀信息 新增加了Intra-Area-Prefix-LSA，用于传递区域内路由前缀新增加了 Link-LSA，用于传递链路范围内的IPv6前缀。 5）明确的LSA泛滥范围 泛滥的范围分为：本地链路范围（Link-local scope），区域范围（Area scope），AS范围（AS scope） 6）提供了对多实例的支持 在OSPFv2中，不同的实例必须运行在不同的链路上；在OSPFv3中，明确的提供了对多实例的支持，同一链路也可以运行多个OSPF实例了，而且互相独立运行不会影响。 配置单区域配置 配置Routert-ID router-id是一个可选的配置，其获取方式依次为1）手动配置2）使用环回地址作为router ID3）如果没有，选择路由器的最高逻辑地址作为routerID 注意：IOS 的某些早期版本无法识别 router-id 命令；因此，为这些路由器设置路由器 ID 的最佳方法是使用环回接口 配置优先级优先级的取值范围为0~255，优先级为0的路由器不能被选举为DR 配置计时器 广播型OSPF网络，缺省hello包间隔为10秒，down机判断间隔为40秒非广播型OSPF网络，缺省hello包间隔为30秒，down机判断间隔为120秒 实际配置的例子如下 12Router（config-if）＃ip ospf hello-interval 5Router（config-if）＃ip ospf dead-interval 10]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算机网络课程总结--IPV6]]></title>
      <url>%2F2016%2F12%2F26%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--IPV6%2F</url>
      <content type="text"><![CDATA[本文主要讲述 IPV6 的一些基础知识，包括 IPV6 的术语，地址，一些基本协议以及从IPV4过渡到IPV6的的一些技术等。 IPV6的出现最主要的原因是因为 IPV4 存在着一些不足，主要有 地址不足 端到端模式无法实施(NAT) Qos和性能问题 配置复杂 安全问题 路由表的膨胀 移动性支持不足 其中最主要的就是地址不足。 在介绍IPV6的具体知识前，首先了解IPV6中的一些基本术语 从上图可知，IPV6 中有以下术语 领节点：不跨越网段的两台主机 局域网段：交换机某个端口下的网络 链路：路由器某个端口下的网络 子网(site)：一个机构管辖的所有网络（不同于IPV4中的子网） IPV6地址IPV4地址讲述IPV6地址前，首先回顾一下IPV4地址，IPV4的地址长度为32位，通过点分十进制表示，点分十进制的规则如下 将32位IP地址分为4个8位组 每个8位组之间用圆点 “.” 分隔 每个8位组转化为对应的十进制数 每个地址被划分为网路地址和主机地址两部分。地址可分为ABCD四类，每一类的地址范围如下图所示 IPV6地址表示IPV6 地址长度是128位，通过冒分16进制表示，冒分16进制将128位划分为8组，组与组之间通过冒号隔开，每一组16位，用四个16进制数表示，如下所示 但是这样表示出来的地址往往在8各组中有很多组是全0的，为了书写的简便，定义了以下的规则：1）忽略前导0，2）忽略全0，用双冒号代替，注意一个地址最多只能有一个双冒号 如下是运用了这两条规则的一个简单例子 除此之外，IPV6中通过/XX表示地址前缀，替代了IPV4中子网掩码的概念。 IPV6中的地址根据其作用域可以分为单播地址(Unicast Address)，组播地址(Multicast Address)和任播地址(Anycast Address)三大类。IPV6中没有广播地址的概念。 单播地址IPV6 单播地址用于唯一标识支持 IPV6 的设备上的接口，与 IPV4 类似，源 IPV6 地址必须是单播地址。在IPV6中，单播地址分为六类 链路本地地址(Link Local Address) 环回地址(Loopback Address ) 未指定地址(Unspecified Address) 站点本地地址(Site Local Address) IPV4兼容地址(IPV4 Compatible Address ) 可聚合全球单播地址(Aggregate Global Unicast Address) 链路本地地址IPV4也有链路本地地址(169.254.0.0/16),但是在IPV4中，该地址的主要被用于地址自动配置：当主机不能从DHCP服务器处获得IP地址时，它会使用链路本地地址作为自己的地址。 但是在IPV6中，链路本地地址用于与同一链路（同一个路由器端口下的网络）中的其他设备通信，而且也只能用于同一链路中，因此，路由器不会转发具有本地链路源地址或目的地址的数据包。需要注意的是，每个支持 IPV6 的网络接口均需要有链路本地地址。有了链路本地地址，就可以与与同一子网中的其他支持 IPV6 的设备通信，包括与默认网关（路由器）通信。所以假如只需要在局域网内通信，可以不需要全局单播地址。 支持 IPV6 的主机在启动时自动创建 IPV6 链路本地地址。链路本地地址格式为 FE80::/64 ，如下图所示 链路本地地址前64位为FE80:0:0:0，后64位为EUI-64地址，EUI-64地址通过mac地址变换过来，变换规则如下所示 注意上图中前24位mac地址中用 u 标记的位表示要对原来的位取反。如下为一个简单的例子 链路本地地址的用途包括：1）主机使用本地路由器的链路本地地址作为默认网关 IPV6 地址。2）路由器使用链路本地地址交换动态路由协议消息。3）转发 IPV6 数据包时，路由器的路由表使用链路本地地址确定下一跳路由器。 环回地址与IPV4的127.0.0.1类似，在IPV6中对应为::1/128 或简单的 ::1。 通过环回地址启用 ping 命令，从而测试本地主机的 IP 协议是否正确安装。 未指定地址未指定地址是全 0 地址，使用压缩格式表示为 ::/128 或 :: 。 未指定地址不能分配给接口，仅可作为 IPV6 数据包的源地址。未指定地址的作用在于当设备尚无永久 IPV6 地址时或数据包的源地址与目的地址不相关时，使用未指定地址用作源地址。 站点本地地址站点本地地址是IPV6的私网地址，就像IPV4中的私网保留地址（10.0.0.0/8, 172.16.0.0/12,192.168.0.0/16）一样。站点本地地址的前缀范围为 FC00::/10。 最初的 IPV6 规范定义了用于特定用途的本地站点地址，但是因为站点本地地址规范中有很多不明之处，因此，IETF 已经弃用本地站点地址，开始倾向于唯一本地地址。 IPV4兼容地址兼容IPV4的IPV6地址，用于过渡时期在IPV4网络上建立自动隧道，以传输IPV6数据包。其中高96位设为0，后面的32位的IPV4地址，即地址为::IPV4. 可聚合全球单播地址可聚合全球单播地址是由IANA分配的可在全球路由的公网IP，地址由格式前缀 001 标识，设计目标是聚合或汇总该地址以便产生有效的路由基础结构 从上图可知，ISP商分配的前缀位为前48位，Site部分则是由ISP下一级的组织机构用于划分子网，最后是64位的接口ID。 接口ID的产生方式有三种 EUI-64(前面讲述过，通过mac生成) 随机生成 手工配置 组播地址IPV6中没有广播的概念，IPV6中通过组播代替广播。 IPV6中组播地址前八位均为1，因此组播地址总是以ff开头，具体的组播地址格式如下 其中 Flags:用来表示permanent(0000)或transient(0001)组播组 Scope:表示组播组的范围 0：预留1：节点本地范围2：链路本地范围5：站点本地范围 Group ID:组播组ID 一些众所周知的组播地址如下图所示 除此之外还有有一种特殊的组播地址：Solicited-node节点地址（被请求节点组播地址），主要用于重复地址检测和获取邻居节点的链路层地址。 地址构成为前104位：FF02::1:FF/104（64个零）后24位：单播地址的后24位 凡是单播地址后24位相同的接口会自动加入相应的请求节点组播组，如下为一个实例 关于组播更详细的信息可参考这篇文章。 任播地址在IP网络上通过一个任播地址标识一组提供特定服务的主机，同时服务访问方并不关心提供服务的具体是哪一台主机(比如DNS或者镜像服务)，访问该地址的报文可以被IP网络路由到这一组目标中的任何一台主机上，一般来说，目标地址为任播地址的数据报将发送给最近的一个服务主机。 综合上面提到的地址，可以知道一个接口上可以具有的IPV6地址如下所示 地址类型 具体地址 链路本地地址 FE80::/10 环回地址 ::1/128 所有节点组播地址 FF01::1,FF02::1 可聚合全球单播地址 2000::/3 被请求节点组播地址 FF02::1:FF00:/104 主机所属组播地址 FF00::/8 IPV6地址配置IPV6中地址配置方式有三种 手工配置 无状态地址自动配置（ND协议） 有状态地址自动配置（DHCPv6） 手动配置一般不常用，各系统均有自己的配置方式，这里不详细展开。下面主要讨论自动配置的两种方式。 无状态地址自动配置（ND协议）无状态地址自动配置指的是无须任何配置即可和外界通信，达到了真正的即插即用。无状态地址自动配置通过ND(Neighbor Discovery,邻居发现)协议实现，主要包括如何自动获取地址以及实现重复地址检测(DAD) RS和RAND协议中有两种重要的消息：RS和RA。其中，RS(Router Solicitation)由主机发出，作用是促使路由器发送RA(Router advertisment)消息,RA消息中包含了路由器前缀等信息，主机会利用路由器的前缀加上自己的EUI-64地址作为自己的IPV6地址。这两种消息都是以ICMPv6报文的形式出现，也是5种ND协议消息中的两种。具体过程如下图所示 注意RS消息的目的地址是FF02::2,也就是主机先整个链路的路由器请求RA消息,RA消息的目的地址是FF01::1，也就是路由器向整个链路的主机发送RS消息。 因此为了避免RS泛滥，节点启动时最多只能发送3个RS，而路由器也会主动周期性地发送RA（默认值200秒）。主机在收到路由器的RA之后，自动设置默认路由器，建立默认路由器列表、前缀列表及其它参数。 需要注意的是，自动配置的IPV6地址在系统中有一个生存周期，跟优先时间和有效时间有关，对应着以下4种状态 在使用的时候需要遵循以下规则 1.在Preferred Lifetime周期内的前缀生成的地址，任何上层应用都可不受限制地使用2.在超过 Preferred Lifetime 但未超过Valid Lifetime周期内的前缀生成的地址，正在使用该地址的上层应用可继续使用，但任何新的上层应用不能使用这个地址3.在超过Valid Lifetime周期内的前缀构造的地址，任何上层应用都不能使用该地址一个链路本地地址的优先时间和有效时间是无限的，即永不超时！ NS和NA在自动配置地址后需要检测该地址在链路上是否与其他地址重复(链路上的主机的mac地址可能重复，从而EUI-64重复)，该过程称为DAD(Duplicate Address Detection),所有的IPV6单播地址，不管是自动配置还是手动配置，都必须要通过DAD。 结合上面地址的生存周期，一个地址在分配给一个接口之后且通过重复地址检测之前称为tentative地址，即试验地址。 DAD机制是ND协议的一部分，因此通过ND协议中的NS/NA两种消息实现。DAD的基本流程: 节点组播发送NS(Neighbor Solicitation)消息 如果收到NA(Neighbor Advertisment)消息，就证明地址重复 如果尝试若干次发送请求，都没有收到邻居通告，即可启用该地址 过程如下所示 注意上面的NS消息的目的地址是被请求节点的组播地址，就是单播地址最后24位相同的主机都会加入的一个组播。而NA消息的目的地址是FF02::1,原因是NS消息中的源地址是未指定地址，发出NA的主机并不知道NS是由那一台主机发出的。 当其他主机收到NS后，会有以下两种情形1）NS接收者如果发现其中的目标地址对它而言是tentative的，则主动放弃使用这个地址；2）NS接收者如果发现其中的目标地址是一个它正在使用的地址，则发送NA消息，请求发起者 将放弃使用这个试验地址 结合上面的RS和RA，4中ND消息交互入下 前缀重新编制前缀重新编制允许网络从以前的前缀平稳地过渡到新的前缀，提供对用户透明的网络重新编址能力。 在前缀重新编址时，路由器会继续通告当前前缀，只是优先时间和有效时间被减小到接近0，同时，路由器开始通告新的前缀，这样，链路中至少有两个前缀共存。 节点收到优先时间和有效时间被减小到接近0的RA时，会发现当前前缀的生命周期较短，停止使用；同时开始用新的前缀配置接口，并进行DAD，通过后，获得新的地址使用。 在转换期间，节点有两个单播地址使用，旧的地址基于旧的前缀，用以维持以前已经建立的连接；新的地址，基于新的前缀，用来建立新的连接。当旧的前缀的有效时间递减为0时，旧的前缀完全废止，此时，RA中只包含新的前缀 有状态地址自动配置（DHCPv6）有状态的自动配置依赖于DHCPv6实现，DHCPv6是DHCP的升级版本。 但是既然有了无状态自动配置，为什么还需要DHCPv6呢？主要有以下几个原因 需要动态指定DNS服务时 当不希望MAC地址成为IPV6地址的一部分时(安全性） 当需要良好的扩展性时 在讲述DHCPv6前，先介绍一下原始的DHCP协议 DHCP的过程非常自然，主要包括下面三步1）DHCP客户发送广播请求2）DHCP服务器单播应答3）DHCP客户接收应答，获取IP等信息 具体过程使用四种package来实现这个过程1）DHCP客户机在本地发送DHCP DISCOVER广播包；2）DHCP服务器单播发送携带租约信息的DHCP OFFER包；3）DHCP客户机确认租约信息并发送DHCP REQUEST广播包；4）DHCP服务器单播送回DHCP ACK確认完成IP地址租用 DHCP有三种地址分配机制：1）自动分配方式－由DHCP分配一个永久的IP2）手动分配方式－网络管理员预先安排分配，由DHCP转达3）动态分配方式－由DHCP分配具有租约期的IP 相比于DHCP，DHCPv6有了以下的改变 使用UDP来交换报文，端口546/547（V4：67/68） 使用本地链路地址或其它机制获得的地址来发送和接收DHCPv6报文 没有了广播，客户机只需发送给保留的链路范围组播地址（FF02::1:2,all dhcp relay agents and servers ） DHCP中使用的四种package在DHCPv6中依次变为DHCP Solicit, DHCP Advertis, DHCP Request, DHCP Relay. 其过程如下所示： 当客户端已经记录了地址和其他配置信息，只需要DNS server、NTP server等信息的时候，可以通过DHCPv6快速配置来快速获得所需地址。这个过程只需要两个消息的交互，过程如下所示： IPV6报文IPV6的报文主要由三部分组成 基本头（固定40字节，v4不固定，为20~60字节范围内） 拓展头（可选，0~n字节，v4中没有） 有效负载（即上层传输数据） 基本头(与v4对比)v4与v6的报文对比如下所示 且v4的报文的具体字段如下所示 v6的报文的具体字段如下所示 v6在v4字段的基础上有删除，也有修正的项，其中 修正的项有 服务类型→业务等级 TTL→跳数限制 数据总长度→净荷长度(因为头部固定长度为40个字节) 地址32位→128位 协议→下一个头(next header,也是指示具体的协议的，不要被名称误导) 删除的项有 报头长 标志和分段偏移量(与分片相关) 报头校验和 增加的项为 流标记 下图是一个抓取到的ICMPv6的具体package 拓展头除此之外，IPV6将一些IP层的可选功能实现在上层封装和IPV6基本头部之后的扩展头部中，主要的扩展报头有： 逐跳选项 路由报头 分段报头 认证报头 封装安全有效载荷报头 目标选项 每一种扩展报头其实也有自己特定的协议号，例如：路由报头为43，AH报头为51。上图中抓到的ICMPv6包的协议号为58(0x3a转为10进制)，其他一些常见的协议号如下所示 协议号 含义 0 逐跳扩展头 1 ICMPv4 6 TCP 17 UDP 43 路由扩展头(使数据分组经过指定的中间节点) 58 ICMPv6 89 OSPF 每一个基本报头和扩展报头的NextHeader字段标识后面紧接的内容，如下图所示 ICMPv6协议ICMPv6协议与回声请求、抑制消息、重定向、参数错误等功能相关，相关的命令为ping、traceroute ICMP报文格式为Type+Code+CheckSum,其报文的格式以及在整个分组的位置如下所示 ICMPv6报文类型可分为两种(1)差错报文(Type=0~127)：通告IPV6分组传输中出现的错误。如目标不可达、数据包超长、超时、参数问题(2)信息报文(Type=128~255)：提供诊断和附加的主机功能。如回声请求(Type=128)和应答(Type=129)，ND协议等 ICMPv6的三个实际应用为 ping tracert(HopLim与v4中的TTL意思相同) 第一个请求：HopLim=1第一跳路由器收到，发送超时(HopLim=0)消息得到第一跳路由器的信息第二个请求：HopLim=2第二跳路由器收到，发送超时消息得到第二跳路由器的信息注意：HopLim的最大值为30，且为了让每一个请求都返回超时信息，通常设置ICMPv6报文的端口不可达。 PMTU发现：通过试探的方式发现路径允许的最大的MTU,如下为一个简单的过程。 1)源机向目的机发送MTU=1500字节的IPV6数据包2)路由器B向源发送超长消息，指定MTU=1400字节3)源机向目的机发送MTU=1400字节的IPV6数据包4)路由器C向源发送超长消息，指定MTU=1300字节5)源机向目的机发送MTU=1300字节的IPV6数据包6)此后，该路径的MTU都使用1300字节 IPV6路由在讲述IPV6的路由之前先回顾IPV4的路由，IPV4的路由可以分为两大类：同一网络的路由和不同网络的路由。 同一网络间主机的通信主要依赖于ARP协议，根据目标IP查询其对应的mac地址，然后两者便可通信，中间可以不经过路由器。 不同网络间的通信则需要借助路由器,其过程如下所示 而对于 IPV6 的路由也可分为两种情况：on-link：源机和目的机在同一链路的数据转发off-link：源机和目的机不在同链路的数据转发 通过地址前缀判断源和目的是否在同一链路。 on-link处于同一链路的两条主机要通信就要知道对方的mac地址，在IPV4中通过ARP实现,ARP是通过广播实现的，但是IPV6中并没有广播的概念。在IPV6中，通过ND协议来完成这个地址解析的工作。 ND协议在我们介绍无状态地址自动配置的时候已经介绍过，但是除了无状态地址自动配置外，ND协议还被用于地址解析和路由重定向。 ND协议共有五种报文，五种报文都是以ICMPv6报文的形式出现，如下图所示 其地址解析过程如下： 1）首先查找邻居缓存表（IPV6 nc），没有则进行地址解析 （类似于查找ARP表）2）源主机发送组播NS报文，该报文的目的地址为目标IPV6地址所对应的被请求节点组播地址（Solicited-node），在其中也包含了自己的链路层地址3）目标主机收到NS报文后，就会了解到发送主机的IPV6地址和相应链路层地址；同时由于目标主机正在使用报文内的目标地址，所以会目标主机向源主机单播发一个邻接点公告报文（NA），该报文中包含自己的链路层地址。 这里需要注意的是最后的NA报文是单播的，而在无状态地址自动配置中NA报文是组播，原因在于无状态地址自动配置的时候发送NS的主机还没有有效的地址，而这里的主机已经有了，只是要找到另外一台主机的mac地址而已。 下图便是上面提的地址解析过程 off-link当源和目的不在同一链路的时候，需要考虑两个问题 1.主机发给哪个路由器？（主机-路由器）2.路由器发给哪个路由器？（路由器-路由器） 对于第一个问题，支持IPV6的主机有一个数据结构DestinationCache，要发送数据到某个目的地址的时候，首先查询这个数据结构，如果查不到，就查路由表，让后将查到的信息记录在这个数据结构中。 如果查询到的目的地址是on-link的，将目的地址本身加入DC表的nexthop域；如果目的地址是off-link的，将路由表中的下一跳加入DC的nexthop域。 在这个过程中，会涉及到重定向的问题，重定向的作用其实就是给主机发送更好的路由。下图为一个简单的例子 当 PC1 要与 PC2 通信时，首先会先向 RT1 查询，RT1查询后发现RT2可以直接提供这个路由，于是RT1告诉PC1以后如果要与PC2通信直接找RT2就好了，效率会更高，这就是路由重定向。 实际中，RT1发现报文的出口和入口相同或者源地址跟报文下一跳同属一个网段，则发出重定向报文，重定向报文就是ND报文中的最后一种报文(其他四种是RS，RA，NS，NA) 对于问题2，也就是路由器与路由器之间的传输，就需要依靠路由表了。路由表中的路由根据是否需要人工配置而分为静态路由和动态路由，其中静态路由需要人工配置，动态路由则通过协议学习，在IPV6中的动态路由协议主要有RIPng，OSPFv3和BGP4+。 RIPng保留了RIP的主要特点 距离矢量采用跳数，16跳为不可达 工作机制不变； 仍然采用水平分割、毒性逆转、触发更新等技术减少路由环的发生 主要改变的地方： 组播代替广播：主机不再受骚扰 下一跳信息由单独的R Table Entry表示 （RTE） 安全考虑：不单独设置验证，由IPV6本身保证 只用于IP网络：不再支持其他网络协议 同样，OSPFv3保留了OSPFv2的主要工作机理 采用链路状态数据库 与邻接路由器同步 DR选举、SPF算法、area区域支持 主要改变的地方 地址信息从LSA中移除；（LSU载荷中包含地址信息） RouterID仍然采用32位，但不再跟地址有关 重新定义了LSA（如增加了link-LSA、Intra-Area-Prefix-LAS等，即LSDB的内容发生了变化） 不再支持认证 过渡技术从IPV4到IPV6的过渡被认为要经过三个阶段 过渡的技术共分为三类 双栈：网络设备上运行IPV6/IPV4双协议栈 隧道：IPV6网络上承载IPV4分组，或相反 翻译/转换：地址、分组、端口的转换 双栈一般基础设施设备，如路由器、交换机、公用服务器等，需要运行和支持双栈，非基础设置则可运行单协议或双协议。 隧道根据创建方式可以大致分为两类1）手动隧道：事前配置2）自动隧道：创建和拆除都依赖当前网络条件 根据实际的网络环境又可以分为两类隧道 IPV6分组通过IPV4网络的隧道 IPV4分组通过IPV6网络的隧道 IPV6分组通过IPV4网络时，IPV6分组作为数据部分搭载到IPV4分组中，在这种情形下，IPV4分组头部的protocol=41，如下图所示。 根据IPV4网络位置的不同，在不同的位置建立 IPV4分组通过IPV6网络的情况跟上面一样。 翻译/转换翻译/转换就是从IPV4转换到IPV6，或反过来，不仅发生在网络层，还有传输层和应用层。如下图所示是一个翻译/转换的例子。 注意：当双栈和隧道都无法使用的时候，才使用翻译/转换技术；适用纯IPV4节点和纯IPV6节点间的通信。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算机网络课程总结--BGP协议]]></title>
      <url>%2F2016%2F12%2F25%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--BGP%E5%8D%8F%E8%AE%AE%2F</url>
      <content type="text"><![CDATA[路由器工作在IP层，其作用是根据IP地址将数据包传输到正确的目的地，因此路由器必须要知道网络的“地图”才能正确投递，而这个网络的“地图”就是存储在路由表中的路由规则，简称为路由。 根据获得路由的方式可以将其分为静态路由和动态路由，静态路由就是管理员静态配置的路由，动态路由则是路由器通过算法动态地学习和调整而得到的路由。而常说的路由协议就是指这些动态路由的学习算法，根据其作用域的不同，又可分为内部网管协议(IGP)和边界网络协议(BGP)；内部网络协议包括RIP，OSPF等，边界网关协议则包括BGP等。 本文主要讲述BGP相关的一些知识。 当网络过大的时候，也会导致路由表过大而难以维护，这时候采用分治的方法，将一个大网络划分为若干个小网络，这些小网络称为自治系统(AS)，BGP的诞生就是用于自治系统间的通信。其在网络中的位置如下 那么是不是AS之间的通信都要使用BGP？答案并不是，只有当两AS间存在多条路径，需要做路由策略和选择才需要BGP；如果AS只有一个出口或者所有出口指向一个ISP的时候，是不需要BGP的。 基本概念BGP允许基于策略（policy-based）的路由选择，策略与政治、安全和经济等因素相关，由AS的网络管理者确定，也就是说人为影响的因素较大。 从上面可知，每个划分后的小网络称为AS，每个AS都有自己独特的AS号码(ASN),ASN的原来使用16位表示。但是由于和和IP地址一样,ASN同样面临分配告罄的危机，自2006年12月1日起,原为16位(1-65535) 的ASN扩展为32位空间。 AS根据其位置的不同，也被分为为不同类型的AS。 一般来说，AS号码只是在一家ISP与至少两家ISP做对等互联，交换路由的时候才需要用到，也就是说一个国家的AS号码的数量实际上是跟大中型ISP的数量有关。 BGP 是运行在TCP协议上的，与其他路由协议对比如下 BGP 是一种距离矢量路由协议，但避免了环路；其避免环路的策略是不仅仅记录路径代价，还记录下全路径信息。如下图所示 BGP有两种邻居，其中运行在同一个AS内的BGP邻居称为IBGP(Interior BGP),不同AS间的邻居称为EBGP(Exterior BGP)，注意无论是IBGP或者EBGP，上面都必须运行着BGP协议，也就是说与BGP路由器直连的内部路由器不一定是它的IBGP，如下图所示 注意：BGP邻居不是自动发现的，而是手动配置的，原因有以下两点：1）可以与对端设备用任何IP地址建立邻居，而不限于某个固定的接口IP。这样，当两台设备采用环回地址而非直连地址建立BGP邻居时，即使主链路中断了，也可以切换到备份链路上，保持邻居不断。这种稳定性正是BGP作为大型网络路由承载的必要特质。2）可以跨越多台设备建立邻居。当一个AS有多个设备运行BGP 建立域内全连接时，不必每台设备物理直连，只要用IGP保证建立邻居的地址可达，即可建立全网连接，减少不必要的链路建设。 BGP报文BGP报文类型有以下四种： Open报文：打招呼，“你好，交个朋友吧”（协商参数） Keepalive报文：我还活着，别不理我（30秒钟交换一次） Update报文：有新闻（链路的变化） Notification报文：我不跟你玩了（异常情况的通报，终止连接） BGP的工作机制也可以通过这四种报文描述：1）通过TCP建立BGP连接时，发送OPEN报文2）连接建立后，如果有路由器需要发送路由或路由发生变化时，发送UPDATE报文3）稳定后，周期发送KEEPALIVE报文，维持连接有效性4）当本地BGP运行中发生错误时，发送NOTIFICATION报文通告BGP对端 路由注入及通告首先要明确一点，BGP路由器的路由注入和通告都是为了修改BGP路由表。 当路由器之间建立BGP邻居之后，就可以相互交换BGP路由。一台运行了BGP协议的路由器，会将BGP得到的路由与普通路由分开存放，所以BGP路由器会同时拥有两张路由表。 一张是存放普通路由的路由表，被称为IGP路由表，就时平时我们使用命令show ip route看到的路由表，IGP路由表的路由信息只能从IGP协议和手工配置获得，并且只能传递给IGP协议；另外一张就是运行BGP之后创建的路由表，称为BGP路由表，需要通过命令show ip bgp才能查看，BGP路由表的路由信息只能传递给BGP协议，如果两台BGP邻居的BGP路由表为空，就不会有任何路由传递。 在初始状态下，BGP的路由表为空，没有任何路由，要让BGP传递相应的路由，只能先将该路由注入BGP路由表，之后才能在BGP邻居之间传递。注入的方式有多种，如1）动态注入：将IGP(如OSPF)发现的路由纯动态地注入到BGP路由表中，这种方式配置简单，但操控性差，可能不稳定。具体过程及配置如下图所示：2）半动态路由注入：通过IGP协议(如OSPF)学习到的路由，再通过 network 发布到BGP中。具体过程及配置如下图所示： 3）静态路由注入：工配置的静态路由，再由network发布到BGP中。具体过程及配置如下图所示： 在BGP路由表注入路由后，BGP路由器之间会将这些路由在BGP路由器间进行通告。通告要遵守以下规则 BGP 路由器只把自己使用的路由通告给相邻体 BGP 路由器从EBGP获得的路由会向它的所有BGP相邻体通告（包括EBGP和IBGP） BGP路由器从IBGP获得的路由不会向它的IBGP相邻体通告（避免内部产生环路） BGP 路由器从IBGP获得的路由是否通告给它的EBGP相邻体要依IGP和BGP同步的情况而定 对于最后一条，只有当 IGP 与 BGP 同步时（也就是该路由可以通过 IGP 获得），才能通告，反之不通告，这样做的目的是为了避免路由黑洞。 路径属性在默认情况下，到达同一目的地，BGP只走单条路径，并不会在多条路径之间执行负载均衡。对于IGP路由协议，当有多条路径可以到达同一目的地时，则根据最小metric值来选择最优路径，而 BGP 存在多条路径到达同一目的地时，对于最优路径的选择，BGP并不会以metric值大小为依据，BGP对于最优路径的选择，需要靠比较路由条目中的Path Attributes，即路径属性，只有在比较多条路由的属性之后，才能决定选择哪条为最优路径。 BGP的路径属性可以划分为以下四类： 公认强制 （Well-Known Mandatory）:所有的路由中都需要写入公认强制属性 公认自选 （Well-Known Discretionary）：能够理解和支持即可，不一定要写入路由 可选可传递 （Optional Transitive）：不一定要理解或支持 可选不可传递（Optional Nontransitive）：只有特定的BGP路由器才能理解和传递 对于任何一台运行BGP的路由器，都必须支持公认强制属性，并且在将路由信息发给其它BGP邻居时，必须在路由中写入公认强制属性，这些属性是被强制写入路由中的，一条不带公认强制属性的路由被BGP路由器被视为无效而被丢弃，一个不支持公认强制属性的BGP，是不正常的，不合法的BGP。BGP路由必须携带的公认强制属性有三个：Origin，Next_Hop，AS-path。 origin属性origin属性为起源属性，描述路由是以何种方式注入到BGP路由表中的，主要有以下两种情况1）以 network 命令注入到BGP路由表中，origin 属性为 IGP2）以 redistribute 命令注入到BGP路由表中，origin 属性为 Incomplete其中，IGP优先级比Incomplete的要高。 AS Path属性描述了该路由经过的AS组成的路径，AS路径中不能算上自己的AS，从离自己最近的AS开始，以目的网络的AS结束。下图为 AS5 到 AS1 的路由的AS Path属性 借助路由的AS Path属性，可以避免环路，具体操作就是收到一条AS Path属性中含有自己AS的路由的时候丢弃该路由。 在选路的时候，优先选AS PATH最短的那条，如果AS PATH距离相等，则优选本AS内到出口路由器最短的那根，如果还相等，则选择Router_ID（发送路由的路由器）最小的那根 但是要注意，这种选择并不总是明智的，如下图所示 Next Hop属性指示下一个AS的路由器入口的网段，同一个AS内Next hop的值不变,如下图所示 local pref属性可选的属性，用于引导流量，local pref的缺省值是100，如下图所示 MED(Multi Exit Distiguisher)属性当AS有多个出口的时候，告诉上游的AS如何选最优的路，MED值越小，优先级越高 BGP 选路的策略为 其他概念BGP过滤BGP 拥有强大的过滤功能，可以按照以下规则进行过滤： 可按照路由的IP地址过滤 可依照路由经过的AS-Path过滤 可以依照路由的属性过滤 可以依照路由到来的接口过滤 BGP聚合由于BGP路由器的路由表庞大，往往超过10万条，通过BGP聚合(BGP支持CIDR)解决这个问题，如下图AS100先将内部的路由聚合再通告给AS200 BGP联盟和反射从前面的描述可知，从IBGP收到的路由不会通告给其他的IBGP（避免环路），所以AS内部的IBGP必须全连接。但是IBGP相邻体过多，逻辑全链接不现实，实际中通过BGP联盟和反射解决这个问题。 BGP联盟就是将大的AS分割成小的AS，从而减少全连接的数目。 BGP反射是通过将网络内的路由器划分为客户机，非客户机以及路由反射器的角色，从而减少IBGP间的连接，如下图所示： BGP反射中需要遵循以下规则 来自客户机的路由通告给其它的客户机和非客户机 来自非客户机的路由只通告给它的客户机 来自EBGP的路由向所有相邻体通告 BGP衰减BGP衰减是为了处理不稳定的路由（如路由频繁更新），避免影响整个互联网络的稳定运行 路由抑制可以阻止公布不稳定的路由，它为每条路由分配一个动态的度量数字用来反映稳定程度，当一条路由出现摆动，就给他分配一个惩罚值，摆动得越多，惩罚值越大。当一段时间不摆动，惩罚值降低，在一个半衰期后，降到原来的一半。如果惩罚值超过抑制上限，该路由就被抑制，只有当一个半衰期后惩罚值降低到重新使用界限时，才重新使用。 BGP配置通常在路由器配置BGP需要开启BGP进程，指定AS号码，指定邻居(EBGP或IBGP)并注入BGP路由，如下是一个简单的例子123456//开启BGP进程，指定AS号Router(config)#router bgp as-number //注入BGP路由Router(config-router)#network network-number [mask network-mask] //指定EBGP或IBGP(as-number 决定)Router(config-router)#neighbor ip-address remote-as as-number 下图是一个简单的例子： 除此以外，还有一些检查BGP工作情况的命令如下所示]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算机网络课程总结--组播基础]]></title>
      <url>%2F2016%2F12%2F20%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93--%E7%BB%84%E6%92%AD%E5%9F%BA%E7%A1%80%2F</url>
      <content type="text"><![CDATA[组播是介于单播和广播间的一种通信方式，单播是单台源机和单台目的机的通信，广播是单台源机和网络中所有其他主机的通信。而组播则是单台源机和网络中部分主机的通信。本文主要介绍组播中的一些基本概念。 基本概念IP组播是指在IP网络中将数据包以尽力传送（best-effort）的形式发送到网络中的某个确定节点子集，这个子集称为一个组播组。 组播传输时，源主机只发送一份数据，这份数据的目的地址为组播组地址。 组播组中的所有成员都可接收到同样的数据拷贝（通过路由器进行复制分发），并且只有组播组内的主机（成员，目标主机）可以接收该数据。 从上图可知，由于每个分支只发送一份报文，所以网络规模（如用户数量）的增大不会额外增加网络的负担。 故组播的优势为1）降低了骨干上的网络流量2）降低了应用服务器的负担 但是组播也存在着以下缺点：1）传送不可靠！（尽力投递/best effort）2）组播报文的复制开销使得路由器的资源消耗增加！3）可控可管性差，用户管理困难，存在安全问题（用户可随意加入某个组，无须密码） 组播地址组播组用D类IP地址标识，以 1110 开头。组播范围为 224.0.0.0~239.255.255.255。 各个地址范围段及其含义如下 地址范围 含义 224.0.0.0～224.0.0.255 预留的组播地址（永久组地址），地址224.0.0.0保留不做分配，其它地址供路由协议使用 224.0.1.0～224.0.1.255 公用组播地址，可以用于Internet 224.0.2.0～238.255.255.255 用户可用的组播地址（临时组地址），全网范围内有效 239.0.0.0～239.255.255.255 本地管理组播地址，也称私人组播地址空间，仅在特定的本地范围内有效 几个常用的组播地址及其含义如下所示 地址 含义 224.0.0.1 子网上所有主机(包括路由器) 224.0.0.2 子网上所有路由器 224.0.0.5 所有ospf路由器 224.2.0.0-224.2.255.255 多媒体会议呼叫 除了IP地址，组播中的mac地址也有特殊的规定，一个组播mac地址通过映射对应一个组播ip地址，其映射规则为：将IP组播地址的低23位代替以太网地址01.00.5e.00.00.00(16进制)中的低23位。，也就是说组播的mac地址一定会以01:00:5e开头。 这种映射方式可能会带来不同的组播IP地址映射成相同的 mac 地址，给定一个组播IP地址，还有几个组播IP地址映射成的mac地址与其相同？除去低位相同的23位以及高位的1110,还剩下5位，因此还有 2^5-1=31 个地址会与给定的组播IP地址映射成相同的mac地址，也就是说有32:1的组播IP地址的mac地址重叠。 组播转发树组播转发树指的是数据从源主机到接受者的传输路径。组播转发树主要有两类：有源树(SPT)和共享树(RPT)。 有源树有源树中源主机构成了源树的根，数据传输的路径构成了有源树的分支，有源树又称为生成树或最短路径树(沿着最短路径传输)。下图中箭头便是有源树 共享树共享树是各个组播组共享的传输路径，如下图所示 有源树和共享树均是无回路的树，且均在分支处复制数据包，但是有源树能够提供一个最优的路径，而共享树路径可能不是最优的，相应的代价是有源树会占用较多的内存。 组播路由组播路由和单播路由是相反的，单播路由关心数据报文要到哪里去，组播路由关心数据报文从哪里来。 组播路由使用 “反向路径转发”机制(RPF, Reverse Path Forwarding)，它是IP组播转发过程的基础，几乎所有的IP组播路由协议都利用RPF作为决定是否转发或丢弃从某个接口上接收到的组播信息包。 RPF的具体机制是路由器收到组播数据报文后，只有确认这个数据报文是从自己到源的出接口（单播）上到来的，才进行转发，否则丢弃报文。实际上就是在路由器中查询到组播报文源地址的路由，假如该路由的出口就是组播报文的入口，RPF检查成功，转发数据包，否则丢弃数据包。 如下为RPF的具体例子RPF检查失败RPF检查成功 TTL阈值IP组播包被路由器转发的时候，IP头中的TTL值要减1；路由器在传输的时候可以在每个接口设置一个TTL阈值，只有数据包的TTL值大于或等于接口的TTL阈值时，路由器才能在出接口转发该数据包。这个机制的目的是为了限制组播的范围。如下图所示 组播协议组播协议分为主机与路由器之间的组成员关系协议和路由器与路由器之间的组播路由协议。 组成员管理协议（主机-路由器）IGMP（internet group management protocol）是IP 协议簇中负责IP组播组成员管理的协议，用来在 IP主机和与其直接相邻的组播路由器之间建立、维护组播组成员关系，所有参与组播的主机必须支持IGMP协议。 注意，IGMP 不包括 组播路由器之间的组成员关系信息的传播与维护，这部分工作由各组播路由协议完成。 IGMP作为TCP/IP第三层的协议，被封装在IP数据包中进行传输。其报文格式如下所示 IGMP协议有三个版本，各个版本的功能和区别如下： IGMP v1：提供成员关系查询/成员关系报告两个基本功能。 IGMP v2：增加了查询器选择/特定组查询/离开组消息及最大响应时间字段等扩展功能。 IGMP v3：增加了对特定(源,组)的加入离开的支持，以提供对组播应用更好的支持 IGMP v1IGMP v1的报文格式如下所示 上面的报文中，类型字段有两种取值：1：成员关系查询(路由器发出)2：成员关系报告(主机发出) 组地址只有在成员关系报告时才填入，注意这个地址不是发送IGMP数据包时用于寻址的地址，用于寻址的地址在IP报头中。 查询报文是周期性发送的，默认为60s一次，过程如下所示 上图出现了报告抑制的情况，这个机制主要是为了使得当前子网中对于每个组只需有一个成员响应成员关系查询，在上图中H1和H2属于同一组的，因此只需要一个成员响应成员报告。 报告抑制的步骤如下1)主机收到 IGMP 成员关系查询后，主机对已经加入的每个组播组启动一个倒数计时器，计时器初始化为一个给定时间（在IGMPv1中为固定值10S）范围内的随机值。2)当计时器计时值为0时，主机发送成员关系报告至与该计时器相关的组播组，以通知路由器本地网中有处于活动状态组播组接收者3)主机在它的倒数计时器达到0之前收到其他主机发送的某一成员关系报告，那么它就停止与这个组播组相关的倒数计时器的计时，这样就抑制了主机到这一组播组的成员关系报告 加入组播组：主机并不等待来自路由器的下一次成员关系查询，可主动向要加入的组播组发送成员关系报告表示加入离开组播组：IGMPv1中没有定义离开机制，主机在任何时候可以默默地离开 为了避免这种默默离开的机制导致路由器给一个空的组播组发送数据包，路由器设置组播组关联定时器（一般为3倍查询周期，3min），超时无组成员报告，则停止转发。 IGMP v2IGMP v2与IGMP v1的最大区别为：1）支持特定组查询2）通过离开消息允许主机主动汇报他们的离开 IGMP v2的报文如下所示： 上面的报文类型有以下四种： 0x11：成员关系查询（与IGMP v1兼容） 0x12：IGMPv1成员关系报告 0x16：IGMPv2成员关系报告 0x17：离开组（组播地址段为目标组播组地址） 而成员关系查询消息又分为两类1）普通查询（组播地址段为零）2）特定组查询：直接对单个组查询（组播地址段为正在查询的组播组地址） 在IGMP v2中组成员的离开并不是默默离开了，而是会主动发送离开的消息，然后路由器会进行特定组的查询已确认这个离开的主机是不是这个组播组最后的一个成员，有响应报文说明还有其他成员，否则这个成员就是最后的成员。 上面的成员查询报文由查询路由器发出。在 IGMP v1 中没有正式的选举规定，它依赖于路由协议，IGMP v2 协议声明了正式的查询路由器选举过程：1）多访问网络上的每个路由器假设自己为查询器并发出查询2）IP地址低（接口）的路由器被选为查询器3）非查询路由器设置定时器，当超时没有收到查询器的周期查询，认为查询器出事了，重新选举 当 IGMP v1 和 IGMP v2 混杂在一个子网的时候，两种协议的交互需要遵循某种规则，这些规则都是因为 IGMP v1 并不能识别IGMP v2 的报文。如 IGMPv2 主机与IGMPv1路由器交互时，有以下规则：1）IGMPv1路由器把v2报告看作无效的信息并且忽略它2）当V1路由器作为查询路由器时，V2的主机必须发送V1成员报告。 IGMPv2路由器与IGMPv1主机交互时，有以下规则：1)V2路由器的查询可被V1的主机所处理，只是忽略第二个八位组的信息，就是忽略特定组的查询，全认为是普通查询。2）v2路由器必须忽略离开报告，否则后果很严重！因为v2路由器收到离开报文后会发出的特定查询，而特定组查询并不被v1主机理会，此时假如剩下的全是IGMP v1的主机，IGMP v2的路由器收不到响应报文，会认为组播组没有成员从而不再发送数据到这个组播组。 基于上面的原因，同一网段上的所有路由器必须运行同一版本的IGMP协议。缺省为V2。但是假如网段上存在其它IGMP v1路由器，所有的路由器必须手工配置为运行IGMP v1。 IGMP v3IGMP v3 与 IGMP v2的最大区别是允许主机只收到组播组内某个特定信源的传输，如下图所示 三个版本的 IGMP 协议比较如下 协议版本 IGMP v1 IGMP v2 IGMPv3 查询器选举 依靠上层路由协议 自己选举 自己选举 离开方式 默默离开 主动发出离开报文 主动发出离开报文 指定组查询 无 有 有 接收组内指定源 无 无 有 路由协议（路由器-路由器）组播路由协议的类型主要有两种：密集模式（Dense-mode）和稀疏模式（Sparse-mode） 密集模式（Dense-mode）1）使用“推”（Push）模型2）组播数据在整个网络的泛滥（Flood）3）下游不想接收的话则剪枝（Prune）4）泛滥、剪枝、泛滥、剪枝…周而复始 (通常3分钟折腾一次) 稀疏模式（Sparse-mode）1）使用 “拉”（Pull）模型2）组播数据只发送到有需要的地方3）有显式的加入（Join）过程 目前主要有4种具体的组播路由协议DVMRP，MOSPF，PIM-DM，PIM-SM。 DVMRP是第一个组播路由协议，一个较为古老，具有实验性质的协议，现已经不常使用。属于密集模式协议。 DVMRP 基于基于距离矢量，类似于RIP，最大不能超过32跳，不支持共享树，不适合于大规模的网络。 MOSPF是对OSPF单播路由协议的扩展，在OSPF链路状态通告中包含组播信息，以此构建组播分发树。MOSOF与单播路由协议相关，仅在OSPF网络内运行，适合在单路由域中使用。不支持共享树，且支持的厂家较少，市场鲜有使用。 目前最常用的组播协议是 PIM（Protocol Independent Multicast，协议无关组播）。PIM有以下特点：1）独立于单播协议，也就是支持所有的单播协议2）扩散和剪枝机制3）无类 PIM 又分为 PIM-DIM 和PIM-SM 两种模式，对应于密集型和稀疏型的PIM。 PIM-DM该协议用PUSH方式，将组播流量周期性扩散到网络中所有设备，建立和维护SPT(short path tree) （假设所有主机都需要接收组播数据）。 主要步骤为以下三个：1）周期性扩散(泛洪,Flood)：为每个路由器创建(S,G)2）剪枝(Prune)：除去不需要组播数据的路径3）嫁接(Graft)：迅速得到数据，而不用到下一周期 泛洪和剪枝的过程如下所示 满足以下任一条件即可发送剪枝消息1）信息到达PIM-DM 路由器的非RPF点对点接口；2）PIM-DM路由器没有下游邻居，且所有叶网络上没有组成员；3）PIM-DM路由器接口上所有的下游邻居已经通过了剪枝表决 嫁接的目的是为了能够迅速得到数据，从而不用等到下一次的泛洪。过程如下图所示 PIM-DM中还有一种断言（Assert）机制。目的是为了避免出现组播流量重复和多份，如下图所示，BCD都会收到重复的数据 为了避免这种情况，断言机制过程如下1）当路由器从其组播“出接口列表”(oiflist)中的某个接口收到与其发送的组播数据相同的数据2）路由器发送 “PIM Assert”消息3）计算distance和 metric值，谁到源的路由最优谁获胜；如果distance和 metric相等，IP地址大的获胜，输的就停止转发 (剪枝接口) 对于上图运行断言机制后，假如C获胜，那么情况如下 原因是B经Assert断言成了loser之后，将自己的loser接口设为剪枝状态，并向winner C发送剪枝消息，D的RPF接口也会收到该剪枝消息，发出join消息，否决。 PIM-DM的优点为1）易于配置2）实现机制简单（泛滥剪枝） 缺点为1）泛滥剪枝过程不够高效2）复杂的Assert机制3）控制和数据平面混合：导致网络内部的所有路由器上都有(S, G)，可能会导致非确定性的拓扑行为4）不支持共享树 PIM-DM适用于1）小规模的网络2）组播源和接收者比较靠近3）源少，接收者多4）数据流大且稳定 PIM-SMPIM-SM协议假设没有主机需要接收组播数据，除非它们明确地发出了请求。 稀疏组播的特点为1）组成员所在的网络数相对少2）组成员跨越的区域太大3）带宽资源还没有富裕到可以忽略DM模式带来的消耗 在PIM-SM中有个重要的概念：汇聚点，RP(Rendezvous Point)，发送者和接收者在RP处进行汇聚，表现为1）发送者的第一跳路由器把发送者注册到RP上（报个到，挂个号）2）接收者的DR（直连网络上的负责人）为接收者加入到共享树 (树根在RP) 接收者加入或离开组播组的行为表现为1)加入:接收者发送加入消息，逐跳上行到RP，沿途路由器记录组播转发状态；2)离开:接收者不想要组播数据时，发送剪枝消息，逐跳上行到RP，沿途路由器更新它的转发状态 从上面接收者离开或加入的行为可以看出SM跟DM本质的差别：路由器转发状态通过组播消息的抵达而建立或更新。 上面的过程中有几个关键问题，（1）如何知道RP(RP发现)？（2）如何让源组播数据到达RP？（3）能否在接收者和源之间建立一个转发树，分担RP的负担？ 针对问题（1），采用自举路由器机制(BSR)来选出RP，通常通过人工配置，将一组路由器配置为候选自举路由器(C-BSR)，另一组路由器配置为候选汇集点（C-RP），通常建议这两组路由器是同样的路由器。C-RP 会定期把候选汇集点通告消息（C-RP-Advs）以单址的形式发送到C-BSR；汇集点通告消息是一种PIM消息，它包括通告 C-RP 的地址、可选的组播组地址和一个掩码长度域（说明组的前缀）；C-BSR 收集这些通告消息并产生相应的自举报文，自举报文也是一种PIM消息，它包括C-RP和相应的组前缀并由自举路由器以一跳一跳的形式发送到所有普通路由器。普通路由器通过接收自举报文便可知道C-RP的地址。 针对问题（2），采用了源注册的机制，过程如下; 1）源的DR(执行注册的源第一跳路由器)将组播数据封装进一个注册消息，单播到RP；2）RP打开注册消息,将组播数据在RPT上转发,发送(S,G)加入消息，沿途建立(S,G)状态3）当RP察觉到从源到RP的SPT树已经建立，RP发送“注册停止”消息给源 上面的过程图示如下所示; 需要注意的是当源的SPT建立起来后，源的DR不会马上停止注册，而是等待收到RP的注册停止消息后才会停止，这时候空注册消息和沿着SPT的组播数据流并存。 在通过组播传输数据的时候，数据的传输方向为 源→SPT→RP→RPT，如下图所示 从上面的传播路径可知，RP可能会成为瓶颈，针对这个问题，也就是问题（3），提出了SPT切换的方法,其过程如下 上面中SPT切换的条件为：最后一跳路由器（和接收者直连的路由器）一旦发现某个特定的组播源的数据量超出了某个界限(阈值)，马上向组播源发送（S，G）Join消息。 共享树剪枝的条件为：最后一跳路由器根据自己的状态表中的（*，G）和（S，G）的入接口情况来判断是否发送剪枝消息（剪共享树），触发条件是：在（S，G）的入接口上收到了相符合的组播数据 （源树已经建立） PIM-SM对于稀疏和密集应用都很高效，其优势为 数据流仅沿“加入”的分支向下发送 可以根据流量等条件动态地切换到源树 与具体的单播路由协议无关 域间组播路由的基础(和MBGP、MSDP共同结合使用可以完成跨域的组播) PIM-SM适用于1）大规模的企业网络2）接收者稀少3）几乎是任何网络的优选方案（目前PIM-SM占主流） 源特定组播(SSM)源特定组播（SSM：Source Specific Multicast）是一种区别于传统组播的新的业务模型，它使用组播组地址和组播源地址同时来标识一个组播会话，而不是像传统的组播服务那样只使用组播组地址来标识一个组播会话，由于源地址的加入，组地址在不同源地址之间可以重用。 SSM保留了传统PIM-SM模式中的主机显示加入组播组的高效性，但是没有 PIM-SM 模式中的共享树和 RP的概念，SSM直接建立由(S,G)标识的一个有源树， 在 SSM 中，主机主动发起对指定(S,G)的加入，由最后一跳路由器直接向源发送（S,G）加入消息。 ipv6组播ipv6 的组播与ipv4的组播非常类似，这里做简单介绍。 ipv6的组播地址规定前8位均为1,也就是以ff开头，其他部分含义如下所示： Flags:用来表示permanent或transient组播组 Scope:表示组播组的范围 0：预留1：节点本地范围2：链路本地范围5：站点本地范围 Group ID:组播组ID ipv6中一些众所周知的组播地址以及与ipv4的对应关系如下： ipv6组播中的mac地址也是通过映射来的，映射规则为33:33:+IPv6组播地址的后32位。同样也存在mac重复问题。 ipv6的组播协议也分为组管理协议和路由协议。组成员管理协议为MLD（Multicast listener Discovery，侦听发现协议）,MLD几乎全盘继承了IGMPv2和IGMPv3，更名为MLDv1和MLDv2，用在路由器和ip主机之间。而路由协议依然是PIM。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[操作系统课程总结]]></title>
      <url>%2F2016%2F12%2F18%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93%2F</url>
      <content type="text"><![CDATA[本文为操作系统的课程小结，主要讲述 Linux 内核的一些知识，参考的主要教材为《Linux内核设计与实现(第3版)》，除此之外还参考了网络上的若干资料，因为Linux本来就是可以写若干本书的内容，所以这里只会涉及到博主上课时接触到的一些知识点，由于博主知识有限，个中定存在错漏地方，望不吝指出。 第一章：内核简介Linux 内核特点 动态加载内核模块 支持对称多处理（SMP） 在计算领域，对称多处理是一种多处理机硬件架构，有两个或更多的相同的处理机（处理器）共享同一主存，由一个操作系统控制 内核可抢占(Preemption): 要理解 Preemption 首先要了解操作系统的 Context Switch Context Switch (上下文切换) 指任何操作系统上下文(上下文简单说来就是一个环境，如进程上下文就是CPU的所有寄存器中的值、进程的状态以及堆栈上的内容，中断上下文就是硬件的参数和内核需要保存的一些其他环境)保存和恢复执行状态，以便于被安全地打断和稍后被正确地恢复执行。当发生进程调度时，进行进程切换就是上下文切换,一般操作系统中通常因以下三种方式引起上下文切换: Task Scheduling (任务调度)。任务调度一般是由调度器代码在内核空间完成的。 通常需要将当前 CPU 执行任务的代码，用户或内核栈，地址空间切换到下一个要运行任务的代码，用户或内核栈，地址空间。 Interrupt (中断) 或 Exception (异常)。中断和异常是由硬件产生但由软件来响应和处理的。这个过程中，涉及到将用户态或内核态代码切换至中断处理代码。 System Call (系统调用)。系统调用是由用户态代码主动调用，使用户进程陷入到内核态调用内核定义的各种系统调用服务。系统调用实质就是通过指令产生中断，也称为软中断。 实际上，进程从用户态进入内核态的方式只有两种:中断和异常。(系统调用实际上最终是中断机制实现的) Preemption (抢占) 是指操作系统允许满足某些重要条件(例如：优先级，公平性)的任务打断当前正在 CPU 上运行的任务而得到调度执行。并且这种打断不需要当前正在运行的任务的配合，同时被打断的程序可以在后来可以再次被调度恢复执行。 线程的实现：内核并不区分线程和其他的一般进程，线程是一个标准的进程，与进程的最大区别在于是否有独立的地址空间 设备管理: 所有设备都是文件 内核版本号主版本号.从版本号.修订版本号，从版本号为偶数时为稳定版本，奇数时为开发版 内核应用 内核开发、移植 驱动 文件系统 云计算与虚拟化 第二章：内核的编译与安装简单教程：http://www.cnblogs.com/hdk1993/p/4910362.html 补丁概念 内核开发特点： 不能访问c库，只能使用c的语法 缺乏内存保护机制 浮点数难以使用 注意同步和并发（原因：竞争条件（可抢占多任务系统），解决方法：自旋锁和信号量） 保持可移植性 第三章：进程管理进程描述符内核通过一个任务队列（task list）组织所有的进程，任务队列是一个双向链表，结构如下图所示 图中链表中每一项都是类型为 task_struct ，称为进程描述符的结构。进程描述符包含了一个具体进程的所有信息，如： 进程状态 进程的地址空间 PID 指向父、子进程的指针 打开的文件 …… task_struct 的存放位置 2.6 之前存放在进程的内核栈底 2.6之后改为了通过内核栈底的一个结构（thread_info），这个结构中有一个指针指向其task_structure 关于进程的内核栈和用户栈参考：http://blog.csdn.net/dlutbrucezhang/article/details/9326857每个进程都有自己的堆栈，内核在创建一个新的进程时，在创建进程控制块task_struct的同时，也为进程创建自己堆栈。一个进程有2个堆栈，用户堆栈和系统堆栈；用户堆栈的空间指向用户地址空间，内核堆栈的空间指向内核地址空间。当进程在用户态运行时，CPU 堆栈指针寄存器指向的 用户堆栈地址，使用用户堆栈，当进程运行在内核态时，CPU堆栈指针寄存器指向的是内核栈空间地址，使用的是内核栈； 当进程由于中断或系统调用从用户态转换到内核态时，进程所使用的栈也要从用户栈切换到内核栈。进程因为中断（软中断或硬件产生中断），使得CPU切换到特权工作模式，此时进程陷入内核态，进程进入内核态后，首先把用户态的堆栈地址保存在内核堆栈中，然后设置堆栈指针寄存器的地址为内核栈地址，这样就完成了用户栈向内核栈的切换。当进程从内核态切换到用户态时，最后把保存在内核栈中的用户栈地址恢复到CPU栈指针寄存器即可，这样就完成了内核栈向用户栈的切换。 task_struct 的组成部分task_struct中包含了进程的所有信息，如进程的状态，优先级，pid，父进程与子进程，运行的时间，与文件系统的交互情况，内存使用情况(mm_struct)等。 这里详细介绍的几个重要组成部分： 进程的状态广义来说，对所有操作系统而言，进程的状态一般可以分为running,ready和block状态，其中running表示进程正在cpu上跑,ready表示进程正在等待cpu分配执行的时间片，一旦分配了时间片即可进入running状态，而block表示当前的进程正在等待某些资源(如用户的输入)，只有得到了这些资源，才可进入ready状态。 但是在 Linux 中，为进程定义了五种状态，与上面所说的状态略有不同，每种状态定义如下： 1. R (task_running) : 可执行状态 只有在该状态的进程才可能在CPU上运行。而同一时刻可能有多个进程处于可执行状态，这些进程的task_struct结构（进程控制块）被放入对应CPU的可执行队列中（一个进程最多只能出现在一个CPU的可执行队列中）。进程调度器的任务就是从各个CPU的可执行队列中分别选择一个进程在该CPU上运行。这种状态包含了正在执行的进程和等待分配时间片的进程，即包含了上面的running和ready状态。 2. S (task_interruptible): 可中断的睡眠状态 处于这个状态的进程因为等待某某事件的发生（比如等待socket连接、等待信号量），而被挂起。这些进程的task_struct结构被放入对应事件的等待队列中。当这些事件发生时（由外部中断触发、或由其他进程触发），对应的等待队列中的一个或多个进程将被唤醒。 通过top命令我们会看到，一般情况下，进程列表中的绝大多数进程都处于task_interruptible状态（除非机器的负载很高）。毕竟CPU就这么一两个，进程动辄几十上百个，如果不是绝大多数进程都在睡眠，CPU又怎么响应得过来。 3. D (task_uninterruptible): 不可中断的睡眠状态 与task_interruptible状态类似，进程处于睡眠状态，但是此刻进程是不可中断的。不可中断，指的并不是CPU不响应外部硬件的中断，而是指进程不响应异步信号。 绝大多数情况下，进程处在睡眠状态时，总是应该能够响应异步信号的。但是task_uninterruptible 状态的进程不接受外来的任何信号，因此无法用 kill 杀掉这些处于D状态的进程，无论是 kill, kill -9还是kill -15，这种情况下，一个可选的方法就是reboot。 处于task_uninterruptible状态的进程通常是在等待IO，比如磁盘IO，网络IO，其他外设IO，如果进程正在等待的IO在较长的时间内都没有响应，那么就被ps看到了，同时也就意味着很有可能有IO出了问题，可能是外设本身出了故障，也可能是比如挂载的远程文件系统已经不可访问了. 而task_uninterruptible状态存在的意义就在于，内核的某些处理流程是不能被打断的。如果响应异步信号，程序的执行流程中就会被插入一段用于处理异步信号的流程（这个插入的流程可能只存在于内核态，也可能延伸到用户态），于是原有的流程就被中断了。 在进程对某些硬件进行操作时，可能需要使用task_uninterruptible状态对进程进行保护，以避免进程与设备交互的过程被打断，造成设备陷入不可控的状态。这种情况下的task_uninterruptible状态总是非常短暂的，通过ps命令基本上不可能捕捉到。 4. T(task_stopped or task_traced)：暂停状态或跟踪状态 向进程发送一个sigstop信号，它就会因响应该信号而进入task_stopped状态（除非该进程本身处于task_uninterruptible状态而不响应信号）。 向进程发送一个sigcont信号，可以让其从task_stopped状态恢复到task_running状态。 当进程正在被跟踪时，它处于task_traced这个特殊的状态。“正在被跟踪”指的是进程暂停下来，等待跟踪它的进程对它进行操作。比如在调试的时候对被跟踪的进程下一个断点，进程在断点处停下来的时候就处于task_traced状态。而在其他时候，被跟踪的进程还是处于前面提到的那些状态。 对于进程本身来说，task_stopped和task_traced状态很类似，都是表示进程暂停下来。而task_traced状态相当于在task_stopped之上多了一层保护，处于task_traced状态的进程不能响应sigcont信号而被唤醒。只能等到调试进程通过ptrace系统调用执行ptrace_cont、ptrace_detach等操作（通过ptrace系统调用的参数指定操作），或调试进程退出，被调试的进程才能恢复task_running状态。 5. Z (task_dead - exit_zombie)：退出状态，进程成为僵尸进程 在Linux进程的状态中，僵尸进程是非常特殊的一种，它是已经结束了的进程，但是没有从进程表中删除。太多了会导致进程表里面条目满了(PID数目有限)，进而导致系统崩溃，倒是不占用其他系统资源。 僵尸进程已经放弃了几乎所有内存空间，没有任何可执行代码，也不能被调度，仅仅在进程列表中保留一个位置，记载该进程的退出状态等信息供其他进程收集，除此之外，僵尸进程不再占有任何内存空间。 进程在退出的过程中，处于TASK_DEAD状态。在这个退出过程中，进程占有的所有资源将被回收，除了task_struct结构（以及少数资源）以外。于是进程就只剩下task_struct这么个空壳，故称为僵尸。 之所以保留task_struct，是因为task_struct里面保存了进程的退出码、以及一些统计信息，而其父进程很可能会关心这些信息。比如在shell中，$?变量就保存了最后一个退出的前台进程的退出码，而这个退出码往往被作为if语句的判断条件。 当然，内核也可以将这些信息保存在别的地方，而将task_struct结构释放掉，以节省一些空间。但是使用task_struct结构更为方便，因为在内核中已经建立了从pid到task_struct查找关系，还有进程间的父子关系。释放掉task_struct，则需要建立一些新的数据结构，以便让父进程找到它的子进程的退出信息。 子进程在退出的过程中，内核会给其父进程发送一个信号，通知父进程来“收尸”。 父进程可以通过 wait 系列的系统调用（如wait4、waitid）来等待某个或某些子进程的退出，并获取它的退出信息。然后wait系列的系统调用会顺便将子进程的尸体（task_struct）也释放掉。 但是如果他的父进程没调用wait或waitpid()等待子进程结束，那么它就一直保持僵尸状态，子进程的尸体（task_struct）也就无法释放掉。 如果这时父进程结束了，那么init进程自动会接手这个子进程，为它收尸，它还是能被清除的。但是如果如果父进程是一个循环，不会结束，那么子进程就会一直保持僵尸状态，这就是为什么系统中有时会有很多的僵尸进程。 当进程退出的时候，会将它的所有子进程都托管给别的进程（使之成为别的进程的子进程）。托管的进程可能是退出进程所在进程组的下一个进程（如果存在的话），或者是1号进程。 1号进程，就是pid为1的进程，又称init进程。linux系统启动后，第一个被创建的用户态进程就是init进程。它有两项使命：1）执行系统初始化脚本，创建一系列的进程（它们都是init进程的子孙）；2）在一个死循环中等待其子进程的退出事件，并调用waitid系统调用来完成“收尸”工作 init进程不会被暂停、也不会被杀死（这是由内核来保证的）。它在等待子进程退出的过程中处于task_interruptible状态，“收尸”过程中则处于task_running状态。 父进程与子进程 进程只有一个父母 ，在进程的 task_struct中的parent表示 进程可以有0个以上的子女，在进程的 task_struct中的children表示比较有趣的一点是在windows 中并没有父子进程的概念。 进程的若干ID pid：进程的ID,唯一标识一个进程,系统中可用的PID 是有限制的, 因此系统中进程的总数也是有限制的 pgrp：进程的组id，进程 uid：启动进程的用户id gid：启动进程的用户所在组的id euid，egid ：euid和egid又称为有效的uid和gid。出于系统安全的权限的考虑，运行程序时要检查euid和egid的合法性。通常，uid等于euid，gid等于egid。有时候，系统会赋予一般用户暂时拥有root的uid和gid(作为用户进程的euid和egid)，以便于进行运作。（特殊权限：suid，sgid） 上面关于task_struct的组成所涉及到的只是很小一部分，更详细的内容可参考 linux进程描述符task_struct详解和task_struct结构体字段介绍–Linux中的PCB。 进程与线程线程基本概念按照教科书上的定义，进程是资源管理的最小单位，线程是程序执行的最小单位。在操作系统设计上，从进程演化出线程，最主要的目的就是更好的支持SMP以及减小（进程/线程）上下文切换开销。 一个进程至少需要一个线程作为它的指令执行体，进程管理着资源（比如cpu、内存、文件等等），而将线程分配到某个cpu上执行。一个进程可以拥有多个线程，此时，如果进程运行在SMP机器上，它就可以同时使用多个cpu来执行各个线程，达到最大程度的并行，以提高效率；同时，即使是在单cpu的机器上，采用多线程模型来设计程序，正如当年采用多进程模型代替单进程模型一样，使设计更简洁、功能更完备，程序的执行效率也更高，例如采用多个线程响应多个输入，而此时多线程模型所实现的功能实际上也可以用多进程模型来实现，而与后者相比，线程的上下文切换开销就比进程要小多了，从语义上来说，同时响应多个输入这样的功能，实际上就是共享了除cpu以外的所有资源的。 线程与进程的比较1) 调度。在传统的操作系统中，拥有资源和独立调度的基本单位都是进程。在引入线程的操作系统中，线程是独立调度的基本单位，进程是资源拥有的基本单位。在同一进程中，线程的切换不会引起进程切换。在不同进程中进行线程切换,如从一个进程内的线程切换到另一个进程中的线程时，会引起进程切换。 2) 系统开销。由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、 I/O设备等，因此操作系统所付出的开销远大于创建或撤销线程时的开销。而线程切换时只需保存和设置少量寄存器内容，开销很小。此外，由于同一进程内的多个线程共享进程的地址空间，因此，这些线程之间的同步与通信非常容易实现，甚至无需操作系统的干预。 3) 地址空间和其他资源（如打开的文件）：进程的地址空间之间互相独立，同一进程的各线程间共享进程的资源（包括地址空间），某进程内的线程对于其他进程不可见。 4) 通信方面：进程间的通信方式有这样几种： 共享内存 消息队列 有名管道 无名管道 信号 文件 socket 线程间的通信方式上述进程间的方式都可沿用，且还有自己独特的几种 互斥量 自旋锁 条件变量 读写锁 线程信号 全局变量 线程的实现方式线程的实现可以分为两类：用户级线程(User-Level Thread, ULT)和内核级线程(Kemel-Level Thread, KLT)。前者更利于并发使用多处理器的资源，而后者则更多考虑的是上下文切换开销。 在用户级线程中，有关线程管理的所有工作都由应用程序完成，内核意识不到线程的存在。应用程序可以通过使用线程库设计成多线程程序。通常，应用程序从单线程起始，在该线程中开始运行，在其运行的任何时刻，可以通过调用线程库中的派生例程创建一个在相同进程中运行的新线程。下图(a)说明了用户级线程的实现方式。 在内核级线程中，线程管理的所有工作由内核完成，应用程序没有进行线程管理的代码，只有一个到内核级线程的编程接口。内核为进程及其内部的每个线程维护上下文信息，调度也是在内核基于线程架构的基础上完成。下图(b)说明了内核级线程的实现方式。 在一些系统中，使用组合方式的多线程实现。线程创建完全在用户空间中完成，线程的调度和同步也在应用程序中进行。一个应用程序中的多个用户级线程被映射到一些（小于或等于用户级线程的数目）内核级线程上。下图(c)说明了用户级与内核级的组合实现方式。 多线程模型有些系统同时支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程和内核级线程的连接方式，如上面的图实际上就包含了三种经典的多线程模型。 1) 多对一模型 将多个用户级线程映射到一个内核级线程，线程管理在用户空间完成。 此模式中，用户级线程对操作系统不可见（即透明）。 优点：线程管理是在用户空间进行的，因而效率比较高。 缺点：当一个线程在使用内核服务时被阻塞，那么整个进程都会被阻塞；多个线程不能并行地运行在多处理机上。 2) 一对一模型 将每个用户级线程映射到一个内核级线程。 优点：当一个线程被阻塞后，允许另一个线程继续执行，所以并发能力较强。 缺点：每创建一个用户级线程都需要创建一个内核级线程与其对应，这样创建线程的开销比较大，会影响到应用程序的性能。 3) 多对多模型 将 n 个用户级线程映射到 m 个内核级线程上，要求 m &lt;= n。 特点：在多对一模型和一对一模型中取了个折中，克服了多对一模型的并发度不高的缺点，又克服了一对一模型的一个用户进程占用太多内核级线程，开销太大的缺点。又拥有多对一模型和一对一模型各自的优点，可谓集两者之所长 需要注意的是在Linux 中，从内核的角度来说，它并没有线程这个概念，内核把所有的线程都当成进程来实现。在内核中，线程看起来就像是一个与其他进程共享了一些资源的普通进程，每一个线程有其唯一的task_struct； 关于这个说法，可参考Linux线程的前世今生 在 Linux 创建的初期，内核一直就没有实现“线程”这个东西。后来因为实际的需求，便逐步产生了LinuxThreads 这个项目，其主要的贡献者是Xavier Leroy。LinuxThreads项目使用了 clone() 这个系统调用对线程进行了模拟，按照《Linux内核设计与实现》的说法，调用 clone() 函数参数是 clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, 0)，即创建一个新的进程，同时让父子进程共享地址空间、文件系统资源、文件描述符、信号处理程序以及被阻断的信号等内容。也就是说，此时的所谓“线程”模型符合以上两本经典巨著的描述，即在内核看来，没有所谓的“线程”，我们所谓的“线程”其实在内核看来不过是和其他进程共享了一些资源的进程罢了。 Linux 的内核线程 内核线程是标准的进程，只存在于内核空间 内核线程没有地址空间 内核线程只能由其他内核线程创建 进程的创建和结束进程创建的两个步骤进程的创建可以分为两个步骤：1) fork:创建一个子进程即复制当前的任务，新进程与其父进程的区别仅在于PID, PPID以及特定的资源(如某些资源的统计量，没有必要继承)。父子进程同时执行，因此调用一次返回两次。 2) exec：将一个程序装入地址空间并执行，只有子进程执行，重建其地址空间，区别于父进程。 fork 操作直接把所有资源复制给新创建的子进程，这种实现大批量的复制无疑或导致执行效率低下，因为行为是非常耗时的，因为它需要： 为子进程的页表分配页面 为子进程的页分配页面 初始化子进程的页表 把父进程的页复制到子进程相应的页中 创建一个地址空间的这种方法涉及许多内存访问，消耗许多CPU周期，并且完全破坏了高速缓存中的内容。在大多数情况下，这样做常常是毫无意义的，因为许多子进程通过装入一个新的程序开始它们的执行，这样就完全丢弃了所继承的地址空间。所以 linux 采用了写时复制(copy-on-write)的策略。 写时拷贝是一种可以推迟甚至免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。也就是说，资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享。这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候。在页根本不会被写入的情况下—举例来说，fork()后立即调用exec()—它们就无需复制了。fork()的实际开销就是复制父进程的页表以及给子进程创建惟一的进程描述符。 实现的时候 fork 通过clone()系统调用实现，clone()通过一系列的参数标志指定父子进程需要共享的资源，clone()调用do_fork(),而do_fork()调用copy_process(),而copy_process()做了以下事情： 调用dup_task_struct复制内核栈、thread_info和task_struct 检查用户进程限额 改变子进程task_struct结构中的部分内容，子进程状态置为TASK_UNINTERRUPTIBLE 为子进程获取一个有效的PID 根据传递给clone()的参数复制资源 父子进程平分剩余的时间片 返回指向子进程的指针 除了 fork 以外，linux中还有一种创建进程的方式 vfork，vfork与fork功能相同，子进程共享父进程的地址空间(内核连子进程的虚拟地址空间结构也不创建)。创建完成后父进程阻塞，直到子进程结束或执行exec。vfork 也是通过向clone系统调用传递特定的标志实现。 进程的结束以下几种情况会出现进程的结束：1）正常结束(显示或隐式地调用exit()系统调用）2）进程收到不能忽略也不能处理的信号或异常 执行exit()函数的过程： 释放进程的地址空间 释放进程使用的资源 给其父进程发送一个信号，并标示自己的状态为TASK_ZOMBIE 调用调度程序，执行其他进程 当父进程收到子进程结束信号时，收回子进程的 task_structure和thread_info,没有回收就称为了僵尸进程。 信号基本概念信号机制是进程之间相互传递消息的一种方法，信号全称为软中断信号，也有人称作软中断。在linux中每个信号有一个名字(以SIG开头)，且定义为一个整数，共有64个信号。 软中断信号（signal，又简称为信号）用来通知进程发生了异步事件。进程之间可以互相通过系统调用kill发送软中断信号。内核也可以因为内部事件而给进程发送信号，通知进程发生了某个事件。注意，信号只是用来通知某进程发生了什么事件，并不给该进程传递任何数据。 信号来源分为硬件类和软件类： 硬件方式 用户输入：比如在终端上按下组合键ctrl+C，产生SIGINT信号； 硬件异常：CPU检测到内存非法访问等异常，通知内核生成相应信号，并发送给发生事件的进程； 软件方式通过系统调用，发送signal信号：kill()，raise()，sigqueue()，alarm()，setitimer()，abort()等 收到信号的进程对各种信号有不同的处理方法。处理方法可以分为三类：1）类似中断的处理程序，对于需要处理的信号，进程可以指定处理函数，由该函数来处理。2）忽略某个信号，对该信号不做任何处理，就象未发生过一样。但是有些信号是不能忽略的，如SIGKILL、SIGSTOP和一些硬件异常信号3）对该信号的处理保留系统的默认值，这种缺省操作，对大部分的信号的缺省操作是使得进程终止。 相关的系统调用上面的第一种处理方式是通过系统调用signal来指定进程对某个信号的处理行为。signal函数的定义如下123456789#include &lt;signal.h&gt;typedef void (*sighandler_t)(int);sighandler_t signal(int signum, sighandler_t handler);(返回值: 如果成功则返回先前的handler，否则返回SIG_ERR)“handler”可取下面的三个值中任意一个：用户定义的函数，或SIG_DEF(恢复参数signum所指信号的处理方法为默认值),或SIG_IGN(忽略参数signum所指的信号) 通过kill()系统调用可以给进程发送一个信号，kill函数的声明如下1234#include &lt;sys/types.h&gt;#include &lt;signal.h&gt;int kill(pid_t pid, int sig);(返回值: 成功为0, 否则为-1) 而raise()系统调用可以说是kill()系统调用的一个特例，用于给当前进程发送一个信号，其定义如下：123#include &lt;signal.h&gt;int raise(int sig); (返回值: 成功为0, 否则为-1) 除此之外，系统调用alarm()的功能是设置一个定时器，当定时器计时到达时，将发出一个信号SIGALRM给进程。该调用的声明格式如下 123#include &lt;unistd.h&gt;unsigned int alarm(unsigned int seconds);(Returned value: 0, or the number of seconds remaining of previous alarm) 而系统调用pause的作用是等待一个信号。该调用使得发出调用的进程进入睡眠，直到接收到一个信号为止。该调用的声明格式如下：1int pause(void); 利用 alarm 函数和 pause 函数实现 sleep,同时可参考这里12345678910unsigned int sleep1(unsigned int nsecs) &#123; if ( signal(SIGALRM, sig_alrm) == SIG_ERR) return(nsecs); alarm(nsecs); /* 开始计时 */ pause(); /*定时信号来时被唤醒*/ return(alarm(0) ); /*关闭定时器 */&#125;int sig_alrm() &#123; signal(SIGALRM, sig_alrm) ;&#125; 可靠的信号机制Linux系统共定义了64种信号，分为两大类：可靠信号与不可靠信号 不可靠信号： 也称为非实时信号，不支持排队，信号可能会丢失, 比如发送多次相同的信号, 进程只能收到一次. 信号值取值区间为1~31； 可靠信号： 也称为实时信号，支持排队, 信号不会丢失, 发多少次, 就可以收到多少次. 信号值取值区间为32~64 信号的注册与注销 注册 在进程task_struct结构体中有一个未决信号的成员变量 struct sigpending pending。每个信号在进程中注册都会把信号值加入到进程的未决信号集。 非实时信号发送给进程时，如果该信息已经在进程中注册过，不会再次注册，故信号会丢失；实时信号发送给进程时，不管该信号是否在进程中注册过，都会再次注册。故信号不会丢失； 注销 非实时信号：不可重复注册，最多只有一个sigqueue结构；当该结构被释放后，把该信号从进程未决信号集中删除，则信号注销完毕；实时信号：可重复注册，可能存在多个sigqueue结构；当该信号的所有sigqueue处理完毕后，把该信号从进程未决信号集中删除，则信号注销完毕； 信号的处理内核处理进程收到的signal是在当前进程的上下文，故进程必须是Running状态。当进程唤醒或者调度后获取CPU，则会从内核态转到用户态时检测是否有signal等待处理，处理完，进程会把相应的未决信号从链表中去掉。 也就是说signal信号处理时机为： 内核态 -&gt; signal信号处理 -&gt; 用户态： 在内核态，signal信号不起作用； 在用户态，signal所有未被屏蔽的信号都处理完毕； 当屏蔽信号，取消屏蔽时，会在下一次内核转用户态的过程中执行； 信号相关函数进程处理某个信号前，需要先在进程中安装此信号。安装过程主要是建立信号值和进程对相应信息值的动作。 123信号安装函数signal()：不支持信号传递信息，主要用于非实时信号安装；sigaction():支持信号传递信息，可用于所有信号安装；（通过sigaction实现signal函数） 信号的发送系统调用123456kill()：用于向进程或进程组发送信号；sigqueue()：只能向一个进程发送信号，不能像进程组发送信号；主要针对实时信号提出，与sigaction()组合使用，当然也支持非实时信号的发送；alarm()：用于调用进程指定时间后发出SIGALARM信号；setitimer()：设置定时器，计时达到后给进程发送SIGALRM信号，功能比alarm更强大abort()：向进程发送SIGABORT信号，默认进程会异常退出。raise()：用于向进程自身发送信号； 信号阻塞函数：123456789sigprocmask(int how, const sigset_t *set, sigset_t *oldset))：检测或更改(或两者)进程的信号掩码不同how参数，实现不同功能SIG_BLOCK：将set中的信号添加到进程阻塞信号集（并集）SIG_UNBLOCK：从进程阻塞信号集删除set中的信号(差集)SIG_SETMASK：将set指向信号集中的信号，设置成进程阻塞信号集sigpending(sigset_t *set))：获取已发送到进程，却被阻塞的所有信号，也就是当前未决的信号集sigsuspend(const sigset_t *mask))：用mask代替进程的原有掩码，并暂停进程执行，直到收到信号再恢复原有掩码并继续执行进程。（pause) 第四章 进程调度调度器用于选择进程运行，分配CPU执行时间 调度器运行的时机 进程阻塞在一个I/O操作上. 硬件中断. 进程时间片到. 内核主动调用调度器 调度目标 有效性：完成尽可能多的工作。 交互性：尽快响应用户 公平性：不允许任何进程饥饿。 哪一个目标最重要取决于取决于目标场景 桌面系统:交互性，尽快相应用户 服务器:有效性，保证每个用户的请求都能够被完成 调度策略进程的类型 I/O密集型进程: 较多的交互性，高优先级，大时间片。如文本编辑 CPU密集型进程: 较少的交互性，低优先级，较小的时间片。如视频解码 进程优先级表示在linux中用top或者ps命令会输出PRI/PR、NI这两个指标值，其含义如下 PRI ：进程优先权，代表这个进程可被执行的优先级，其值越小，优先级就越高，越早被执行NI ：进程Nice值，可用于改变PRI的值，PRI(new)=PRI(old)+nice。 在Linux系统中，Nice值的范围从-20到+19（不同系统的值范围是不一样的），每个进程都在其计划执行时被赋予一个nice值。在通常情况下，子进程会继承父进程的nice值，比如在系统启动的过程中，init进程会被赋予0，其他所有进程继承了这个nice值（因为其他进程都是init的子进程）。 进程的nice值是可以被修改的，修改命令分别是nice和renice, 对非 root 用户，只能将其底下的进程的 nice 值变大而不能变小,若想变小，得要有相应的权限。对root用户，可以给其子进程赋予更小的nice值。 进程抢占的时机 当一个进程的优先级高于当前正在运行的进程的优先级 当一个进程的时间片为0. 调度器内核2.4是O(n)调度器，2.5及后改成了O(1)调度器，采用的新的数据结构为运行队列和优先级数组，同时改善了SMP的可拓展性。 两种数据结构的解释如下运行队列(struct runqueue )：给定处理器上可执行进程的链表，运行队列进行操作前要先锁住。优先级数组：Linux 调度器维护两个优先级数组：活跃的和过期的数组。优先级数组是提供 O(1) 调度的数据结构 优先级数组是一个结构体，其定义如下所示：12345struct prio_array &#123; int nr_active; /* 任务数目*/ unsigned long bitmap[BITMAP_SIZE]; /* 优先级位图*/ struct list_head queue[MAX_PRIO]; /* 优先级队列*/ &#125;; 调度器为每一个CPU维护了两个进程队列数组：active数组和expire数组。数组中的元素着保存某一优先级的进程队列指针。系统一共有140个不同的优先级，因此这两个数组大小都是140。同时该调度算法为每个优先级都设置一个可运行队列, 即包含140个可运行状态的进程链表，每一条优先级链表上的进程都具有相同的优先级，而不同进程链表上的进程都拥有不同的优先级。 除此之外, 每个优先级数组还包括一个优先级位图bitmap。该位图使用一个位(bit)来代表一个优先级，而140个优先级最少需要5个32位来表示， 因此只需要一个int[5]就可以表示位图，该位图中的所有位都被置0，当某个优先级的进程处于可运行状态时，该优先级所对应的位就被置1。 优先级数组中分为了活跃和过期两种，过期的优先级数组存放过期队列，活跃的优先级数组存放实际队列。 过期队列是所有用完了时间片的进程。实际队列是没有用完时间片的进程。当一个进程用完了时间片时，重新计算其时间片，并放入到过期队列中。当实际进程队列为空时，交换过期队列和实际队列。 重新计算时间片过程 动态优先级用于计算优先级nice+进程交互性的奖励或罚分为了确定一个进程是否是交互性的, Linux记录了一个进程用于休眠和用于执行的时间（0-MAX_SLEEP_AVG，默认10ms）。一个进程从休眠恢复到执行时，优先级增加；运行一段时间后会减小。 静态优先级用于计算时间片进程创建时，子进程与父进程均分父进程剩余的时间片.任务的时间片用完时，基于任务的静态优先级重新计算时间片 负载平衡程序12345找最繁忙的运行队列选择一个优先级数组（过期的优先）选择优先级最高的链表选择一个不是正在运行的，不在高速缓冲的，可移动的进程抽取重复上述步骤，直至平衡 抢占可分为用户抢占和内核抢占：用户抢占发生在 从系统调用返回用户态 从中断服务程序返回用户态 内核抢占发生在 中断服务程序正在执行，且返回内核空间之前 内核代码再一次具有可抢占性时处于核心态的任务直接调用schedule() 内核中的任务阻塞 实时调度策略SCHED_FIFO:先入先出方式调度的实时进程，即该进程一旦执行便一直运行到结束。SCHED_RR: 通过时间片轮转的方式调度的实时进程。在运行了指定的时间片后会被抢占并重新调度。但如果没有其他优先级高于或等于它的实时进程与其竞争，它还会得到继续运行的机会。 第五章 系统调用基本概念系统调用为用户空间进程提供一个访问内核接口，在Linux中，系统调用是唯一合法访问内核的入口。 系统调用的目的主要有两个：1）为用户空间提供一个统一接口。2）保证系统的安全和稳定 API、POSIX、C库API/POSIX/C库的区别与联系 一般情况下应用程序通过应用编程接口API，而不是直接通过系统调用来编程。在Unix世界，最流行的API是基于POSIX标准的,即POSIX说明API和系统调用之间关系，。 api是函数的定义，规定了这个函数的功能，跟内核无直接关系。它们可以实现成一个系统调用，也可以通过调用多个系统调用来实现，而完全不使用任何系统调用也不存在问题。实际上，API可以在各种不同的操作系统上实现，给应用程序提供完全相同的接口，而它们本身在这些系统上的实现却可能迥异。 Linux的系统调用接口,像大多数Unix系统一样，以C 库的形式提供，C库实现了 Unix系统的主要API，包括标准C库函数和系统调用。 系统调用的实现在Linux中, 每一个系统调用分配有一个syscall 号.这是一个唯一的整数，用于指定系统调用. syscall号分配后，不能够改变或回收. 一般地，系统调用都是通过软中断实现：产生一个异常，系统切换到内核模式，执行异常处理程序，即系统调用处理程序，在x86上，定义的软中断是函数system_call()。 system_call()函数检查系统调用号syscall，如合法，调用指定的系统调用。 增加一个系统调用的过程 首先在系统调用表的最后加入一个表项。 对于每一种支持的体系结构，系统调用号必须定义在 &lt;asm/unistd.h&gt;. 系统调用必须被编译进内核映像。 实现一个新的系统调用的好处：1）系统调用容易使用容易实现。2）系统调用的性能在Linux中非常快。缺点:1）系统调用号需要官方授权给你。2）系统调用一旦进入稳定的内核，其接口就不能再改变，否则会影响用户空间的应用.3）需要将系统调用分别注册到每个需要支持的体系结构。4）系统调用在脚本中不宜使用，不能直接从文件系统访问。5）如果仅仅进行简单的信息交换，系统调用就大材小用 从用户空间访问系统调用：Linux 提供了一组宏，用于直接访问系统调用。它设置寄存器内容，并执行trap指令。这些宏是 _syscalln(), 这里n：0-6。 第六章 内核数据结构 链表 队列 映射 红黑树 映射是键到值的关联关系。Linux中的映射是将一个唯一的标识符（UID）映射到一个指针，实现方式有： 数组 散列表 自平衡二叉树 Linux采用的方式：radix树（） 数据结构的选择原则如下： 链表：主要操作是遍历数据 队列：生产者消费者模式 映射：映射一个UID到一个对象 红黑树：存储大量数据，并且迅速检索 红黑树是一种自平衡二叉搜索树，具有以下性质： 所有的节点或者红色，或者黑色 所有叶节点都是黑色 叶节点不包含数据 所有非叶节点都有两个字节点 如果一个节点是红色，则其子节点都是黑色 在一个节点到其叶子节点的路径中，如果总是包含同样数目的黑色节点，则该路径相比其他路径是最短的 第七、八章 中断和中断处理基本概念操作系统要管理硬件，就必须要能够与硬件通信。中断能够使硬件能够发出通知给处理器(CPU)，例如按下键盘时，键盘控制器就会发出一个中断请求；处理器(CPU)接收到中断请求后，会马上通知内核进行处理，因此硬件设备生成中断时并不会考虑与处理器的时钟同步，也就是说中断可以随时产生。 不同情况下对应的中断不同，每个中断都有一个唯一的数字标示，通常被称为中断号（IRQ），如键盘和硬盘的中断号就不同。中断号的不同，内核处理的程序也不同，因此有一张表格用于记录每个中断号及其对应的处理程序，称为中断向量表。 说到中断，常常会提及到异常。异常与中断不同，异常产生的时候必须考虑与处理器的时钟同步。实际上异常也常被称为同步中断。常见的异常有处理器执行过程中由于代码的缺陷而执行了错误指令（如除上0），或者是在执行期间出现了特殊情况（如缺页），必须依靠内核来处理的时候，处理器就会产生一个异常。前面说到的系统调用实际上就是通过异常来实现的，异常也可称为软中断。 中断处理程序由前面的简介可知，响应一个特定的中断，内核会执行一个与之对应的特定函数，这个函数就叫做中断处理程序。 在Linux中，一个设备的中断处理程序是它设备驱动程序（管理设备的内核代码）的一部分，因此，中断处理程序实际上就是普通的C函数，与其他内核函数的真正区别在于这些程序运行于被称为中断上下文的特殊上下文中，该上下文不可被阻塞，也就是说进入中断服务程序后, 不会被其他响应中断。 上半部和下半部中断要求尽快处理，但是往往中断又要完成较多的工作量。考虑两者间做一个权衡，中断处理被分成了两个部分：上半部和下半部，上半部完成那些重要、有严格时限的、与硬件相关的工作，下半部则完成那些允许被稍后完成的工作。接收到一个中断后，上半部（实际上就是中断处理程序）会立即执行，然后返回中断前原先运行的程序，而下半部会在合适的时机在执行（通常下半部分在中断处理程序一返回就会马上运行）。中断处理程序的下半部分（如果有的话）几乎做了中断处理程序所有的事情。它们最大的不同是上半部分不可中断，而下半部分可中断。下面以网卡为例做简单说明： 当网卡接收到来自网络的数据包的时候，网卡会向内核发出中断，内核通过网卡已注册的中断处理程序来做出应答，中断开始的时候，内核会快速拷贝网络数据包到系统内存，因为网卡上接收的网络数据包的缓存是固定的，而且相比于内存来说很小，假如数据包占满了缓存，后续的数据包只能被丢弃，所以这个任务是最紧急的，当网络数据包全被拷到内存后，中断任务算是完成了，这个它会将控制权返回给系统中断前原先运行的程序。至于处理数据包的操作会在下半部进行。 尽管上半部和下半部的结合能够改善系统的响应能力，但是，Linux设备驱动中的中断处理并不一定要分成两个半部。如果中断要处理的工作本身就很少，则完全可以直接在上半部全部完成。 上半部注册与释放中断程序对于设备的每一种中断，设备的驱动程序需要为其注册一个相关的中断处理程序，以便通知内核该如何处理该中断。 驱动程序通过request_irq()函数注册一个中断处理程序。 卸载驱动程序的时候，需要注销相应的中断处理程序，并释放中断线（设备的中断处理器与CPU的直连线），通过调用free_irq() 实现 编写中断处理程序典型的定义123456static irqreturn_t intr_handler (int irq, void *dev_id, struct pt_regs *regs)irq: 中断号dev_id: 区分共享中断线的多个设备regs: 保存中断前的处理器的寄存器和状态irqreturn_t：IRQ_NONE, IRQ_HANDLED Linux中的中断处理程序都是无须重入的，也就是说一个给定的中断处理程序正在执行的时候，相应的中断线在所有的处理器上都会被屏蔽，以防止在同一中断线上接收另外一个新的中断，但是其他的中断线上的中断能够被处理。 中断上下文在讨论中断上下文之前先讨论一下进程上下文。 用户空间的应用程序，通过系统调用，进入内核空间。这个时候用户空间的进程要传递 很多变量、参数的值给内核，内核态运行的时候也要保存用户进程的一些寄存 器值、变量等。所谓的“进程上下文”，可以看作是用户进程传递给内核的这些参数以及内核要保存的那一整套的变量和寄存器值和当时的环境等。 相对于进程而言，就是进程执行时的环境。具体来说就是各个变量和数据，包括所有的寄存器变量、进程打开的文件、内存信息等。一个进程的上下文可以分为三个部分:用户级上下文、寄存器上下文以及系统级上下文。 （1）用户级上下文: 正文、数据、用户堆栈以及共享存储区；（2）寄存器上下文: 通用寄存器、程序寄存器(IP)、处理器状态寄存器(EFLAGS)、栈指针(ESP)；（3）系统级上下文: 进程控制块task_struct、内存管理信息(mm_struct、vm_area_struct、pgd、pte)、内核栈。 当发生进程调度时，进行进程切换就是上下文切换(context switch).操作系统必须对上面提到的全部信息进行切换，新调度的进程才能运行。 而对于中断上下文而言，硬件通过触发信号，导致内核调用中断处理程序，进入内核空间。这个过程中，硬件的 一些变量和参数也要传递给内核，内核通过这些参数进行中断处理。所谓的“ 中断上下文”，其实也可以看作就是硬件传递过来的这些参数和内核需要保存的一些其他环境（主要是当前被打断执行的进程环境）。中断时，内核不代表任何进程运行，它一般只访问系统空间，而不会访问进程空间，内核在中断上下文中执行时一般不会阻塞。 Linux 内核工作在进程上下文或者中断上下文。提供系统调用服务的内核代码代表发起系统调用的应用程序运行在进程上下文；另一方面，中断处理程序则代表硬件运行在中断上下文。中断上下文和特定进程无关。 运行在进程上下文的内核代码是可以被抢占的（Linux2.6）。但是一个中断上下文，通常都会始终占有CPU，不可以被打断。正因为如此，运行在中断上下文的代码就要受一些限制，不能做下面的事情：1、睡眠或者放弃CPU：这样做的后果是灾难性的，因为内核在进入中断之前会关闭进程调度，一旦睡眠或者放弃CPU，这时内核无法调度别的进程来执行，系统就会死掉。除此之外，在中断处理函数中调用一个内核API之前，应该仔细分析它以确保其内部不会触发阻塞等待。2、尝试获得信号量：如果获得不到信号量，代码就会睡眠，会产生和上面相同的情况3、执行耗时的任务：中断处理应该尽可能快，因为内核要响应大量服务和请求，中断上下文占用CPU时间太长会严重影响系统功能。4、访问用户空间的虚拟地址：因为中断上下文是和特定进程无关的，它是内核代表硬件运行在内核空间，所以在终端上下文无法访问用户空间的虚拟地址 中断处理的实现中断处理系统在linux中的实现是非常依赖于体系结构的，例如要依赖于处理器，所使用的的中断控制器的类型，体系结构的设计及机器本身。 下图是中断从硬件到内核进行处理的一个流程 下半部下半部的任务主要是执行与中断相关的工作，这些工作没有被中断服务程序本身完成.下半部分并不需要指明一个确切时间，只要把这些任务推迟一点，让它们在系统不太忙并且中断恢复后执行就可以了。通常下半部分在中断处理程序一返回就会马上运行。内核中实现下半部的手段不断演化，目前已经从最原始的BH（bottom half）衍生出BH（在2.5中去除）、软中断（softirqs在2.3引入）、tasklet（在2.3引入）、工作队列（work queue在2.5引入）。 softirqs机制软中断结构定义如下：1234struct softirq_action &#123; void (*sction) (struct softirq_action *);/*待执行的函数*/ void *data; /*传给函数的指针*/ &#125; kernel/softirq.c定义了一个32个元素的结构数组, 每个被注册的软中断都占据该数组的一项，因此最多可能有32个软中断，且下标小的软中断优先级高。 中断处理程序在返回前触发它的软中断，使其在稍后执行。在执行过程中，一个软中断不会抢占另外一个软中断，唯一可以抢占软中断的是中断处理程序。执行就是遍历上面提到的结构数组，并处理那些有软中断的位置（值为1）。 软中断保留给对时间要求最严格和最重要的下半部使用，如网络和SCSI设备。使用软中断的过程大致为分配索引号-&gt;注册处理程序-&gt;触发软中断 tasklet机制tasklet 是基于软中断实现的，与软中断相比，tasklet更常用，软中断一般用于那些执行频率很高和连续型要求很高的情况。 引入tasklet，最主要的是考虑支持SMP，提高SMP多个cpu的利用率；不同的tasklet可以在不同的cpu上运行。tasklet可以理解为softirq的派生，所以它的调度时机和软中断一样。 每个处理器都有一个用于辅助处理软中断的内核线程:ksoftirqd/n,当内核中出现大量软中断的时候，这些内核线程就会辅助处理他们。这个内核线程是对于重新触发的软中断是否立即处理的问题的一个折中，最终是不会立即处理这些重新触发的软中断，而是添加这样一个线程使得在软中断数目过多时也能够被迅速处理。 work queue 机制工作队列可以把工作推后，然后交给一个内核线程去执行，这些内核线程被称为工作线程。工作队列一个很重要的特性就是允许工作重新调度和睡眠。 选择何种机制从设计上讲，Softirq提供最少的顺序保证，这需要Softirq处理函数采取一些额外的步骤保证数据安全，因为两个以上的同类型softirqs只能同时运行于不同的CPU。Softirq多用于时间要求严格和使用频度高的场合。 如果代码不能很好地线程化，tasklet意义较大Tasklets 有一个简单的接口，由于两个同类型的不能同时运行，他们非常易于实现。 如果你的延期的工作需要运行于进程上下文（重新调度和睡眠）, 唯一的选择是work queue 第九、十章 内核同步基本概念在现代操作系统里，同一时间可能有多个内核执行流在执行，因此内核也需要一些同步机制来同步各执行单元对共享数据的访问。尤其是在多处理器系统上，更需要一些同步机制来同步不同处理器上的执行单元对共享的数据的访问。 临界区和竞争条件临界区：访问和操作共享数据的代码段。竞争条件：多个执行线程处于同一个临界区中。 同步就是保证不安全的并发不发生，即竞争条件不发生。需要同步的情况有： 中断 Softirqs和tasklets 内核抢占 用户空间的睡眠和同步 SMP 死锁的产生条件1.互斥(mutual exclusion)：系统存在着临界资源；2.占有并等待(hold and wait)：已经得到某些资源的进程还可以申请其他新资源；3.不可剥夺(no preemption)：已经分配的资源在其宿主没有释放之前不允许被剥夺；4.循环等待(circular waiting)：系统中存在多个（大于2个）进程形成的封闭的进程链，链中的每个进程都在等待它的下一个进程所占有的资源； 死锁预防与死锁避免死锁预防防止死锁的发生只需破坏死锁产生的四个必要条件之一即可1) 破坏互斥条件 如果允许系统资源都能共享使用，则系统不会进入死锁状态。但有些资源根本不能同时访问，如打印机等临界资源只能互斥使用。所以，破坏互斥条件而预防死锁的方法不太可行，而且在有的场合应该保护这种互斥性。2) 破坏不剥夺条件 当一个已保持了某些不可剥夺资源的进程，请求新的资源而得不到满足时，它必须释放已经保持的所有资源，待以后需要时再重新申请。这意味着，一个进程已占有的资源会被暂时释放，或者说是被剥夺了，或从而破坏了不可剥夺条件。 该策略实现起来比较复杂，释放已获得的资源可能造成前一阶段工作的失效，反复地申请和释放资源会增加系统开销，降低系统吞吐量。这种方法常用于状态易于保存和恢复的资源，如CPU的寄存器及内存资源，一般不能用于打印机之类的资源。3) 破坏请求和保持条件 釆用预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不把它投入运行。一旦投入运行后，这些资源就一直归它所有，也不再提出其他资源请求，这样就可以保证系统不会发生死锁。 这种方式实现简单，但缺点也显而易见，系统资源被严重浪费，其中有些资源可能仅在运行初期或运行快结束时才使用，甚至根本不使用。而且还会导致“饥饿”现象，当由于个别资源长期被其他进程占用时，将致使等待该资源的进程迟迟不能开始运行。4) 破坏循环等待条件 为了破坏循环等待条件，可釆用顺序资源分配法。首先给系统中的资源编号，规定每个进程，必须按编号递增的顺序请求资源，同类资源一次申请完。也就是说，只要进程提出申请分配资源Ri，则该进程在以后的资源申请中，只能申请编号大于Ri的资源。 这种方法存在的问题是，编号必须相对稳定，这就限制了新类型设备的增加；尽管在为资源编号时已考虑到大多数作业实际使用这些资源的顺序，但也经常会发生作业使甩资源的顺序与系统规定顺序不同的情况，造成资源的浪费；此外，这种按规定次序申请资源的方法，也必然会给用户的编程带来麻烦。 死锁避免 死锁的预防是通过破坏产生条件来阻止死锁的产生，但这种方法破坏了系统的并行性和并发性。。 死锁产生的前三个条件是死锁产生的必要条件，也就是说要产生死锁必须具备的条件，而不是存在这3个条件就一定产生死锁，那么只要在逻辑上回避了第四个条件就可以避免死锁。 避免死锁采用的是允许前三个条件存在，但通过合理的资源分配算法来确保永远不会形成环形等待的封闭进程链，从而避免死锁。 其主要思想如下：1.如果一个进程的当前请求的资源会导致死锁，系统拒绝启动该进程；2.如果一个资源的分配会导致下一步的死锁，系统就拒绝本次的分配； 在这个思想下诞生出来的便是著名的银行家算法：把操作系统看做是银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。操作系统按照银行家制定的规则为进程分配资源，当进程首次申请资源时，要测试该进程对资源的最大需求量，如果系统现存的资源可以满足它的最大需求量则按当前的申请量分配资源，否则就推迟分配。当进程在执行中继续申请资源时，先测试该进程已占用的资源数与本次申请的资源数之和是否超过了该进程对资源的最大需求量。若超过则拒绝分配资源，若没有超过则再测试系统现存的资源能否满足该进程尚需的最大资源量，若能满足则按当前的申请量分配资源，否则也要推迟分配。 同步的机制原子操作 基本的操作 不可中断 不能分割的指令 原子操作的两组接口 原子整数操作：使用一种特殊的类型 atomic_t 原子位操作: 在位级别上进行操作 自旋锁原子位和原子整数仅能对简单的整形变量进行原子操作，对于复杂的数据复杂的操作并不适用。需要更复杂的同步方法实现保护机制——锁。 自旋锁：同一时刻只能被一个可执行线程持有，获得自旋锁时，如果已被别的线程持有则该线程进行循环等待锁重新可用然后继续向下执行。 使用自旋锁时要防止死锁： 自旋锁不可递归，自旋处于等待中造成死锁； 中断处理程序中，获取自旋锁前要先禁止本地中断，中断会打断正持有自旋锁的任务，中断处理程序有可能争用已经被持有的自旋锁，造成死锁。 读写自旋锁：将锁的用途直接分为读取和写入。 多个读者能同时持有读锁 没有读者时只能有一个写者能持有写锁 信号量信号量是睡眠锁。如果有一个任务试图获取信号量时，有一下两种情况：1）信号量未被占用：该任务获得成功信号量；2）信号量已被占用：信号量将任任务推进等待队列，让其睡眠，处理器继续工作；当信号量被释放后，唤醒信号量队列中的任务，并获取该信号量。 信号量适用于长时间的持有。持有信号量的进程可以睡眠，但是不能试图获自旋锁。 读写信号量，与读写自旋锁类似 自旋锁与信号量的对比自旋锁与信号量对比如下： 需求 建议使用锁 低开销加锁 优先使用自旋锁 短期锁定 优先使用自旋锁 长期锁定 优先使用信号量 中断上下文加锁 自旋锁 持有锁需要睡眠 使用信号量 完成变量信号量的简易方法。一个任务在等待完成变量，另一个进程在进行某种工作。当一个进程完成工作后，使用完成变量去唤醒在等待的任务，使两个任务得以同步。 BKL: 大内核锁大内核锁本质上是一个全局自旋锁，但是它又不同于自旋锁，自旋锁是不可以递归获得锁的，因为那样会导致死锁。但大内核锁可以递归获得锁。大内核锁用于保护整个内核，而自旋锁用于保护非常特定的某一共享资源。同时持有BLK是也可睡眠。 整个内核只有一个大内核锁，其实不难理解，内核只有一个，而大内核锁是保护整个内核的，当然有且只有一个就足够了。 大内核锁是历史遗留，内核中用的非常少，一般保持该锁的时间较长，因此不提倡使用它。 顺序锁内核2.6引入，类似于读者自旋锁，但是为写者赋予了较高的优先级，写者优先，读者正在读时允许写者写。 禁止抢占内核是可抢占的，被抢占的进程可能处于临界区。 解决方法：使用自旋锁作为非抢占区的标志，因为一个自旋锁被持有，内核便不能进行抢占。 顺序和屏障当处理器和硬件交互时，时常需要确保一个给定的读操作发生在其他读或写操作之前。在多处理器上，可能需要按写数据的顺序读数据。但是编译器和处理器为了提高效率，可能对读和写重新排序。但是，处理器提供了机器指令来确保顺序，指示编译器不要对给定点周围的指令序列进行重新排序。这些确保顺序的指令称为屏障(barrier)。 rmb()方法提供了一个“读”内存屏障，也就是说，在rmb()之前的读操作不会被重新排在该调用之后，同理，在rmb()之后的读操作不会被重新排在该调用之前。wmb()方法提供了一个“写”内存屏障，这个函数的功能和rmb()类似，区别仅仅是它是针对写而非读。 总结 保证同步的最简单的方法, 原子操作 自旋锁, 最常用的方式，轻量级，只能有一个保持者，忙等 信号量, 睡眠锁，适用稍少 第十二章 内存管理连续分配连续分配是指为一个用户程序分配连续的内存空间。连续分配有单一连续存储管理和分区式储管理两种方式。 单一连续存储管理在这种管理方式中，内存被分为两个区域：系统区和用户区。应用程序装入到用户区，可使用用户区全部空间。其特点是，最简单，适用于单用户、单任务的操作系统。CP／M和 DOS 2．0以下就是采用此种方式。这种方式的最大优点就是易于管理。但也存在着一些问题和不足之处，例如对要求内存空间少的程序，造成内存浪费；程序全部装入，使得很少使用的程序部分也占用—定数量的内存。伙伴系统 分区式存储管理为了支持多道程序系统和分时系统，支持多个程序并发执行，引入了分区式存储管理。分区式存储管理是把内存分为一些大小相等或不等的分区，操作系统占用其中一个分区，其余的分区由应用程序使用，每个应用程序占用一个或几个分区。分区式存储管理虽然可以支持并发，但难以进行内存分区的共享。 分区式存储管理引人了两个新的问题：内碎片和外碎片。内碎片是占用分区内未被利用的空间，外碎片是占用分区之间难以利用的空闲分区(通常是小空闲分区)。 为实现分区式存储管理，操作系统应维护的数据结构为分区表或分区链表。表中各表项一般包括每个分区的起始地址、大小及状态(是否已分配)。分配方式有固定分区和动态分区。 固定分区(nxedpartitioning) 固定式分区的特点是把内存划分为若干个固定大小的连续分区。分区大小可以相等：这种作法只适合于多个相同程序的并发执行(处理多个类型相同的对象)。分区大小也可以不等：有多个小分区、适量的中等分区以及少量的大分区。根据程序的大小，分配当前空闲的、适当大小的分区。 动态分区(dynamic partitioning) 动态分区的特点是动态创建分区：在装入程序时按其初始要求分配，或在其执行过程中通过系统调用进行分配或改变分区大小。 与固定分区相比较其优点是：没有内碎片。但它却引入了另一种碎片——外碎片。动态分区的分区分配就是寻找某个空闲分区，其大小需大于或等于程序的要求。若是大于要求，则将该分区分割成两个分区，其中一个分区为要求的大小并标记为“占用”，而另一个分区为余下部分并标记为“空闲”。分区分配的先后次序通常是从内存低端到高端。动态分区的分区释放过程中有一个要注意的问题是，将相邻的空闲分区合并成一个大的空闲分区。 伙伴系统固定分区和动态分区方式都有不足之处。固定分区方式限制了活动进程的数目，当进程大小与空闲分区大小不匹配时，内存空间利用率很低。动态分区方式算法复杂，回收空闲分区时需要进行分区合并等，系统开销较大。伙伴系统方式是对以上两种内存方式的一种折衷方案。 伙伴系统规定，无论已分配分区或空闲分区，其大小均为 2 的 k 次幂，k 为整数， l≤k≤m，其中： 2^1 表示分配的最小分区的大小，2^m 表示分配的最大分区的大小， 假设系统的可利用空间容量为2^m个字， 则系统开始运行时， 整个内存区是一个大小为2^m的空闲分区。在系统运行过中， 由于不断的划分，可能会形成若干个不连续的空闲分区，将这些空闲分区根据分区的大小进行分类，对于每一类具有相同大小的所有空闲分区，单独设立一个空闲分区双向链表。这样，不同大小的空闲分区形成了k(0≤k≤m)个空闲分区链表。 当需要为进程分配一个长度为n 的存储空间时: 首先计算一个i 值，使 2^(i－1) &lt; n ≤ 2^i，然后在空闲分区大小为2^i的空闲分区链表中查找。若找到，即把该空闲分区分配给进程。否则，表明长度为2^i的空闲分区已经耗尽，则在分区大小为2^(i＋1)的空闲分区链表中寻找。若存在2^(i＋1)的一个空闲分区，则把该空闲分区分为相等的两个分区，这两个分区称为一对伙伴，其中的一个分区用于配，而把另一个加入分区大小为2^i的空闲分区链表中。若大小为2^(i＋1)的空闲分区也不存在，则需要查找大小为2^(i＋2)的空闲分区， 若找到则对其进行两次分割，以此类推。 在最坏的情况下，可能需要对 2^k的空闲分区进行 k 次分割才能得到所需分区。合并空闲内存的过程与分割的过程类似。 离散分配前面的几种存储管理方法中，为进程分配的空间是连续的，使用的地址都是物理地址。如果允许将一个进程分散到许多不连续的空间，就可以避免内存紧缩(将各个占用分区向内存一端移动，然后将各个空闲分区合并成为一个空闲分区)，减少碎片。基于这一思想，通过引入进程的逻辑地址，把进程地址空间与实际存储空间分离，增加存储管理的灵活性。地址空间和存储空间两个基本概念的定义如下： 地址空间：将源程序经过编译后得到的目标程序，存在于它所限定的地址范围内，这个范围称为地址空间。地址空间是逻辑地址的集合。 存储空间：指主存中一系列存储信息的物理单元的集合，这些单元的编号称为物理地址存储空间是物理地址的集合。 根据分配时所采用的基本单位不同，可将离散分配的管理方式分为以下三种：页式存储管理、段式存储管理和段页式存储管理。其中段页式存储管理是前两种结合的产物。 页式管理将程序的逻辑地址空间划分为固定大小的页(page)，而物理内存划分为同样大小的页框(page frame)。程序加载时，可将任意一页放人内存中任意一个页框，这些页框不必连续，从而实现了离散分配。 该方法需要CPU的硬件支持，来实现逻辑地址和物理地址之间的映射。在页式存储管理方式中地址结构由两部构成，前一部分是页号，后一部分为页内地址w（位移量）. 页式管理方式的优点是： 1）没有外碎片，每个内碎片不超过页的大小2）一个程序不必连续存放。3）便于改变程序占用空间的大小(主要指随着程序运行，动态生成的数据增多，所要求的地址空间相应增长)。 缺点是：要求程序全部装入内存，没有足够的内存，程序就不能执行。 每个进程有一个页表，用于完成逻辑页号(本进程的地址空间)到物理页面号(实际内存空间，也叫块号)的映射，如下图所示 段式管理在段式存储管理中，将程序的地址空间划分为若干个段(segment)，为每个段分配一个连续的分区，而进程中的各个段可以不连续地存放在内存的不同分区中。程序加载时，操作系统为所有段分配其所需内存，这些段不必连续，物理内存的管理采用动态分区的管理方法。 段式存储管理也需要硬件支持，实现逻辑地址到物理地址的映射。 程序通过分段划分为多个模块，如代码段、数据段、共享段，这样做的优点是：可以分别编写和编译源程序的一个文件，并且可以针对不同类型的段采取不同的保护，也可以按段为单位来进行共享。 段式存储管理的优点是：没有内碎片，外碎片可以通过内存紧缩来消除；便于实现内存共享。缺点与页式存储管理的缺点相同，进程必须全部装入内存。 类似页式管理的进程页表，段式管理中每个进程也有一张进程段表。 页式管理 vs 段式管理页式和段式系统有许多相似之处。比如，两者都采用离散分配方式，且都通过地址映射机构来实现地址变换。但概念上两者也有很多区别，主要表现在： 1)、需求：是信息的物理单位，分页是为了实现离散分配方式，以减少内存的碎片，提高内存的利用率。或者说，分页仅仅是由于系统管理的需要，而不是用户的需要。段是信息的逻辑单位，它含有一组其意义相对完整的信息。分段的目的是为了更好地满足用户的需要。因为一条指令或一个操作数可能会跨越两个页的分界处，而不会跨越两个段的分界处。 2)、大小：页大小固定且由系统决定，把逻辑地址划分为页号和页内地址两部分，是由机器硬件实现的。段的长度不固定，且决定于用户所编写的程序，通常由编译系统在对源程序进行编译时根据信息的性质来划分。 3)、逻辑地址表示：页式系统地址空间是一维的，即单一的线性地址空间，程序员只需利用一个标识符，即可表示一个地址。分段的作业地址空间是二维的，程序员在标识一个地址时，既需给出段名，又需给出段内地址。 4)、段比页大，因而段表比页表短，可以缩短查找时间，提高访问速度。 linux 中的内存管理页和区linux 采用上面提到的页式管理方法。MMU以页为单位管理内存。对于32位，每个页的大小为 4K；而对于64位而言每个页的大小为8K。内核用struct page结构体表示每个物理页,它们组织在mem_map数组中 内核把页划分在不同的区(zone),总共3个区，具体如下： 区 描述 物理内存（MB） ZONE_DMA DMA使用的页 &lt;16 ZONE_NORMAL 可正常寻址的页 16 ~896 ZONE_HIGHMEM 动态映射的页 >896 执行DMA操作的内存必须从ZONE_DMA区分配一般内存，既可从ZONE_DMA，也可从ZONE_NORMAL分配，但不能同时从两个区分配 内存分配和释放页的分配与释放：页的分配通过 alloc_pages 函数实现，而释放则通过__free_pages 实现 字节的分配与释放：字节的分配可通过kmalloc，vmalloc实现 1）kmalloc1void * kmalloc(size_t size, gfp_t flags) 该函数返回的是一个指向内存块的指针，其内存块大小至少为size,所分配的内存在物理内存中连续且保持原有的数据(不清零)，释放时通过kfree释放 其中部分flags取值说明： GFP_USER： 用于用户空间的分配内存，可能休眠；GFP_KERNEL：用于内核空间的内存分配，可能休眠；GFP_ATOMIC：用于原子性的内存分配，不会休眠；典型原子性场景有中断处理程序，软中断，tasklet等 2）vmalloc1void * vmalloc(unsigned long size) 该函数返回的是一个指向内存块的指针，其内存块大小至少为size,所分配的内存是逻辑上连续的，物理上可能连续也可能不连续。 与kmalloc不同，该函数没有flags,默认是可以休眠的。 Slab分配器 slab分配器是一种策略，用于缓存内核对象，其主要工作是针对一些经常分配并释放的对象，如进程描述符等，这些对象的大小一般比较小，如果直接采用伙伴系统来进行分配和释放，不仅会造成大量的内碎片，而且处理速度也太慢。而slab分配器是基于对象进行管理的，相同类型的对象归为一类(如进程描述符就是一类)，每当要申请这样一个对象，slab分配器就从存储某一类对象的高速缓存组中分配一个这样大小的单元出去，而当要释放时，将其重新保存在该列表中，而不是直接返回给伙伴系统。slab分配对象时，会使用最近释放的对象内存块，因此其驻留在CPU高速缓存的概率较高。 其中高速缓存(cache),slab 和 对象的关系如下：123Cache: 存储某种类型的对象.Slab: 包含有缓存的对象（由cache划分出来）Object: 经常使用的数据结构，例如inode slab有三种状态Full：没有自由的对象Partial：部分自由，先从这里分配(减少了页的分配和释放)，再找empty的，如果两者都找不到，创建一个新的slabEmpty：含有未分配的对象 选择哪种方法分配 频繁分配和释放 Slab分配器. 需要以页为单位分配内存 alloc_pages() 从高端内存分配 alloc_pages() 默认 kmalloc() 不需要连续的页 vmalloc() 第十五章 进程地址空间基本概念进程地址空间指进程能够使用的地址范围，每一个进程看到的是一个不同的线性地址，一个进程使用的地址与另一个进程使用的地址无关。内核会通过增加或删除线性地址中的区域，动态修改进程地址空间。 进程地址空间由进程可寻址的虚拟内存组成，linux采取的虚拟内存技术使得所有进程以虚拟方式共享内存。对于某个进程，它好像可以访问所以物理内存，而且它的地址空间可以远远大于物理内存. 进程只能访问有效区域中的内存地址。如果进程访问的内存地址不再有效内存区域，或者以非法的方式访问有效区域，内核会杀掉这个进程并输出一个“Segmentation Fault” 信息 内存描述符内核用一个称之为内存描述符的数据结构（mm_struct）表示一个进程的地址空间。mm_struct部分组元素如下： mmap和mm_rb字段是不同的数据结构，但含有同样的东西，即地址空间中的所有内存区域 mm_users 字段表示使用这个地址空间的进程数目 mmlist 链表将所有mm_struct连接在一起 内存描述符的分配与释放 copy_mm() 函数用于在 fork()时复制父进程的内存描述符（使用vfork()的时候进程与子进程共享地址空间），当与指定的地址空间相关联的进程结束时，会调用exit_mm() 函数 进程的内存区域进程的地址空间可划分为不同的区域，应用在不同的场景，如下就是常见的几种内存区域： 1）文本段（text section），存放可执行文件的操作指令，也就是可执行文件的代码的内存映像，包含一些字符串、常量和只读数据2）数据段（data section），数据段用来存放可执行文件中已初始化全局变量，也就是存放程序静态变量和全局变量3）bss section，未初始化全局变量的内存映像4）堆（heap），堆是用于存放进程运行中被动态分配的内存区域，它的大小并不固定，可动态扩张或缩减。当进程调用malloc等函数分配内存时，新分配的内存就被动态添加到堆上（堆被扩张）；当利用free等函数释放内存时，被释放的内存从堆中被剔除（堆被缩减）5）栈（stack），栈是用户存放程序临时创建的局部变量，除此以外，在函数被调用时，其参数也会被压入发起调用的进程栈中 每个内存区域均由以下参数刻画1）起始地址2）长度3）访问权利 内存区域由内存区域对象表示，存于vm_area_struct 结构，也叫VMA，描述给定的地址空间上的一个独立的内存区域。VMA 结构能够表示上面提到的多种类型的内存区域。 task_struct，mm_struct, vm_area_struct的关系如下所示 VMA中还有VMA标志，用于指定内存区域中页的行为或信息，各标志及其含义如下VM_READ：页可读VM_WRITE：页可写VM_EXEC：页可执行对于代码: 可被映射为VM_READ和VM_EXEC，但不能映射为VM_WRITE对于数据：可被映射为VM_READ和VM_WRITE，但不能映射为VM_EXEC Linux的分页下面先以32位的系统为例讲述原始的分页 每个地址可以通过一个32位的整数描述，其中整数的32为分配如下 页目录包含1024（2^10）个页表 页表描述1024（2^10）个页 每页大小 4 KB（2^12） (PAGE_SIZE) 1024 1204 4KB = 4GB CR3 (在task_struct的TSS)含有页目录的物理基地址 寻址时利用线性地址的低22~31位从页目录找到对应的页表，然后利用线性地址的低12~21为从页表找到对应的页，最后用低12位从页中找到最终地址。过程如下所示： 上面是原始的两级分页，但是Linux使用3级分页，顶层页表是page global directory (PGD)，二级页表是page middle directory (PMD)，最后一级是 PTE，整体如下所示 与task_struct 等关系如下图所示 第十三章 文件系统ext2 文件系统ext2于1993 年创建，是ext的改进版本，具有如下的特点： 支持UNIX所有标准的文件特征 可管理大硬盘，一个分区的容量最大可达4TB 它使用变长的目录项 ，支持255个字符的长文件名，可扩展到1012个字符 使用位图来管理数据块和节点的使用情况 使用了块组的概念，从而使数据的读和写更快，更有效，也使系统变得更安全可靠 文件系统结构整个ext2文件系统结构如下所示： 文件系统中存储的最小单位是块（ Block），一个块究竟多大是在格式化时确定的，例如 mke2fs的 -b选项可以设定块大小为 1024、 2048或 4096字节。而上图中引导块（Boot Block）的大小是确定的，就是 1KB，引导块是由 PC标准规定的，用来存储磁盘分区信息和启动信息，任何文件系统都不能使用启动块。 启动块之后才是 ext2文件系统的开始，ext2将磁盘分区看成是由块组组成，每个块组包含一个或多个块。每个块组大小相同，顺序存放，且每个块组都由以下部分组成。 1）超级块(Super Block)：描述整个分区的文件系统信息，例如块大小、文件系统版本号、上次 mount的时间等等。超级块在每个块组的开头都有一份拷贝。2）组描述符(Group Descriptor Table)：由很多块组描述符组成，整个分区分成多少个块组就对应有多少个块组描述符。每个块组描述符存储一个块组的描述信息，例如在这个块组中从哪里开始是 inode 表，从哪里开始是数据块，空闲的 inode 和数据块还有多少个等等。和超级块类似，块组描述符表在每个块组的开头也都有一份拷贝，这些信息是非常重要的，一旦超级块意外损坏就会丢失整个分区的数据，一旦块组描述符意外损坏就会丢失整个块组的数据，因此它们都有多份拷贝。 3）块位图(Block Bitmap)。块位图就是用来描述整个块组中哪些块是空闲可用的，它本身占一个块，其中的每个 bit代表本块组中的一个块，这个 bit为 1表示该块已用，这个 bit为 0表示该块空闲可用。 与此相联系的另一个问题是：在格式化一个分区时究竟会划出多少个块组呢？主要的限制在于块位图本身必须只占一个块。用 mke2fs格式化时默认块大小是 1024字节，可以用 -b参数指定块大小，现在设块大小指定为 b字节，那么一个块可以有 8b个 bit，这样大小的一个块位图就可以表示 8b个块的占用情况，因此一个块组最多可以有 8b个块，如果整个分区有 s个块，那么就可以有 s/(8b)个块组。格式化时可以用 -g参数指定一个块组有多少个块，但是通常不需要手动指定， mke2fs工具会计算出最优的数值。 4）索引节点位图(inode Bitmap)。和块位图类似，本身占一个块，其中每个 bit表示一个 inode是否空闲可用。 5）索引接点表(inode table)。一个文件除了数据需要存储之外，一些描述信息也需要存储，例如文件类型（常规、目录、符号链接等），权限，文件大小，创建/ 修改/访问时间等，也就是 ls -l命令看到的那些信息，这些信息存在 inode中而不是数据块中。每个文件都有一个 inode，一个块组中的所有 inode组成了 inode表。 inode表占多少个块在格式化时就要决定并写入块组描述符中， mke2fs格式化工具的默认策略是一个块组有多少个 8KB 就分配多少个 inode。由于数据块占了整个块组的绝大部分，也可以近似认为数据块有多少个 8KB就分配多少个 inode，换句话说，如果平均每个文件的大小是 8KB，当分区存满的时候 inode表会得到比较充分的利用，数据块也不浪费。如果这个分区存的都是很大的文件（比如电影），则数据块用完的时候 inode会有一些浪费，如果这个分区存的都是很小的文件（比如源代码），则有可能数据块还没用完 inode就已经用完了，数据块可能有很大的浪费。如果用户在格式化时能够对这个分区以后要存储的文件大小做一个预测，也可以用 mke2fs的 -i 参数手动指定每多少个字节分配一个 inode。 6）数据块(Data Block)。数据块用来存储文件的具体内容，在linux中文件类型有以下几种 普通文件 目录 符号连接 字符设备特殊文件 块设备特殊文件 命名管道 Socket 于普通文件，文件的数据存储在数据块中。 对于目录，该目录下的所有文件名及其inode存储在数据块中，除文件名之外， ls -l命令看到的其它信息都保存在该文件的inode中。注意这个概念：目录也是一种文件，是一种特殊类型的文件。 对于符号链接，如果目标路径名较短则直接保存在 inode中以便更快地查找，如果目标路径名较长则分配一个数据块来保存。 设备文件、FIFO和socket 等特殊文件没有数据块，即文件大小为0，设备文件的主设备号和次设备号保存在 inode中。 文件查找文件查找的流程：1）从当前进程的根目录项中（current→ fs → root）找到它的根目录的inode编号2）根据该编号和超级块的s_inodes_per_group，计算出该inode所在的块组3）查找该块组中的inode表，找到描述根目录文件的inode4）根据该inode的描述，读取其数据块（如果是文件）或得到目录项列表（如果是目录，然后返回步骤（2）直到读到最终的文件） ext2的内存数据结构为提高效率，尽量减少访问磁盘次数，在安装Ext2分区时，内存中存放着部分磁盘数据结构，并使用缓存技术保持磁盘数据结构更新。 缓存模式共有4种 Always cached：一直缓存 Fixed limit：固定数量的数据结构保存在缓存中 Dynamic：只要索引节点或块使用，相关数据就保存在缓存中 Never：任何高速缓存中都不保存 ext2 中的数据结构及其缓存情况如下所示 文件系统的创建创建Ext2文件系统实际上就是建立Ext2文件系统的磁盘数据结构与内存数据结构，在linux中通过mke2fs程序实现，这个程序实际上执行了以下两个步骤：1）格式化2）创建文件系统 ext3与ext4ext2 文件系统的下一个版本是 ext3 文件系统，它和 ext2 文件系统在硬盘布局上是一样的，其差别仅仅是 ext3 文件系统在硬盘上多出了一个特殊的 inode（可以理解为一个特殊文件），用来记录文件系统的日志，也即所谓的 journal。Ext4 支持更大的文件和无限量的子目录。 虚拟文件系统（VFS）基本概念为支持各种文件系统，Linux内核在用户进程（或C标准库）和具体的文件系统之间引入了一个抽象层，使得文件、目录、读写访问等概念成为抽象层的概念，因此各种文件系统看起来用起来都一样，该抽象层称之为“虚拟文件系统（VFS）”。类似于类和对象的关系，其中VFS是类，各种文件系统是具体的对象。 VFS一方面提供一种操作文件、目录及其他对象的统一方法，使用户进程不必知道文件系统的细节。另一方面，VFS提供的各种方法必须和具体文件系统的实现达成一种妥协，毕竟对几十种文件系统类型进行统一管理并不是件容易的事。为此，VFS中定义了一个通用文件模型，以支持文件系统中对象（或文件）的统一视图。 Linux对Ext文件系统族的支持是最好的，因为VFS抽象层的组织与Ext文件系统类似，这样在处理Ext文件系统时可以提高性能，因为在Ext和VFS之间转换几乎不会损失时间。 task_struct 结构中 VFS 相关的字段 fs – 包括root, pwd,指向dentrie的指针 files – 文件描述符矩阵 fd[ ],每一个元素指向打开文件对象的指针 主要的数据结构1）超级块对于每个已经挂载的文件系统，VFS 在内核中都生成一个超级块结构（struct super_block实例），超级块代表一个已经安装的文件系统，用于存储文件系统的控制信息，例如文件系统类型、大小、所有inode对象、脏的inode链表等。 超级块相关的文件系统操作为读、写、清除和删除inode。 2）inodeVFS处理文件的关键是inode，每个文件或目录都有且只有一个对应的inode（struct inode实例），其中包含元数据(权限，拥有者，时间信息，大小，连接计数)和指向文件数据的指针，但inode并不包含文件名。系统中所有的inode都有一个特定的编号，用于唯一的标识各个inode。文件名可以随时更改，但是索引节点对文件是唯一的，并且随文件的存在而存在。 inode和super block在存储介质中都是有实际映射的，即存储介质中也存在超级块和inode。但是由于不同类型的文件系统差异，超级块和inode的结构不尽相同。而VFS的作用就是通过具体的设备驱动获得某个文件系统中的超级块和inode节点，然后将其中的信息填充到内核中的struct super_block和struct inode中，以此来试图对不同文件系统进行统一管理。 inode的相关操作包括：create – 创建一个普通文件link/unlink/rename – 增加、删除、修改目录内容symlink, readlink, follow_link – 软连接操作mkdir/rmdir – 创建目录文件mknod – 创建设备文件truncate – 修改文件大小permission – 检查访问权限 3）dentryVFS把目录当做文件对待，为了方便路径查找，VFS引入了目录项的概念，每个目录项代表路径中的一个特定部分如(/bin/vi中包含了/,bin和vi三个目录项目对象)。目录项对象通过 dentry 结构体表示。dentry 结构的主要用途就是建立文件名和相关的 inode 之间的联系。 目录项有三种有效状态 used:表示该目录项对应一个有效的索引节点，且该对象正在被使用 unused: 表示该目录项对应一个有效的索引节点，但是该对象没有被使用 negative:表示该目录项没有对应的有效索引节点 由于块设备速度较慢（于内存而言），可能需要很长时间才能找到与一个文件名关联的inode。Linux使用目录项缓存来快速访问此前的查找操作结果。在VFS读取了一个目录或文件的数据之后，则创建一个 dentry 实例（struct dentry），以缓存找到的数据。Dentry缓存通过哈希表访问，因此时间较快。 4）打开的文件对象文件对象主要用于关联文件和进程，在打开文件的时候创建，主要描述一个打开文件的相关信息，包括当前的读写指针等，通过struct file结构体表示。 在task_struct中有一个数组，数组中的每一个元素都是指向一个打开的文件对象，这个数组就称为文件描述符数组。 上面提到的数据结构之间的关系如下所示 共享数据结构的情况： 在不同的目录上挂载同一个文件系统 ：共享超级块 通过不同的硬连接打开同一个文件：共享 inodes 打开同一个文件两次：共享 dentries 调用 dup()：共享打开文件对象，例如: 2&gt;&amp;1 参考：task_struct结构体字段介绍–Linux中的PCBLinux 进程状态说明Linux 线程实现机制分析Linux进程管理——fork()和写时复制linux中断处理的上半部和下半部死锁的产生、预防和避免linux内存管理总结之进程地址空间Ext2文件系统布局，文件数据块寻址，VFS虚拟文件系统Linux 虚拟文件系统（VFS）介绍]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Reducing the Dimensionality of Data with Neural Networks》阅读笔记]]></title>
      <url>%2F2016%2F11%2F28%2F%E3%80%8AReducing%20the%20Dimensionality%20of%20Data%20with%20Neural%20Networks%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[Reducing the Dimensionality of Data with Neural Networks是对深度学习有重要影响的一篇论文，可以说是拉开了深度学习的帷幕，该论文出自 Hinton 大神之手。本文是读了论文后结合其他一些参考资料整理成的读书笔记。 简介该论文主要讲述了通过神经网络对数据进行降维，并通过多项实验结果证明通过神经网络进行降维的效果要优于传统的降维方法 PCA(Principal component analysis, 主成成分分析)。但是要达到这种效果，需要有一个前提，那就是神经网络中的参数在初始化的时候不能随机初始化，而是要有一个预训练的过程，论文中通过 RBM(Restricted Boltzmann machine，受限玻尔兹曼机)来实现这个预训练的过程，利用 RBM 对神经网络中的参数进行逐层预训练，然后将训练出来的参数作为神经网络的初始化参数。 数据的降维在机器学习中，原始数据往往会存在着各种各样的问题，样本的特征数目过多是其中之一，当样本的特征过多的时候往往会存在冗余的信息和噪声；而当特征数目原大于样本数目的时候容易导致过拟合，使得模型的泛化能力弱；除此之外，特征数目过多的样本也需要更长的训练时间，训练的成本较高。 基于上述的原因，在训练模型之前往往需要对数据进行一个降维的操作，常见的降维方法有 PCA 等。降维直观的反映就是样本特征数目的减少，同时原始的信息（包括有用的信息和噪声）也会有损失；从另外一个角度来看降维就是提取原始数据的主要特征，而神经网络的结构特点恰恰为其进行特征提取提供了可能性，下面就讲述如何通过神经网络进行特征的提取，也就是论文的主要工作。 自编码器与逐层预训练自编码器(autoencoder)是论文提出的一种特殊的神经网络，由 Encoder 和 Decoder 两部分构成，其中 Encoder 的作用是降维，而 Decoder 的作用是从降维的后的特征中恢复出原始特征。其结构如下所示： 上图主要展示了通过自编码器对图像进行特征压缩并复原的过程。其中左边部分是初始训练时候的状态，Encoder 将原图像2000维的特征压缩到了30维， 而 Decoder 将压缩后得到的30维的图像恢复成原来的2000维，由于还没对网络进行训练，所以此时的图像会比较模糊。右边部分则是通过经典的 BP(Backpropagation, 反向传播) 算法对网络进行训练后的恢复效果，得到的效果与原图像已经非常接近了。 从上面的描述看来，自编码器的训练方法与传统的神经网络的训练没有差别。但是论文中指出了网络的初始化参数要足够好，才能利用这种训练方法得到比较好的效果。 网络的初始化参数就是上图中的 $W_1,W_2,W_3,W_4$, 我们知道，神经网络的主要是通过前向传播和反向传播这两个过程来训练网络中层与层之间的参数，通过这些网络间的参数来拟合数据的内在特性。但是在开始训练前，必须要给网络中的参数赋一个初始值，由于对数据没有任何的先验知识，这种初始化赋值往往是随机的，在多层网络中随机初始化参数存在着以下问题：当随机初始化的值过大时容易陷入局部最优，当随机初始化的值过小时训练会比较困难（在反向传播的时候梯度很快趋于0，错误信息传不到前面的层）。 而论文中所说的“网络的初始化参数足够好”其实是要通过逐层训练的方法先训练出一批参数值作为初始值赋给 $W_1,W_2,W_3,W_4$，然后再进行后面的前向传播和反向传播来训练整个网络。 下面主要讲述如何训练出网络的初始化参数，而这也是本文的最重要的工作。 逐层预训练过程在逐层预训练中采用的模型是 RBM, RBM 的结构图如下所示 从上面的的结构图可知， RBM 是一个二层全连接的双向网络，二层指的是隐藏层(h节点)和可视层(v节点)，其中各层节点的大小关系没有要求（也就是 m可以大于n也可以小于n），双向指数据既可从可视层传播到隐藏层，也可从隐藏层传输到可视层。 RBM包含的参数有 权重矩阵 $W_{nm}$ 隐藏层偏置量 $c = (c_1, c_2, c_3, … c_n)$ 可视层偏置量 $b = (b_1, b_2, b_3, … b_m)$ 其中偏置量的取值为0或1。 由于传播方向是双向的，这里先不加证明给出两个方向的传播的公式，具体的证明看下一节的原理与推导。 从可视层传到隐藏层$$P(h_i=1|v) = \sigma(\sum_{j=1}^mw_{ij}v_j+c_i)\tag{3-1}$$从隐藏层传到可视层$$P(v_j=1|h) = \sigma(\sum_{i=1}^nw_{ij}h_i+b_j)\tag{3-2}$$其中$$ \sigma(x) = 1/(1+e^{-x})\tag{3-3}$$ 由上面的传播公式可知，两个传播过程计算出来的都是一个概率值，就是传播的目标点取1的概率，实际中赋值时按照均匀分布产生一个0到1之间的随机浮点数，如果它小于$P(v_j=1|h)，v_j$ 的取值就是1，否则就是 0。 有了上面关于 RBM 的基础知识，下面就是逐层预训练的具体过程 正向过程：样本 $v$ 通过公式 (3-1) 从可视层输入得到 $h$ 反向过程：隐藏层 $h$ 通过公式 (3-2) 回传到可视层得到 $v’$, 利用 $v’$ 再进行一次正向传播得到隐藏层的 $h’$ 权重更新过程：更新公式为(其中 $\alpha$ 为学习率)$$W(t+1) = W(t) + \alpha(vh^T-v’h’^T)\tag{3-4}$$ 迭代上面过程直至权重 $W$ 收敛 上面的公式中的 $v,h,v’,h’$ 均为向量，且公式 (3-4) 在原文表述为 $\Delta W_{ij} = \varepsilon ( (v_jh_j)_{data} - (v_jh_j)_{recon})$，但是含义是一致的，就是利用被压缩后再恢复的数据与原始数据的误差来调整二层网络间的参数，使得恢复出来的数据尽可能与原始数据接近，也就是要让被压缩后的数据尽可能的保留着原始数据的特征。 上面的训练过程中只是训练了相邻两层网络间的参数，而神经网络一般是有多层的，所以需要利用这种方法逐层进行训练，这也是逐层预训练说法的来由。所以上面通过自编码器对图像进行特征压缩并复原的完整过程如下图所示： 首先利用 RBM 逐层训练出网络的初始化参数，后面就是传统神经网络的训练过程了，通过前向传播和反向传播来调整网络间的参数，从而达到收敛。 原理与推导上面主要讲述了参数逐层预训练的具体过程，下面主要讲述这种方法的思想以及推导对上面不加证明给出的传播过程的公式。 在自编码器预训练的过程中， RBM 的主要作用是在隐藏层尽可能保留从可视层输入的数据的主要特征（因为特征维度的压缩会导致数据的损失），而度量其保留程度的指标就是利用压缩了的特征恢复出来的图像与原图像的概率分布的差别，差别越小，保留的特征就越好。利用这个差别，可以调整 RBM 中的参数，从而使得误差逐步减小。 因此，上面的正向过程( $v \rightarrow h$ )是一个特征压缩过程，影响了真实数据的特征，而反向过程( $h \rightarrow v’, v’\ \rightarrow h’$ )就是利用压缩后的特征($h$)重现真实数据的特征（$v’$）,权重更新过程则是利用他们的误差来更新权重矩阵，误差在这里表示为 $(vh^T-v’h’^T)$。 上面的正向过程和反向过程中的两个关键公式 (3-1) 和 (3-2) 我们是不加证明的使用的，下面对其进行简单推导，推导的思路是从 RBM 的能量函数推导出概率模型，再从概率模型推导极大似然估计。 首先， RBM 诞生于统计力学，统计力学中为其定义的一个能量函数,针对下图的RBM 其能量函数表示如下：$$E(v,h) = -\sum_{i=1}^n\sum_{j=1}^m w_{ij}h_iv_j-\sum_{j=1}^mb_jv_j-\sum_{i=1}^nc_ih_i \tag{3-5}$$ 定义出这个能量函数又有什么作用呢？根据参考资料，原因有以下几个 第一、RBM网络是一种无监督学习的方法，无监督学习的目的是最大可能的拟合输入数据，所以学习RBM网络的目的是让RBM网络最大可能地拟合输入数据。 第二、对于一组输入数据来说，现在还不知道它符合那个分布，那是非常难学的。例如，知道它符合高斯分布，那就可以写出似然函数，然后求解，就能求出这个是一个什么样个高斯分布；但是要是不知道它符合一个什么分布，那可是连似然函数都没法写的，问题都没有，根本就无从下手。 第三，统计力学的结论表明，任何概率分布都可以转变成基于能量的模型，而且很多的分布都可以利用能量模型的特有的性质和学习过程，有些甚至从能量模型中找到了通用的学习方法。换句话说，就是使用能量模型使得学习一个数据的分布变得容易可行了。 因此，基于上面的能量函数，可以定义出一个可视节点与隐藏节点间的联合概率$$P(v,h) = \frac{e^{-E(v,h)}}{\sum_{v,h}e^{-E(v,h)}} \tag{3-6}$$ 该公式也是根据统计热力学给出的，具体参看参考文献。有了联合概率，就可以求出其条件概率如下所示： $$P(v) = \frac{\sum_he^{-E(v,h)}}{\sum_{v,h}e^{-E(v,h)}} \tag{3-7}$$$$P(h) = \frac{\sum_ve^{-E(v,h)}}{\sum_{v,h}e^{-E(v,h)}} \tag{3-8}$$$$P(v|h) = \frac{e^{-E(v,h)}}{\sum_ve^{-E(v,h)}} \tag{3-9}$$$$P(h|v) = \frac{e^{-E(v,h)}}{\sum_he^{-E(v,h)}} \tag{3-10}$$ 上面的这些概率分布也叫Gibbs分布，这样就完成了从能量模型到概率模型的推导，下面是从概率模型推导出极大似然估计。 现在回到求解的目标: 让RBM网络的表示Gibbs分布最大可能的拟合输入数据的分布。那么这两个分布的KL散度如下所示,KL散度主要用于表示两个分布的一个相似度，其值越小，表示两个分布越相似：$$KL(q||p) = \sum_{x \epsilon \Omega} q(x) ln\frac{q(x)}{p(x)} =\sum_{x \epsilon \Omega} q(x)lnq(x)-\sum_{x \epsilon \Omega} q(x)lnp(x) \tag{3-11}$$ 上式中的 $q(x)$ 是输入样本的分布，样本确定的时候，该分布也确定了下来，而$p(x)$表示通过 RBM 后输出样本的分布，也就是公式 (3-8) 表示的隐藏层的分布。当输入样本确定的时候，要最小化公式 (3-9)的KL距离，实际上就是要最大化公式(3-9)中的 $lnp(x)$,而 $lnp(x)$ 与 RBM网络中的参数相关，实际上就是进行一个极大似然估计求出网络中的参数。具体的数学推导过程参见这里。通过求解便可以得到公式(3-1) 和 (3-2),也就完成了从概率模型到最大似然的推导。 实验效果对比论文通过若干的实验证明了通过自编码器对图像进行压缩后再恢复的效果要优于PCA，需要注意的是图像的压缩实际上就是特征的压缩，也就是一个特征提取或者说降维的过程，下面是具体的实验结果。 实验一：曲线图像的压缩与恢复下图是实验数据 上图中从上到下每一行对应于下表中从上到下的每一行 方法 特征数 均方误差 原图像 786 自编码器 6 1.44 PCA 6 7.64 Logistic PCA 18 2.45 PCA 18 5.90 实验二：手写数字图片的压缩、恢复与分类该实验采用 MNIST 数据集，下图是实验数据 上图中从上到下每一行对应于下表中从上到下的每一行 方法 特征数 均方误差 原图像 786 自编码器 30 3.00 Logistic PCA 30 8.01 PCA 30 13.87 上面是对图像进行压缩与恢复的实验，下面是提取每个手写数字两维的特征（原始维度为786维）进行分类的结果，图A是原始数据进行分类后的结果，图B是通过自编码器中的 Encoder 压缩到两维后再分类的结果。可以看到，通过自编码器得到的两维特征已经能够将各个数字较好分离开。 实验三：人脸图像的压缩与恢复实验数据如下所示 上图中从上到下每一行对应于下表中从上到下的每一行 方法 特征数 均方误差 原图像 625 自编码器 30 126 PCA 30 135 实验四：词向量的降维与分类上面的实验均是针对图像的，但是实际上通过自编码器中的 Encoder 对原始数据进行提取特征后，可利用这些特征进行分类和回归。这个实验就是对词向量进行降维后并进行分类，主要比较自编码器和LSA( Latent Semantic Analysis, 隐性语义分析)对词向量降维后分类的效果。 上图中 A 是文档相似性判断的准确率，通过LSA分类要提取文档向量的50维才能达到自编码器提取前10维进行分类的效果，图 B 是采用LSA对提取了2维的词向量(原始为2000维)进行分类的结果，可以看到完全无法分开，而图c是自编码器提取2维后的分类结果，可以看到分类结果要大大优于图B的效果。 总结这篇论文由深度学习的开山鼻祖Geoffrey E. Hinton 2006 年发表在 science 上，论文虽然只有短短的四页，但是做了两个非常重要的工作 (1) 多层的神经网络具有优秀的特征学习能力，能够学习到数据更本质的特征(2) 多层神经网络的初始化参数可通过逐层预训练获得 从上面的四个实验结果中可以看到自编码器提取的特征均要优于传统的PCA和LSA，也就是上面说的第(1)点；但是多层的神经网络很早就已经提出了，只是因为一直存在着初始化参数赋值的困难（过大陷入局部最优，过小梯度消失）而无法应用到实际中，本论文通过 RBM 逐层预训练得到多层神经网络的初始化参数，从而解决了这个问题，也就是上面说的第(2)点，也正是这个工作为多层神经网络或者说深度学习在实际中的应用拉开了帷幕。 参考文献：Reducing the Dimensionality of Data with Neural Networks深度学习读书笔记之RBM（限制波尔兹曼机）能量模型(EBM)、限制波尔兹曼机(RBM)深度学习方法：受限玻尔兹曼机RBM（一）基本概念深度学习方法：受限玻尔兹曼机RBM（二）网络模型]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[概率论与数理统计知识整理(5)--样本及抽样分布]]></title>
      <url>%2F2016%2F11%2F18%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(5)--%E6%A0%B7%E6%9C%AC%E5%8F%8A%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83%2F</url>
      <content type="text"><![CDATA[前面几部分主要总结了概率论中的一些知识，后面主要讲述与数理统计相关的知识。 概率论与数理统计的主要区别为，在概率论中所研究的随机变量，其分布都是假设已知的，在这一前提下去研究它的性质（数字特征，分布函数等）；而在数理统计中研究的随机变量其分布是未知的，通过对所研究的随机变量进行重复独立的试验和观察，得到许多观察值，再对观察值进行分析，从而对所研究的随机变量的分布做出各种推断。 因此数理统计的主要内容包括两部分，一是如何收集，整理数据资料，二是如何对得到的数据资料进行分析和研究，从而对所研究的对象的性质和特点做出推断。第二部分其实就是统计推断的问题，也是后面主要讲述的内容。本文主要讲述数理统计中的两个基本概念：样本和抽样分布。 样本从前面可知，数理统计就是通过数据来推断变量的分布，比如说现在要求求出全国成年男人的身高的一个分布，那只需要测出每个成年男人的身高后进行统计即可。 但是在实际中，受限于人力物力和测试的难度，我们往往不会对每个成年男人进行身高的测试，而是在全国男人中选择部分的男人进行测试(如根据每个地区的人口数量按比例测试)，然后用这部分男人的身高分布来推断全国男人的分布，这样的推断肯定会存在误差，但是通过增加样本的数量，可以减少这种误差(大数定理)。 上面其实就是一个很简单的数理统计过程，当中有几个概念需要注意，例子中的全国男人的身高是一个总体，选择出来实际测试身高的男人是一个样本，测试得到的身高称为样本值（观测值），总体和样本中的数目分别称为他们的容量。 其严格定义如下： 设 $X$ 是具有分布函数 $F$ 的随机变量, 若 $X_1, X_2, …,X_n$ 是具有同一分布函数 $F$ 的相互独立的随机变量，则称 $X_1, X_2,…X_n$ 为从分布函数 $F$ 得到的容量为n的简单随机样本，简称样本，他们的观测值 $x_1, x_2,…x_n$ 称为样本值，又称为 $X$ 的 $n$ 个独立的观测值。 由定义可知样本 $ X_1,X_2,…,X_n $ 相互独立，且他们的分布函数均为 $F$, 所以 $ (X_1,X_2,…,X_n) $的分布函数为 $$ F^*(x_1,x_2,…,x_n) = \prod_{i=1}^nF(x_i)$$ 同样,$ (X_1,X_2,…,X_n) $的概率密度函数为： $$ f^*(x_1,x_2,…,x_n) = \prod_{i=1}^nf(x_i) $$ 抽样分布统计量样本是进行统计推断的依据，但是在应用中，往往不是直接使用样本本身，而是针对不同问题构造适当的样本的函数，利用这些样本的函数进行统计推断。 当这些样本的函数中不含未知变量时，我们称其为统计量，如下面就是几个常用的统计量，其中 $ X_1,X_2,….,X_n $ 为总体的一个样本。 样本平均值： $$\overline X = \frac{1}{n} \sum_{i=1}^{n} X_i$$ 样本方差：$$S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline X)^2$$ 样本标准差：$$S = \sqrt {S^2}$$ 样本 k 阶原点矩： $$A_k = \frac{1}{n} \sum_{i=1}^{n} X_i^k (k=1,2,…)$$ 样本 k 阶中心矩: $$B_k = \frac{1}{n} \sum_{i=1}^{n}(X_i - \overline X)^k (k=2,3,4…..)$$ 这些统计量的定义与概率论中的基本相似，唯一比较奇怪的是为什么样本方差的分母是 $n - 1$ 而不是 n，原因是通过数学证明可以得到只有当分母取n-1时，用样本来估计总体才是无偏的(无偏指的是估计量的期望与总体的参数一致)，下面是分母取n时得到的有偏估计的证明过程（$S_1^2$为样本方差） 更多的解释可参考这里 除了上面的统计量还定义了与总体分布函数 $F(x)$ 相应的统计量–经验分布函数。其定义如下 设 $ X_1,X_2,….,X_n $ 为总体的一个样本，$ S(x)(-\infty &lt; x &lt; \infty) $表示为样本中不大于 $x$ 的随机变量的个数，则经验分布函数 $F_n(x)$ 的定义如下$$F_n(x) = \frac{1}{n}S(x) (-\infty &lt; x &lt; \infty)$$ 下面是关于经验分布函数的一个简单例子。 设总体 $F$ 具有一个样本值1,2,3，则经验分布函数 $F_3(x)$ 的观测值为 $$f(y) = \begin{cases}0 &amp;{x&lt;1} \\\frac{1}{3} &amp;{1\le x&lt;2}\\\frac{2}{3} &amp;{2\le x&lt;3}\\1 &amp;{x \ge 3}\end{cases}$$ 经验分布函数的意义在于当 n 充分大的时候，$F_n(x)$ 以概率1收敛于总体分布函数 $F(x)$ 统计量的分布使用统计量进行统计推断时，常常需要知道其分布，统计量的分布也称为抽样分布，下面介绍三种来自正态分布的抽样分布： $\chi^2$ 分布，$t$ 分布和 $F$ 分布。 $\chi^2$ 分布$\chi^2$ 分布的定义如下 设 $X_1, X_2,…X_n$ 是来自总体 $N(0,1)$ 的样本，则称统计量$$\chi^2 = X_1^2 + X_2^2 +….X_n^2$$为服从自由度为 $n$ 的 $\chi^2$ 分布 上面的自由度指的是右端独立变量的个数。 $\chi^2(n)$ 的概率密度函数为 $$f(y) = \begin{cases} \frac{1}{2^{\frac{n}{2}}\Gamma(n/2)}y^{n/2-1}e^{-y/2} &amp;{y&gt;0} \\ 0&amp;{其他}\end{cases}$$ 上式的 $\Gamma$ 函数定义为 $$\Gamma = \int_{0}^{\infty} \frac{t^z - 1}{e^t} dt$$ $f(y)$ 的图像如下所示 关于 $\chi^2(n)$ 有以下几个有用的结论： 可加性 设 $\chi_1^2$~$\chi^2(n_1), \chi_2^2$~$\chi^2(n_2)$, 并且 $\chi_1^2, \chi_2^2$相互独立，则有 $$\chi_1^2 + \chi_2^2 \sim \chi^2(n_1 + n_2)$$ 期望和方差 若$\chi^2$~$\chi^2(n)$，则$\chi^2$的期望和方差如下所示$$E(\chi^2) = n, D(\chi^2)=2n$$ 分位点 分位点的定义如下，给定正数 $a, 0&lt;a&lt;1$, 称满足下面条件 $$P(\chi^2 \gt \chi_a^2(n)) = \int_{\chi_a^2(n)}^{\infty}f(y)dy= a$$ 的 $\chi_a^2(n)$ 为 $\chi^2(n)$上的a分位点，其图像如下所示 由定义可知，分位点由 $a,n$ 共同决定，因此对于不同的 $a，n$ 可以查阅表格得到其 $a$ 分位点。 $t$ 分布$t$分布的定义如下： 设 $X \sim N(0,1), Y \sim \chi^2(n)$, 且 $X,Y$ 相互独立，则称随机变量 $$ t = \frac{X}{\sqrt{Y/n}}$$ 服从自由度为 $n$ 的 $t$ 分布, 记为 $t \sim t(n)$ 其概率密度函数和对应的图像如下所示： 其分位点的定义与上面讲述的一样， $$P(t \gt t_a(n)) = \int_{t_a(n)}^{\infty}h(t)dt= a$$ 且由于其概率密度函数的对称性可知,总是存在这样对称的两个分位点 ： $t_{1-a}(n) = -t_a(n)$ $F$ 分布F 分布的定义如下 设 $U \sim \chi^2(n_1), V \sim \chi^2(n_2)$， 且 $U,V$相互独立，则称随机变量$$F = \frac{U/n_1}{V/n_2}$$服从自由度为$(n_1,n_2)$的$F$分布，记为$F \sim F(n_1, n_2)$ 其概率密度函数为： $$\psi(y) = \begin{cases} \frac{\Gamma((n_1+n_2)/2)(n_1/n_2)^{n_1/2}y^{n_1/2-1}}{\Gamma(n_1/2)\Gamma(n_2/2)[1+(n_1y/n_2)]^{(n_1+n_2/)2}} &amp;{y&gt;0} \\ 0&amp;{其他}\end{cases}$$ 概率密度函数的图像如下所示 其分位点定义同上$$P(F \gt F_a(n_1,n_2)) = \int_{F_a(n_1,n_2)}^{\infty}\psi(y)dy= a$$ 且具有以下性质$$F_{1-a}(n_1,n_2) = \frac{1}{F_a(n_2,n_1)}$$ 上面只是简单地介绍了三大抽样分布，并未介绍其作用，实际上三大抽样分布主要用于参数的区间估计中，而这主要基于从正态分布中抽取的样本所构造的统计量服从这三大分布这一事实，从下面要介绍的定理中可以看到了这三大抽样分布的作用。更详细的作用会在区间估计中进一步体现。 正态总体的样本均值与样本方差的分布由于正态分布的普遍性，这里特意指出从服从正态分布的总体中抽取出的样本的所服从的分布。 假设上面的 $X$ 服从正态分布 $N(\mu, \sigma^2)$, 则有以下几条定理，这几条定理在数理统计的区间估计中起了重要作用。 定理一： 设$X_1, X_2,….X_n$服从$N(\mu, \sigma^2)$，$\overline X$是样本均值，则有 $$\overline X \sim N(\mu,\sigma^2/n)$$ 证明如下：$$E(\overline X) = E(\frac{1}{n}\sum_{i=1}^{n} X_i) = \frac{1}{n}E(\sum_{i=1}^{n} X_i) = \frac{1}{n}n E(X) = \mu$$ $$D(\overline X) = D(\frac{1}{n}\sum_{i=1}^{n} X_i) = \frac{1}{n^2}D(\sum_{i=1}^{n} X_i) = \frac{1}{n}n D(X) = \sigma^2/n$$ 定理一通常用于区间估计中已知总体（服从正态分布）的期望 $\mu$ 来估计其未知的方差 $\sigma^2$ ,或已知方差 $\sigma^2$ 来估计未知的期望 $\mu$。 定理二 ：设$X_1, X_2,….X_n$服从$N(\mu, \sigma^2)$，$\overline X$是样本均值，$S^2$ 是样本的方差，则 $\overline X$ 和 $S^2$ 相互独立，且有 $$\frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)$$ 由于该定理的证明部分较为冗长，这里略去证明过程，感兴趣的读者可参考相关书籍。定理二主要用于区间估计中总体（服从正态分布）的期望、方差均未知时，估计其方差的范围，这也是 $\chi^2$ 分布的作用之一。 定理三：设$X_1, X_2,….X_n$服从$N(\mu, \sigma^2)$，$\overline X$是样本均值，$S^2$ 是样本的方差，则$$\frac{\overline X - \mu}{S/\sqrt{n}} \sim t(n-1)$$ 证明：根据定理一，易知 $\overline X - \mu \sim N(0, \sigma^2/n)$, 则 $\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim N(0,1)$, 从定理二可知$$\frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)$$则根据t分布的定义有$$ \frac{\frac{\overline X - \mu}{\sqrt{\sigma^2/n}}} {\sqrt{\frac{(n-1)S^2}{\sigma^2(n-1)}}} \sim t(n-1)$$ 化简可得$$\frac{\overline X - \mu}{\sqrt{\sigma^2/n}} \sim t(n-1)$$ 定理三主要用于区间估计中总体（服从正态分布）的期望、方差均未知时，估计其期望的范围，这也是 $t$ 分布的作用之一,注意前面讲到的$\chi^2$分布估计的是方差。 定理四：设 $X_1, X_2…X_n$与$Y_1,Y_2,…Y_n$分别是来自正态总体 $N(\mu_1, \sigma_1^2)$和$N(\mu_2, \sigma_2^2)$的样本, $\overline X, \overline Y$分别是其样本均值，$S_1^2, S_2^2$分别是其样本方差。则有$$\frac{S_1^2/S_2^2}{\sigma_1^2/ \sigma_2^2} \sim F(n_1 - 1, n_2 - 1)$$且当$\sigma_1^2 = \sigma_2^2 = \sigma^2$ 时，$$\frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{S_w\sqrt{1/n_1+1/n_2}} \sim t(n_1+n_2-2)$$其中，$S_w = \sqrt{\frac{(n_1 -1)S_1^2+(n_2 -1)S_2^2}{n_1+n_2-2}}$ 证明如下:由定理二可知$$\frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2(n_1-1), \frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)$$ 由 F 分布的定义可知$$ \frac{(n_1-1)S_1^2}{\sigma_1^2(n_1-1)} / \frac{(n_2-1)S_2^2}{\sigma_2^2(n_2-1)} \sim F(n_1-1, n_2-1)$$化简可得$$\frac{S_1^2/S_2^2}{\sigma_1^2/ \sigma_2^2} \sim F(n_1 - 1, n_2 - 1)$$ 当$\sigma_1^2 = \sigma_2^2 = \sigma^2$ 时, 易知 $(\overline X - \overline Y) \sim N(\mu_1 - \mu_2,\sigma_1^2/n_1 + \sigma_2^2/n_2)$ 则$\frac{(\overline X - \overline Y)- (\mu_1 - \mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}} \sim N(0,1)$ 由定理二可知$$\frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2(n_1-1), \frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)$$, 由 $\chi^2$ 分布的可加性可知：$$\frac{(n_1-1)S_1^2}{\sigma_1^2} + \frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2(n_1+n_2-2)$$ 由t分布的定义可知：$$\frac{(\overline X - \overline Y)- (\mu_1 - \mu_2)}{\sqrt{\sigma_1^2/n_1 + \sigma_2^2/n_2}} / (\sqrt{(\frac{(n_1-1)S_1^2}{\sigma_1^2} + \frac{(n_2-1)S_2^2}{\sigma_2^2})/(n_1+n_2-2)}) \sim t(n_1+n_2-2)$$ 将 $\sigma_1^2 = \sigma_2^2 = \sigma^2$代入到上式化简即可得到$$\frac{(\overline X - \overline Y) - (\mu_1 - \mu_2)}{S_w\sqrt{1/n_1+1/n_2}} \sim t(n_1+n_2-2)$$其中，$S_w = \sqrt{\frac{(n_1 -1)S_1^2+(n_2 -1)S_2^2}{n_1+n_2-2}}$ 定理四的作用是在区间估计时估计两个均服从正态分布的总体的方差的比值（期望未知）以及两者期望的差距（方差未知） 小结本文主要介绍了数理统计的概念，数理统计主要做的事情就是通过有限的样本构造的统计量去推断总体分布的参数。同时介绍了数理统计中的三大分布 $\chi^2$分布， $t$分布和$F$分布，这三大分布与前面讲的随机变量的分布不同（伯努利分布，泊松分布，正态分布等等），随机变量的分布可以认为是整体的分布，而三大分布则是描述样本的分布情况。这三大分布在区间估计中有重要作用： 其中 $\chi^2$分布主要解决总体期望未知时估计其方差的问题， $t$分布主要解决总体方差未知时估计其期望的问题，$F$主要解决期望未知时两个正态分布的方差比值问题。需要注意的是上面估计的前提是总体服从正态分布。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(10, 44)--正则表达式的匹配与通配符的匹配]]></title>
      <url>%2F2016%2F10%2F23%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(10%2C%2044)--%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E5%8C%B9%E9%85%8D%E4%B8%8E%E9%80%9A%E9%85%8D%E7%AC%A6%E7%9A%84%E5%8C%B9%E9%85%8D%2F</url>
      <content type="text"><![CDATA[正则表达式和通配符均是用来匹配字符串的，但是两者使用的范围不一样，通配符一般用在 Linux 命令行shell中，而正则表达式使用则更加广泛，在各种编程语言和工具中均有支持。下面主要讲述如何实现正则表达式和通配符的简单匹配。 题目选自 LeetCode 的 10. Regular Expression Matching 和 44. Wildcard Matching，之所以说是简单匹配，是因为两者的实现的只是两个符号的功能。正则匹配要求实现 *和 .的匹配功能，而通配符匹配要求实现?和 *的匹配。 解决这种两个字符串的比较问题一般可考虑动态规划。以两个字符串的长度分别作为长和宽建立一个二维矩阵，通过二维动态规划解决。 正则表达式在正则表达式中,.表示任意一个单一字符，*表示0个或若干个前一个字符（表示为0个的时候前一字符也失效，也就是 a* 可以匹配出空字符串）。 首先我们建立了一个 m*n 的dp矩阵，其中m表示匹配模式字符串 p 的长度，n表示待匹配字符串 s 的长度。则 dp[i][j] 表示子字符串 p[:i] 和 s[:j] (均包含i和j)是否匹配(true/false)。假设目前已知 dp[i][j-1] 及其前面的所有情况的匹配关系，那么要求dp[i][j]通过动态规划的递推关系如下： 123451. 假如 p[i] == &apos;.&apos;，则dp[i][j] = dp[i-1][j-1]2. 假如 p[i] == letter(a-zA-Z)，则dp[i][j] = dp[i-1][j-1] &amp;&amp; (p[i]==s[j])3. 假如 p[i] == &apos;*&apos;,则 dp[i][j] = dp[i-2][j] || dp[i-1][j] || (dp[i][j-1] &amp;&amp; (p[i-1] == s[j])) 上面的1,2 均比较好理解，关键是出现 * 时要分三种情况讨论，分别是 * 匹配了0个，1个，和若干个前一字符。假如匹配了0个前一字符，那么当前位置的匹配结果与dp[i-2][j]相同；匹配了1个前一字符，则当前位置的匹配结果与 dp[i-1][j] 相同；关键是假如匹配了多个前一字符，该如何判断，此时我们无法知道到底匹配了多少个前一字符，但是换个角度去想这个问题，假如匹配了多个前一字符，那么前一字符要与当前的 s[j] 匹配（p[i-1]==s[j] 或 p[i-1]=’.’），此时要想匹配成功(dp[i][j]为true)，则当前的匹配串(p[:i])必须能够匹配s[:j-1],也就是dp[i][j-1]为true。对于这三种情况出现任意一种均可认为匹配，因此取或操作。 在具体实现中还要注意数组越界的问题，可以看到上面出现了 i-1，j-1，i-2的下标，那么在实现的时候要在原二维矩阵中各增加一行和一列，表示第0个字符也就是空字符从而避免了i-1的越界；同时只有在遇到*时才会出现i-2的下标，且这种情况下只有当*出现在匹配串第一个的时候才会越界，而当*出现在匹配串的第一个字符的时候表示为空字符串，除了待匹配字符串为空时一律为false。具体实现的Java代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637public class Solution &#123; public boolean isMatch(String s, String p) &#123; int m = p.length()+1, n = s.length()+1; boolean[][] dp = new boolean[m][n]; for (int i=0; i&lt;m; i++) &#123; for (int j=0;j&lt;n; j++) &#123; if(i==0) &#123; if(j==0) dp[i][j] = true; else dp[i][j] = false; &#125; else if(j==0) &#123; if(p.charAt(i-1)!='*') dp[i][j] = false; else dp[i][j] = dp[i-1][j] || dp[i-2][j]; &#125; else &#123; if (p.charAt(i-1)=='.') dp[i][j] = dp[i-1][j-1]; else if (p.charAt(i-1) == '*') &#123; if (i==1) dp[i][j] = false; else dp[i][j] = dp[i-2][j] || dp[i-1][j] || ((p.charAt(i-2)== '.' || p.charAt(i-2)==s.charAt(j-1)) &amp;&amp; dp[i][j-1]); &#125; else dp[i][j] = (s.charAt(j-1)==p.charAt(i-1)) &amp;&amp; dp[i-1][j-1]; &#125; &#125; &#125; return dp[m-1][n-1]; &#125;&#125; 通配符在通配符中 ？ 表示一个字符，而 * 与正则表示式中的含义不一样，在这里 * 表示任意字符串（包括空字符串）。 采用上面的符号(dp,s,p)的定义，在求 dp[i][j],有以下的递推关系 1231. 假如 p[i] == &apos;?&apos;，则dp[i][j] = dp[i-1][j-1]2. 假如 p[i] == letter(a-zA-Z)，则dp[i][j] = dp[i-1][j-1] &amp;&amp; (p[i]==s[j])3. 假如 p[i] == &apos;*&apos;,则 dp[i][j] = dp[i-1][j] || dp[i][j-1] 前面的两种情况都比较好理解，这里的关键点是当遇到 * 时，需要讨论两种情况，第一种是*表示空字符，这时候匹配结果与dp[i-1][j]相同；第二种是*表示任意字符串，这时候假如 dp[i-1][0] 到 dp[i-1][j-1] 有一个为真，则dp[i][j]为真,但是这样遍历的话遇到 * 时会导致时间复杂度变得很大，这时候用到了一个技巧，就是dp[i-1][0] 到 dp[i-1][j-1]的结果已经包含在dp[i-1][j] 中了（根据递推式可知道），所以此时只需要或上dp[i-1][j]即可。 实现时也需要注意越界问题，解决方法同上面提到的添加空字符，具体实现的Java代码如下所示： 12345678910111213141516171819202122232425262728293031public class Solution &#123; public boolean isMatch(String s, String p) &#123; int m = p.length()+1, n = s.length()+1; boolean[][] dp = new boolean[m][n]; for (int i=0; i&lt;m; i++) &#123; for (int j=0; j&lt;n; j++) &#123; if(i==0) &#123; if(j==0) dp[i][j] = true; else dp[i][j] = false; &#125; else if(j==0) &#123; if(p.charAt(i-1)=='*') dp[i][j] = dp[i-1][j]; else dp[i][j] = false; &#125; else &#123; if (p.charAt(i-1)=='?') dp[i][j] = dp[i-1][j-1]; else if(p.charAt(i-1)=='*') dp[i][j] = dp[i-1][j] || dp[i][j-1]; else dp[i][j] = (s.charAt(j-1)==p.charAt(i-1)) &amp;&amp; dp[i-1][j-1]; &#125; &#125; &#125; return dp[m-1][n-1]; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[概率论与数理统计知识整理(4)--大数定律和中心极限定理]]></title>
      <url>%2F2016%2F10%2F18%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(4)--%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86%2F</url>
      <content type="text"><![CDATA[大数定律和中心极限定理都是与极限有关的定理，其中大数定律主要描述了当样本的数量足够多时，其均值(频率)可以用来逼近总体的期望（概率）；而中心极限定理则描述了在某些条件下，大量独立同分布的随机变量的和的分布逼近于正态分布。 大数定律大数定律有弱大数定律和强大数定律，两者描述的都是样本数量越多，则其平均就越趋近期望值。两个简单的区别就是弱大数定律表示样本均值依概率收敛于总体均值，而强大数定律表示了样本均值可以以概率为1收敛于总体均值。弱大数定律比较早被证明出来，强大数定律是比较晚被证明出来的，通俗来说就是数学家先证明了弱大数定律，后来在没有改变前提的情况下把弱大数定律推进了一步，更加确定了这个收敛，也就是强大数定律。 这里主要讲几个弱大数定律的定义 弱大数定理（辛钦大数定理） 设 $X_1,X_2,…$ 是独立同分布的随机变量序列，且具有数学期望 $E(X_k) = \mu(k=1,2,,….)$,取前 n 个变量的算术平均 $\frac{1}{n} \sum_{k=1}^{n}X_k$, 对于任意的 $\epsilon$,有$$\lim_{n \rightarrow \infty} P(|\frac{1}{n} \sum_{k=1}^{n}X_k - \mu| &lt; \epsilon) = 1$$ 定义描述的就是当样本数n足够大时，样本均值与总体期望的差可以无限小，也就是可以通过样本均值估计总体期望。基于上面的辛钦弱大数定理可以推出下面的伯努利大数定理 伯努利大数定理 设 $f_A$ 是 n 次独立重复试验中事件 $A$ 发生的次数，$p$ 是事件 $A$ 在每次试验中发生的概率，则对于任意正数 $\epsilon &gt; 0 $, 有$$\lim_{n \rightarrow \infty}P(|\frac{f_A}{n} - p| &lt; \epsilon) = 1$$或$$\lim_{n \rightarrow \infty}P(|\frac{f_A}{n} - p| \ge \epsilon) = 0$$ 伯努利大数定理主要描述当样本数足够大时，可以用样本的频率来估计总体的概率，其本质跟辛钦弱大数定理是一样的。 中心极限定理一般来说，n个独立同分布的随机变量的和的分布函数是比较难求的，而通过中心极限定理，可以描述当n足够大的时候，这些随机变量的和的分布近似服从正态分布。下面主要讲述两条中心极限定理的 独立同分布的中心极限定理 随机变量 $X_1,X_2,…X_n$ 独立同分布，且具有数学期望$E(X_k) = \mu$, 和方差 $D(X_k) = \sigma^2 &gt; 0(k=1,2,3…)$, 则随机变量之和 $\sum_{k=1}^n X_k$ 的标准化变量$$Y_n = \frac{\sum_{k=1}^n X_k - E(\sum_{k=1}^n X_k)}{\sqrt{D(\sum_{k=1}^n X_k)}} = \frac{\sum_{k=1}^n X_k - n\mu}{\sqrt{n}\sigma}$$的分布函数 $F_n(x)$ 对于任意 $x$ 满足$$\lim_{n \rightarrow \infty} F_n(x) = \lim_{n \rightarrow \infty}P(\frac{\sum_{k=1}^n X_k - n\mu}{\sqrt{n}\sigma}\le x) = \int_{-\infty}^x\frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt$$ 也就是说，当上面的 $n$ 充分大的时候，$\frac{\sum_{k=1}^n X_k - n\mu}{\sqrt{n}\sigma}$ 服从正态分布 $N(0,1)$ 也可以将上面分布写成下面的形式 $\frac{ \overline X- \mu}{\sigma/\sqrt{n}}$~$N(0,1)$ 或 $\overline X$~$N(\mu, \sigma^2/n)$ 也就是说，当样本的数量n足够大的时候，样本均值服从均值为 $\mu$, 方差为 $\sigma^2/n$ 的正态分布， 其中 $\mu$ 和 $\sigma$ 分别是原来随机变量的所服从的分布的期望和方差，这一结果是数理统计中大样本统计推断的基础。 上面的独立同分布中每个随机变量都是同分布的，也就是具有同样的期望和方差，那么如果随机变量的分布独立呢？下面是对应这种情况的中心极限定理。 李雅普诺夫定理 设随机变量 $X_1,X_2,…X_n$ 相互独立，具有数学期望和方差$$E(X_k) = \mu_k, D(X_k) = \sigma_k^2 &gt; 0,k=1,2,…$$,记$$B_n^2 = \sum_{k=1}^n \sigma_k^2$$ 若存在正数 $\delta$, 使得当 $n \rightarrow \infty$ 时，$$\frac{1}{B_n^{2+\delta}}\sum_{k=1}^{n} E(|X_k - \mu_k|^{2+\delta}) \rightarrow 0$$定义随机变量 $Z_n$ 为$$Z_n = \frac{\sum_{k=1}^n X_k - \sum_{k=1}^n \mu_k}{B_n}$$那么当n很大时,只要满足定理中的条件，那么随机变量 $Z_n$ 服从正态分布 $N(0,1)$。 也就是说当 n 很大的时候，随机变量的和 $\sum_{k=1}^{n}X_k$ 近似服从正态分布$N(\sum_{k=1}^n\mu_k, B_n^2)$ 下面是应用中心极限定理的一个例子]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[概率论与数理统计知识整理(3)--随机变量的统计特征]]></title>
      <url>%2F2016%2F10%2F14%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(3)--%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81%2F</url>
      <content type="text"><![CDATA[随机变量的统计特征主要包括期望，方差，协方差以及相关系数。 期望离散型随机变量：$$E(X) = \sum_{k=1}^{ +\infty}p_kx_k$$ 连续型随机变量：$$E(X) = \int_{-\infty}^{ +\infty} {xf(x)dx} $$ 期望有以下性质(C为常数,其他均为随机变量): $E(C) = C$ $E(CX) = CE(X)$ $E(X+Y) = E(X)+E(Y)$ $E(XY) = E(X)E(Y) $ （$X,Y$ 相互独立） 前面讨论随机变量的分布函数时，同时讨论了随机变量的函数的分布函数，这里同样对于随机变量 $X$ 的函数的期望进行讨论，其定义及求法如下所示。 设Y是随机变量X的函数：$Y=g(X)$(g是连续函数) (1) 如果 $X$ 是离散型随机变量，它的分布律为$$P(X=x_k) = p_k, k = 1,2,…$$若 $\sum_{k=1}^{\infty}g(x_k)p_k $绝对收敛，则有$$E(Y) = E[g(X)] = \sum_{k=1}^{\infty}g(x_k)p_k $$ (2) 如果 X 是连续型随机变量，它的概率密度函数为 $f(x)$, 若 $\int_{-\infty}^{\infty}g(x)f(x)dx$ 绝对收敛，则有$$E(Y) = E[g(X)] = \int_{-\infty}^{\infty}g(x)f(x)dx$$ 这个定理的重要意义在于求 $E(Y)$ 的时候，不用再求 Y 的分布律或概率密度函数，直接利用 X 的分布律或概率密度函数即可。 方差方差的原始定义为 $D(X) = E[(X-E(X))^2] = E(X^2) - E(X)^2$ 方差有以下性质： $D(C) = 0$ $D(CX) = C^2D(X)$ $D(X+Y) = D(X) + D(Y) + 2E([X-E(X)][Y-E(Y)]) $ 如果 $X，Y$ 是相互独立的，那么$E([X-E(X)][Y-E(Y)]) = 0$, 当这一项不为0的时候，称作变量 $X,Y$ 的协方差。 常见分布的期望和方差前面我们提到了若干种典型的离散分布和连续分布，下面是这几种分布的期望和方差，记住这些常用的期望和方差能够在使用的时候省去推导过程。 分布类型 概率密度函数 期望 方差 伯努利分布~$B(1,p)$ $p = p^x(1-p)^{1-x}$ $p$ $p(1-p)$ 二项分布~$B(n,p)$ $p_i = C_n^i p^i(1-p)^{n-i}(i=1,2,3…)$ $np$ $np(1-p)$ 泊松分布~$P(\lambda)$ $p_i = \frac{\lambda^ki e^{-\lambda}}{i!}(i = 1,2,3,) $ $\lambda$ $\lambda$ 均匀分布~$U(a,b)$ $f(x) = \frac{1}{b-a}$ $\frac{a+b}{2}$ $\frac{(b-a)^2}{12}$ 正态分布~$N(\mu,\sigma^2)$ $f(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ $\mu$ $\sigma^2$ 指数分布~$E(\lambda)$ $$f(x) = \begin{cases} \lambda e^{-x\lambda} &amp;{x&gt;0} \\ 0&amp;{其他}\end{cases}$$ $\frac{1}{\lambda}$ $\frac{1}{\lambda^2}$ 切比雪夫不等式切比雪夫不等式的定义如下： 设随机变量 $X$ 具有数学期望 $E(X) = \mu$, 方差 $D(X) = \sigma^2$, 则对于任意正数 $\epsilon$, 下面的不等式成立$$P(|X-\mu|\ge \epsilon) \le \frac{\sigma^2}{\epsilon^2}$$ 从定义可知，切比雪夫不等式也可写成如下的形式： $$P(|X-\mu| \le \epsilon) \ge 1 - \frac{\sigma^2}{\epsilon^2}$$ 切比雪夫不等式的一个重要意义在于当随机变量 X 的分布未知，只知道 $E(X)$ 和 $D(X)$ 的情况下，对于事件 $(|X-\mu| \le \epsilon) $ 概率的下限的估计。 协方差协方差表达了两个随机变量的相关性，正的协方差表达了正相关性，负的协方差表达了负相关性。协方差为0 表示两者不相关，对于同样的两个随机变量来说，计算出的协方差的绝对值越大，相关性越强。 协方差的定义入下: $Cov(X,Y) = E{[X-E(X)][Y-E(Y)]}$ 由定义可以知下面等式成立: $Cov(X,Y) = Cov(Y,X)$$Cov(X,Y) = E(XY) - E(X)E(Y)$ 协方差有以下性质： $Cov(aX,bY) = abCov(X,Y)$（a，b是常数） $Cov(X_1+X_2, Y) = Cov(X_1, Y) + Cov(X_2,Y)$ 假如我们现在有身高和体重这两个未知变量，对于一系列的样本我们算出的的协方差为30，那这究竟是多大的一个量呢？如果我们又发现，身高与鞋号的协方差为5，是否说明，相对于鞋号，身高与体重的的相关性更强呢？ 为了能进行这样的横向对比，我们计算相关系数(correlation coefficient)， 相关系数相当于是“归一化”的协方差。 $$\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)D(Y)}}$$ 相关系数是用协方差除以两个随机变量的标准差。相关系数的大小在-1和1之间变化，等于0表示不相关。再也不会出现因为计量单位变化，而数值变化较大的情况，而相关系数的大小的含义与协方差是一样的。 需要注意的是上面提到的相关均指线性相关，$X, Y$ 不相关是指 $X,Y$ 之间不存在线性关系，但是他们还可能存在除线性关系以外的关系。因此，有以下结论: $X,Y$ 相互独立则 $X,Y$ 一定不相关；反之 $X,Y$ 不相关，两者不一定相互独立。 简单的证明如下：当 $X,Y$ 相互独立的时候有 $E(XY) = E(X)E(Y)$ ， 根据上面协方差的展开式 $Cov(X,Y) = E(XY) - E(X)E(Y)$ 此时协方差为零，两者不相关。 而当 $X, Y$ 不相关的时候举一个反例如下： 上面的例子来源于https://www.zhihu.com/question/26583332, 可知计算出来的协方差为0，即两者不相关，但是 $P(XY) \neq P(X)P(Y)$,即 两者不独立，注意 $E(XY) = E(X)E(Y)$ 不是 $X,Y$ 独立的充分条件。 矩和协方差矩阵下面介绍概率论中几种矩的定义 设 $X,Y$ 为随机变量,则 $E(X^k), k=1,2,3….$ 称为 $X$ 的 $k$ 阶原点矩，简称 $k$ 阶矩 $E((X-E[X])^k), k=1,2,3….$ 称为 $X$ 的 $k$ 阶中心距 $E(X^kY^l),k,l=1,2,…$ 称为 $X$ 和 $Y$ 的 $k+l$ 阶混合矩 $E((X-E[X])^k(Y-E[Y])^l)),k,l=1,2,…$称为$X$ 和 $Y$ 的 $k+l$ 阶混合中心矩 由以上定义我们可以知道，随机变量的期望是其一阶原点矩，方差是其二阶中心距，协方差是其二阶混合中心矩。 除此之外，另外一个常用的概念是协方差矩阵， 其定义如下： 对于 $n$ 维随机变量 $(X_1,X_2,X_3…,X_n)$ 构成的矩阵 $$C=\begin{bmatrix}c_{11} &amp; c_{12} &amp; \cdots &amp; c_{1n} \\c_{21} &amp; c_{22} &amp; \cdots &amp; c_{2n} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\c_{n1} &amp; c_{n2} &amp; \cdots &amp; c_{nn} \\\end{bmatrix}$$ 其中各个元素为$$c_{ij} = Cov(X_i,X_j) = E((X_i - E[X_i])(X_j - E[X_j]))，i,j=1,2,3..n$$ 则称矩阵 $C$ 为协方差矩阵，由于$c_{ij} = c_{ji}$ ， 因此上面的矩阵为一个对称矩阵。 协方差矩阵其实是将二维随机变量的协方差一般化后拓展到了 $n$ 维随机变量上的一种表示形式，但是除了作为一种表示形式以外，协方差矩阵还存在着某些性质使得其在多个领域均有应用，如主成成分分析。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[中文维基百科语料库词向量的训练]]></title>
      <url>%2F2016%2F10%2F12%2F%E4%B8%AD%E6%96%87%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E8%AE%AD%E7%BB%83%2F</url>
      <content type="text"><![CDATA[要通过计算机进行自然语言处理，首先就需要将这些文本数字化。目前用得最广泛的方法是词向量，根据训练使用算法的不同，目前主要有 Word2Vec 和 GloVe 两大方法，本文主要讲述通过这两个方法分别训练中文维基百科语料库的词向量。 获取并处理中文维基百科语料库下载中文维基百科语料库的下载链接为：https://dumps.wikimedia.org/zhwiki/, 本试验下载的是最新的zhwiki-latest-pages-articles.xml.bz2。这个压缩包里面存的是标题、正文部分，该目录下还包括了其他类型的语料库，如仅包含标题，摘要等。 抽取内容Wikipedia Extractor 是一个开源的用于抽取维基百科语料库的工具，由python写成，通过这个工具可以很容易地从语料库中抽取出相关内容。使用方法如下： 12$ git clone https://github.com/attardi/wikiextractor.git wikiextractor$ wikiextractor/WikiExtractor.py -b 2000M -o zhwiki_extracted zhwiki-latest-pages-articles.xml.bz2 由于这个工具就是一个python脚本，因此无需安装，-b 参数指对提取出来的内容进行切片后每个文件的大小，如果要将所有内容保存在同一个文件，那么就需要把这个参数设得大一下，-o 的参数指提取出来的文件放置的目录，抽取出来的文件的路径为zhwiki_extract/AA/wiki_00。更多参数可参考其github主页的说明。 抽取后的内容格式为每篇文章被一对&lt;doc&gt; &lt;/doc&gt;包起来，而&lt;doc&gt;中的包含了属性有文章的id、url和title属性，如&lt;doc id=&quot;13&quot; url=&quot;https://zh.wikipedia.org/wiki?curid=13&quot; title=&quot;数学&quot;&gt;。 繁简转换由上一步提取出来的中文维基百科中的语料中既有繁体字也有简体字，这里需要将其统一变为简体字，采用的工具也是开源的 OpenCC 转换器。使用方法如下： 123$ git clone https://github.com/BYVoid/OpenCC.git$ cd OpenCC &amp;&amp; make &amp;&amp; make install$ opencc -i zhwiki_extract/AA/wiki_00 -o zhwiki_extract/zhs_wiki -c /home/nlp/OpenCC/data/config/t2s.json 我使用的是 centos，yum源中找不到这个软件，因此通过编译安装最新的版本，需要注意的是编译OpenCC 要求gcc的版本最低为 4.6 。其中 -i表示输入文件路径， -o表示输出的文件 ，-c表示转换的配置文件，这里使用的繁体转简体的配置文件，OpenCC自带了一系列的转换配置文件，可参考其github主页的说明。 去除标点符号去除标点符号有两个问题需要解决，一是像下面这种为了解决各地术语名称不同的问题1他的主要成就包括Emacs及後來的GNU Emacs，GNU C 編譯器及-&#123;zh-hant:GNU 除錯器;zh-hans:GDB 调试器&#125;-。 另外一个就是将所有标点符号替换成空字符，通过正则表达式均可解决这两个问题，下面是具体实现的python代码。 1234567891011121314151617181920212223242526#!/usr/bin/python# -*- coding: utf-8 -*- import sysimport reimport ioreload(sys)sys.setdefaultencoding('utf-8')def pre_process(input_file, output_file): multi_version = re.compile(ur'-\&#123;.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\&#125;-') punctuation = re.compile(u"[-~!@#$%^&amp;*()_+`=\[\]\\\&#123;\&#125;\"|;':,./&lt;&gt;?·！@#￥%……&amp;*（）——+【】、；‘：“”，。、《》？「『」』]") with io.open(output_file, mode = 'w', encoding = 'utf-8') as outfile: with io.open(input_file, mode = 'r', encoding ='utf-8') as infile: for line in infile: line = multi_version.sub(ur'\2', line) line = punctuation.sub('', line.decode('utf8')) outfile.write(line)if __name__ == '__main__': if len(sys.argv) != 3: print "Usage: python script.py input_file output_file" sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] pre_process(input_file, output_file) 分词经过上面的步骤基本得到了都是简体中文的纯净文本，下面需要对其进行分词并且整理成每行一篇文本的格式，从而方便后续的处理。 分词采用 python 的分词工具 jieba，通过 pip install jieba 安装即可。且将一篇文章分词后的结果存储在一行，由前面可知，每篇文章存储在一对&lt;doc&gt;&lt;/doc&gt;标签中，由于前面去掉了标点，所以现在变成了doc doc,所以只要判断当前行为doc时即可认为文章结束，从而开始在新的一行记录下一篇文章的分词结果。实现的python代码如下: 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/python# -*- coding: utf-8 -*-import sysimport ioimport jiebareload(sys)sys.setdefaultencoding('utf-8')def cut_words(input_file, output_file): count = 0 with io.open(output_file, mode = 'w', encoding = 'utf-8') as outfile: with io.open(input_file, mode = 'r', encoding = 'utf-8') as infile: for line in infile: line = line.strip() if len(line) &lt; 1: # empty line continue if line.startswith('doc'): # start or end of a passage if line == 'doc': # end of a passage outfile.write(u'\n') count = count + 1 if(count % 1000 == 0): print('%s articles were finished.......' %count) continue for word in jieba.cut(line): outfile.write(word + ' ') print('%s articles were finished.......' %count)if __name__ == '__main__': if len(sys.argv) &lt; 3: print "Usage: python script.py input_file output_file" sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] cut_words(input_file, output_file) 通过 Word2Vec 训练词向量Word2vec中包含了两种训练词向量的方法：Continuous Bag of Words(CBOW)和Skip-gram。CBOW的目标是根据上下文来预测当前词语的概率。Skip-gram刚好相反，根据当前词语来预测上下文的概率。这两种方法都利用人工神经网络作为它们的分类算法。起初，每个单词都是一个随机N维向量。训练时，该算法利用CBOW或者Skip-gram的方法获得了每个单词的最优向量。 最初 Google 开源的 Word2Vec 是用C来写的，后面陆续有了Python ，Java 等语言的版本，这里采用的是 Python 版本的 gensim。通过 gensim 提供的 API 可以比较容易地进行词向量的训练。gensim的建议通过conda安装，步骤如下: 1234$ wget https://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh$ bash Anaconda2-4.1.1-Linux-x86_64.sh$ conda update conda$ conda install gensim Linux 系统一般原来会带有 python，直接执行 python 命令可能会调用系统内置的 python 解释器，因此如果要使用conda安装的 python， 执行 python 命令的时候需要输入指定其通过 conda 安装的完整目录，或者将这个路径添加在环境变量$PATH之前。 下面是对上面处理后的语料库进行训练的一个简单例子。 123456789101112131415161718192021222324#!/usr/bin/python# -*- coding: utf-8 -*-import os, sysimport multiprocessingimport gensim reload(sys)sys.setdefaultencoding('utf-8')def word2vec_train(input_file, output_file): sentences = gensim.models.word2vec.LineSentence(input_file) model = gensim.models.Word2Vec(sentences, size=300, min_count=10, sg=0, workers=multiprocessing.cpu_count()) model.save(output_file) model.save_word2vec_format(output_file + '.vector', binary=True)if __name__ == '__main__': if len(sys.argv) &lt; 3: print "Usage: python script.py infile outfile" sys.exit() input_file, output_file = sys.argv[1], sys.argv[2] word2vec_train(input_file, output_file) 上面的训练过程首先将输入的文件转为 gensim 内部的 LineSentence 对象，要求输入的文件的格式为每行一篇文章，每篇文章的词语以空格隔开。 然后通过gensim.models.Word2Vec 初始化一个 Word2Vec 模型，size参数表示训练的向量的维数；min_count表示忽略那些出现次数小于这个数值的词语，认为他们是没有意义的词语，一般的取值范围为（0，100）；sg表示采用何种算法进行训练，取0时表示采用CBOW模型，取1表示采用skip-gram模型；workers表示开多少个进程进行训练，采用多进程训练可以加快训练过程，这里开的进程数与CPU的核数相等。 最后将训练后的得到的词向量存储在文件中，存储的格式可以是 gensim 提供的默认格式(save方法)，也可以与原始c版本word2vec的 vector 相同的格式(save_word2vec_format方法)，加载时分别采用 load 方法和 load_word2vec_format 方法即可。更详细的API可参考 https://rare-technologies.com/word2vec-tutorial/ 和 http://radimrehurek.com/gensim/models/word2vec.html。 假设我们训练好了一个语料库的词向量，当一些新的文章加入这个语料库时，如何训练这些新增的文章从而更新我们的语料库？将全部文章再进行一次训练显然是费时费力的，gensim提供了一种类似于“增量训练”的方法。即可在原来的model基础上仅对新增的文章进行训练。如下所示为一个简单的例子： 12model = gensim.models.Word2Vec.load(exist_model)model.train(new_sentences) 上面的代码先加载了一个已经训练好的词向量模型，然后再添加新的文章进行训练，同样新增的文章的格式也要满足每行一篇文章，每篇文章的词语通过空格分开的格式。这里需要注意的是加载的模型只能 是通过model.save()存储的模型，从model.save_word2vec_format()恢复过来的模型只能用于查询. 通过 Glove 训练词向量除了上面的 Word2Vec ，通过 Glove 也可以训练出词向量，只是这种方法并没有 Word2Vec 用得那么广泛。这里简单提及，也算是为训练词向量提供多一个选择。 首先需要下载并编译 Glove，步骤如下：123$ wget http://www-nlp.stanford.edu/software/GloVe-1.2.zip$ unzip Glove-1.2.zip $ cd Glove-1.2 &amp;&amp; make 编译后会在 Glove-1.2 目录下生成一个 build 目录，里面包含了训练需要用到的工具。目录结构如下所示：12345build/|-- cooccur|-- glove|-- shuffle`-- vocab_count 训练过程总共分为四步，对应上面的四个工具，顺序依次为vocab_count --&gt; cooccur --&gt; shuffle --&gt; glove，下面是具体的训练过程 1234567$ build/vocab_count -min-count 5 -verbose 2 &lt; zhs_wiki_cutwords &gt; zhs_wiki_vocab$ build/cooccur -memory 4.0 -vocab-file zhs_wiki_vocab -verbose 2 -window-size 5 &lt; zhs_wiki_cutwords &gt; zhs_wiki_cooccurence.bin$ build/shuffle -memory 4.0 -verbose 2 &lt; zhs_wiki_cooccurence.bin &gt;zhs_wiki_shuff.bin$ build/glove -save-file zhs_wiki_glove.vectors -threads 8 -input-file zhs_wiki_shuff.bin -vocab-file zhs_wiki_vocab -x-max 10 -iter 5 -vector-size 300 -binary 2 -verbose 2 上面四条命令分别对应于训练的四个步骤，每个步骤含义如下 vocab_count从语料库(zhs_wiki_cutwords是上面第一步处理好的语料库)中统计词频，输出文件 zhs_wiki_vocab，每行为词语 词频；-min-count 5指示词频低于5的词舍弃，-verbose 2控制屏幕打印信息的，设为0表示不输出 cooccur 从语料库中统计词共现，输出文件 zhs_wiki_cooccurence.bin，格式为非文本的二进制；-memory 4.0指示bigram_table缓冲器，-vocab-file指上一步得到的文件，-verbose 2同上，-window-size 5指示词窗口大小。 shuffle 对 zhs_wiki_cooccurence.bin 重新整理，输出文件zhs_wiki_shuff.bin glove 训练模型，输出词向量文件。-save-file 、-threads 、-input-file 和-vocab-file 直接按照字面应该就可以理解了，-iter 表示迭代次数，-vector-size 表示向量维度大小，-binary 控制输出格式0: save as text files; 1: save as binary; 2: both 训练后得到的二进制词向量模型格式与原始c版本word2vec的 vector 格式也相同，可以通过下面的方法统一加载使用。 使用词向量模型训练好的词向量可以供后续的多项自然语言处理工作使用，下面是通过 gensim 加载训练好的词向量模型并进行查询的例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 加载模型&gt;&gt;&gt; from gensim.models import Word2Vec&gt;&gt;&gt; model = Word2Vec.load_word2vec_format('/home/nlp/zhs_wiki_trained.vector',binary = True)# 词向量维度&gt;&gt;&gt; len(model[u'男人'])300# 具体词向量的值&gt;&gt;&gt; model[u'男人']array([ 0.56559366, -1.96017861, -1.57303607, 1.2871722 , -1.38108838.....# 词语相似性&gt;&gt;&gt; model.similarity(u'男人',u'女人')0.86309866214314379# 找某个词的近义词，反义词&gt;&gt;&gt; words = model.most_similar(u"男人")&gt;&gt;&gt; for word in words:... print word[0], word[1]... 女人 0.863098621368女孩 0.67369633913女孩子 0.658665597439陌生人 0.654322624207小女孩 0.637025117874小孩 0.630155563354男孩 0.625135600567男孩子 0.617452859879小孩子 0.613232254982老婆 0.584552764893&gt;&gt;&gt; words = model.most_similar(positive=[u"女人", u"皇后"], negative=[u"男人"], topn=5)&gt;&gt;&gt; for word in words:... print word[0], word[1]... 皇太后 0.630089104176太后 0.613425552845王妃 0.581929504871贵妃 0.581658065319王后 0.577878117561# 若干个词中剔除与其他最不相关的&gt;&gt;&gt; print model.doesnt_match(u"早餐 晚餐 午餐 食堂".split())食堂&gt;&gt;&gt; print model.doesnt_match(u"早餐 晚餐 午餐 食堂 教室".split())教室# 多个词语的相似性&gt;&gt;&gt; model.n_similarity([u"女人", u"皇帝"], [u"男人", u"皇后"])0.76359309631510597 这里并没有对训练出来的词向量质量进行评估，虽然 Google 提供了一种测试集，约20000句法和语义的测试实例（questions-words.txt），检查如A对于B类似C对于D这种线性平移关系。由于测试集是英文的，因此可以考虑翻译过来然后对中文的采用同样的评估方法，但是实际的效果还是要看具体应用中的效果。 参考：https://flystarhe.github.io/2016/09/04/word2vec-test/https://flystarhe.github.io/2016/08/31/wiki-corpus-zh/http://radimrehurek.com/gensim/models/word2vec.htmlhttps://rare-technologies.com/word2vec-tutorial/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[概率论与数理统计知识整理(2)--二维随机变量的分布]]></title>
      <url>%2F2016%2F10%2F08%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(2)--%E4%BA%8C%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%88%86%E5%B8%83%2F</url>
      <content type="text"><![CDATA[前面讲到的随机变量都是一维的，但是某些试验中随机变量可能有多个，这里主要讨论二维的随机变量。 联合分布函数假设 $X$ 和 $Y$ 都是随机变量，那么我们定义其分布函数如下： $$ F(x,y) = P ((X \le x)\cap(Y \le y)) = P (X \le x, Y \le y )$$ 上面的 $F(x,y)$ 称作随机变量(X,Y)的分布函数，也叫作联合分布函数。 离散型随机变量联合分布如果上面的 $X$ 和 $Y$ 都是离散随机变量，那么对于 $(X,Y)$ 的所有取值可记为 $$P(X=x_i, Y=y_i) = p_{ij},i,j=1,2,….$$ 上面的所有P的取值为二维离散随机变量的分布律，也叫联合分布律。直观用表格表示如下所示 连续型随机变量联合分布类似地，如果上面的X和Y都是连续随机变量，那么分布函数可定义为 $$ F(x,y) = \int_{-\infty}^y\int_{-\infty}^xf(u,v)dudv $$ 其中 $f(x,y)$ 被称为概率密度函数，也叫联合概率密度函数。 其性质与一维随机变量的概率密度函数非常相似 $$f(x,y) \ge 0$$ $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y)dxdy = F(\infty,\infty)$$ 3.设 $G$ 是 $xOy$ 平面上的区域，点 $(X,Y)$ 落在G内的概率为 $$P((X,Y)\in G) = \int\int f(x,y)dxdy$$ 4.若 $f(x,y)$ 在点 $(x, y)$ 连续，则 $$\frac{\partial^2F(X,Y)}{\partial x \partial y} = f(x, y)$$ 边缘分布函数二维随机变量 $(X,Y)$ 作为一个整体的时候，其分布函数为联合分布函数，但是 $X$ 和 $Y$ 是随机变量，各自也有分布函数，将其分别记为 $F_X(x),F_Y(y)$，称为随机变量 $(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘分布函数。 边缘分布函数可通过联合分布函数确定，关系如下 $$F_X(x) = P(X \le x) = P(X \le x,Y \lt \infty) = F(x, \infty)$$ 即 $$F_X(x) = F(x,\infty)$$ 也就是说在联合分布函数 $F(x,y)$ 中令 $y \rightarrow \infty$ 即可得到边缘分布 $F_X(x)$, 同理$$F_Y(y) = F(\infty, y)$$ 下面分别以离散型随机变量和连续性随机基量为例说明 离散型随机变量边缘分布假如 $X$ 和 $Y$ 是离散型随机变量，那么随机变量 $(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘分布定义下 $$p_{i.} = \sum_{j=1}^{\infty} p_{ij} = P(X = x_i), i=1,2,3…..n$$ $$p_{.j} = \sum_{i=1}^{\infty} p_{ij} = P(Y = y_j), j=1,2,3…..n$$ 上面的式子分别称为随机变量 $(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘分布率。 连续型随机变量边缘分布假如 $X$ 和 $Y$ 分别是连续性随机变量，那么随机变量 $(X,Y)$ 关于 $X$ 的边缘分布函数定义为 $$F_X(x) = F(x,\infty) = \int_{-\infty}^{x}(\int_{-\infty}^{\infty}f(x,y)dy)dx$$ 而 $$ f_X(x) = \int_{-\infty}^{\infty}f(x,y)dy$$ 则被称为随机变量 $(X,Y)$ 关于 $Y$ 的 边缘概率密度函数 条件分布由条件概率可以比较容易推导出条件分布的含义，其定义如下： 离散型随机变量的条件分布对于离散型随机变量，条件分布的定义如下： 设 $(X,Y)$ 是二维离散型随机变量，对于固定的 $j$，若 $P(Y=y_j) \gt 0$, 则称$$P(X = x_i|Y= y_j) = \frac{P(X = x_i, Y=y_j)}{P(Y=y_j)} = \frac{p_{ij}}{p_{.j}}, i = 1,2,3$$为在 $Y=y_j$ 条件下随机变量X的条件分布律。同理，交换 $X$ 和 $Y$ 的位置得到的是在 $X=x_i$ 条件下随机变量 $Y$ 的条件分布律。 连续型随机变量的条件分布对于连续型的随机变量，条件分布的定义如下： 设二维随机变量 $(X,Y)$ 的概率密度函数为 $f(x,y),(X,Y)$ 关于 $Y$ 的边缘概率密度为 $f_Y(y)$ .若对于固定的 $y，f_Y(y) &gt;0$ ，则称 $\frac{f(x,y)}{f_Y(y)}$ 为在 $Y=y$ 的条件下 $X$ 的条件概率密度。记为$$f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)}$$ 有了条件概率密度(就是条件概率密度函数)，我们也可以定义出条件分布函数如下 $$\int_{-\infty}^x f_{X|Y}(x|y)dx = \int_{-\infty}^x \frac{f(x,y)}{f_Y(y)}dx$$ 上面的函数为在 $Y=y$ 的条件下 $X$ 的条件分布函数，记为 $F_{X|Y}(x|y) = P(X \le x| Y=y)$ 相互独立的随机变量两个随机变量 $X,Y$ 相互独立的充要条件如下： $F(x,y) = F_X(x)F_Y(y)$ 上面的 $F(x,y),F_X(x),F_Y(y)$ 分别是二维随机变量的联合分布函数及关于 $X$ 和 $Y$ 的边缘分布函数。 除了通过分布函数，对于具体的连续型随机变量或离散型随机变量，还可通过概率密度函数和分布律来定义相互独立的条件。 对于连续型随机变量，上面的式子等价于 $f(x,y) = f_X(x)f_Y(y)$ 式子中的 $f(x,y),f_X(x),f_Y(y)$ 分别为 随机变量 $(X,Y)$ 的条件概率密度函数和边缘概率密度函数。 对于离散型随机变量则有： $P(X = x_i, Y = y_j) = P(X=x_i)P(Y=y_j)$ 二维随机变量的函数的分布在讨论一维随机变量的分布函数的时候，也讨论了一维随机变量的函数的分布函数，同样对于二维随机变量，我们也可以讨论其函数的分布函数。下面主要讨论 $Z=X+Y$，$Z=XY$，$Z=Y/X$，$M=max(X,Y)$，$N=min(X,Y)$ 这几个函数的分布函数（$X，Y$ 为相互独立的随机变量），这里主要给出具体的公式，证明省略。 $Z = X + Y$ 的分布设 $(X,Y)$ 是二维连续型随机变量，其概率密度函数为 $f(x,y)$， $Z = X+Y$仍然为连续性随机变量，其概率密度函数为 $$f_{X+Y}(z) = \int_{-\infty}^{\infty} f(z-y,y)dy$$或$$f_{X+Y}(z) = \int_{-\infty}^{\infty} f(x,z-x)dx$$ 当 $X,Y$ 相互独立时，其边缘概率密度函数具有以下性质 $f(x,y) = f_X(x)f_Y(y)$ 因此上面的式子也可以化成下面的形式 $$f_{X+Y}(z) = \int_{-\infty}^{\infty} f_X(z-y)f_Y(y)dy$$ $$f_{X+Y}(z) = \int_{-\infty}^{\infty} f_X(x)f_Y(z-x)dx$$ $Z=XY$ 和 $Z=Y/X$ 的分布设 $(X,Y)$ 是二维连续型随机变量，其概率密度函数为 $f(x,y)$， $Z = \frac{Y}{X},Z = XY$仍然为连续性随机变量，其概率密度函数为 $$f_{Y/X}(z) = \int_{-\infty}^{\infty} |x|f(x,xz)dx$$ $$f_{XY}(z) = \int_{-\infty}^{\infty} \frac{1}{|x|}f(x,z/x)dx$$ 当 $X,Y$ 相互独立时，同样有下面的性质 $$f_{Y/X}(z) = \int_{-\infty}^{\infty} |x|f_X(x)f_Y(xz)dx$$ $$f_{XY}(z) = \int_{-\infty}^{\infty} \frac{1}{|x|}f_X(x)f_Y(z/x)dx$$ $M = max(X,Y)$ 和 $N = min(X,Y)$ 的分布讨论 $max(X,Y)$ 和 $min(X,Y)$ 的分布的时候， 一般假设 $X, Y$ 相互独立，因为这样才有下面的性质。 对于 $M = max(X,Y)$ 的分布有 $F_{max}(z) = P(M \le z) = P(X \le z, Y \le z) = P(X \le z)P(Y \le z)$ 由于 $X$ 和 $Y$ 相互独立，因此有 $F_{max}(z) = F_X(z)F_Y(z)$ 同样对 $N = min(X,Y)$ 有 $F_{min}(z) = P(N \le z) = 1 - P(N \gt z) = 1 - P(X &gt; z)P(Y&gt;z)$ 即 $F_{min}(z) = 1 - (1 - F_X(z))(1 - F_Y(z))$ 推广到 $n$ 个相互独立的随机变量有下面的性质 $M = max \lbrace X_1,X_2…,X_n \rbrace$ 及 $N = min\lbrace X_1,X_2…,X_n \rbrace$ 的分布函数分别为 $$F_{max}(z) = F_{X_1}(z)F_{X_2}(z)…F_{X_n}(z)$$ $$F_{min}(z) = 1 - (1 - F_{X_1}(z))(1 - F_{X_2}(z))…(1 - F_{X_n}(z))$$ 而当 $ X_1,X_2…,X_n $ 独立同分布的时候，上式变为如下所示 $$F_{max}(z) = [F(z)]^n$$ $$F_{min}(z) = 1 - (1 - F(z))^n$$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[概率论与数理统计知识整理(1)--一维随机变量的分布类型]]></title>
      <url>%2F2016%2F10%2F03%2F%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86(1)--%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E5%88%86%E5%B8%83%E7%B1%BB%E5%9E%8B%2F</url>
      <content type="text"><![CDATA[本文主要讲述三种离散型随机变量的分布(伯努利分布,二项分布,泊松分布)和三种连续型随机变量的分布(均匀分布,指数分布,正态分布)。 离散型随机变量的分布伯努利分布伯努利分布又名两点分布或者0-1分布，只能取两种结果，一般记为0或1。设取1的概率为 $p$，其分布规律为 $P(X=k) = p^k(1-p)^{1-k}, k = 0,1 (1&lt;p&lt;1)$ 如果试验E只有两种结果，则称试验E为伯努利试验。如果独立的进行 $n$ 次试验E，则称为n重伯努利试验。 二项分布以 $X$ 表示 $n$ 重伯努利试验中事件 A 出现的次数，则 $X$ 为一个随机变量，令 $p$ 为事件 A 出现的概率，则事件 $X$ 服从以下分布： $P(X=k) = C_n^k p^k q^{n-k}, k = 0,1,2,…,n$ 其中 $q = 1-p$,则我们称 X 服从参数为 n,p 的二项分布，记为 $X$~$b(n,p)$。其概率和根据以下公式为1 $\sum_{k=1}^nP(X=k) = \sum_{k=1}^nC_n^k p^k q^{n-k} = (p+q)^n = 1$ 泊松分布随机变量 $X$ 的所有可能取值为 0,1,2,3….,且各个取值的概率为 $P(X=k) = \frac {\lambda^ke^{-\lambda}}{k!}, k=0,1,2….$ 则我们称 $X$ 服从参数为 $\lambda$ 的泊松分布，记为 $X$~$\pi(\lambda)$ 其中 $\lambda$ 为 $X$ 的期望，即 $E(X)$ 其分布规律通过图像直观表示为 泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数，电话交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA序列的变异数、放射性原子核的衰变数等等。从上面的图像也可以看出，对于给定泊松分布的强度 $\lambda$ ,其在单位时间内发生的次数的概率有一个峰值，也就是说发生的次数很多或很少的可能性都不大，且当强度越大，其最大可能发生的次数的值也越大。 下面介绍用泊松分布来逼近二项分布的定理，也就是泊松定理。 泊松定理：设 $\lambda &gt; 0$是一个常数，$n$ 是任意正整数，设 $np = \lambda$,则对于任一非负整数 $k$，有$$\lim_{n \to \infty} C_n^kp^k(1-p)^{n-k} = \frac{\lambda^ke^{-\lambda}}{k!}$$ 即当 $n$ 很大且 $p$ 很小的时候，我们可以使用上面公式等号右边部分来逼近左边部分，从而简化计算。 连续型随机变量的分布分布函数和概率密度函数首先需要清楚，分布函数是针对随机变量的，离散或连续都可以；而概率密度函数是针对连续随机变量的。下面是这两者的定义以及联系 分布函数的定义如下： $X$ 是一个随机变量，$x$ 是任意实数，则 $X$ 的分布函数定义为 $F(x) = P(X \le x), (-\infty&lt;x&lt;\infty)$ 则对于任意实数 $x_1,x_2(x_1 &lt; x_2)$,有 $P(x_1 &lt; X \le x_2) = p(X \le x_2) - p(X \le x_1) = F(x_2) - F(x_1)$ 即如果我们知道了 $X$ 的分布函数，则可以知道 $X$ 落在任一区间上的概率，也就是说通过分布函数可以完整描述随机变量的统计规律特性。 概率密度函数定义如下： 对于分布函数 $F(X)$，假如存在 $f(x)$ 使得以下公式成立： $F(x) = \int_{-\infty}^x f(t) dt$ 则 X 为连续型随机变量，$f(x)$为随机变量的概率密度函数。$f(x)$ 具有以下性质 $f(x) \ge 0$ $\int_{-\infty}^{\infty} f(x) dx = 1$ 3.对于任意实数 $x_1,x_2(x_1 &lt; x_2)$，有$$P(x_1&lt; X \le x_2) = F(x_2) - F(x_1) = \int_{x_1}^{x_2} f(x) dx$$ 且当 $f(x)$ 在点 $x$ 连续的时候，有 $F’(x) = f(x)$ 通过上面这个性质可以推导出下面的约等式$P(x &lt;X \le x+\Delta x) \approx f(x)\Delta x$ 也就是说概率密度函数在某点的值的大小一定程度上反映了随机变量落在该点附近的概率的大小。 上面讲了随机变量的分布函数，下面讲一下随机变量的函数的分布函数，例如 $X$ 为一个随机变量，则 $Y = (X-1)^2$ 是随机变量 $X$ 的函数，且 $Y$ 也是一个随机变量， 因此 $Y$ 也有自己的分布律。下面是两个例子，其中一个是离散型随机变量，一个是连续型随机变量。 离散型随机变量的函数的分布律容易求，对于连续型随机变量的函数的分布律的求法一般是先按定义写出 $Y$ 的分布函数 $F(Y&lt;=y)$, 然后替换成 $F(g(X)&lt;=Y)$, 再转换成 $X$ 的分布函数和概率密度函数。 均匀分布若随机变量 $X$ 的概率密度函数为 $$f(x) = \begin{cases} \frac{1}{b-a} &amp;{a&lt;x&lt;b} \\ 0&amp;{其他}\end{cases}$$ 则称X在区间 $(a,b)$ 上服从均匀分布，记为 $X$~$U(a,b)$ 指数分布若随机变量 $X$ 的概率密度函数为 $$f(x) = \begin{cases} \frac{1}{\theta}e^{-x / \theta} &amp;{x&gt;0} \\ 0&amp;{其他}\end{cases}$$ 则称 $X$ 服从参数为 $\theta$ 的指数分布。 其图像如下所示: 其分布函数为 $$F(x) = \begin{cases} 1-e^{-x / \theta} &amp;{x&gt;0} \\ 0&amp;{其他}\end{cases}$$ 如果说上面的泊松分布是描述某个时间段内事件发生次数的概率分布，那么指数分布描述的就是事件发生的时间间隔的概率分布。指数分布是连续的分布，反映在其实际意义上就是时间是连续的。更详细的描述可查看这里 关于指数分布的一个有趣的性质为： $P(X&gt;s+t | X&gt;s) = P(X&gt;t)$ 该性质也称为无记忆性，假设 $X$ 是某一原件的寿命，上面的式子表示的就是该元件在使用了 s 个小时后，至少还能使用 t 个小时的条件概率。而这一条件概率又等于该元件从刚开始使用的算起至少能使用 t 个小时的概率。也就是说原件对使用过的s个小时无记忆性，这个特性与随机过程中的平稳过程非常相似，而这个特性也是指数分布有广泛应用的重要原因。 正态分布正态分布也叫高斯分布，其概率密度函数为 $$f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$ 则称 $X$ 服从参数为 $\mu, \sigma$ 的正态分布，记为$X$~$N(\mu,\sigma^2)$，而且 $\mu, \sigma$ 分别是正态分布的期望和标准差。其图像如下所示 从图像可知，当$ X = \mu $时，取值最大，也就是说随机变量落在这个值附近的概率最大，而这个值也就是正态分布的期望。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(402,316,321)--删除字符串k个字符使剩余字符串取最值]]></title>
      <url>%2F2016%2F09%2F25%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(402%2C316%2C321)--%E5%88%A0%E9%99%A4%E5%AD%97%E7%AC%A6%E4%B8%B2k%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%BD%BF%E5%89%A9%E4%BD%99%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8F%96%E6%9C%80%E5%80%BC%2F</url>
      <content type="text"><![CDATA[本文主要讲述如何解决这一类问题：给定一个含有数字或英文字母的字符串，从中删除k个字符，使得剩下的字符取得最小值或最大值。数字的大小的比较容易理解，而字母的大小则是按照其ASCII码来排列，如’abc’&gt;’abd’。 下面以 LeetCode 上的几道题目为例进行讲解：402. Remove K Digits316. Remove Duplicate Letters321. Create Maximum Number 解决这类题目的关键点是借助栈这种数据结构，遍历给出的字符串，将当前元素与栈顶元素比较大小，从而决定是否要将当前元素出栈，最后栈内剩余元素就是所需结果。这只是大致的过程，具体的步骤需要根据题目的具体要求。下面以上面的题目为例讲解 402. Remove K Digits 这个题目要求去掉给定字符串（全是数字）中的 k 个字符，使得剩余的字符串表示的数字最小。根据我们上面说到的大致流程，这道题目的解决步骤如下： 1.创建一个栈，以及记录当前已经删除的字符数量的计数器2.对于字符串中的每个字符，记为当前字符，先将其与栈顶元素比较（假如栈不为空），若当前字符小于栈顶元素，则将栈顶元素出栈，将计数器加一，重复该操作直到栈为空或栈顶元素小于当前元素或计数器等于k，然后将当前元素入栈 具体的 python 代码如下所示：123456789101112131415class Solution(object): def removeKdigits(self, num, k): """ :type num: str :type k: int :rtype: str """ stack = [] remain = len(num) - k for dig in num: while k and stack and stack[-1] &gt; dig: stack.pop() k -= 1 stack.append(dig) return ''.join(stack[:remain]).lstrip('0') or '0' 最后return语句之所以要选取前len(num) - k个字符是因为 result 中可能会有多余这个数目的字符，如对于从小到大排列的字符串就会出现这种情况，另外还需要处理掉前缀0以及当结果为空的时候返回 ‘0’ 316. Remove Duplicate Letters 的要求跟 402. Remove K Digits 类似，只是这次要求删除的是字母，而且每个字符要求出现一次且只能出现一次。 解决的思路跟上面的一样，只是因为要求每个字符出现且只出现一次，在入栈和出栈的时候需要特殊的限制条件。具体步骤入下 1.创建一个栈，记录每个字符在字符串中出现的次数的table2.对于字符串中的每个字符，先判断其是否已在栈内，若已在栈内，将table中对应该字符的计数减去1，然后跳到字符串的下一字符；若不在栈内，栈顶元素大于当前字符且table内剩余的栈顶元素的个数大于1时，将栈顶元素出栈，重复该操作直到栈为空或栈顶元素小于当前元素，然后将当前字符入栈 实现的 python 代码如下所示：123456789101112131415161718192021class Solution(object): def removeDuplicateLetters(self, s): """ :type s: str :rtype: str """ result, stored, count = [], set(), &#123;&#125; for char in s: count.setdefault(char, 0) count[char] += 1 for char in s: if char in stored: count[char] -= 1 continue else: while result and result[-1] &gt; char and count[result[-1]] &gt; 1: count[result[-1]] -= 1 stored.remove(result.pop()) stored.add(char) result.append(char) return ''.join(result) 321. Create Maximum Number 题目要求与上面两题相比不是相似性不高，但是也是利用这种思想的，题目要求从两个数组 nums1 和 nums2 中共选出k个数字，从而使得这k个数字组成的数最大。 解决方法就是先从 nums1 中选出 i 个数（0 &lt;= i &lt;= k）使得这i个数组成的数字最大，然后从 nums2 中选出 k-i 个数，同样使得这 k-i 个数组成的数字最大，最后将从两个数组中抽出的最大数字合并起来。 实现的代码如下所示：1234567891011121314151617181920212223class Solution(object): def maxNumber(self, nums1, nums2, k): """ :type nums1: List[int] :type nums2: List[int] :type k: int :rtype: List[int] """ return max(self.merge(self.single_max(nums1, i), self.single_max(nums2, k-i)) for i in xrange(k+1) if i &lt;= len(nums1) and (k-i) &lt;= len(nums2)) def single_max(self, nums, k): drop = len(nums) - k stack = [] for digit in nums: while drop and stack and stack[-1] &lt; digit: stack.pop() drop -= 1 stack.append(digit) return stack[:k] def merge(self, nums1, nums2): return [max(nums1,nums2).pop(0) for _ in xrange(len(nums1)+len(nums2))] 上面的代码中 max(num1, num2)中的 nums1 和 nums2是两个数组，比较的时候回比较两个数组的第一个元素，然后返回第一个元素较大的数组，若第一个元素相等，则比较第二个元素，依此类推；pop(0)则是删除并返回下标为0的元素，也就是第一个元素。通过这些语法可以简化代码]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[常用数学符号的 LaTeX 表示方法]]></title>
      <url>%2F2016%2F09%2F18%2F%E5%B8%B8%E7%94%A8%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7%E7%9A%84%20LaTeX%20%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95%2F</url>
      <content type="text"><![CDATA[在 Markdown 中编辑数学公式一般是使用LaTeX 来渲染和排版的，但是一些数学符号的 LaTeX 比较特殊，常常会忘掉，因此在这里特意记录这些数学符号用LaTeX 的表示方法。 指数和下标指数和下标可以用 ^ 和 _ 后加相应字符来实现,如果指数或下边多于一个字符， 那么需要用 {} 将其括起来 平方根平方根（square root）的输入命令为：\sqrt，n 次方根相应地为: \sqrt[n]。方根符号的大小由LATEX自动加以调整。也可用\surd 仅给出符号。比如： 分数分数（fraction）使用\frac{…}{…} 排版。 积分、求和、连乘积分运算符用\int 来生成。求和运算符由\sum 生成。乘积运算符由\prod 生成。上限和下限用^ 和_来生成，类似于上标和下标 表达式的上、下方画出水平线命令\overline 和\underline 在表达式的上、下方画出水平线。比如 表达式的上、下方给出一水平的大括号命令\overbrace 和\underbrace 在表达式的上、下方给出一水平的大括号。 比如： 向量上方的箭头向量通常用上方有小箭头（arrow symbols）的变量表示。这可由\vec 得到。另两个命令\overrightarrow 和\overleftarrow在定义从A 到B 的向量时非常有用。 其他一些数学符号 参考：http://mohu.org/info/symbols/symbols.htm]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 中的可迭代对象(iterable)、迭代器(iterator)与生成器(generator)]]></title>
      <url>%2F2016%2F09%2F08%2Fpython%20%E4%B8%AD%E7%9A%84%E5%8F%AF%E8%BF%AD%E4%BB%A3%E5%AF%B9%E8%B1%A1(iterable)%E3%80%81%E8%BF%AD%E4%BB%A3%E5%99%A8(iterator)%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8(generator)%2F</url>
      <content type="text"><![CDATA[本文主要讲述python中的几个概念：可迭代对象(iterable)、迭代器(iterator)与生成器(generator)。 可迭代对象(iterable) 与 迭代器(iterator)对于 string、list、dict、tuple 等这类容器对象，可以使用 for 循环对其进行遍历。像这种可以被遍历的对象被称为可迭代对象。 通过 for 语句对遍历可迭代对象时，实际上是调用可迭代对象内部的 __iter__() 方法（因此一个可迭代对象必须要实现 __iter__() 方法），调用了这个方法会返回一个迭代器(iterator)，通过迭代器便可遍历可迭代对象。见下面的例子： 12345678910111213&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; y = iter(x)&gt;&gt;&gt; z = iter(x)&gt;&gt;&gt; next(y)1&gt;&gt;&gt; y.next()2&gt;&gt;&gt; next(z)1&gt;&gt;&gt; type(x)&lt;class 'list'&gt;&gt;&gt;&gt; type(y)&lt;class 'list_iterator'&gt; 这里 x 是一个列表，是一个可迭代对象。y 和 z 是两个独立的迭代器，迭代器内部持有一个状态，该状态用于记录当前迭代所在的位置，以方便下次迭代的时候获取正确的元素。 迭代器也分具体的迭代器类型，比如 list_iterator，set_iterator。iter(x)语句实际上是调用了 x 内部的 __iter__ 方法的, 调用 __iter__ 方法后会返回一个迭代器，由于迭代器内部实现了 next 方法 （python2中是 next 方法，python3是 __next__ 方法,一个迭代器必须实现此方法），因此可通过 next() 方法来遍历可迭代对象。 因此，执行下面语句： 123 x = [1, 2, 3]for elem in x: ... 相当于以下流程 上图中调用 next() 方法直到没有后续元素时，next() 会抛出一个 StopIteration 异常，通知for语句循环结束。如 12345678910&gt;&gt;&gt; a = [1,3]&gt;&gt;&gt; b = iter(a)&gt;&gt;&gt; b.next()1&gt;&gt;&gt; next(b)3&gt;&gt;&gt; next(b)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 上面说的都是 python 自带的容器对象，它们都实现了相应的迭代器方法，那如果是自定义类需要遍历怎么办？ 方法很简单，假如我们需要自定义一个有遍历功能的类 IterClass，那么只需要在这个类的内部实现一个 __iter__(self) 方法，使其返回一个带有 __next__(self) 方法的对象就可以了。如果你在 IterClass 刚好也定义了 __next__(self) 方法（一般使用迭代器都会定义），那在 __iter__() 里只要返回 self 就可以。下面是具体的实例： 1234567891011121314151617181920class IterClass: def __init__(self, max): self.max = max def __iter__(self): self.a = 0 self.b = 1 return self def next(self): fib = self.a if fib &gt; self.max: raise StopIteration self.a, self.b = self.b, self.a + self.b return fibif __name__ == '__main__': fib = IterClass(10) for i in fib: print i 上面输出的结果为:12345670112358 上面的代码定义了一个 IterClass 类，用于生成 fibonacci 序列。用for遍历时会逐个打印生成的fibonacci数，max是生成的fibonacci序列中数字大小的上限。 在类的实现中，定义了一个 __iter__(self) 方法，这个方法是在遍历时被 iter() 调用，返回一个迭代器。因为在遍历的时候，是直接调用 python 的内置函数 iter()，由 iter() 通过调用 __iter__(self) 获得对象的迭代器。 有了迭代器，就可以逐个遍历元素了。而逐个遍历的时候，也是使用 python 的内置 的 next() 函数，next() 函数通过调用对象的 next(self) 方法（python 3 为 __next__(self) 方法）对迭代器对象进行遍历。因为同时实现 __iter__(self) 和 next(self) ， 所以 IterClass 既是可迭代对象，也是迭代器，在实现 __iter__(self) 的时候，直接返回self就可以。 为了更好地理解，对上面的内容的小结如下：在循环遍历自定义容器对象时,会使用 python 内置函数 iter() 调用遍历对象的 __iter__(self) 获得一个迭代器,之后再循环对这个迭代器使用 next() 调用迭代器对象的 next(self) 或 __next__(self)。__iter__ 只会被调用一次,而 __next__ 会被调用 n 次。 生成器(generator)生成器其实是一种特殊的迭代器，不过这种迭代器更加简洁和高效,它自动创建了 __iter__() 和 next() 方法（因此生成器其实既是一个可迭代对象，也是一个迭代器）, 除了创建和保存程序状态的自动方法,当发生器终结时,还会自动抛出 StopIteration 异常。它不需要再像上面的类一样写 __iter__() 和 next () 方法了，只需要一个 yiled 关键字。生成器一定是迭代器（反之不成立）。 一个带有关键词 yield 的函数就是一个生成器,它和普通函数不同,生成一个 generator 看起来像函数调用,但不会执行任何函数代码,直到对其显式或隐式地调用 next() (在 for 循环中会隐式自动调用 next()) 才开始执行。虽然执行流程仍按函数的流程执行,但每执行到一个 yield 语句就会中断,并返回一个迭代值,下次执行时从 yield 的下一个语句继续执行。看起来就好像一个函数在正常执行的过程中被 yield 中断了数次,每次中断都会通过 yield 返回当前的迭代值（yield暂停一个函数，next()从其暂停处恢复其运行）。见下面的例子： 123456789101112&gt;&gt;&gt; def reverse(data):... for index in range(len(data)-1, -1, -1):... yield data[index]... &gt;&gt;&gt; for char in reverse('hello'):... print(char)... olleh 用生成器来实现上面的斐波那契数列的例子是： 123456789def fib(): prev, curr = 0, 1 while True: yield curr prev, curr = curr, curr + prev&gt;&gt;&gt; f = fib()&gt;&gt;&gt; list(itertools.islice(f, 0, 10))[1, 1, 2, 3, 5, 8, 13, 21, 34, 55] 生成器在Python中是一个非常强大的编程结构，可以用更少地中间变量写流式代码，此外，相比其它容器对象它更能节省内存和CPU，它也可以用更少的代码来实现相似的功能。如果构造一个列表的目的仅仅是传递给别的函数, 那么就可以用生成器来代替。但凡看到类似：12345def something(): result = [] for ... in ...: result.append(x) return result 都可以用生成器函数来替换：123def iter_something(): for ... in ...: yield x 只需要在接收函数返回值的时候将其转为 list 类型即可。 另外对于生成器，python还提供了一个生成器表达式(generator expression)：类似与一个 yield 值的匿名函数。表达式本身看起来像列表推导式, 但不是用方括号而是用圆括号包围起来, 它返回的是一个生成器对象而不是列表对象。见下面的例子： 1234567891011&gt;&gt;&gt; a = (i*i for i in xrange(5))&gt;&gt;&gt; for num in a:... print num...014916&gt;&gt;&gt; a&lt;generator object &lt;genexpr&gt; at 0x02B707D8&gt; 参考：https://segmentfault.com/a/1190000002900850http://foofish.net/blog/109/iterators-vs-generators]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 中 * 与 ** 的参数传递]]></title>
      <url>%2F2016%2F09%2F01%2Fpython%20%E4%B8%AD%E6%98%9F%E5%8F%B7%E5%8F%82%E6%95%B0%E4%BC%A0%E9%80%92%2F</url>
      <content type="text"><![CDATA[在 python 中，经常可以看到定义函数或调用函数时f(*arg)和f(**args)类型的参数，本文主要讲述这两个形式的参数的含义以及应用。 定义函数时参数加上 * 和 **首先这两个类型的参数都表示不确定具体参数个数，怎么理解这句话呢？通常在定义函数的时候，定义了几个参数，调用是也要传入几个参数(默认参数除外，可传可不传)，但是只要在定义函数的时候将参数写成*或**的形式，就可以传入多个参数。如下面的例子： 12345678910111213141516&gt;&gt;&gt; def a(*args):... print type(args)... for i in args:... print i...&gt;&gt;&gt; a(1,2,3,4)&lt;type 'tuple'&gt;1234&gt;&gt;&gt; a([3,2,3])&lt;type 'tuple'&gt;[3, 2, 3]&gt;&gt;&gt; a()&lt;type 'tuple'&gt; 从上面的例子可以看到，通过 * 声明的参数在调用是可以传入0~n个参数，且不管传入的参数为何类型，在函数内部都被存放在以形参名为标识符的tuple中，无法进行修改。 同理,通过**声明的参数也是可以传入多个参数，但是传入的参数类型需要为k1=v1,k2=v2.....的类型,且参数在函数内部将被存放在以形式名为标识符的dictionary中，这种方法在需要声明多个默认参数的时候特别有用。 123456789101112131415&gt;&gt;&gt; def a(**args):... print type(args)... print args... for k, v in args.items():... print k,v... print args['k1']...&gt;&gt;&gt; a(k1=1, k2=2, k3=3, k4=4)&lt;type 'dict'&gt;&#123;'k3': 3, 'k2': 2, 'k1': 1, 'k4': 4&#125;k3 3k2 2k1 1k4 41 下面显示了如何通过这两个参数使代码变得简洁(注意 *args 和 **kwargs 可以同时在函数的定义中,但是 args 必须在 *kwargs 前面) 1234567891011def sum(*values, **options): s = 0 for i in values: s = s + i if "neg" in options and options["neg"]: s = -s return ss = sum(1, 2, 3, 4, 5) # returns 15s = sum(1, 2, 3, 4, 5, neg=True) # returns -15s = sum(1, 2, 3, 4, 5, neg=False) # returns 15 除此之外，*args 和 **kwargs 也可以和命名参数一起混着用。命名参数首先获得参数值，然后所有的其他参数都传递给 *args 和 **kwargs .命名参数在列表的最前端.例如: def table_things(titlestring, *args, **kwargs) 调用函数时参数加上 * 和 **除了在定义函数时可以加上 *或**, 还可以在调用函数时加上*或**, 表示将输入的 集合（序列）类型参数拆开，见下面的例子： 1234567891011121314def sum(a, b): return a + b# values = set()# values.add(1)# values.add(2)values = (1, 2)# values = [1,2]# values = &#123;1:3, 2:4&#125;s = sum(*values) 无论是集合、列表、元组还是字典, 在作为参数输入时加上*，表示将里面的元素拆开，然后一个个传进去，所以上面执行的结果相当于s = sum(1, 2),由于字典比较特殊，传入参数时只会拆开 key 然后传入。下面结合上面定义函数时参数加上*来讲述这个 * 的含义,例子如下：12345678&gt;&gt;&gt; def sum(*args):... print args[0]...&gt;&gt;&gt; val = (1,2)&gt;&gt;&gt; sum(val)(1, 2)&gt;&gt;&gt; sum(*val)1 上面的sum函数输出传入的第一个参数，由于sum(val)将整个val元组作为参数传入，相当于sum((1,2)),所以会输出(1,2);而sum(*val)则会将val拆开，相当于sum（1,2），因此输出为 1 而在调用参数时加上**,作用也是将传入的参数拆开，只是输入的参数必须为字典，且每个 key 必须要为函数的某个形参，key对应的value为该参数的值。详见下面的例子: 12345678&gt;&gt;&gt; def parrot(voltage, state='a stiff', action='voom'):... print("-- This parrot wouldn't", action, end=' ')... print("if you put", voltage, "volts through it.", end=' ')... print("E's", state, "!")...&gt;&gt;&gt; d = &#123;"voltage": "four million", "state": "bleedin' demised", "action": "VOOM"&#125;&gt;&gt;&gt; parrot(**d)-- This parrot wouldn't VOOM if you put four million volts through it. E's bleedin' demised ! 上面的parrot(**d)相当于parrot(voltage = &quot;four million&quot;, state = &quot;bleedin&#39; demised&quot;, action = &quot;VOOM&quot;)通过上面的方法，可以先将所有参数用字典封装，再通过 ** 传递。 结合定义函数时参数加上** 有以下例子： 12345678&gt;&gt;&gt; def b(**args):... for k,v in args.items():... print k,v...&gt;&gt;&gt; val = &#123;'keke':1, 'hehe':2&#125;&gt;&gt;&gt; b(**val)keke 1hehe 2 参考：http://stackoverflow.com/questions/2921847/what-does-the-star-operator-mean-in-pythonhttps://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Supervisor的简介与使用]]></title>
      <url>%2F2016%2F08%2F23%2FSupervisor%E7%9A%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[Supervisor 是 Linux 下一个进程管理的工具，主要的功能包括让程序自动启动、程序奔溃后自动重启，指定进程的数目等。本文主要讲述Supervisor在Linux下的安装与使用。 安装由于 Supervisor 是用 python 写的，因此推荐利用 easy_install supervisor或pip install supervisor 进行安装。除此之外，还可通过 Linux 的包管理命令进行安装（源中需要包含这个包），如 Centos 下可通过 yum install supervisor进行安装，Ubuntu 下可以通过apt install supervisor安装。 配置Supervisor 的配置文件就只有一个，在安装完成后通过echo_supervisord_conf &gt; /etc/supervisord.conf 将创建一个默认的配置文件/etc/supervisord.conf,当然也可以指定配置文件在其他目录下。配置文件以[]来隔离每部分的配置内容，并且以;为注释符号。 因为 Supervisor 由三部分组成：supervisord、supervisorctl、inet_http_server。因此配置文件也分别根据这三部分阐述。需要注意的是supervisorctl、inet_http_server并非是必须要配置的，这两个均是连接supervisord的客户端，用于观察和管理 supervisord 监控的程序。 supervisordsupervisord 是Supervisor的核心，主要用与启动程序，在程序奔溃时自动重启，设定程序的进程数目、输出的日志文件路径等。 supervisored 有多个参数，下面主要讲述其中几种较为重要的最简配置，每个参数的含义可参考官方文档。 下面就是supervisord配置的一个简单例子1234567[supervisord]logfile=/var/log/supervisor/supervisord.log ;日志文件的目录pidfile=/var/run/supervisord.pid ; pid文件目录; logfile_maxbytes = 50MB ;默认的日志文件的大小; loglevel = info ; 默认的日志的记录等级; umask = 002; 默认的进程umask; nodaemon = false ;默认在后台启动，若为true则在前台启动 上面注释掉的配置项为 supervisord 的默认配置项，可以不配置。 除了配置 supervisord 外，还需要配置被其管理的程序。详细的参数可见官方文档，下面是一个简单的例子123456789101112[program:robot] ; 标示一个程序[program:XXX],XXX为自定义的程序名称command = /usr/bin/xvfb-run python /home/amazon/v0/Robot.py;运行程序所需命令;autostart=true ;默认跟随supervisord启动而启动autorestart=true ;程序退出后自动重启; startretries = 3; 程序出错时，连续重启的最大次数，超过该次数后，进程进入FATAL转态; startsecs = 1; 程序启动后多少秒内认为其启动成功; numprocs = 1 ;启动的进程数目，默认为1; priority = 999 ; 程序的优先级，默认为999，该值越小，表示优先级越高user=root ;程序启动的用户，只用root用户才能指定这一项；不指定时该值为启动supervisord的用户stdout_logfile = /home/amazon/log/Robot.log ;存储程序标准输出流的文本文件stderr_logfile = /home/amazon/log/Robot_err.log ; 存储程序出错时错误提示的文件; stopasgroup = false;以进程组的方式停止进程，默认为false，以上面为例，假如为false时，停止该程序时只会停止 python运行的程序，而不会停止 xvfb 程序 上面注释的配置项为程序默认的配置，可以不配置。上面给出的supervisord和program为最简配置，仅配置这两项就可以运行supervisor。运行方式为 supervisord -c /etc/supervisord.conf, -c参数指定了配置文件的路径，不指定该参数时会以一定的路径顺序寻找配置文件，并且会抛出warning，因此建议启动时要带有此参数。 上面的配置虽然能启动这些程序，但是当supervisord管理多个程序时，需要关闭或开启其中的一个程序就必须关闭supervisord，然后修改配置文件并重启。为了单独管理这些程序，并直观看到每个程序的运行状态，就需要配置下面的supervisorctl和inet_http_server。 inet_http_serverinet_http_server是supervisord 内置的一个http浏览器，用于查看、管理每个程序的运行状态，配置项如下： 1234[inet_http_server]port = 110.64.55.128:9001username = XXXXpassword = XXXX 上面的配置应该比较容易理解，访问port后输入用户名和密码验证身份后即可观察到程序运行的状态，下图为访问时观察到的效果。 上图可以看到每个程序的运行状态，pid以及运行时长，还可以改变程序的运行状态。 supervisorctlsupervisorctl的功能与inet_http_server一样，只是inet_http_server是有界面的，而supervisorctl是在命令行下使用的，配置项如下： 1234[supervisorctl]serverurl = http://110.64.55.128:9001 ;http服务器的地址username = XXXX ;与 [inet_http_server] 配置项的username相同password = XXXX ;与 [inet_http_server] 配置项的password相同 通过 supervisorctl即可观察到程序运行的状态，如下图所示 同时可以通过 supervisorctl start|stop|restrt XXX来启动、停止、重启程序XXX，其中XXX为配置[program:XXX]指定的名称。 下面是综合以上所说的完整的supervisord.conf配置文件12345678910111213141516171819[supervisord]logfile=/var/log/supervisor/supervisord.log pidfile=/var/run/supervisord.pid [program:robot] command = /usr/bin/xvfb-run python /home/amazon/v0/Robot.pyautorestart=true stdout_logfile = /home/amazon/log/Robot.log stderr_logfile = /home/amazon/log/Robot_err.log [inet_http_server]port = 110.64.55.128:9001username = XXXXpassword = XXXX[supervisorctl]serverurl = http://110.64.55.128:9001 username = XXXX password = XXXX 从上面可知，既然supervisorctl提供的功能和inet_http_server的相同，那么是否可以不启动http服务器，仅仅通过supervisorctl进行观察呢? 答案是可以的，但是要通过 unix socket 与 supervisord 通信，将上面的[inet_http_server]部分改成[unix_http_server]，并修改[supervisorctl]的serverurl部分，完整的配置文件如下。 12345678910111213141516171819[supervisord]logfile=/var/log/supervisor/supervisord.log pidfile=/var/run/supervisord.pid [program:robot] command = /usr/bin/xvfb-run python /home/amazon/v0/Robot.pyautorestart=true stdout_logfile = /home/amazon/log/Robot.log stderr_logfile = /home/amazon/log/Robot_err.log [unix_http_server]file=/var/run/supervisor.sock ; (the path to the socket file)username = XXXXpassword = XXXX[supervisorctl]serverurl = unix:///var/run/supervisor.sockusername = XXXX password = XXXX 使用综上，supervisor的一般的使用方法为如下： 1）配置好需要启动的程序2）通过supervisord -c /etc/supervisord.conf启动supervisord3）通过supervicorctl和日志文件查看每个程序状态，通过supervicorctl start|stop|restart XXX在不影响其他程序的情况下改变某个程序的运行状态。 更详细的内容请参考官方文档]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL的锁机制]]></title>
      <url>%2F2016%2F08%2F16%2FMySQL%E7%9A%84%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
      <content type="text"><![CDATA[在计算机科学中，锁是在执行多线程时用于强行限制资源访问的同步机制，即用于在并发控制中保证对互斥要求的满足。 本文主要以MySQL为例，讲述几个锁的概念(行级锁、页级锁、表级锁、共享锁、排它锁等)，这些概念的范畴不限于MySQL，在并发系统上均有应用。 行级锁，页级锁，表级锁在DBMS中，可以按照锁的粒度把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。 行级锁行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。 特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。BDB支持页级锁 特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 常用存储引擎及其锁机制 MyISAM和MEMORY采用表级锁(table-level locking) BDB采用页面锁(page-level locking)或表级锁，默认为页面锁 InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁 Innodb中的行锁与表锁前面提到过，在Innodb引擎中既支持行锁也支持表锁，那么什么时候会锁住整张表，什么时候或只锁住一行呢？ InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 值得注意的是，DBMS 对于主键会自动生成唯一索引，所以主键也是一个特殊的索引。即通过主键进行查询也能实现行级锁。 在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。 行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁。行级锁的缺点是：由于需要请求大量的锁资源，所以速度慢，内存消耗大。 行级锁与死锁MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 在MySQL中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。 在UPDATE、DELETE操作时，MySQL不仅锁定WHERE条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的next-key locking。 当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。 发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。 避免死锁如何避免死锁，这里只介绍常见的三种 1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率 共享锁，排它锁，意向锁行级锁分为共享锁和排他锁两种，下面将详细介绍共享锁及排他锁的概念、使用方式及注意事项等。 共享锁(Share Lock，SLock)共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。 如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。 语法：SELECT ... LOCK IN SHARE MODE; 在查询语句后面增加LOCK IN SHARE MODE，Mysql会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。 排他锁（eXclusive Lock，XLock）排他锁又称写锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。 语法：SELECT ... FOR UPDATE; 在查询语句后面增加FOR UPDATE，Mysql会对查询结果中的每行都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。 意向锁（Intent Lock）InnoDB还有两个表锁： 意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说给一个数据行加共享锁前必须先取得该表的IS锁 意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁 意向锁是InnoDB自动加的，不需要用户干预。 对于insert、update、delete，InnoDB会自动给涉及的数据加排他锁（X）；对于一般的Select语句，InnoDB不会加任何锁，事务可以通过以下语句给显示加共享锁或排他锁。 共享锁：SELECT ... LOCK IN SHARE MODE; 排他锁：SELECT ... FOR UPDATE; 乐观锁，悲观锁数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。 乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 无论是悲观锁还是乐观锁，都是人们定义出来的概念，可以认为是一种思想。其实不仅仅是关系型数据库系统中有乐观锁和悲观锁的概念，像memcache、hibernate、tair等都有类似的概念。 针对于不同的业务场景，应该选用不同的并发控制方式。所以，不要把乐观并发控制和悲观并发控制狭义的理解为DBMS中的概念，更不要把他们和数据中提供的锁机制（行锁、表锁、排他锁、共享锁）混为一谈。其实，在DBMS中，悲观锁正是利用数据库本身提供的锁机制来实现的。 悲观锁悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法。 悲观锁，正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度(悲观) ，因此，在整个数据处理过程中，将数据处于锁定状态。 悲观锁的实现，往往依靠数据库提供的锁机制 （也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据） 在数据库中，悲观锁的流程如下：（1）在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。（2）如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。（3）如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 下面讲述在MySQL InnoDB中使用悲观锁，要使用悲观锁，我们必须关闭MySQL数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。 123456789101112//0.关闭自动提交属性set autocommit=0;//1.开始事务begin;/begin work;/start transaction; //2.查询出商品信息select status from t_goods where id=1 for update;//3.根据商品信息生成订单insert into t_orders (id,goods_id) values (null,1);//4.修改商品status为2update t_goods set status=2;//4.提交事务commit;/commit work; 上面的查询语句中，我们使用了select…for update的方式，这样就通过排他锁的实现了悲观锁。此时在t_goods表中，id 为 1 的那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。 优点与不足：悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会；另外，在只读型事务处理中由于不会产生冲突，也没必要使用锁，这样做只能增加系统负载；还有会降低了并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数 乐观锁乐观并发控制（又名“乐观锁”，Optimistic Concurrency Control，缩写“OCC”）是一种并发控制的方法。 乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。 相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现乐观锁的方式就是记录数据版本。实现数据版本可以通过使用版本号或使用时间戳。实现流程如下： （1）为数据表增加一个表示版本标识的字段，用于存储版本号或时间戳（2）当读取数据时，将版本标识的值一同读出（3）当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的版本标识进行比对，如果数据库表当前版本标识与第一次取出来的版本标识值相等，则同时更新数据和版本号，否则认为是过期数据，返回错误给用户处理 下图为该流程的示意过程： 参考:MySQL中的行级锁,表级锁,页级锁MySQL中的共享锁与排他锁深入理解乐观锁与悲观锁mysql悲观锁总结和实践mysql乐观锁总结和实践]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过Selenium和PhantomJS抓取带JS的网页]]></title>
      <url>%2F2016%2F08%2F10%2F%E9%80%9A%E8%BF%87Selenium%E5%92%8CPhantomJS%E6%8A%93%E5%8F%96%E5%B8%A6JS%E7%9A%84%E7%BD%91%E9%A1%B5%2F</url>
      <content type="text"><![CDATA[爬虫一般通过获取网页的源码，然后通过正则表达式或html解释器获取所需的信息，但是有的网页，不能直接通过 linux下的 wget 命令、或者使用 Python 中的requests.get这样的函数库来直接获取其真正展现给用户的信息，因为里面包含有JavaScript脚本,而该JS和页面数据的生成相关，需要通过Firefox、Chrome等浏览器渲染后才能得到想要看的结果。 如亚马逊的商品列表 https://www.amazon.com/s/ref=nb_sb_noss_1?url=search-alias%3Daps&amp;field-keywords=lightning+cable 通过上面提到的方法直接获取其网页源码是无法获取每个商品的标题信息的。 一般来说，对于这种网页如下两种方案： 通过Selenium启动真正的浏览器（如：IE、Firefox）来打开该网页，然后调用webdriver获取想要的页面元素。 通过浏览器渲染引擎解释网页，能够让其解析网页并执行网页中需要初始化JS，然后将JS、CSS等执行后的HTML代码输出出来。 启动真正的浏览器，可能带来两个问题：一个是需要的时间较长，另一个是UI自动化易受干扰、不够稳定。因此本文主要讲述通过第二种方法，也就是不启动浏览器进行获取这种网页上的信息。 主要用到的库是Selenium，通过selenium 中的 PhantomJS 作为webdriver能够不启动浏览器来解释js文件并获取其解释后的源码。下面以上面的亚马逊商品列表页作为例子，通过PhantomJS来获取其页面上的商品标题。 首先需要在官网下载PhantomJS的可执行文件。 然后获取页面 https://www.amazon.com/s/ref=nb_sb_noss_1?url=search-alias%3Daps&amp;field-keywords=lightning+cable 上的商品列表的python代码如下：12345678910111213141516171819# -*- coding: utf-8 -*-# @Author: LC# @Date: 2016-07-30 10:08:31# @Last modified by: LC# @Last Modified time: 2016-08-22 17:08:46# @Email: liangchaowu5@gmail.comfrom selenium import webdriverfrom bs4 import BeautifulSouptarget_url = r'https://www.amazon.com/s/ref=nb_sb_noss_1?url=search-alias%3Daps&amp;field-keywords=lightning+cable'driver = webdriver.PhantomJS(executable_path = r'H:/PythonModule/phantomjs/phantomjs-2.1.1-windows/bin/phantomjs.exe')driver.get(target_url)text = driver.page_source # 获取解释后的网页源码soup = BeautifulSoup(text, 'lxml')titles = soup.find_all('h2')for title in titles: print title.get('data-attribute', '')driver.quit() 上面的代码首先通过指定本地的PhantomJS的可执行文件可解释目标url，获取其网页html代码，然后通过BeautifulSoup提取网页代码中的商品标题 参考：http://smilejay.com/2013/12/try-phantomjs-with-selenium/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Segment Tree 简介]]></title>
      <url>%2F2016%2F08%2F05%2FSegment%20Tree%20%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[简介本文主要通过实际例子介绍segment tree这种数据结构及其应用。以LeetCode上的一道题目 307. Range Sum Query - Mutable 为例说明。 这道题目307. Range Sum Query - Mutable要求求数组的区间和，但是有个额外条件，就是会进行多次数组区间求和以及数组元素的更新的操作。 从正常的思路出发，每次求和的时间复杂度为$O(n)$， 更新数组元素的时间复杂度为$O(1)$, 因此总体的时间复杂度为 $O(n)$。而参考 303. Range Sum Query - Immutable 可以实现求和的时间复杂度为$O(1)$, 但更新数组元素的时间复杂度为$O(n)$，所以总体的时间复杂度也是 $O(n)$。 上面两种方法的总体时间复杂度均为$O(n)$, 但是通过我们下面要介绍的Segment Tree，能够将求和以及更新数组元素操作的时间复杂度均变为 $O(log_2n)$。 Segment Tree是一棵二叉树，其特点为叶子节点个数与数组的长度相同 从左到右依次为数组中下标从小到大的元素的值，父节点的值为其左右的叶子节点的值的和。如下图是一个简单的例子 因此可以看到每个非叶子节点的值均是代表了数组某个区间的和。下面分别讲述如何构造这棵树，更新某个元素的值以及对特定区间求和。 建树虽然逻辑上是一棵二叉树，但是实际存储时可以通过数组来实现，通过父子节点的下标的数值关系可以访问父节点的子节点。然后需要求出数组的大小，因为这是一棵满二叉树（full binary tree，具体定义见下），而且数组下标必须是连续的，因此需要的最大空间为${\displaystyle \sum _{k=0}^{m}2^k}$,其中m为二叉树的高度(从0开始计算，如上图的高度为3)。 具体实现则通过递归，每次记录当前的节点的下标以及表示的数组的范围，如下为建树的python代码,其中seg_tree为建立的segment tree，nums为原数组，curr 为segmen tree中当前节点的下标，start、end 为以 curr 包含的 nums 数组的下标范围。 12345678def build_tree(start, end, curr): if start &gt; end: return if start == end: seg_tree[curr] = nums[start] else: mid = start + (end - start)/2 seg_tree[curr] = build_tree(start, mid, curr*2+1) + build_tree(mid+1, end, curr*2+2) return seg_tree[curr] 更新元素更新元素需要更新两个地方，一是原数组对应的下标的值，另外一个是包含了这个元素的segment tree中的节点的值。具体也是通过递归实现，下面是更新segment tree中所有包含原数组下标为 idx 的元素的节点的值的python代码， diff是下标为idx的新值与旧值之差。可见时间复杂度为$O(log_2n)$,n为原数组元素的个数。 12345678def update_sum( start, end, idx, curr, diff): seg_tree[curr] += diff if start == end: return mid = start + (end - start)/2 if start &lt;= idx &lt;= mid: update_sum(start, mid, idx, curr*2+1, diff) else: update_sum(mid+1, end, idx, curr*2+2, diff) 求区间和求区间和也是通过递归实现，关键在于根据当前节点表示的范围以及需要求的区间的范围的关系进行求和。下面是实现的求区间[qstart, qend]的和的python代码。可见时间复杂度为$O(log_2n)$,n为原数组元素的个数。 12345678def get_sum(start, end, qstart, qend, curr): mid = start + (end - start)/2 if qstart &gt; end or qend &lt; start: return 0 elif start &gt;= qstart and end &lt;= qend: return seg_tree[curr] else: return get_sum(start, mid, qstart, qend, curr*2+1) + get_sum(mid+1, end, qstart, qend, curr*2+2) 实际例子下面结合上面讲述的三个步骤以及LeetCode上的题目307. Range Sum Query - Mutable 给出完整的AC代码入下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class NumArray(object): def __init__(self, nums): """ initialize your data structure here. :type nums: List[int] """ n = len(nums) if n == 0: return max_size = 2 * pow(2, int(math.ceil(math.log(n, 2)))) - 1 self.seg_tree = [0 for i in xrange(max_size)] self.nums = nums[:] self.build_tree(0, n-1, 0) def build_tree(self, start, end, curr): if start &gt; end: return # empty list if start == end: self.seg_tree[curr] = self.nums[start] else: mid = start + (end - start)/2 self.seg_tree[curr] = self.build_tree(start, mid, curr*2+1) + self.build_tree(mid+1, end, curr*2+2) return self.seg_tree[curr] def update(self, i, val): """ :type i: int :type val: int :rtype: int """ diff = val - self.nums[i] self.nums[i] = val self.update_sum(0, len(self.nums)-1, i, 0, diff) def update_sum(self, start, end, idx, curr, diff): self.seg_tree[curr] += diff if start == end: return mid = start + (end - start)/2 if start &lt;= idx &lt;= mid: self.update_sum(start, mid, idx, curr*2+1, diff) else: self.update_sum(mid+1, end, idx, curr*2+2, diff) def sumRange(self, i, j): """ sum of elements nums[i..j], inclusive. :type i: int :type j: int :rtype: int """ return self.get_sum(0, len(self.nums)-1, i, j, 0) def get_sum(self, start, end, qstart, qend, curr): mid = start + (end - start)/2 if qstart &gt; end or qend &lt; start: return 0 elif start &gt;= qstart and end &lt;= qend: return self.seg_tree[curr] else: return self.get_sum(start, mid, qstart, qend, curr*2+1) + self.get_sum(mid+1, end, qstart, qend, curr*2+2)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(207,210)--拓扑排序(Topological Sort)]]></title>
      <url>%2F2016%2F07%2F27%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(207%2C210)--%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%2F</url>
      <content type="text"><![CDATA[拓扑排序是有向图中一种较常用的排序方法，本文主要以LeetCode上的两道题目 207. Course Schedule 和 210. Course Schedule II 讲述如何拓扑排序的概念与使用方法。 LeetCode 上的两道题目是关于拓扑排序（Topological Sort）比较经典的两道题目，大意就是有0~(n-1)的n门课程需要修，但是有部分课程需要先修了别的课程才能修，问是否能修完所有课程。从题意可知，将这些课程及依赖关系表示成一个有向图，那么当图中存在闭环时是不可能修完所有课程的，因为形成闭环的几门课程相互依赖，无法选择第一门开始修的课程。 因此题目就是要求判断一个有向图中是否存在闭环。 最直观的思路就是通过深度优先搜索以每个点为起点遍历，判断每次的遍历中是否存在闭环，若存在则无法修完，反之则可以。 针对207. Course Schedule，利用上面的思路实现的代码如下所示12345678910111213141516171819202122232425262728class Solution(object): def canFinish(self, numCourses, prerequisites): """ :type numCourses: int :type prerequisites: List[List[int]] :rtype: bool """ course = [[0 for i in xrange(numCourses)] for j in xrange(numCourses)] for s in prerequisites: course[s[0]][s[1]] = 1 visited = [0 for i in xrange(numCourses)] for i in xrange(numCourses): if self.dfs(course, i, visited)==False: return False return True def dfs(self, course, num, visited): for i in xrange(len(course[num])): if course[num][i] == 1: visited[num] = 1 if visited[i] == 1: return False else: if self.dfs(course, i, visited) == False: return False visited[num] = 0 这个方法的时间复杂度是$O(n^2)$,n是图中所有点的数目。在提交时会提示TLE。 其实解决这种问题还有更好的方法，就是下面要介绍的拓扑排序，拓扑排序实际上返回的结果是有向图中所有点依据有向边指向顺序的排列而成的点。下面是拓扑排序的两个例子，第一个是合法的拓扑排序的结果，第二个是非法的拓扑排序结果。 那么该如何获得一个有向图的拓扑排序的结果？ 最直观的方法步骤如下（1） 找到图中入度（indegree）为0的点，然后记录这个点并删除这个点的所有出度（outdegree），也就是连接了这个点的其他点的入度（2） 检查图中是否有入度为0的点，如果有重复步骤（1） 重复步骤（1）-（2）直至无法找到入度为0的点，此时如果记录的点的数目与所有点的数目相同，那么说明图中不存在闭合的环，反之则存在。 上面这种方法虽然比较直观，但是每次找到入度为0的点的时间复杂度是$O(n)$,因此总的时间复杂度是$O(n^2)$，n为图中所有点的数目。下面介绍一种利用队列将时间复杂度降为$O(n+e)$,e为图中所有边的数目。 这种方法特殊的地方在于利用了一个队列存储当前入度为0的点，每次队首元素出列时，将队首元素出度相连的所有点的入度减去1。因此还需要一个额外的空间存储每个点出度连接的所有点。整体的数据结构如下： 图中的In-Degree array就是存储每个点的入度数的队列，而Adjacency list则是存储每个点通过出度连接的所有点的数据结构。 这个算法的具体流程如下：(1)遍历有向图中所有的边可以构造出上面提到的两个数据结构，时间复杂度为$O(e)$(2)从In-Degree array中找所有点中入度数为0的点，构成初始队列，时间复杂度$O(n)$(3)通过队首出列，调整与队首元素出度相连的所有点的入度数，并检查这些点的入度数是否为0，若是则入队列(4)重复步骤(3)直至队列为空，此时若从队列中出列的所有点的数目为原来有向图中所有点的数目，则图中不存在闭合的环，且出列的顺序是一种可行的遍历方法。步骤(3)(4)的时间复杂度为$O(n+e)$ 因此这个方法的时间复杂度为$O(n+e)$,除了能够判断有向图中是否有闭合的环，也能够给出一种合理的遍历方法，因此能解决上面提到的207. Course Schedule 和 210. Course Schedule II两个问题。 下面是207. Course Schedule 的解决方法123456789101112131415161718192021222324252627class Solution(object): def canFinish(self, numCourses, prerequisites): """ :type numCourses: int :type prerequisites: List[List[int]] :rtype: bool """ indegree = [0 for i in xrange(numCourses)] connection = &#123;i:[] for i in xrange(numCourses)&#125; for link in prerequisites: connection[link[1]].append(link[0]) indegree[link[0]] += 1 zero_indegree = [] for i in xrange(numCourses): if indegree[i] == 0: zero_indegree.append(i) i = 0 while i&lt;len(zero_indegree): for node in connection[zero_indegree[i]]: indegree[node] -= 1 if indegree[node] == 0: zero_indegree.append(node) i += 1 if len(zero_indegree) == numCourses: return True else: return False 210. Course Schedule II的解决方法跟上面一样，只需要将return True改为return zero_indegree即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python内置的排序函数]]></title>
      <url>%2F2016%2F07%2F23%2Fpython%E5%86%85%E7%BD%AE%E7%9A%84%E6%8E%92%E5%BA%8F%E5%87%BD%E6%95%B0%2F</url>
      <content type="text"><![CDATA[排序是非常常见的操作，常见的排序算法的时间复杂度一般为$O(n^2)$（冒泡、选择、插入）或$O(nlogn)$(快排、归并等)。虽然这些算法对于编程人员来说是基础，但是在实际工程中往往会使用语言内置的排序函数，主要是考虑到编程效率和自己编写排序函数时涵盖情况不全的问题。因此本文主要讲述python中的内置函数。 python内置的排序函数主要有两个：sort和sorted，两者主要以下几点区别（1）针对的数据类型不同，sort只能对list类型排序，sorted可对list、tuple、dictionary以及自定义的类等数据类型排序（2）sort会修改原来的list，sorted不会修改原来的数据结构，而是将排好序的结果以list形式返回,因此sorted才能对不可变的数据类型tuple进行排序（3）语法不同，sort的使用方法是list.sort(),sorted的使用方法是sorted(list|tuple|dict) 除此之外，两者的参数完全一致，都含有reverse，key，cmp这几个参数，这几个参数的用法在两个函数中一致，下面以sorted为例分别讲述这几个参数的作用。 reverse 参数reverse参数的作用是决定从小到大还是从大到小排列结果，默认情况下reverse=False，从小到大排列结果。如果要从大到小排列结果，添加reverse=True参数即可。示例如下所示：12345678&gt;&gt;&gt; a = ['a','A','b','B']&gt;&gt;&gt; sorted(a)['A', 'B', 'a', 'b']&gt;&gt;&gt; a['a', 'A', 'b', 'B']&gt;&gt;&gt; a = ['abundunt','Array','bunch','&gt;&gt;&gt; sorted(a, reverse = True)['bunch', 'abundunt', 'But', 'Array'] 数字大小的判断很容易理解，这里比较的是字符串的大小，比较时会根据字符串首字符的ASCII码的大小进行排序。 key 参数上面的例子是根据单个元素来排序的，假如需要对多个元素组合而成的元素来排序（如b = [(2,1),(1,3),(4,2)]），就需要用key来指定以哪个元素作为排序的依据。 下面是对tuple组成的tuple的排序12345678&gt;&gt;&gt; b = ((2,1),(1,3),(4,2))# 不用key参数时也不会报错，这时排序依据默认是组合元素中的第一个元素，但是为了程序的清晰性，还是建议使用key参数&gt;&gt;&gt; sorted(b) [(1, 3), (2, 1), (4, 2)]&gt;&gt;&gt; sorted(b, key = lambda x:x[0])[(1, 3), (2, 1), (4, 2)]&gt;&gt;&gt; sorted(b, key = lambda x:x[1])[(2, 1), (4, 2), (1, 3)] 给key参数传进去的是一个函数，上面使用lambda实现，传入的是需要排序的组合元素，返回的是根据组合元素中哪个元素排序，从下标0开始计算。 对于字典的key或value排序也是如此，但此时的下标就只有0和1了，分别代表key和value。 123456789&gt;&gt;&gt; d = &#123;2:1, 4:3, 1:6&#125;&gt;&gt;&gt; sorted(d.items(), key = lambda x:x[0])[(1, 6), (2, 1), (4, 3)]&gt;&gt;&gt; d&#123;1: 6, 2: 1, 4: 3&#125;&gt;&gt;&gt; sorted(d.items(), key = lambda x:x[1])[(2, 1), (4, 3), (1, 6)]&gt;&gt;&gt; d&#123;1: 6, 2: 1, 4: 3&#125; 除了内部的list、tuple等数据类型，key还可以针对自定义的数据类型进行排序。实例如下1234567891011121314151617&gt;&gt;&gt; class Student: def __init__(self, name, grade, age): self.name = name self.grade = grade self.age = age def __repr__(self): return repr((self.name, self.grade, self.age)) def weighted_grade(self): return 'CBA'.index(self.grade) / float(self.age)&gt;&gt;&gt; student_objects = [ Student('john', 'A', 15), Student('jane', 'B', 12), Student('dave', 'B', 10),]&gt;&gt;&gt; sorted(student_objects, key=lambda student: student.age) # sort by age[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)] cmp 参数利用上面两个参数已经能解决大部分的问题了，还有一些较特殊的排序需要编写特定的排序函数，这时就需要利用cmp参数。 如LeetCode上的这道题目179. Largest Number，就是一道排序的题目，且排序的要求是对于两个字符串s1，s21234排序规则如下：(1)假如 s1+s2 &gt; s2+s1,则 s1 &gt; s2(2)假如 s1+s2 &lt; s2+s1,则 s1 &lt; s2(3)以上两种情况均不符合，则s1 = s2 这种情况下上面所提到的两种方法都无法实现，因为上面的两个参数都是针对单个元素的特性的，而这种方法则是针对两个元素之间的关系。因此需要定义自己的排序函数。不指定cmp参数的时候，cmp = lambda x,y: cmp(x, y)，这里需要注意第一个cmp是sort函数的参数，第二个cmp则是python的一个内置函数。其定义如下：123456&gt;&gt;&gt; cmp(1,2)-1&gt;&gt;&gt; cmp(2,1)1&gt;&gt;&gt; cmp(2,2)0 对于cmp(x, y),当 x 大于、等于、小于 y 时，分别会返回 1,0,-1。这个默认的cmp函数也可以写成cmp = lambda x,y: x-y,这种情况下是从小到大排序的，那么从大到小的排序可以写成cmp = lambda x,y: y-x。这等价于reverse = True。 回到题目179. Largest Number上，利用内置的sorted函数，我们可以写出下面较为简洁的代码 1234567class Solution: # @param &#123;integer[]&#125; nums # @return &#123;string&#125; def largestNumber(self, nums): snums = [str(num) for num in nums] snums.sort(cmp=lambda x,y: cmp(y+x,x+y)) return ''.join(snums).lstrip('0') or '0' 关键点在snums.sort(cmp=lambda x,y: cmp(y+x,x+y))这句话，首先是cmp(y+x,x+y),当 y+x &gt; x+y时, y&gt;x, 此时cmp(y+x,x+y)返回1，而由上面的知识可知这是从大到小的排序。 上面可以说是比较抽象的代码，下面是通过自己实现的快排解决上面的题目，跟上面的答案一样能够AC。 123456789101112131415161718192021222324class Solution: # @param &#123;integer[]&#125; nums # @return &#123;string&#125; def largestNumber(self, nums): snums = [str(num) for num in nums] self.quick_sort(0, len(snums)-1, snums) tmp = ''.join(snums).lstrip('0') return tmp if tmp else '0' def quick_sort(self, left, right, nums): if left &gt; right: return pivot, start, end = left, left, right while left &lt; right: while left &lt; right and nums[right]+nums[pivot] &lt;= nums[pivot]+nums[right]: right -= 1 while left &lt; right and nums[left]+nums[pivot] &gt;= nums[pivot]+nums[left]: left += 1 if left &lt; right: nums[left], nums[right] =nums[right], nums[left] if left == right: nums[pivot], nums[left] = nums[left], nums[pivot] self.quick_sort(start, left-1, nums) self.quick_sort(left+1, end, nums)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(105. 106)--树的重构]]></title>
      <url>%2F2016%2F07%2F20%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(105.%20106)--%E6%A0%91%E7%9A%84%E9%87%8D%E6%9E%84%2F</url>
      <content type="text"><![CDATA[本文主要从LeetCode两道题出发讲解如何从树的周游结果来重构树。这两道题分别是105. Construct Binary Tree from Preorder and Inorder Traversal和106. Construct Binary Tree from Inorder and Postorder Traversal 105. Construct Binary Tree from Preorder and Inorder Traversal要求从树的前序周游和中序周游的结果重构树。由两种周游的遍历顺序可知，两种周游方式得到的结果的结构如下 其中left_tree是root的左子树， right_tree是root的右子树。 因此通过树的根节点root以及左子树的最后一个节点，可以从两个周游结果中分别找到左子树的两种周游结果和右子树的两种周游结果。然后将问题转化为原问题的子问题，通过递归解决即可。 python实现的代码如下所示,需要注意的是下面的代码通过下标指出子树的范围，而不是通过slice，也就是preorder[i:j]的方式来将子树范围切出来，因为slice会在内存开辟新的空间，递归时会开辟大量空间而导致MLE错误： 1234567891011121314151617181920class Solution(object): def buildTree(self, preorder, inorder): """ :type preorder: List[int] :type inorder: List[int] :rtype: TreeNode """ return self.helper(preorder, 0, len(preorder)-1, inorder, 0, len(inorder)-1) def helper(self, preorder, pleft, pright, inorder, ileft, iright): if pleft &gt; pright: return None if pleft == pright: return TreeNode(preorder[pleft]) inx = inorder.index(preorder[pleft]) left_len = inx - ileft root = TreeNode(preorder[pleft]) root.left = self.helper(preorder, pleft + 1, pleft + left_len, inorder, ileft, inx-1) root.right = self.helper(preorder, pleft + left_len + 1, pright, inorder, inx+1, right) return root 106. Construct Binary Tree from Inorder and Postorder Traversal的结题思路与上面一致。两种周游结果的结构如下： 同样可以通过树的根节点root以及左子树的最后一个节点，分别找到两棵子树的两种周游结果，进而将问题转化为原来问题的子问题，通过递归解决。 python实现代码如下所示123456789101112131415161718class Solution(object): def buildTree(self, inorder, postorder): """ :type inorder: List[int] :type postorder: List[int] :rtype: TreeNode """ return self.helper(inorder, 0, len(inorder)-1, postorder, 0, len(postorder)-1) def helper(self, inorder, ileft, iright, postorder, pleft, pright): if ileft &gt; iright: return None if ileft == iright: return TreeNode(inorder[ileft]) inx = inorder.index(postorder[pright]) left_len = inx - ileft root = TreeNode(postorder[pright]) root.left = self.helper(inorder, ileft, inx-1, postorder, pleft, pleft+left_len-1) root.right = self.helper(inorder, inx+1, iright, postorder, pleft+left_len, pright-1) return root]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(200,130)--图的孤立子图]]></title>
      <url>%2F2016%2F07%2F17%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(200%2C130)--%E5%9B%BE%E7%9A%84%E5%AD%A4%E7%AB%8B%E5%AD%90%E5%9B%BE%2F</url>
      <content type="text"><![CDATA[200. Number of Islands题目描述如下： Given a 2d grid map of ‘1’s (land) and ‘0’s (water), count the number of islands. An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water.Example 1: 11110110101100000000 Answer: 1 上面的问题是一个经典的图论问题，需要求孤立的岛屿的个数，而岛屿仅可通过上下左右来连接，这种问题可以通过深度优先搜索(DFS)或广度优先搜索(BFS)来解决，每次遇到一个没访问过的岛屿就对当前岛屿进行DFS或DFS，并记录这个过程中访问过的岛屿，直至最终所有岛屿都被访问 采用DFS的的python代码如下所示： 123456789101112131415161718192021222324252627class Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ if len(grid) == 0: return 0 m, n, result = len(grid), len(grid[0]), 0 visited = [[0 for j in xrange(n)] for i in xrange(m)] for i in xrange(m): for j in xrange(n): if grid[i][j] == '1' and visited[i][j] == 0: self.dfs(grid, visited, i, j) result += 1 return result def dfs(self, grid, visited, i, j): visited[i][j] = 1 m, n = len(grid), len(grid[0]) if 0&lt;= i-1 &lt;m and grid[i-1][j] == '1' and visited[i-1][j] == 0: self.dfs(grid, visited, i-1, j) if 0&lt;= i+1 &lt;m and grid[i+1][j] == '1' and visited[i+1][j] == 0: self.dfs(grid, visited, i+1, j) if 0&lt;= j-1 &lt;n and grid[i][j-1] == '1' and visited[i][j-1] == 0: self.dfs(grid, visited, i, j-1) if 0&lt;= j+1 &lt;n and grid[i][j+1] == '1' and visited[i][j+1] == 0: self.dfs(grid, visited, i, j+1) 假如将上面的题目改为一个岛屿邻接的岛屿除了上下左右，还有左上、左下、右上、右下，也就是有八个可能连接的地方，那么答案该如何修改？ 实际上方法也很简单，对上面代码中的dfs方法增加检查左上、左下、右上、右下这四个点的代码即可。 上面的DFS中利用了visited数组检查某个点是否已经被访问过，空间复杂度为O(m*n)。实际上在原来的grid可修改的情况下,可以通过修改grid中的值为’1’,’0’以外的值，从而使得空间复杂度为O(1)。下面的BFS方法利用了这点 12345678910111213141516171819202122232425262728293031323334353637from collections import dequeclass Solution(object): def numIslands(self, grid): """ :type grid: List[List[str]] :rtype: int """ m = len(grid) if m == 0: return 0 n = len(grid[0]) count = 0 for i in xrange(m): for j in xrange(n): if grid[i][j] == '1': self.bfs(i, j, grid) count += 1 return count def bfs(self, row, col, grid): m, n = len(grid), len(grid[0]) que = deque() que.append((row, col)) grid[row][col] = '2' while que: row, col = que.popleft() if row&gt;0 and grid[row-1][col]=='1': que.append((row-1, col)) grid[row-1][col] = '2' if row&lt;m-1 and grid[row+1][col] == '1': que.append((row+1, col)) grid[row+1][col] = '2' if col&gt;0 and grid[row][col-1]=='1': que.append((row, col-1)) grid[row][col-1] = '2' if col&lt;n-1 and grid[row][col+1] == '1': que.append((row, col+1)) grid[row][col+1] = '2' 此外，130. Surrounded Regions跟上面的题目也类似，只是需要对边上的点进行BFS或DFS，具体代码见这里]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Binary Indexed Trees 简介]]></title>
      <url>%2F2016%2F07%2F12%2FBinary%20Indexed%20Trees%20%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[Binary Indexed Trees（中文名为树状数组，下文简称为BIT）是一种特殊的数据结构，可多用于高效计算数列的前缀和， 区间和。对于长度为n的数组，它可以以$O(logn)$的时间得到任意前缀和 $ {\displaystyle \sum _{i=1}^{j}a[i],1&lt;=j&lt;=N}$，并同时支持在 $ O(log n)$时间内支持动态单点值的修改。空间复杂度 $O(n)$ 虽然BIT名称中带有tree这个词，但是实际存储时是利用两个数组进行存储，记这两个数组为nums和 BIT。假设我们现在需要对原始数组 arr 进行前缀求和和区间求和，那么可以按照以下步骤进行。 1.初始化 $nums[i] = arr[i]$$BIT[i] = {\displaystyle \sum _{k=i-lowestbit(i)+1}^{i}arr[k]}$ 上面的lowestbit(i)指将i转为二进制后,最后一个1的位置所代表的数值。如lowestbit(1)=1、lowestbit(6)=2，具体的实现可通过(i&amp;-i)获取。 下图就是初始化后的情况，横轴为数组的下标(记为i)，纵轴为下标数值对应的lowestbit（i&amp;-i），长方形表示BIT[i]涵盖的求和的范围 可以看到每个数组下标的lowestbit（也就是图中描黑的部分）在形态上构成了一棵树的形状，这也是名称中tree的来源。并且对于每个下标的lowestbit表示成的tree node有以下特性。 (1)假如i是左子节点，那么其父节点下标为i+(lowestbit(i))(2)假如i是右子节点，那么其父节点下标为i-(lowestbit(i)) 上面这两个特性非常重要，也是我们进行后文分析的重要基础。 2. 更新一个数值假如要修改原始数组 arr 中的下标为i的值，那么需要修改nums数组中对应下标的值。除此之外还需要修改BIT数组中涵盖了arr[i]的值。结合上图可以知道，BIT数组中涵盖了arr[i]的值为下标i及其所有父节点，伪代码如下123while i &lt; n: BIT[i] += new_value i += (i&amp;-i) 3. 区间求和 假如要求arr数组下标区间为[i,j]的数值之和，那么可以先求下标为[0,i-1]的数值之和，再求下标为[0,j]的数值之和，然后用后者减去前者即可。 通过观察上面初始化后的图可以知道求[0, i]可以通过下面的方法：1234count = 0while i&gt;0: count += BIT[i] i -= (i&amp;-i) 通过上面的操作，通过利用额外的两个数数组，将原来的区间求和的操作从时间复杂度$O(n)$变为了$O(logn)$,但是更新数组的值的操作的时间复杂度也从原来的$O(1)$变为了$O(logn)$,所以这种数据结构更适合用于区间求和频繁的应用场景。 下面是LeetCode上的一道利用了BIT的题目,有兴趣的读者可以尝试做一下，验证刚刚学的理论知识。 Given an integer array nums, find the sum of the elements between indices i and j (i ≤ j), inclusive. The update(i, val) function modifies nums by updating the element at index i to val. 实现的python代码如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class NumArray(object): def __init__(self, nums): """ initialize your data structure here. :type nums: List[int] """ self.nums = nums[:] self.count = [0 for i in xrange(len(nums)+1)] for i in xrange(len(nums)): self.initialize(i, nums[i]) def initialize(self, i, val): i += 1 while i &lt; len(self.nums)+1: self.count[i] += val i += (i &amp; -i) def update(self, i, val): """ :type i: int :type val: int :rtype: int """ diff = val - self.nums[i] self.nums[i] = val self.initialize(i, diff) def left_sum(self, i): i += 1 total = 0 while i&gt;0: total += self.count[i] i -= (i &amp; -i) return total def sumRange(self, i, j): """ sum of elements nums[i..j], inclusive. :type i: int :type j: int :rtype: int """ return self.left_sum(j) - self.left_sum(i-1)# Your NumArray object will be instantiated and called as such:# numArray = NumArray(nums)# numArray.sumRange(0, 1)# numArray.update(1, 10)# numArray.sumRange(1, 2)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[爬虫抓取代理IP]]></title>
      <url>%2F2016%2F07%2F05%2F%E7%88%AC%E8%99%AB%E4%BB%A3%E7%90%86IP%E7%9A%84%E8%8E%B7%E5%8F%96%2F</url>
      <content type="text"><![CDATA[由于某些网站对会对爬虫做限制，因此常常需要通过代理将爬虫的实际IP隐蔽起来，代理也有分类，如透明代理，高匿代理等。本文主要讲述如何获取代理IP，并且如何存储和使用。 某些网站会免费提供代理IP，如下面的几个 http://www.xicidaili.com http://www.kuaidaili.com https://proxy.peuland.com 获取这些页面上的代理IP及端口也是通过爬虫抓取，下面以第一个网站http://www.xicidaili.com为例，解释如何获取并存储这些代理IP。一般的流程为：解析当前页面--&gt;存储当前页面的代理IP--&gt;跳转到下一页面，重复该流程即可。 解析页面首先要解析页面，由于网页中显示代理IP时是在表格中显示的，因此只需要通过找出网页源码中相关的表格元素即可。下面是通过python中的requests和bs4获取页面http://www.xicidaili.com/nt/上显示的IP及端口。 12345678910111213141516import requestsfrom bs4 import BeautifulSoupuser_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'referer = 'http://www.xicidaili.com/'headers = &#123;'user-agent': user_agent, 'referer': referer&#125;target = 'http://www.xicidaili.com/nt/'# 获取页面源码r = requests.get(target, headers = headers)# 解析页面源码soup = BeautifulSoup(r.text, 'lxml')for tr in soup.find_all('tr')[1:]: tds = tr.find_all('td') proxy = tds[1].text+':'+tds[2].text print proxy 输出如下：123456736.235.1.189:3128219.141.225.149:80125.44.132.44:9999123.249.8.100:3128183.54.30.186:9999110.211.45.228:9000........... 代理IP的存储上面代码获取的代理IP可以通过在代码一开始建立一个集合（set）来存储，这种情况适用于一次性使用这些代理IP，当程序发生异常或正常退出后，这些存储在内存中的代理IP也会丢失。但是爬虫中使用代理IP的情况又是非常多的，所以有必要把这些IP存储起来，从而可以让程序多次利用。 这里主要通过redis数据库存储这些代理IP，redis是一个NOSQL数据库，具体使用参照官方文档，这里不做详细解释。 下面是ConnectRedis.py文件，用于连接redis12345678910import redisHOST = 'XXX.XXX.XXX.XXX' # redis所在主机IPPORT = 6379 # redis服务监听的端口PASSWORD = 'XXXXXX' # 连接redis的密码DB = 0 # IP存储的DB编号def get_connection(): r = redis.Redis(host = HOST, port = PORT, password = PASSWORD, db= DB) return r 下面是在上面的代码基础上将IP存储到redis中，12345678910111213141516171819202122232425import requestsfrom bs4 import BeautifulSoupfrom ConnectRedis import get_connection# 获取redis连接try: conn = get_connection()except Exception: print 'Error while connecting to redis' returnuser_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36'referer = 'http://www.xicidaili.com/'headers = &#123;'user-agent': user_agent, 'referer': referer&#125;target = 'http://www.xicidaili.com/nt/'# 获取页面源码r = requests.get(target, headers = headers)# 解析页面源码soup = BeautifulSoup(r.text, 'lxml')for tr in soup.find_all('tr')[1:]: tds = tr.find_all('td') proxy = tds[1].text+':'+tds[2].text conn.sadd("ip_set", proxy) print '%s added to ip set'%proxy 上面的conn.sadd(&quot;ip_set&quot;, proxy)将代理proxy加入到redis的集合&quot;ip_set&quot;，这个集合需要预先在redis中创建，否则会出错。 页面跳转上面的代码获取的只是一个页面上显示的代理，显然这个数量不够，一般通过当前页面中的下一页的超链接可以跳转到下一页，但是我们测试的由于每页的的url都有规律，都是http://www.xicidaili.com/nt/page_number,其中的page_number表示当前在哪一页，省略时为第一页。因此，通过一个for循环嵌套上面的代码即可获取多个页面的代理。但是更一般的方法是通过在当前页面获取下一页的超链接而跳转到下一页。 代理IP的使用当我们需要通过代理访问某一网站时，首先需要从redis中随机选出一个代理ip，然后尝试通过代理ip是否能连到我们需要访问的目标网站，因为这些代理IP是公共使用的，所以往往也会被封的很快，假如通过代理无法访问目标网站，那么就要从数据库中删除这个代理IP。反之即可通过此代理访问目标网站 下面是实现上面所说流程的代码：1234567891011121314151617181920212223242526272829import requestsfrom ConnectRedis import get_connection# 判断IP是否能访问目标网站def is_valid(url, ip): proxy = &#123; 'http': 'http://%s' %ip &#125; user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36' headers = &#123;'user-agent': user_agent&#125; try: r = requests.get(url, headers = headers, proxies = proxy, timeout = 6) return True except Exception: return Falseif __name__ == '__main__': my_proxy, proxies, ip_set = None, None, 'amazon_ips' conn = get_connection() target = 'https://www.amazon.com/' while not is_valid(target, my_proxy): if my_proxy: conn.srem(ip_set, my_proxy) #删除无效的代理IP if proxies: my_proxy = proxies.pop() else: proxies = conn.srandmember(ip_set, 5) #从redis中随机抽5个代理ip my_proxy = proxies.pop() print 'valid proxy %s' %my_proxy requests.get(url, headers = headers, proxies = proxy, timeout = 6)是通过代理去访问目标网站，超时时间设为6s，也就是说在6秒内网站没有回应或返回错误信息就认为这个代理无效。 除此之外，在爬取免费提供代理的网站上的代理IP的时候，爬取的速度不要太快，其中的一个原因是爬取太快有可能会被封，另外一个原因是如果每个人都无间隙地从这种网站上爬取，那么网站的负担会比较大，甚至有可能垮掉，因此采用一个可持续爬取的策略非常有必要，我爬取的时候是没爬完一个页面后让程序sleep大概2分钟，这样下来不会被封而且爬取的代理的量也足够使用。实际中可以根据自己使用代理的频率来进行调整。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java 面向对象的几个概念]]></title>
      <url>%2F2016%2F07%2F01%2FJava%20%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%87%A0%E4%B8%AA%E6%A6%82%E5%BF%B5%2F</url>
      <content type="text"><![CDATA[本文主要记录Java面向对象中几个容易混淆的概念。主要包括重写(override)与重载(overload)，多态，抽象类与接口。 继承在Java中，类的继承是单一继承，一个子类只能拥有一个父类。通过extends关键字实现类的继承。所有Java的类均是由java.lang.Object类继承而来的，所以Object是所有类的祖先类，而除了Object外，所有类必须有一个父类。 通过instanceof关键字可以判断一个对象是不是一个类的实例。见下面的例子： 123456789101112131415//A.javaclass A&#123;&#125;//B.javaclass B extends A&#123; public static void main(String[] args)&#123; A a = new A(); B b = new B(); System.out.println(a instanceof A); System.out.println(b instanceof B); System.out.println(a instanceof B); System.out.println(b instanceof A); &#125;&#125; 上述代码的输出为1234truetruefalsetrue 也就是说子类的对象也是父类的一个实例。 重写(override)与重载(overload)重写(override)重写(override)是子类对父类的允许访问的方法的实现过程进行重新编写,返回值和形参都不能改变。 重写有以下几点规则 参数列表和返回类型必须完全与被重写方法相同； 访问权限不能比父类中被重写的方法的访问权限更低。例如：如果父类的一个方法被声明为public，那么在子类中重写该方法就不能声明为protected。 子类只能重写有访问权限的父类方法，在此基础上声明为final的方法不能被重写。 重写的方法能够抛出任何非强制异常，无论被重写的方法是否抛出异常。但是，重写的方法不能抛出新的强制性异常，或者比被重写方法声明的更广泛的强制性异常，反之则可以 构造方法不能被重写。 如果不能继承一个方法，则不能重写这个方法 详见下面的例子：1234567891011121314151617181920class Animal&#123; public void move()&#123; System.out.println("动物可以移动"); &#125;&#125;class Dog extends Animal&#123; public void move()&#123; System.out.println("狗可以跑和走"); &#125;&#125;public class TestDog&#123; public static void main(String args[])&#123; Animal a = new Animal(); // Animal 对象 Animal b = new Dog(); // Dog 对象 a.move();// 执行 Animal 类的方法 b.move();//执行 Dog 类的方法 &#125;&#125; 在上面的例子中可以看到，尽管b属于Animal类型，但是它运行的是Dog类的move方法。这是由于在编译阶段，只是检查参数的引用类型。然而在运行时，Java虚拟机(JVM)指定对象的类型并且运行该对象的方法。因此在上面的例子中，之所以能编译成功，是因为Animal类中存在move方法，然而运行时，运行的是特定对象的方法。而这就是一个典型的多态例子。 再看看下面的例子能更好地理解上面的话123456789101112131415161718192021222324class Animal&#123; public void move()&#123; System.out.println("动物可以移动"); &#125;&#125;class Dog extends Animal&#123; public void move()&#123; System.out.println("狗可以跑和走"); &#125; public void bark()&#123; System.out.println("狗可以吠叫"); &#125;&#125;public class TestDog&#123; public static void main(String args[])&#123; Animal a = new Animal(); // Animal 对象 Animal b = new Dog(); // Dog 对象 a.move();// 执行 Animal 类的方法 b.move();//执行 Dog 类的方法 b.bark(); &#125;&#125; 以上实例编译运行结果如下：12345TestDog.java:30: cannot find symbolsymbol : method bark()location: class Animal b.bark(); ^ 该程序将抛出一个编译错误，因为b的引用类型Animal没有bark方法。 当需要在子类中调用父类的被重写方法时，要使用super关键字。 12345678910111213141516171819class Animal&#123; public void move()&#123; System.out.println("动物可以移动"); &#125;&#125;class Dog extends Animal&#123; public void move()&#123; super.move(); // 应用super类的方法 System.out.println("狗可以跑和走"); &#125;&#125;public class TestDog&#123; public static void main(String args[])&#123; Animal b = new Dog(); // Dog 对象 b.move(); //执行 Dog类的方法 &#125;&#125; 以上实例编译运行结果如下：12动物可以移动狗可以跑和走 重载(overload)重载(overloading) 是在一个类里面，方法名字相同，而参数不同，返回类型可以相同也可以不同。 重载有以下几点规则 被重载的方法必须改变参数列表； 被重载的方法可以改变返回类型； 被重载的方法可以改变访问修饰符，没有限制权限只能变大或变小的限制 方法能够在同一个类中或者在一个子类中被重载。 见下面的例子 123456789101112131415161718192021222324252627282930public class Overloading &#123; public int test()&#123; System.out.println("test1"); return 1; &#125; public void test(int a)&#123; System.out.println("test2"); &#125; //以下两个参数类型顺序不同 public String test(int a,String s)&#123; System.out.println("test3"); return "returntest3"; &#125; public String test(String s,int a)&#123; System.out.println("test4"); return "returntest4"; &#125; public static void main(String[] args)&#123; Overloading o = new Overloading(); System.out.println(o.test()); o.test(1); System.out.println(o.test(1,"test3")); System.out.println(o.test("test4",1)); &#125;&#125; 重写(override)与重载(overload)的区别 区别点 重写(override) 重载(overload) 参数列表 不能改变 必须改变 返回类型 不能改变 可以改变 范围 只能在子类中重写 可以在当前类或子类中重载 权限 重写的方法的访问权限只能变大 重载方法的访问权限无变化限制 异常 可以减少或删除，不能抛出新的或者更广的异常 无添加减少的限制 多态从字面上的意思解释，多态是同一个行为具有多个不同表现形态的能力。反映在Java面向对象中指的是同一方法（参数列表和返回类型都相同）有具有多种实现方式。 因此，结合上面说到的内容，多态存在有以下三个必要条件: 继承 重写 父类引用指向子类对象 当使用多态方式调用方法时，首先检查父类中是否有该方法，如果没有，则编译错误；如果有，再去调用子类的同名方法。 上面的重写所提到的例子就是一个典型的多态例子。 抽象类与接口抽象类抽象类不能实例化对象，抽象类的用途在于声明了一系列需要被继承并实现的抽象方法，然后被其他类继承并实现。也是因为这个原因，通常在设计阶段决定要不要设计抽象类。 抽象类通过abstract class来定义，同时需要注意如果一个类包含抽象方法，那么该类一定要声明为抽象类；但是抽象类可以不包含抽象方法，也可以同时包含抽象方法和非抽象方法。 见下面的例子1234567891011121314151617// Employee.javapublic abstract class Employee&#123; private String name; private String address; private int number; public abstract double computePay();&#125;// seller.javapublic class seller&#123; public double computePay &#123; &#125;&#125; 继承抽象类后需要注意下面两点： 如果一个类包含抽象方法，那么该类必须是抽象类。 任何子类必须重写父类所有的抽象方法，否则需要声明自身为抽象类。 接口（interface）接口（英文：Interface），在JAVA中是抽象方法的集合，接口通常以 interface 来声明。一个类通过继承接口的方式，从而来继承接口的抽象方法。 接口并不是类，编写接口的方式和类很相似，但是它们属于不同的概念。类描述对象的属性和方法。接口则包含类要实现的方法。 接口有以下特性： 接口是隐式抽象的，当声明一个接口的时候，不必使用abstract关键字。 接口中每一个方法也是隐式抽象的，声明时同样不需要abstract关键子。 接口中的方法都是公有的。 如下面就声明了一个接口：1234interface Animal &#123; public void eat(); public void travel();&#125; 当类实现接口的时候，类要实现接口中所有的方法。否则，类必须声明为抽象的类。类使用implements关键字实现接口,且一个类可以实现多个接口。 下面是实现上面的接口的一个例子：1234567891011121314151617181920public class MammalInt implements Animal&#123; public void eat()&#123; System.out.println("Mammal eats"); &#125; public void travel()&#123; System.out.println("Mammal travels"); &#125; public int noOfLegs()&#123; return 0; &#125; public static void main(String args[])&#123; MammalInt m = new MammalInt(); m.eat(); m.travel(); &#125;&#125; 一个接口能继承另一个接口，和类之间的继承方式比较相似。接口的继承使用extends关键字，见下面的例子 1234567891011121314// Sports.javapublic interface Sports&#123; public void setHomeTeam(String name); public void setVisitingTeam(String name);&#125;// Football.javapublic interface Football extends Sports&#123; public void homeTeamScored(int points); public void visitingTeamScored(int points); public void endOfQuarter(int quarter);&#125; 除此之外，接口还允许多继承，但是 Java 中是不允许类的多继承的。如下面的接口就继承了上面的两个接口1234public interface Socer extends Sports, Football&#123; &#125; 接口与抽象类非常相似，两者的区别入下： 区别 接口 抽象类 继承(实现)的个数 一个类可实现多个接口 一个类仅能继承一个抽象类 内部是否可以含有实现的方法 没有实现的方法 可以有实现的方法 参考：http://www.runoob.com/java]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java 中的修饰符]]></title>
      <url>%2F2016%2F06%2F20%2FJava%20%E4%B8%AD%E7%9A%84%E4%BF%AE%E9%A5%B0%E7%AC%A6%2F</url>
      <content type="text"><![CDATA[Java的修饰符主要分为两种 访问修饰符 非访问修饰符 访问修饰符访问修饰符用于控制对类、方法、变量的访问权限，使用中有private、default、protected、public四个修饰符，其访问权限从小到大。其中default不是一个保留字，而是当任何修饰符都不加的时候的默认权限。 四种修饰符的区别如下所示 修饰符 属性 private 仅对同一个类（class）内部可见 default 对同一个包（package）内的所有类可见 protected 对同一个包内的所有类及其子类可见（即使子类在另外一个包） public 对所有包的所有类均可见 访问权限的继承的一个原则就是子类从父类继承过来的方法和变量的访问权限只能变大，private除外，因此就有了以下几条原则 父类中声明为public的方法在子类中也必须为public。 父类中声明为protected的方法在子类中要么声明为protected，要么声明为public。不能声明为private。 父类中声明为private的方法，不能够被继承。 非访问修饰符非访问修饰符指的是一些实现其他功能的修饰符，为了与控制访问权限的访问修饰符区别，就命名为了非访问修饰符。 非访问修饰符主要包含static，final， abstract， synchronized和volatile。其主要区别如下： 修饰符 属性 static 声明方法或变量属于整个类而不是变量 final 声明方法或变量不可被修改 abstract 仅仅声明了类或方法的名称而让子类对抽象类进行拓展和修改 synchronized和volatile 用于线程的同步 关于上面的几个修饰符有以下几点需要注意： static 静态（static）方法不能使用类的非静态变量 静态变量或方法可通过类直接调用：如class.staticVariable或class.staticMethod final final变量需要在声明的时候显式初始化并且只能初始化一次 final方法可以被子类继承，但是不能被子类修改，或者说重写(override) final类不能被继承，没有类能够继承final类的任何特性 final修饰符通常和static修饰符一起使用来创建类常量 abstract 抽象（abstract）方法是一种没有任何实现的方法（没有花括号），该方法的的具体实现由子类提供 抽象方法的声明以分号结尾，且例如：public abstract sample(); 任何继承抽象类的子类必须实现父类的所有抽象方法，除非该子类也是抽象类。 如果一个类包含抽象方法，那么该类一定要声明为抽象类；但是抽象类可以不包含抽象方法，也可以同时包含抽象方法和非抽象方法 一个抽象类的定义12345678abstract class Caravan&#123; private double price; public abstract void goFast(); //抽象方法 public abstract void changeColor() &#123; // 抽象类中可以包含实现了的方法 &#125;&#125; 抽象方法:12345678910public abstract class SuperClass&#123; abstract void m(); //抽象方法&#125; class SubClass extends SuperClass&#123; //实现抽象方法 void m()&#123; ......... &#125;&#125; 这里顺便穿插抽象类和接口（interface）的一些差异： 接口（interface）就是给出一些没有内容的方法，封装到一起，到某个类需要使用的时候，再根据具体情况将这些方法写出。 与抽象类不同的是抽象类中的方法可以有主体（也就是实现了该方法）；而接口中的所有方法都不可以有主体（也就是所有方法都不可以实现） 除此之外，一个类可以实现多个接口，实现了类似于多继承的特性；但是一个类不能继承多个类，并且一个类在实现一个接口的同时也可以继承一个类，如：class son extends father implements interface{} synchronized和volatile synchronized关键字声明的方法同一时间只能被一个线程访问，通过synchronized修饰符可以实现线程锁的功能 volatile 修饰的成员变量在每次被线程访问时，都强制从共享内存中重新读取该成员变量的值。而且，当成员变量发生变化时，会强制线程将变化值回写到共享内存。这样在任何时刻，两个不同的线程总是看到某个成员变量的同一个值 关于violate的一个列子如下所示12345678910111213141516public class MyRunnable implements Runnable&#123; private volatile boolean active; public void run() &#123; active = true; while (active) // 第一行 &#123; // 代码 &#125; &#125; public void stop() &#123; active = false; // 第二行 &#125;&#125; 通常情况下，在一个线程调用 run() 方法（在 Runnable 开启的线程），在另一个线程调用 stop() 方法。 如果 第一行 中缓冲区的 active 值被使用，那么在 第二行 的 active 值为 false 时循环不会停止。但是以上代码中我们使用了 volatile 修饰 active，所以该循环会停止。 在对变量和方法使用修饰符时，访问修饰符和非访问修饰符可以混用，且对访问修饰符只能选择其中一个，而非访问修饰符则没有这个限制。因此以下修饰符都是合法的1234public static void main() // 执行类的入口--main 方法public static final pi = 3.14 //不可修改的变量πpublic final class Test // 不能被继承的类............. Referer http://www.runoob.com/java/java-modifier-types.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(95,96)--构造二叉搜索树]]></title>
      <url>%2F2016%2F06%2F11%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(95%2C96)--%E6%9E%84%E9%80%A0%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%2F</url>
      <content type="text"><![CDATA[两个题目均是要求利用给出的整数[1, n]构造出所有的二叉搜索树（BST），其中95. Unique Binary Search Trees II要求返回所有的二叉树的根节点，96. Unique Binary Search Trees则仅要求返回所有二叉树的数目。 两个题目均可以通过递归实现，思路如下：1）遍历[1,n]中每个整数,并将正在遍历的整数 i 设为根节点2）将 [1,i-1] 的整数构建出来的子树作为整数 i 构建的根节点的左子树，将 [i+1,n] 的整数构造出来的子树作为整数 i 构建的根节点的右子树3）对得到的左右子树进行组合，这样便可得到以整数i为根节点的所有二叉树 因此，95. Unique Binary Search Trees II实现代码如下: 1234567891011121314151617181920212223242526272829303132# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def generateTrees(self, n): """ :type n: int :rtype: List[TreeNode] """ if n==0: return [] return self.helper(1, n) def helper(self,begin,end): if begin &gt; end: return [None] if begin == end: return [TreeNode(begin)] result = [] for i in xrange(begin,end+1): tmp = [] left = self.helper(begin, i-1) right = self.helper(i+1, end) for m in left: for n in right: root = TreeNode(i) root.left, root.right = m, n result.append(root) return result 96. Unique Binary Search Trees 的解决思路类似于上面提到的递归的方法，只是这道题目仅仅要求返回二叉树的数量。因此可以通过一个技巧缩短计算量，这个技巧就是整数 i 作为根节点和整数 n-i 作为根节点的时候，两者构造的二叉树的数目是一样的。因此整数 i 作为根节点的左右子树的节点数目与整数 n-i 作为根节点的左右子树的节点数目对称相等。 实现的代码如下：123456789101112131415161718192021222324# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def numTrees(self, n): """ :type n: int :rtype: int """ result = 0 if n&lt;=1: return 1 for i in xrange(n/2): left = self.numTrees(i) right = self.numTrees(n-i-1) result += left * right result *= 2 if n%2 == 1: result += pow(self.numTrees(n/2),2) return result 要注意的是当 n 为基数的时候，整数(n+1)/2没有这种对称关系，需要额外计算。 从上面也可以衍生出动态规划的解法，令 dp[m] 表示 m 个节点能够构造的二叉树的数目。那么dp[m+1]的计算方法如下12for i in xrange(0, m+1): dp[m+1] += dp[i]*dp[m+1-i-1] 实现的代码如下：123456789101112class Solution(object): def numTrees(self, n): """ :type n: int :rtype: int """ dp = [0 for i in xrange(n+1)] dp[0],dp[1] = 1, 1 for i in xrange(2,n+1): for j in xrange(i): dp[i] += dp[j]*dp[i-j-1] return dp[n]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HTTP协议简介]]></title>
      <url>%2F2016%2F06%2F10%2FHTTP%E5%8D%8F%E8%AE%AE%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[HTTP协议采用了非常简单的请求-响应模式。由浏览器向网站发出请求（称为http request），网站根据请求将相关的资源返回给浏览器（称为http response）。这样周而复始就形成了网络通信。 HTTP协议是无状态的，也就是说同一个客户端的这次请求和上次请求是独立的，对http服务器来说，它并不知道这两个请求来自同一个客户端。为了解决这个问题，在HTTP中通过Session和Cookie机制来维护状态。 HTTP 请求（http request）消息的结构HTTP消息可分为三部分，第一部分叫request line（请求行）， 第二部分叫http header, 第三部分是body。下面通过一个例子解释 12345678910# 第一部分：request lineGET http://www.google.com/ HTTP/1.1 # 第二部分：http headerHost: www.google.comProxy-Connection: keep-aliveAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36...# 第三部分：body（由于请求方法为get，所以body为空） 从上面的例子可以看到：request line 可分为三部分，分别是请求方法 请求资源 使用的协议，上面的例子中，请求的方法是GET，请求的资源是http://www.google.com/，使用的HTTP协议是1.1版本的，也有1.0版本的HTTP协议，主要区别在于1.1版本允许多个HTTP请求复用一个TCP连接，以加快传输速度。 http header 是格式为key:value的一系列kv对，这些kv对主要作用是告知服务器关于浏览器的一些基本信息，如请求的host、使用的终端类型、页面缓存情况等。具体作用要根据具体的kv对分析 body 是发送给服务器的query信息 当使用的是”GET” 方法的时候，body是为空的（GET只能读取服务器上的信息，post能写入) 上面提到的三部分通过换行符\r\n，其中request line永远都是占第一行，接下来每个header一行一个，换行符是\r\n，当遇到连续两个\r\n时，Header部分结束，后面的数据全部是Body 请求的方法上面提到的request line的最后一个部分为请求的方法，在HTTP中请求的方法最基本的有4种，分别是GET,POST,PUT,DELETE。 一个URL地址用于描述一个网络上的资源，而HTTP中的GET, POST, PUT, DELETE 就对应着对这个资源的查，改，增，删4个操作。 我们最常见的就是GET和POST了。GET一般用于获取/查询资源信息，而POST一般用于更新资源信息.GET方法和POST方法的区别如下： GET 提交的数据会放在URL之后，以?分割URL和传输数据，参数之间以&amp;相连，如EditPosts.aspx?name=test1&amp;id=123456。POST 方法是把提交的数据放在HTTP包的Body中。 GET 提交的数据大小有限制（因为浏览器对URL的长度有限制），而POST方法提交的数据没有限制. GET 方式提交数据，会带来安全问题，比如一个登录页面，通过GET方式提交数据时，用户名和密码将出现在URL上，如果页面可以被缓存或者其他人可以访问这台机器，就可以从历史记录获得该用户的账号和密码. HTTP 响应（http response）消息的结构HTTP也分为三部分，第一部分叫response line, 第二部分叫response header，第三部分是body。下面是对应着上面的请求的一个响应 12345678910111213# 第一部分：response lineHTTP/1.1 302 Found # 第二部分：response headerLocation: http://www.google.com.hk/url?sa=p&amp;hl=zh-CN&amp;pref=hkredirect&amp;pval=yes&amp;q=http://www.google.com.hk/%3Fgws_rd%3Dcr&amp;ust=1465567219496491&amp;usg=AFQjCNGmfj-b-0AJ0D2coSy_40k76XajIwCache-Control: privateContent-Type: text/html; charset=UTF-8Date: Fri, 10 Jun 2016 13:59:49 GMTServer: gwsContent-Length: 390Proxy-Connection: keep-alive# 第三部分：body.... 从上面的例子可以看到：response line可以分为两部分，分别是使用的HTTP协议，状态码及其含义。在上面的例子中使用的协议版本为1.1，状态码为302，表示重定向，也就是会向response中的location表示的url发出新的请求。 response header也是格式为key:value的一系列kv对，表示服务器的状态和返回的内容的一些信息：如Content-Type表明返回的内容的类型，该类型也决定了body的内容，如果是网页，Body就是文本，如果是图片，Body就是图片的二进制数据；Content-Length表明返回的内容的类型；Content-Encoding表示，Body数据是被压缩的，最常见的压缩方式是gzip，如content-Encoding: gzip，压缩的目的在于减少Body的大小，加快网络传输。 body 则是返回给浏览器的实际内容，由content决定其类型。如上面的例子中返回的类型是text\html,则body的内容是该网页的html代码。如果该html代码中还有其他的资源如图片等，浏览器会发送一个新的http请求来获取这个资源。 状态码response line中的状态码表示对浏览器的请求的回应状况，状态码由三位数字组成，第一个数字定义了响应的类别其中 1XX 提示信息 - 表示请求已被成功接收，继续处理 2XX 成功 - 表示请求已被成功接收，理解，接受 3XX 重定向 - 要完成请求必须进行更进一步的处理 4XX 客户端错误 - 请求有语法错误或请求无法实现 5XX 服务器端错误 - 服务器未能实现合法的请求 一些常见的状态码及其含义如下所示： 200 OK请求被成功地完成，所请求的资源发送回客户端 302 Found重定向，新的URL会在response中的Location中返回，浏览器将会使用新的URL发出新的Request 304 Not Modified文档已经被缓存，直接从缓存调用 400 Bad Request客户端请求与语法错误，不能被服务器所理解 403 Forbidden服务器收到请求，但是拒绝提供服务 404 Not Found请求资源不存在 500 Internal Server Error服务器发生了不可预期的错误 503 Server Unavailable服务器当前不能处理客户端的请求，一段时间后可能恢复正常 更多状态码详细解释见http://tool.oschina.net/commons?type=5]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 中使用SQLAlchemy]]></title>
      <url>%2F2016%2F06%2F08%2Fpython%20%E4%B8%AD%E4%BD%BF%E7%94%A8SQLAlchemy%2F</url>
      <content type="text"><![CDATA[数据库表是一个二维表，包含多行多列。通过python获取数据库中的内容时，可以用一个list表示获取的多行记录，每一个元素的类型是tuple，表示一行记录，比如，包含id和name的user表： 12345[ (&apos;1&apos;, &apos;Michael&apos;), (&apos;2&apos;, &apos;Bob&apos;), (&apos;3&apos;, &apos;Adam&apos;)] 通过ORM技术：Object-Relational Mapping，可以把关系数据库的表结构映射到对象上。如： 12345678910class User(object): def __init__(self, id, name): self.id = id self.name = name[ User('1', 'Michael'), User('2', 'Bob'), User('3', 'Adam')] ORM框架就是用来完成这种装换的，在Java中最常用的是Hibernate，而在Python中，最有名的ORM框架是SQLAlchemy。SQLAlchemy 的简单使用如下 创建表123456789101112131415161718192021# 导入:from sqlalchemy import Column, String, create_enginefrom sqlalchemy.orm import sessionmakerfrom sqlalchemy.ext.declarative import declarative_base# 创建对象的基类:Base = declarative_base()# 定义User对象:class User(Base): # 表的名字: __tablename__ = 'user' # 表的结构: id = Column(String(20), primary_key=True) name = Column(String(20))# 初始化数据库连接:engine = create_engine('mysql+mysqlconnector://root:password@localhost:3306/test')# 创建DBSession类型:DBSession = sessionmaker(bind=engine) create_engine()用来初始化数据库连接。SQLAlchemy用一个字符串表示连接信息： 数据库类型+数据库驱动名称://用户名:口令@机器地址:端口号/数据库名 插入记录由于有了ORM，我们向数据库表中添加一行记录，可以视为添加一个User对象：12345678910# 创建session对象:session = DBSession()# 创建新User对象:new_user = User(id='5', name='Bob')# 添加到session:session.add(new_user)# 提交即保存到数据库:session.commit()# 关闭session:session.close() session对象可视为当前数据库连接。关键是获取session，然后把对象添加到session，最后提交并关闭。 查询记录通过ORM查询出来的可以不再是tuple，而是User对象。SQLAlchemy提供的查询接口如下：123456789# 创建Session:session = DBSession()# 创建Query查询，filter是where条件，最后调用one()返回第一行，如果调用all()则返回所有行:user = session.query(User).filter(User.id=='5').one()# 打印类型和对象的name属性:print 'type:', type(user)print 'name:', user.name# 关闭Session:session.close() 运行结果如下：12type: &lt;class '__main__.User'&gt;name: Bob 关SQLAlchemy更详细的用法可参考官方文档 参考：http://www.liaoxuefeng.com/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 中使用 SQLite]]></title>
      <url>%2F2016%2F06%2F07%2Fpython%20%E4%BD%BF%E7%94%A8SQLite%2F</url>
      <content type="text"><![CDATA[SQLite是一款轻量级的关系型数据库，相比MySQL等CS模式的数据库，SQLite有以下特点： 不需要一个单独的服务器进程或操作的系统（无服务器的）。 SQLite 不需要配置，这意味着不需要安装或管理。 SQLite 是非常小的，是轻量级的，完全配置时小于400KiB，省略可选功能配置时小于250KiB。 一个完整的 SQLite数据库是存储在一个单一的跨平台的磁盘文件。 SQLite事务是完全兼容 ACID 的，允许从多个进程或线程安全访问。这里科普一下ACID的定义，摘自维基百科 ACID，是指数据库管理系统（DBMS）在写入/更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。隔离性：数据库允许多个并发事务同时对齐数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 SQLite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。 由于SQLite本身是C写的，而且体积很小，所以，经常被集成到各种应用程序中，甚至在iOS和Android的App中都可以集成。 Python就内置了SQLite3，所以，在Python中使用SQLite，不需要安装任何东西，直接使用。 python中使用SQLite的步骤与使用MySQL的步骤非常类似，主要分为下面三步： 1.获取连接2.获取游标3.执行语句并提交事务 下面是操作SQLite的一个简单例子 123456789101112131415161718import sqlite3DB = 'test.db'try: conn = sqlite3.connect(DB) cursor = conn.cursor() cursor.execute('create table student(id varchar(10),name varchar(20))') cursor.execute('insert into student values("2012","lc")') cursor.execute('select * from student where name=?',('lc',)) result = cursor.fetchall() for row in result: print rowexcept sqlite3.Error as e: print efinally: cursor.close() conn.commit() conn.close() 以下几点将有助于更好地理解上面的代码： 由于SQLite是一个嵌入式的本地数据库，所以连接时不需要指定服务器地址、用户等。只需要通过sqlite3.connect(DB)便可连接数据库DB，假如没有该数据库时会自动创建。 由于SQLite是关系型数据库，所以绝大部分的SQL语句规范与MySQL等类似 如果执行的SQL语句需要从外部传入变量，则需要在SQL语句中将变量替换成？,并在execute方法增加第二个参数（tuple类型） fetchall()会返回一个list，list中的每个元素都是一个tuple，代表数据库中的一行记录 执行完语句后需要关闭cursor和conn，并且在关闭conn前需要进行commit(),否则修改不会生效]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[二叉树的遍历方法]]></title>
      <url>%2F2016%2F06%2F05%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86%E6%96%B9%E6%B3%95%2F</url>
      <content type="text"><![CDATA[二叉树的遍历方法有三种，分别是前序遍历，中序遍历和后序遍历。其中的前、中、后分别表示根节点在遍历中被访问的次序。因此，各个遍历方式的访问顺序如下所示： 前序遍历：根节点–&gt;左子树–&gt;右子树中序遍历：左子树–&gt;根节点–&gt;右子树后序遍历：左子树–&gt;右子树–&gt;根节点 下面通过 python 分别实现这三种遍历的递归方法和非递归方法，首先定义每个节点如下所示 123456# Definition for a binary tree node.class TreeNode(object): def __init__(self, x): self.val = x self.left = None self.right = None 前序遍历递归实现递归实现的方法非常简单，就是先访问根节点，然后访问左子树，最后访问右子树即可。实现代码如下所示：123456789101112def preorderTraversal(root): """ :type root: TreeNode :rtype: List[int] """ result = [] if root == None: return result result.append(root.val) result += preorderTraversal(root.left) result += preorderTraversal(root.right) return result 非递归实现非递归的方法通过栈来实现，流程如下（1）将根节点设为当前节点（2）记录当前节点C的值，然后将C入栈，将当前节点设为C的左子节点（3）重复步骤（2）直到访问到空叶子节点，然后从栈中弹出一个元素D，并将当前节点设为D的右子节点,重复步骤（2）（4)重复步骤（2）~（3）直到栈为空 实现代码如下所示：12345678910111213141516def preorderTraversal(root): """ :type root: TreeNode :rtype: List[int] """ stack, result = [], [] curr_node = root while len(stack) != 0 or curr_node!= None: if curr_node != None: result.append(curr_node.val) stack.append(curr_node) curr_node = curr_node.left else: tmp = stack.pop() curr_node = tmp.right return result 中序遍历递归实现递归实现也非常简单，按照顺序访问即可，实现代码如下：123456789101112def inorderTraversal(root): """ :type root: TreeNode :rtype: List[int] """ result = [] if root == None: return result result += inorderTraversal(root.left) result.append(root.val) result += inorderTraversal(root.right) return result 非递归实现非递归实现的过程类似于前序遍历的非递归实现，只是在记录节点的时间不同，中序周游记录是在元素从栈里弹出的时候才记录元素的值。实现的步骤如下： （1）将根节点设为当前节点（2）将当前节点C入栈，然后将当前节点设为C的左子节点（3）重复步骤（2）直到访问到空叶子节点，然后从栈中弹出一个元素D，记录元素D的值，并将当前节点设为D的右子节点,重复步骤（2）（4)重复步骤（2）~（3）直到栈为空 12345678910111213141516def inorderTraversal( root): """ :type root: TreeNode :rtype: List[int] """ stack, result= [], [] curr_node = root while len(stack) != 0 or curr_node != None: if curr_node != None: stack.append(curr_node) curr_node = curr_node.left else: tmp = stack.pop() result.append(tmp.val) curr_node = tmp.right return result 后序遍历递归实现后序遍历的实现也非常简单，只需要按照顺序访问即可，实现代码如下所示：123456789101112def postorderTraversal(root): """ :type root: TreeNode :rtype: List[int] """ result = [] if root == None: return result result += postorderTraversal(root.left) result += postorderTraversal(root.right) result.append(root.val) return result 非递归实现后续遍历的非递归实现利用了一点小技巧，就是先进行根节点--&gt;右子树--&gt;左子树的遍历,然后将得到的结果进行反转（reverse）即可。这是因为当根节点最后访问时无法确定何时该记录这个值。 实现的过程也类似于前序遍历，具体过程如下：（1）将根节点设为当前节点（2）记录当前节点C的值，然后将C入栈，将当前节点设为C的右子节点（3）重复步骤（2）直到访问到空叶子节点，然后从栈中弹出一个元素D，并将当前节点设为D的左子节点,重复步骤（2）（4)重复步骤（2）~（3）直到栈为空 实现代码如下所示：1234567891011121314151617def postorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ stack, result = [], [] curr_node = root while len(stack)!=0 or curr_node != None: if curr_node != None: result.append(curr_node.val) stack.append(curr_node) curr_node = curr_node.right else: tmp = stack.pop() curr_node = tmp.left result.reverse() return result]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[并行算法的设计]]></title>
      <url>%2F2016%2F06%2F04%2F%E8%AE%BE%E8%AE%A1%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[本文主要讲述并行算法设计的一些注意事项。 设计并行算法首先需要要确认问题是否可以被并行化，最典型的情况是在处理一个任务队列时，队列中的前后任务是独立的，这时候就可以通过并行化让多个进程或线程同时处理其中的多个任务。关键点在于大任务可以被分解为若干个互相独立的小任务。 设计并行算法可以从以下四方面着手1）分治(divide and conquer)2）数据分解(data decomposition)3）管道分解任务(decomposing tasks with pipeline)4）任务分配(processing and mapping) 分治(divide and conquer)分治法是一个非常常用的思想，通过将一个问题分解成规模更小的子问题，然后并行解决子问题，再合并子问题的结果即可。如快速排序、归并排序都是分治法的经典例子。下图是归并排序的一个例子 数据分解(data decomposition)数据分解指当要处理的数据的最小单元间相互独立时，可通过并行化来实现。如将一个2X2的矩阵中的每个元素都乘上4时，如果采取串行化需要按顺序将每个元素逐一乘上4，但是采取并行化时可以同时将每个元素乘上4。如下所示，图中的worker指进程或线程。 上面的例子比较简单，仅仅是为了说明数据分解的概念，在实际应用中，往往还需要考虑数据量与worker数量的不对称性问题，并且在将各自的结果合并时中需要考虑不同worker间的通信问题。 管道分解任务(decomposing tasks with pipeline)管道（pipeline），也可以称为流水线技术，是类似于工厂生产的流水线的一种设计方式。如下图所示就是流水线的一个例子： 上图首先将一个大任务分成了四个小任务，然后每个worker分别处理其中的一个任务，每个任务的输出是下一个任务的输入。 假设每个小任务消耗的时间是t，那么当完成1个大任务的时候串行和并行方式消耗的时间均是4t。但是当完成k个大任务时，串行化消耗的时间是4t*k,但是并行化需要的时间是4t+(k-1)*t,当k很大时，并行化节省的时间就相当可观了。 任务分配(processing and mapping)上面提到的三种方法均是将大任务分解，然后通过并行完成小任务来实现并行化。但是在将任务分解后,需要注意的是如何分配任务给各个 worker，使得每个 worker 的负载均衡，从而达到最优的效果。 在这个问题主要是要区分独立的任务和需要交换数据（通信）的任务。独立的任务可以分配给不同的worker去完成，因为这些任务不需要通信的成本；而将需要经常进行通信的任务让单独一个worker完成，考虑到网络通信的开销，这样能够提高性能。 总结上面主要提到了并行化算法的几个关键点，包括将大任务进行分解的几种方法（分治、数据分解、管道）以及将分解后的任务分配给worker时的注意事项。要注意的是这几种方法在实际中常常会混用，举一个实际一点的例子，如果要对一个很大的数组排序，单台机器的内存都放不下这个数组了，那该怎么办？ 首先将数组分成k份，然后分配给k台机器分别进行排序，排序完毕后我们有了k个sorted list，然后将k个sorted list两两合并，当合并后的数据越来越大时，单台机器内存不足时，可以采取外排序，将两个sorted list存储在硬盘中，每次取出前n个进行合并。 也许在实际中有更好的方法，但是上面的例子中实现的并行化就是利用到了分治法和数据分解法，实际中还可以根据机器的配置情况分配不同的任务负载。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python网络编程]]></title>
      <url>%2F2016%2F06%2F03%2Fpython%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[网络编程根据协议划分可以划分为TCP编程和UDP编程。两者的主要区别在于效率和可靠性，下面分别讲述两者在python中的实现 TCP编程客户端要创建一个基于TCP连接的Socket，可以这样做：12import sockets = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 创建Socket时，AF_INET指定使用IPv4协议，如果要用更先进的IPv6，就指定为AF_INET6。SOCK_STREAM指定使用面向流的TCP协议，这样，一个Socket对象就创建成功，但是还没有建立连接。 客户端要主动发起TCP连接，必须知道服务器的IP地址和端口号.接着上面的代码，通过创建的socket连接到本地服务器上。 123host = ('127.0.0.1', 80) # tuple类型s.connect(host)s.send('GET / HTTP/1.1\r\nHost: 127.0.0.1\r\nConnection: close\r\n\r\n') 建立TCP连接后，我们就可以向服务器发送请求。但是由于TCP连接创建的是双向通道，双方都可以同时给对方发数据。但是谁先发谁后发，怎么协调，要根据具体的协议来决定。例如，HTTP协议规定客户端必须先发请求给服务器，服务器收到后才发数据给客户端。 发送的文本格式必须符合HTTP标准，如果格式没问题，接下来就可以接收服务器返回的数据了：1234567891011# 接收数据:buffer = []while True: # 每次最多接收1k字节: d = s.recv(1024) if d: buffer.append(d) else: breakdata = ''.join(buffer)s.close() 接收数据时，调用recv(max)方法，一次最多接收指定的字节数，因此，在一个while循环中反复接收，直到recv()返回空数据，表示接收完毕，退出循环。当我们接收完数据后，调用close()方法关闭Socket，这样，一次完整的网络通信就结束了： 服务器端服务器进程首先要绑定一个端口并监听来自其他客户端的连接。如果某个客户端连接过来了，服务器就与该客户端建立Socket连接，随后的通信就靠这个Socket连接了 服务器需要同时响应多个客户端的请求，所以，每个连接都需要一个新的进程或者新的线程来处理，否则，服务器一次就只能服务一个客户端了。 首先，创建一个基于IPv4和TCP协议的Socket： 1s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 然后，我们要绑定监听的地址和端口。服务器可能有多块网卡，可以绑定到某一块网卡的IP地址上，也可以用0.0.0.0绑定到所有的网络地址，还可以用127.0.0.1绑定到本机地址。127.0.0.1是一个特殊的IP地址，表示本机地址，如果绑定到这个地址，客户端必须同时在本机运行才能连接，也就是说，外部的计算机无法连接进来。 端口号需要预先指定。因为我们写的这个服务不是标准服务，所以用9999这个端口号。请注意，小于1024的端口号必须要有管理员权限才能绑定,紧接着，调用listen()方法开始监听端口，传入的参数指定等待连接的最大数量： 1234# 绑定并监听端口:s.bind(('127.0.0.1', 9999))s.listen(5)print 'Waiting for connection...' 接下来，服务器程序通过一个永久循环来接受来自客户端的连接，accept()会等待并返回一个客户端的连接:123456while True: # 接受一个新连接: sock, addr = s.accept() # 创建新线程来处理TCP连接: t = threading.Thread(target=tcplink, args=(sock, addr)) t.start() 每个连接都必须创建新线程（或进程）来处理，否则，单线程在处理连接的过程中，无法接受其他客户端的连接：1234567891011def tcplink(sock, addr): print 'Accept new connection from %s:%s...' % addr sock.send('Welcome!') while True: data = sock.recv(1024) time.sleep(1) if data == 'exit' or not data: break sock.send('Hello, %s!' % data) sock.close() print 'Connection from %s:%s closed.' % addr 要测试这个服务器程序，示例的客户端程序如下所示：1234567891011s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 建立连接:s.connect(('127.0.0.1', 9999))# 接收欢迎消息:print s.recv(1024)for data in ['Michael', 'Tracy', 'Sarah']: # 发送数据: s.send(data) print s.recv(1024)s.send('exit')s.close() UDP编程客户端客户端使用UDP时，首先要创建基于UDP的Socket，然后，不需要调用connect()，直接通过sendto()给服务器发数据：1234567s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)for data in ['Michael', 'Tracy', 'Sarah']: # 发送数据: s.sendto(data, ('127.0.0.1', 9999)) # 接收数据: print s.recv(1024)s.close() 创建Socket时，SOCK_DGRAM指定了这个Socket的类型是UDP,TCP则是SOCK_STREAM。 从服务器接收数据仍然调用recv()方法。 服务器端TCP是建立可靠连接，相对TCP，UDP则是面向无连接的协议。 使用UDP协议时，不需要建立连接，只需要知道对方的IP地址和端口号，就可以直接发数据包。但是，能不能到达就不知道了。 虽然用UDP传输数据不可靠，但它的优点是和TCP比，速度快，对于不要求可靠到达的数据，就可以使用UDP协议。 和TCP类似，使用UDP的通信双方也分为客户端和服务器。服务器首先需要绑定端口：123s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)# 绑定端口:s.bind((&apos;127.0.0.1&apos;, 9999)) 创建Socket时，SOCK_DGRAM指定了这个Socket的类型是UDP,TCP则是SOCK_STREAM。绑定端口和TCP一样，但是不需要调用listen()方法，而是直接接收来自任何客户端的数据： 123456print 'Bind UDP on 9999...'while True: # 接收数据: data, addr = s.recvfrom(1024) print 'Received from %s:%s.' % addr s.sendto('Hello, %s!' % data, addr) recvfrom()方法返回数据和客户端的地址与端口，这样，服务器收到数据后，直接调用sendto()就可以把数据用UDP发给客户端。 参考：http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/00138683226192949cd41410a6d4f1ebfa9ba40bbd1399d000]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python 并行编程概述]]></title>
      <url>%2F2016%2F05%2F29%2Fpython%20%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0%2F</url>
      <content type="text"><![CDATA[程序并行化的形式程序并行化有以下三种形式，分别是：并发编程（Concurrent Programming）、并行编程（Parallel Programming ）、分布式编程(Distributed Programming) 并发编程（Concurrent Programming）并发编程（Concurrent Programming）的模型图如下所示： 从图中可知，并发编程类似操作系统中的伪并行，任一时刻只有一个进程占用CPU，通过调度控制不同的进程在不同时刻访问CPU。 并行编程（Parallel Programming ）并行编程的模型图如下所示： 并行编程指在多核环境中，同一时间每个核都可以允许一个进程运行，这可以认为是真正意义上的并行 分布式编程(Distributed Programming)分布式编程的模型图如下所示： 分布式编程指在不同机器上同时完成同一项任务，是物理上的隔离。如Hadoop中的MapReduce就是分布式编程的一个典型例子。 并行化编程的通信方式由于并行化后的进程要完成的是同一项任务，所以程序间的通信是必须的。程序的通信方式一般有以下两种：共享状态（shared state）、消息传递（message passing） 共享状态（shared state）这种方法就是共享进程间的资源，类似于同一进程里所有的线程共享进程的资源一样。 这种方法有以下不足：任一进程对共享资源的错误操作都会影响其他的进程；难以应用在分布式编程中。 在这种通信方式下，对于只读的数据可以不加保护措施，但是对于可写的数据，必须要防止多个进程同时修改这个数据。如在操作系统中的互斥量（mutex），线程锁等就是这类型的防护措施。 消息传递（message passing）消息传递能够避免上面提到的问题，而且也能够应用在分布式编程中。每进行一次消息传递，都会复制一份数据，因此数据的一致性大大提升。 虽然这种方法占用的内存比第一种要大，但是这种方法有以下优势： 数据的一致性大大增强 消息能够在本地传输（多进程）或者在分布式环境中传输 解决可伸缩问题并允许不同系统间的相互操作 对于编程人员来说便于实现 并行化编程存在的问题在并行化编程中有可能会遇到以下问题 死锁(DeadLock)与操作系统中的死锁问题一样，发生在多个进程中每个都需要其他进程的资源，同时又不肯释放自己的资源，导致资源的需求关系形成闭合的环状。 如下图所示，进程A需要进程C的资源，进程C需要进程B的资源，而进程B需要进程A的资源。并且在进程释放自己的资源前，其他进程无法获取，而进程需要获得其他进程的资源才能完成自己的任务并释放资源，这样就是一个死锁的局面。 饿死(Starvation)饿死概念与操作系统中的也是一样，指的是某个进程一直得不到自己的资源，无法继续运行。 如进程 A 的优先级比 B 要高，所以优先运行A，但是进程A由于需要完成的任务繁重，所以一直占用着CPU，导致进程B一直无法运行，就称为进程B被饿死。 竞争条件(Race conditons)竞争条件是操作系统和电子电路中的一个常见概念，维基百科对其定义如下： A race condition or race hazard is the behavior of an electronic, software or other system where the output is dependent on the sequence or timing of other uncontrollable events. It becomes a bug when events do not happen in the order the programmer intended. The term originates with the idea of two signals racing each other to influence the output first. 大意就是多个具有不确定性（无法知道何时会到达或执行指定操作）的对象（进程或电子信号），必须要按照时间序列（time sequence）执行,假如这种同步被破坏，那么多个进程会没有顺序地修改同一个变量，导致数据出错。如下面的简单例子： 假设图中的husband和wife是两个进程，两者同时操作账户里的钱，正常情况下是这样的 而假如两者不按照time sequences执行操作，同步会被破坏，导致出现race conditions，如下图所示： python中实现并行化编程工具 threading 模块，python自带的多线程模块 multiprocessing模块，python自带的多进程模块 parallel Python模块，具有运行过程调整进程数目、动态负载平衡的第三方模块 Celery 模块，用于分布式编程的一个分布式任务队列模块 参考：Parallel Programming with Python]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关键词抽取算法的研究]]></title>
      <url>%2F2016%2F05%2F28%2F%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96%E7%AE%97%E6%B3%95%E7%9A%84%E7%A0%94%E7%A9%B6%2F</url>
      <content type="text"><![CDATA[关键词抽取的一般步骤为： 分词–&gt;过滤停止词,得到候选关键词–&gt;从候选关键词中选出文章的关键词 从候选关键词中选出文章的关键词需要通过关键词抽取算法实现，而关键词抽取算法可以根据是否需要人工标注的语料进行训练而分为有监督的提取和无监督的提取。有监督的提取需要人工标注的语料进行训练，人工预处理的代价较高。而无监督的抽取算法直接利用需要提取关键词的文本即可进行关键词的提取，因此适用性较强。 关键词抽取中无监督的抽取算法可分为三大类：1）基于统计特征的，如TF-IDF2）基于词图模型的，如TextRank3）基于主题模型的，如LDA 本文主要讲述TF-IDF算法、TextRank算法、以及通过组合两者得到的三种新方法，然后通过Java实现这几种方法并比较这几种方法在特定语料库上进行关键词抽取的效果。 TF-IDF算法TF-IDF（Term Frequency-Inverse Document Frequency）算法是一种基于统计特征的非常经典的算法，通过计算一个词的TF值和IDF值的乘积作为该值的得分，然后根据得分从大到小对词语排序，选择分数高的词语作为关键词。 TF值指词语在文本中出现的频率，如某篇文章分词并过滤停止词后的词语的数量为n，而其中的某个词语w出现的个数为m,则词w的TF值为 IDF值则指词语在整个语料库中的出现的频率大小。这里首先要指出的是TF-IDF算法是针对一个语料库（也就是多篇文档进行）进行关键词提取的算法。假如语料库中共有N篇文档，而出现了词语w的文档数为M。则词w的IDF值为 $TF(w)*IDF(w)$则为词w的TF-IDF值，根据这个值对候选词从大到小排序，选择前n个作为候选关键词即可。 通过Java的实现并不难，主要是利用Java的集合框架Map、List等存储词语的中间得分、以及候选关键词等。 实现的完整代码见:https://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TFIDF.java TextRank算法TextRank算法是借鉴PageRank算法在语言处理中的一个算法，关于PageRank算法可参考这篇文章。无论是PageRank还是TextRank，其关键思想都是重要性传递。 以PageRank为例，假如一个大型网站有一个超链接指向了某个小网站，那么小网站的重要性会上升，而上升的量则依据指向它的大网站的重要性。下图所示的就是一个例子： 假设网页A,B原来的重要性为100和9，那么根据他们指向的网页，传递给C、D的重要性分别为53和50。 在TextRank中将上图的网页替换成词语，将网页间的超链接换成词语间的语义关系；假如两个词的距离小于预设的距离，那么就认为这两个词间存在语义关系，否则不存在。这个预设的距离在TextRank算法中被称为同现窗口（co-occurance window）。这样便可构建出一个词的图模型。 但是在实际中应用时我们是无法预先知道网页A、B的重要性的，又或者说假如我们已经知道了网页的重要性，那么也不需要通过算法计算出网页的重要性了。这就成了一个先有鸡还是先有蛋的问题。 PageRank的原始论文提出了解决这个问题的方法，这篇文章中通过具体的例子提到了相关的理论依据，就是幂法求特征向量与初始值无关。具体做法就是，先给每个网页随机附一个初值，然后通过迭代计算直至收敛，理论证明了收敛的值与初始值无关。 同样的，TextRank也采取了相同的方法，就是先随机赋初值，然后通过迭代计算得到每个词的重要性的得分。词语$V_i$的得分计算公式如下所示： 上式中各符号表示如下 实现的一个关键点在于构建词的图模型，在Java中通过队列实现，队列大小即为同现窗口的大小，移动队列的过程中将队列内部的词语互相连接。连接的形式通过java的Map&lt;String,Set&lt;String&gt;&gt;类型实现，表示指向词语（第一个String）的所有其他词语（Set&lt;String&gt;）的实现的关键代码如下： 12345678910111213141516171819202122232425262728Map&lt;String, Set&lt;String&gt;&gt; words = new HashMap&lt;String, Set&lt;String&gt;&gt;();Queue&lt;String&gt; que = new LinkedList&lt;String&gt;();for (String w : wordList) //wordList为候选关键词&#123; if (!words.containsKey(w)) &#123; words.put(w, new HashSet&lt;String&gt;()); &#125; que.offer(w); // 入队 if (que.size() &gt; coOccuranceWindow) &#123; que.poll(); // 出队 &#125; for (String w1 : que) &#123; for (String w2 : que) &#123; if (w1.equals(w2)) &#123; continue; &#125; words.get(w1).add(w2); words.get(w2).add(w1); &#125; &#125;&#125; 另外一个实现关键点就是判断算法是否收敛，可以认为前后两次计算出来的值小于指定的阈值（一般取值较小，如0.000001）时算法收敛，或者超过设定的最大迭代次数时停止。实现的关键代码如下所示：1234567891011121314151617181920212223242526min_diff = 0.000001Map&lt;String, Float&gt; score = new HashMap&lt;String, Float&gt;(); for (int i = 0; i &lt; max_iter; ++i) &#123; Map&lt;String, Float&gt; m = new HashMap&lt;String, Float&gt;(); float max_diff = 0; for (Map.Entry&lt;String, Set&lt;String&gt;&gt; entry : words.entrySet()) &#123; String key = entry.getKey(); Set&lt;String&gt; value = entry.getValue(); m.put(key, 1 - d); for (String other : value) &#123; int size = words.get(other).size(); if (key.equals(other) || size == 0) continue; m.put(key, m.get(key) + d / size * (score.get(other) == null ? 0 : score.get(other))); &#125; max_diff = Math.max(max_diff, Math.abs(m.get(key) - (score.get(key) == null ? 1 : score.get(key)))); &#125; score = m; //exit once recurse if (max_diff &lt;= min_diff) break; &#125; 完整的实现代码见：https://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TextRank.java 综合TextRank 多同现窗口由于TextRank的同现窗口的大小会影响提取的效果，如下图是同现窗口为2~10的时候评估值为F1值的变化情况。（测试语料） 而原始的TextRank算法仅仅是建议该值设为2~10，无法知道对于一篇文章的最优同现窗口，因此本方法会综合TextRank多同现窗口的结果，将一个词语在不同大小的窗口下的得分相加，作为该词的总得分，然后根据总得分对词语排序，选择得分较高的前n个词作为候选关键词 。 该算法的效果与原始的TextRank算法的效果对比如下（测试语料） 图中的textrank表示原始的TextRank算法的效果，而multi_window_textrank表示综合了大小为2~10的同现窗口的结果的效果。从图中可知，在提取关键词个数大于4个的时候，该方法的效果要优于原始的TextRank算法，但是F1值的提升幅度不大，并且实际运行的时候，综合多同现窗口的方法花费的时间是原始TextRank算法的14倍左右。 代码的具体实现见：https://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TextRankWithMultiWin.java TextRank 与 TF-IDF 综合考虑词语的IDF值由于TextRank算法仅考虑文档内部的结构信息，导致一些在各个文档的出现频率均较高且不属于停止词的词语最总的得分较高。原因是没有考虑词语在整个语料库中的权重。因此在TextRank算法得到的每个词的得分基础上，乘上这个词在整个语料库的IDF值，IDF值是TF-IDF算法中的一个概念，该值越大，表示这个词在语料库中出现的次数越少，越能代表该文档。 将词语的TextRank得分乘上这个词的IDF值后作为该词的新得分，然后根据得分从大到小排序，选择得分高的前n个词作为关键词即可。 下面是考虑了词语的IDF值的方法与原始的TextRank算法的效果对比图（测试语料） 从图中可知，考虑了词语的IDF值后的方法的效果要优于原始的TextRank算法，运行时间约为TextRank算法的两倍。 完整的代码实现见：https://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TextRankWithTFIDF.java TextRank与TF-IDF投票这种方法也是针对TextRank算法仅考虑文档内部的信息而忽略了文档外部的信息，综合TextRank算法和TF-IDF算法提取出来的结果。 具体的流程为：确定要抽取的关键词个数n,通过TextRank算法和TF-IDF算法对语料库分别提取2n个关键词,选择同时在两个算法得到的结果中出现的词语作为关键词，假如同时出现的词语不足n个，那么剩下的词语从TextRank的结果或TF-IDF的结果中补。 下面是TextRank和TF-IDF投票方法的结果与原始的TextRank算法的结果的对比图（测试语料） 从结果可知，两个算法综合投票的方法的效果要优于原始的TextRank算法。运行的时间约为原始的TextRank的两倍。 完整的代码实现见：https://github.com/WuLC/KeywordExtraction/blob/master/src/com/lc/nlp/keyword/algorithm/TextRankWithTFIDF.java 总结本文主要讲述了TextRank算法以及对其进行简单改进的三种方法：综合多同现窗口的结果、考虑词语的IDF值、TF-IDF与TextRank共同投票。通过Java实现并比较其效果（评判指标为F1值）。下图是这几个算法的总效果对比图。（测试语料） 综合多同现窗口的改进方案后的效果虽然要略优于原始的 TextRank 算法，但是消耗的时间是原始 TextRank 算法的 14 倍左右；综合 TextRank 算法和 TF-IDF 算法后的结果是改进算法后最优的，其次是考虑 TextRank 提取出的关键词的 IDF 值的改进方案，两者的效果均要优于原始的 TextRank 算法， 消耗的时间也比原始的 TextRank 算法要多。 因此，若需要对单篇文档提取的关键词时，可采用原始的TextRank算法或综合多同现窗口的方法， 假如对提取效果的要求较高且对时间要求不高时，可以采用综合多同现窗口的方法， 反之直接采用原始的TextRank算法。如果需要对多文档进行关键词抽取时，四种方法都可以采用，但是考虑提取的效果以及消耗的时间， 建议使用 TextRank 算法和 TF-IDF 算法综合投票的方法或 TextRank 结合IDF值的方法，并且根据着重点是时间还是提取的精度，选择 TF-IDF 算法综合投票的方法或 TextRank 结合 IDF 值的方法。 上文提到的所有代码的地址为：https://github.com/WuLC/KeywordExtraction除了算法的实现，还包括了语料库的导入、F1值的计算方法的实现等。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一些有意思的题目]]></title>
      <url>%2F2016%2F05%2F28%2F%E4%B8%80%E4%BA%9B%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E9%A2%98%E7%9B%AE%2F</url>
      <content type="text"><![CDATA[从互联网搜集的一些比较有趣的题目，通过python实现，换成其他语言大多也能实现。题目会尽量保持更新,如果您有好的项目推荐，欢迎在评论区留言。 github地址:https://github.com/WuLC/show-me-the-code Talk is cheap. Show me the code.–Linus Torvalds 第 0000 题：将你的 QQ 头像（或者微博头像）右上角加上红色的数字，类似于微信未读信息数量那种提示效果。类似于图中效果 第 0001 题：做为 Apple Store App 独立开发者，你要搞限时促销，为你的应用生成激活码（或者优惠券），使用 Python 如何生成 200 个激活码（或者优惠券）？ 第 0002 题：将 0001 题生成的 200 个激活码（或者优惠券）保存到 MySQL 关系型数据库中。 第 0003 题：将 0001 题生成的 200 个激活码（或者优惠券）保存到 Redis 非关系型数据库中。 第 0004 题：任一个英文的纯文本文件，统计其中的单词出现的个数。 第 0005 题：你有一个目录，装了很多照片，把它们的尺寸变成都不大于 iPhone5 分辨率的大小。 第 0006 题：你有一个目录，放了你一个月的日记，都是 txt，为了避免分词的问题，假设内容都是英文，请统计出你认为每篇日记最重要的词。 第 0007 题：有个目录，里面是你自己写过的程序，统计一下你写过多少行代码。包括空行和注释，但是要分别列出来。 第 0008 题：一个HTML文件，找出里面的正文。 第 0009 题：一个HTML文件，找出里面的链接。 第 0010 题：使用 Python 生成类似于下图中的字母验证码图片,同时写出识别验证码的程序。 阅读资料 第 0011 题： 敏感词文本文件 filtered_words.txt，里面的内容为以下内容，当用户输入敏感词语时，则打印出 Freedom，否则打印出 Human Rights。 北京 程序员 公务员 领导 牛比 牛逼 你娘 你妈 love sex jiangge 第 0012 题： 敏感词文本文件 filtered_words.txt，里面的内容 和 0011题一样，当用户输入敏感词语，则用 星号 替换，例如当用户输入「北京是个好城市」，则变成「*是个好城市」。 第 0013 题： 用 Python 写一个爬图片的程序,爬这个链接的壁纸 第 0014 题： 纯文本文件 student.txt为学生信息, 里面的内容（包括花括号）如下所示： { &quot;1&quot;:[&quot;张三&quot;,150,120,100], &quot;2&quot;:[&quot;李四&quot;,90,99,95], &quot;3&quot;:[&quot;王五&quot;,60,66,68] } 请将上述内容写到 student.xls 文件中，如下图所示： 阅读资料 腾讯游戏开发 XML 和 Excel 内容相互转换 第 0015 题： 纯文本文件 city.txt为城市信息, 里面的内容（包括花括号）如下所示： { &quot;1&quot; : &quot;上海&quot;, &quot;2&quot; : &quot;北京&quot;, &quot;3&quot; : &quot;成都&quot; } 请将上述内容写到 city.xls 文件中，如下图所示： 第 0016 题： 纯文本文件 numbers.txt, 里面的内容（包括方括号）如下所示： [ [1, 82, 65535], [20, 90, 13], [26, 809, 1024] ] 请将上述内容写到 numbers.xls 文件中，如下图所示： 第 0017 题： 将第 0014 题中的 student.xls 文件中的内容写到 student.xml 文件中，如 下所示： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;root&gt; &lt;students&gt; &lt;!-- 学生信息表 &quot;id&quot; : [名字, 数学, 语文, 英文] --&gt; { &quot;1&quot; : [&quot;张三&quot;, 150, 120, 100], &quot;2&quot; : [&quot;李四&quot;, 90, 99, 95], &quot;3&quot; : [&quot;王五&quot;, 60, 66, 68] } &lt;/students&gt; &lt;/root&gt; 第 0018 题： 将 第 0015 题中的 city.xls 文件中的内容写到 city.xml 文件中，如下所示： &lt;?xmlversion=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;root&gt; &lt;citys&gt; &lt;!-- 城市信息 --&gt; { &quot;1&quot; : &quot;上海&quot;, &quot;2&quot; : &quot;北京&quot;, &quot;3&quot; : &quot;成都&quot; } &lt;/citys&gt; &lt;/root&gt; 第 0019 题： 将 第 0016 题中的 numbers.xls 文件中的内容写到 numbers.xml 文件中，如下 所示： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;root&gt; &lt;numbers&gt; &lt;!-- 数字信息 --&gt; [ [1, 82, 65535], [20, 90, 13], [26, 809, 1024] ] &lt;/numbers&gt; &lt;/root&gt; 第 0020 题： 使用 Python 语言开发服务器端口扫描器，用来检测目标服务器上有哪些端口开放。 第 0021 题： 通常，登陆某个网站或者 APP，需要使用用户名和密码。密码是如何加密后存储起来的呢？请使用 Python 对密码加密。 阅读资料 用户密码的存储与 Python 示例 阅读资料 Hashing Strings with Python 阅读资料 Python’s safest method to store and retrieve passwords from a database 第 0022 题： 使用 Python 的 Web 框架，做一个 Web 版本 留言簿 应用。 阅读资料：Python 有哪些 Web 框架 第 0023 题： 通过有道翻译提供的API写一个支持命令行翻译单词的工具，效果如下图：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python语法杂记]]></title>
      <url>%2F2016%2F05%2F23%2Fpython%20%E8%AF%AD%E6%B3%95%E6%9D%82%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[本文主要记录一些学习过程中遇到的一些比较零碎的python语法知识。 常见易错用法for循环中修改下标的值python中的for循环一般会写成这样12for i in range(10): .... 上面的语句中循环了10此，i的值从0增到9。在Java中可以在for循环中修改i的值，从而跳过一些i的值不处理，但是在上面的语法中无效,因为range实际上生成了一个0到9的list，每次i会取其中的一个值，所以如果没有break的话，i会取遍10个值。 如果要达到修改i的值跳过一些值不处理，建议使用while语句。 字典初始化可通过{}或dict()函数进行初始化，通过dict()初始化时，可以选择是否传入参数，传入参数初始化时，参数格式为包含若干kv的一个list，每个kv用一个tuple表示，如12345678&gt;&gt;&gt;d = dict( [('foozelator', 123), ('frombicator', 18), ('spatzleblock', 34), ('snitzelhogen', 23) ])&gt;&gt;&gt;d&#123;'foozelator': 123, 'frombicator': 18, 'snitzelhogen': 23, 'spatzleblock': 34&#125; 删除一个key－ 从字典中删除一个key：dict.pop(key[, default]),存在key时返回key对应的value，不存在时返回default。不存在且没有default时返回KeyError。 遍历字典 dict.keys()返回字典dict所有keys组成的一个list dict.values()返回字典dict所有values组成的一个list dict.items()返回字典dict所有kv组成的一个list，kv以tuple的形式存储 对字典排序通过sorted函数可以根据字典的key或value对字典排序，并返回一个元素类型为tuple为的list，每个tuple代表字典中的一个元素。排序不会改变原来字典中的值。 12345678a = &#123;1:2,2:1&#125;# 根据key对字典排序,reverse = True表示从大到小，默认是从小到大sorted(a.items(), key = lambda x:x[0], reverse = True)# 输出为[(2,1),(1,2)]# 根据value对字典从小到大排序sorted(a.items(), key = lambda x:x[1])# 输出为[(2:1),(1:2)] 集合可变集合用{}或set()函数来生成可变集合，集合中不含有相同元素。123s=&#123;&#125; # 非法s=&#123;1,2,3,4&#125; # 合法s=set() # 也可用s = set(list),用一个集合提取list中的不重复元素 不可变集合对应于元组（tuple）与列表（list）的关系，对于集合（set），Python提供了一种叫做不可变集合（frozen set）的数据结构。 创建一个不可变集合123&gt;&gt;&gt;s = frozenset([1, 2, 3, 'a', 1])&gt;&gt;&gt;sfrozenset(&#123;1, 2, 3, 'a'&#125;) 不可变集合的一个主要应用是用来作为字典的键，例如用一个字典来记录两个城市之间的距离：123456789&gt;&gt;&gt;flight_distance = &#123;&#125;&gt;&gt;&gt;city_pair = frozenset(['Los Angeles', 'New York'])&gt;&gt;&gt;flight_distance[city_pair] = 2498&gt;&gt;&gt;flight_distance[frozenset(['Austin', 'Los Angeles'])] = 1233&gt;&gt;&gt;flight_distance[frozenset(['Austin', 'New York'])] = 1515&gt;&gt;&gt;flight_distance&#123;frozenset(&#123;'Austin', 'New York'&#125;): 1515, frozenset(&#123;'Austin', 'Los Angeles'&#125;): 1233, frozenset(&#123;'Los Angeles', 'New York'&#125;): 2498&#125; 集合的一些方法 添加元素，s.add(item) 交集，s1&amp;s2或s1.intersection(s2),返回集合s1和集合s2的交集 并集，s1|s2或s1.union(s2),返回集合s1和集合s2的并集 差集，s1-s2或s1.defference(s2)返回s1中有但s2中没有的元素的集合 对称差集,s1^s2或s1.symmetric_difference(s2)返回s1中有但s2中没有的元素和s2中有但s1中没有的元素的合集 子集，s1.issubset(s2)或s1&lt;=s2判断s1是否s2的子集；反之也可用s2.issuperset(s1)达到上面的效果 删除一个元素s.remove(element)或s.pop(element)后者会返回这个值元素的值而前者不会；不存在该元素时均会报错。s.discard(element)作用跟remove一样，区别在于不存在该元素时discard()不会报错 列表列表合并可通过加号+按顺序合并两个列表，如1234&gt;&gt;&gt;a = [1, 2, 3]&gt;&gt;&gt;b = [3.2, 'hello']&gt;&gt;&gt;a + b[1, 2, 3, 3.2, 'hello'] 列表的extend()方法也能实现相同功能。如：12345&gt;&gt;&gt;a = [1, 2, 3]&gt;&gt;&gt;b = [3.2, 'hello']&gt;&gt;&gt;a.extend(b)&gt;&gt;&gt;a[1, 2, 3, 3.2, 'hello'] 列表排序列表可用内置函数，分为两种类型：排序后改变原列表和排序后不改变列表 排序后改变原列表的方法是listName.sort(),不改变原列表的方法是sorted(listName)。具体见下面例子123456789&gt;&gt;&gt;s = [1,4,3,2]&gt;&gt;&gt;s.sort()&gt;&gt;&gt;s[1, 2, 3, 4]&gt;&gt;&gt;s = [1,4,3,2]&gt;&gt;&gt;sorted(s)[1, 2, 3, 4]&gt;&gt;&gt;s[1, 4, 3, 2] 默认是从小到大排序，也可从大到小排序，只需要加入reverse=True的参数即可12345678&gt;&gt;&gt;s=[1, 4, 3, 2]&gt;&gt;&gt;sorted(s,reverse=True)[4, 3, 2, 1]&gt;&gt;&gt;s[1, 4, 3, 2]&gt;&gt;&gt;s.sort(reverse=True)&gt;&gt;&gt;s[4, 3, 2, 1] 列表推导式(List comprehension)也叫列表生成式。 将多条语句写成一条，如要求列表a中所有偶数的和，可写成12a = [1,2,3,4,5,6]print sum([i for i in a if i%2==0]) sum()是求一个列表内所有元素的和的内置函数，传入的参可以为一个列表，而[i for i in a if i%2==0]则是列表推导式，该语句生成了列表[2,4,6] 但是，Python会生成这个列表，然后再将它放到垃圾回收机制中（因为没有变量指向它），这毫无疑问是种浪费。 为了解决这种问题，与rang()和xrange()的问题类似，Python使用生成器（generator）表达式来解决这个问题： 将sum中代表list的括号去掉即可，修改后如下所示：12a = [1,2,3,4,5,6]print sum(i for i in a if i%2==0) 上面的(i for i in a if i%2==0)就是一个生成器，与列表生成式最大的不同是列表生成式会在执行语句的时候生成完整的列表，而生成器会在在循环的过程中不断推算出后续的元素 除了上面这种定义生成器的方法，还可以在函数中通过yield关键字实现一个生成器。如下面生成斐波那契数列的例子：12345678def print_fibona(n): a,b=0,1, for i in range(n): yield b a,b = b,a+bfor i in print_fibona(8): print i, 输出结果为：11 1 2 3 5 8 13 21 print_fibona不是普通函数，而是generator，在执行过程中，遇到yield就中断，下次又继续执行。 列表的其他一些方法 查找某一元素在列表中出现了几次，list.count(element)返回element在list中出现的次数 查找某一元素在列表中第一次出现的位置，list.index(element)返回element在list中第一次出现的位置,不存在element元素时会报错 在特定位置插入某一元素，其他元素依次往后移动一步，list.insert(index,element)在index处插入element，原来在index处及后面的元素依次往后移动一位 删除元素，有两种方法，list.remove(element)会将list中第一次出现的element删除；list.pop(index)则会将list中下标为index的元素删除且返回该元素的值。 列表反转，list.reverse()回将list中的元素反转 map方法生成序列可以通过 map 的方式利用函数来生成序列,例子如下： 12345def sqr(x): return x ** 2a = [2,3,4]print map(sqr, a) 输出为[4, 9, 16] 其用法为map(aFun, aSeq),将函数 aFun 应用到序列 aSeq 上的每一个元素上，返回一个列表，不管这个序列原来是什么类型。 事实上，根据函数参数的多少，map 可以接受多组序列，将其对应的元素作为参数传入函数,例子如下：123456def add(x, y): return x + ya = (2,3,4)b = [10,5,3]print map(add,a,b) 结果为[12, 8, 7] 序列赋值序列（list,tuple,str)可以将其值逐一赋值给变量，详见下面的例子123456789&gt;&gt;&gt; a,b,c=[1,2,3]&gt;&gt;&gt; print a,b,c1 2 3&gt;&gt;&gt; a,b,c=(1,2,3)&gt;&gt;&gt; print a,b,c1 2 3&gt;&gt;&gt; a,b,c="123"&gt;&gt;&gt; print a,b,c1 2 3 数值整形（int）和长整形（long）整型数字的最大最小值 在 32 位系统中，一个整型 4 个字节，最小值 -2,147,483,648，最大值 2,147,483,647。 在 64 位系统中，一个整型 8 个字节，最小值 -9,223,372,036,854,775,808，最大值 9,223,372,036,854,775,807。 当整型超出范围时，Python会自动将整型转化为长整型，长整型就是在数字后面加上一个大写的L 复数Python 使用 j 来表示复数的虚部：12345a = 1 + 2jtype(a)a.real # 实部a.imag # 虚部a.conjugate() # 共轭 内置的一些数值函数abs(n)求n的绝对值round(n)求n的整数部分，返回的是float类型max(n,m)求m，n的最大值min(n,m)求m,n的最小值 其他的一些表示方法 科学计数法，1e-6表示 $10^{-6}$ 16进制，前面加上0x修饰，后面的数字范围为0~F 8进制，前面加上0修饰，后面的数字范围为0~7 2进制，前面加上0b修饰，后面数字范围为0~1 字符串常用的字符串方法 s.split(c),以符号c为分隔符将字符串s分割，返回字符串列表 c.join(sList)，作用跟上面的相反，以符号c为连接符将字符串数组sList连接起来 s.repalce(a,b),将字符串中的a替换为b，并返回替换后的字符串，注意s本身不变 s.upper()，将s中的英文字母转为大写的并返回，但是s本身不变 s.lower()，将s中的英文字母转为小写的并返回，但是s本身不变 s.strip()，去掉字符串s前后的空格并返回，但是s本身不变 s.lstrip()，去掉字符串s前的空格并返回，但是s本身不变 s.rstrip()，去掉字符串s后的空格并返回，但是s本身不变可通过dir(str)查找更多方法 数字与字符的转换整数转字符串 16进制：hex(255)返回’0xff’ 8进制：oct(255)返回’0377’ 2进制：bin(255)返回’0b11111111’ 字符串转整数通过int(s)转换，还可以指定特定的进制，默认是十进制。如下面的方法均返回255 int(‘ff’,16) int(‘377’,8) int(‘111111111’,2) int(‘255’) ASCII码与字符的转换 数字转ASCII码：chr(97) --&gt; &#39;a&#39; ASCII码转数字：ord(&#39;A&#39;) --&gt; 65 字符串的分片与索引索引指的是可以通过下标来寻找字符串中的某个字符，0下标代表第一个，-1下标代表倒数第一个，-2下标代表倒数第二个 分片指的是提取子字符串，一般格式为[start:end:step],start和end都是指字符串的下标，省略时默认为字符串的头和尾；step指每次取字符串的步长，省略时为1，也即是从start到end-1每个字符串都取，step也可取负值，表示从后往前按step的绝对值来取。如s[::-1]表示反转字符串 函数高阶函数（Higher-order function）把另外一个函数作为参数传入的函数称为高阶函数，函数式编程就是指这种高度抽象的编程范式。 map/reduce 函数 map函数：两个参数，第一个参数为接收一个参数的函数，第二个参数为一个序列，利用第一个参数所代表的函数对序列中的每个元素操作，返回操作后的序列 reduce函数：两个参数，第一个参数为接收两个参数的函数，第二个参数为一个序列，利用第一个参数代表的函数对序列中的两个首元素操作，返回的结果与序列的下一元素再进行函数的操作，直到遍历完序列。 例子：1234567# 利用map函数对列表中每个数进行平方操作&gt;&gt;&gt; map(lambda x:x**2,[1,2,3])[1, 4, 9]# 利用reduce函数实现sum()函数的功能&gt;&gt;&gt; reduce((lambda x,y:x+y),[1,2,3,4,5])15 上面均利用了lambda函数，也可以将lambda函数改成def定义好的函数。 filter函数Python内建的filter()函数用于过滤序列。和map()类似，filter()也接收一个函数和一个序列。和map()不同的时，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。 例子：过滤掉1~100中的素数并返回结果1234567import mathdef is_prime(num): for i in range(2,int(math.sqrt(num))+1): if num%i == 0: return Trueprint filter(is_prime,range(1,101)) sorted函数sorted函数除了可以用来给列表排序外，还可以通过排序函数作为传入参数，进行指定的排序。 排序函数通常规定，对于两个元素x和y，如果认为x &lt; y，则返回-1或负数，如果认为x == y，则返回0，如果认为x &gt; y，则返回1或正数。python内部定义的排序函数规则就是这样的，根据这样的原理，我们可以自定义一个排序函数进行降序排序。例子如下： 1234567def descend_sort(x,y): if x&gt;y: return -1 else: return 1print sorted(range(1,101),descend_sort) 上面的代码也可以简单写成sorted(range(1,101),lambda x,y:y-x) 返回函数高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回1234567def lazy_sum(*args): def sum(): s = 0 for i in args: s+=i return s return sum 调用lazy_sum()时，返回的并不是求和结果，而是求和函数： 123&gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function sum at 0x10452f668&gt; 调用函数f时，才真正计算求和的结果： 12&gt;&gt;&gt; f()25 在这个例子中，我们在函数lazy_sum中又定义了函数sum，并且，内部函数sum可以引用外部函数lazy_sum的参数和局部变量，当lazy_sum返回函数sum时，相关参数和变量都保存在返回的函数中，这种程序称为闭包（Closure） 另一个需要注意的问题是，返回的函数并没有立刻执行，而是直到调用了f()才执行。例子如下123456789def count(): fs = [] for i in range(1, 4): def f(): return i*i fs.append(f) return fsf1, f2, f3 = count() 在上面的例子中，每次循环，都创建了一个新的函数，然后，把创建的3个函数都返回了。 你可能认为调用f1( )，f2( )和f3( )结果应该是1，4，9，但实际结果是：12&gt;&gt;&gt;print f1(),f2(),f3()9 9 9 全部都是9！原因就在于返回的函数引用了变量i，但它并非立刻执行。等到3个函数都返回时，它们所引用的变量i已经变成了3，因此最终结果为9。 返回闭包时牢记的一点就是：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变.如下面的例子123456def count(): fs = [lambda x=y:x**2 for y in range(1,4)] return fsf1,f2,f3 = count()print f1(),f2(),f3() 最后打印出来的结果是1 4 9 装饰器有一个函数我们希望将其运行前后打印某些信息，却又不希望改变这个函数的代码，那么久可以通过装饰器（decorator）来实现这个功能。例子如下：1234567891011def log(func): def wrapper(): print 'before %s' %func.__name__ return func() return wrapper@logdef hello(): print 'hello'hello() 输出结果为：12before hello_worldhello 实际上执行hello()时相当于执行了hello=log(hello)，即将hello指向了返回的wrapper函数，而这也带来了一个问题，就是hello的__name__属性变为了wrapper的__name__属性。也就是加入在上面的程序的最后加上print hello_world.__name__打印出来的是wrapper。所以，需要把原始函数的name等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。 这些事情不用我们自己做，Python内置的functools.wraps就是干这个事的，所以，上面的规范写法如下：1234567891011121314import functoolsdef log(func): @functools.wraps(func) def wrapper(): print 'before %s' %func.__name__ return func() return wrapper@logdef hello_world(): print 'hello'hello_world()print hello_world.__name__ 这时打印出来的结果是123before hello_worldhellohello_world 上面的例子中装饰器均没有参数，下面给出装饰器带有参数的例子：123456789101112131415def log_a(text): def decorator_a(func): @functools.wraps(func) def wrapper_a(): print 'args in the decorator is %s'%text func() return wrapper_a return decorator_a@log_a('haha')def hello_world_a(): print 'hello'hello_world_a()print hello_world_a.__name__ 输出结果如下：123args in the decorator is hahahellohello_world_a 执行hello_world_a()相当于执行了hello_world_a()=log(&#39;haha&#39;)(hello_world_a()),其中的log(&#39;haha&#39;)返回了装饰器函数decorator_a. 类与对象私有变量python中没有private关键字来限定变量的私有性，假如变量名是以两根下划线开头，那么就认为是私有变量，如为类s定义了一个__age的变量，那么不能通过s.__age在外部修改这个变量，只能通过在类的内部定义set和get方法。 获取对象的信息通过type(object)函数或isinstance(object，type)函数可以判断一个类或对象的类型，通过dir(object)函数可以找到一个对象的所有属性和方法。通过hasattr(object, &#39;x&#39;) 判断object是否有属性x 通过dir(object)列出一个类的所有属性和方法会发现有很多__XXX___方法，类似__xxx__的属性和方法在Python中都是有特殊用途的，比如__len__方法返回长度。在Python中，如果你调用len()函数试图获取一个对象的长度，实际上，在len()函数内部，它自动去调用该对象的len()方法，所以，下面的代码是等价的：1234&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; 'ABC'.__len__()3 如果试图获取不存在的属性，会抛出AttributeError的错误 动态绑定属性和方法定义了一个class，或者创建了一个class的实例后，我们可以给该类或实例绑定任何属性和方法，这就是动态语言的灵活性。如下面的例子先定义一个类123&gt;&gt;&gt; class Student(object):... pass... 然后，尝试给实例绑定一个属性：1234&gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name = 'hello' # 动态给实例绑定一个属性&gt;&gt;&gt; print s.namehello 如果要限定能够绑定的属性，可以在原来的类中添加__slots__变量，变量的内容设为能够动态绑定的属性即可。 隐藏getter和setter为类的属性通过装饰器@property可以隐藏类对某个属性的get方法和set方法，见下面的例子123456789101112class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2014 - self._birth 把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@birth.setter，这个装饰器负责把一个setter方法变成属性赋值，并且在这个方法内可以限制复制的的范围等。 调用方法如下所示1234&gt;&gt;&gt;s = Student()&gt;&gt;&gt;s.birth = 2001 &gt;&gt;&gt;print s.birth,s.age2001 13 上面的s.birth = 2001 实际上是执行了装饰器@birth.setter装饰的birth方法，因此可在这个方法内加上赋值的限制条件,过滤不合法的赋值。 也可以将一个属性定义为只读属性，只定义getter方法即可。如上面的age方法。 类的一些内部函数__str__该函数是在直接打印对象时输出的内容，如下例子所示12345class Student(object): passs = Student()print s 输出内容为&lt;__main__.Student object at 0x02124DF0&gt;,表示对象在内存中的地址，可以重写这个函数的输出，见下面的例子123456class Student(object): def __str__(self): return 'object student's = Student()print s 再次执行的时候会输出object student。 __repr__该函数与__str__函数很类似，只是在直接显示变量调用的不是__str__()，而是__repr__()，两者的作用的区别是__str__()返回用户看到的字符串，而__repr__()返回程序开发者看到的字符串，也就是说，__repr__()是为调试服务。1234567`&gt;&gt;&gt; class Student(object): def __str__(self): return 'object student'... ... ... &gt;&gt;&gt; s=Student()&gt;&gt;&gt; s&lt;__main__.Student object at 0x02859930&gt; __iter__如果一个类想被用于for ... in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的next()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。 以斐波那契数列为例，写一个Fib类，可以作用于for循环：123456789101112class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def next(self): self.a, self.b = self.b, self.a + self.b # 计算下一个值 if self.a &gt; 100000: # 退出循环的条件 raise StopIteration(); return self.a # 返回下一个值 把Fib实例作用于for循环：1234567891011&gt;&gt;&gt; for n in Fib():... print n...11235...4636875025 __getitem__Fib实例虽然能作用于for循环，看起来和list有点像，但是，把它当成list来使用还是不行，比如，取第5个元素：1234&gt;&gt;&gt; Fib()[5]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'Fib' object does not support indexing 要表现得像list那样按照下标取出元素，需要实现getitem()方法：123456789101112131415161718class Fib(object): def __getitem__(self, n): a, b = 1, 1 for x in range(n): a, b = b, a + b return a``` 现在，就可以按下标访问数列的任意一项了：```py&gt;&gt;&gt; f = Fib()&gt;&gt;&gt; f[0]1&gt;&gt;&gt; f[1]1&gt;&gt;&gt; f[2]2&gt;&gt;&gt; f[3]3 __call__一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用instance.method()来调用。能不能直接在实例本身上调用呢？类似instance()？在Python中，答案是肯定的。 任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用。请看示例：123456class Student(object): def __init__(self, name): self.name = name def __call__(self): print('My name is %s.' % self.name) 调用方式如下：123&gt;&gt;&gt; s = Student('Michael')&gt;&gt;&gt; s()My name is Michael. __call__()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样 判断一个对象是否能被调用，能被调用的对象就是一个Callable对象，比如函数和我们上面定义的带有call()的类实例：12345678910&gt;&gt;&gt; callable(Student())True&gt;&gt;&gt; callable(max)True&gt;&gt;&gt; callable([1, 2, 3])False&gt;&gt;&gt; callable(None)False&gt;&gt;&gt; callable('string')False 通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。 错误、调试和测试概念常用的调试结构 1234567try....except...else # 没有捕捉到exception时执行该语句....finally Python所有的错误都是从BaseException类派生的，常见的错误类型和继承关系看这里：https://docs.python.org/2/library/exceptions.html#exception-hierarchy logging模块可以把错误记录到日志文件里，方便事后排查 抛出错误因为错误是class，捕获一个错误就是捕获到该class的一个实例。因此，错误并不是凭空产生的，而是有意创建并抛出的。Python的内置函数会抛出很多类型的错误，我们自己编写的函数也可以抛出错误。 如果要抛出错误，首先根据需要，可以定义一个错误的class，选择好继承关系，然后，用raise语句抛出一个错误的实例： 12345678class FooError(StandardError): passdef foo(s): n = int(s) if n==0: raise FooError('invalid value: %s' % s) return 10 / n 只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型（比如ValueError，TypeError），尽量使用Python内置的错误类型。 另一种错误处理的方式： 123456789101112131415def foo(s): n = int(s) return 10 / ndef bar(s): try: return foo(s) * 2 except StandardError, e: print 'Error!' raisedef main(): bar('0')main() 在bar()函数中，我们明明已经捕获了错误，但是，打印一个Error!后，又把错误通过raise语句抛出去了，这不有病么？ 其实这种错误处理方式不但没病，而且相当常见。捕获错误目的只是记录一下，便于后续追踪。但是，由于当前函数不知道应该怎么处理该错误，所以，最恰当的方式是继续往上抛，让顶层调用者去处理。 raise语句如果不带参数，就会把当前错误原样抛出。此外，在except中raise一个Error，还可以把一种类型的错误转化成另一种类型： 1234try: 10 / 0except ZeroDivisionError: raise ValueError('input error!') 只要是合理的转换逻辑就可以，但是，决不应该把一个IOError转换成毫不相干的ValueError。 调试print、assert语句最基础的调试就是通过print语句打印出变量的值，但是这样每次调试后都要注释或删除print语句。 因此也可使用assert语句，该语句的结构为assert condition,&#39;message&#39;,只有当condition为False时，才会抛出一个AssertionError并打印出message logging模块和assert比，logging不会抛出错误，而且可以输出到文件。并且可以指定输出的信息的级别，包括有debug，info，warning，error等几个级别 123456import logginglogging.basicConfig(level=logging.INFO)s = '0'n = int(s)logging.info('n = %d' % n)print 10 / n 上面的logging.basicConfig就是设置输出的日志的等级，logging.info为输出的内容。 输出的内容如下：12345INFO:root:n = 0Traceback (most recent call last): File "XX.py", line X, in &lt;module&gt; print 10 / nZeroDivisionError: integer division or modulo by zero pdbpdb(Python Debugger)是Python的调试器，可以让程序以单步方式运行，并随时查看运行状态。 通过python -m pdb XXX.py可以启动调试器调试XXX.py，n命令执行当前代码并转到下一行，p 变量名打印出具体的变量，q命令退出调试程序。 除了上面的使用方法，还可以在可能出错的地方放一个pdb.set_trace()，相当于设置一个断点。运行代码时，程序会自动在pdb.set_trace()暂停并进入pdb调试环境，可以用命令p查看变量，或者用命令c继续运行 多进程和多线程多进程python提供的跨平台多进程模块为multiprocessing, 使用的方式如下：12345678from multiprocessing import Processdef target_func(arg1,arg2): .... p1 = Process(target=target_func,args=(arg1,arg2))p2 = Process()p1.start()p1.join() 上面启动了一个进程，并执行任务target_func,注意同时执行任务的最大进程数等于该机器的核数。 更详细的用法参考这篇文章 多线程python提供的多线程模块为thread模块和threading模块，后者是高级模块，除了封装了前者还封装了很多其他方法。 一般的使用有两种：1）继承threading.Thread构造自己的线程类。2）类似多进程将需要执行的任务作为参数构造线程。 详细的语法可参考这篇文章。需要注意的是多线程同时修改进程中的公共变量时记得加线程锁。 ThreadLocal在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。 但是局部变量也有问题，就是在函数调用的时候必须要通过参数传递。如下面的例子：1234567891011121314import threadingdef process_student(name): print 'Hello, %s (in %s)' % (name, threading.current_thread().name)def process_thread(name): process_student(name)t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 当参数多了的时候，这样一层层传下去就会显得比较麻烦。因此引入了ThreadLocal的概念，可将上面的代码改写成如下的样式实现相同的功能。12345678910111213141516171819import threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): print 'Hello, %s (in %s)' % (local_school.student, threading.current_thread().name)def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。 可以理解为全局变量local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，如local_school.teacher等等。 ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。 常用内建模块collectionscollections 提供了许多有用的集合类 namedtuplenamedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素 1234567&gt;&gt;&gt;from collections import namedtuple&gt;&gt;&gt;Coordinate = namedtuple("corr",['x','y'])&gt;&gt;&gt;c = Coordinate(1,2)&gt;&gt;&gt;c.x1&gt;&gt;&gt;c.y2 dequedeque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x']) defaultdict使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict： 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')&gt;&gt;&gt; dd['key1'] = 'abc'&gt;&gt;&gt; dd['key1'] # key1存在'abc'&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值'N/A' 注意默认值是调用函数返回的，而函数在创建defaultdict对象时传入。 除了在Key不存在时返回默认值，defaultdict的其他行为跟dict是完全一样的。 OrderedDict使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict： 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d # dict的Key是无序的&#123;'a': 1, 'c': 3, 'b': 2&#125;&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([('a', 1), ('b', 2), ('c', 3)]) CounterCounter是一个简单的计数器，例如，统计字符出现的个数： 1234567&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter(&#123;'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1&#125;) Counter实际上也是dict的一个子类 base64Base64是一种用64个字符来表示任意二进制数据的方法。 首先要理解的问题就是为什么要用字符来表是二进制的数据。维基百科的解释如下： A binary-to-text encoding is encoding of data in plain text. More precisely, it is an encoding of binary data in a sequence of characters. These encodings are necessary for transmission of data when the channel does not allow binary data, such as when one might attach an image file to an e-mail message, to accomplish this, the data is encoded in some way, such that eight-bit data is encoded into seven-bit ASCII characters 大意就是在数据传输时，某些协议或系统只支持字符的传输（如email），因此如果需要传输二进制的数据，就要将二进制数据转为字符格式。而Base64是一种最常见的二进制编码方法。 Base64的原理很简单，首先，准备一个包含64个字符的数组：如[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, ... &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, ... &#39;0&#39;, &#39;1&#39;, ... &#39;+&#39;, &#39;/&#39;]然后对二进制数据进行处理，每3个字节一组，一共是3x8=24bit，划为4组，每组正好6个bit, 计算6个bit表示的数字大小（范围在0~63）之间，然后查上面的表，这样我们得到4个数字作为索引，然后查表，获得相应的4个字符，就是编码后的字符串。 因此，Base64编码会把3字节的二进制数据编码为4个字符的文本数据 如果要编码的二进制数据的字节数不是3的倍数，最后会剩下1个或2个字节怎么办？Base64用\x00字节在二进制数据末尾补足后，再在编码的末尾加上1个或2个=号，表示补了一个会两个字节，解码的时候，会自动去掉。 例子如下所示： 123456789&gt;&gt;&gt; import base64 as b64&gt;&gt;&gt; b64.b64encode(',')'LA=='&gt;&gt;&gt; b64.b64encode('l,')'bCw='&gt;&gt;&gt; b64.b64encode('ll,')'bGws'&gt;&gt;&gt;b64.b64decode('LA==')',' 由于标准的Base64编码后可能出现字符+和/，在URL中就不能直接作为参数，所以又有一种”url safe”的base64编码，其实就是把字符+和/分别变成-和_：123456&gt;&gt;&gt; base64.b64encode('i\xb7\x1d\xfb\xef\xff')'abcd++//'&gt;&gt;&gt; base64.urlsafe_b64encode('i\xb7\x1d\xfb\xef\xff')'abcd--__'&gt;&gt;&gt; base64.urlsafe_b64decode('abcd--__')'i\xb7\x1d\xfb\xef\xff' Base64适用于小段内容的编码，比如数字证书签名、Cookie的内容等。 由于=字符也可能出现在Base64编码中，但=用在URL、Cookie里面会造成歧义，所以，很多Base64编码后会把=去掉,如下所示：1234# 标准Base64:'abcd' -&gt; 'YWJjZA=='# 自动去掉=:'abcd' -&gt; 'YWJjZA' 去掉=后怎么解码呢？因为Base64是把3个字节变为4个字节，所以，Base64编码的长度永远是4的倍数，因此，需要加上=把Base64字符串的长度变为4的倍数，就可以正常解码了。如下面的例子1234567891011&gt;&gt;&gt; base64.b64decode('YWJjZA==')'abcd'&gt;&gt;&gt; base64.b64decode('YWJjZA')Traceback (most recent call last): ...TypeError: Incorrect padding&gt;&gt;&gt; def safe_b64decode(s):... return base64.b64decode(s+'='*(4-len(s)%4))...&gt;&gt;&gt; safe_b64decode('YWJjZA')'abcd' structpython中的struct模块的主要作用就是对python基本类型值与用python字符串格式表示的C语言中struct类型间的转化（This module performs conversions between Python values and C structs represented as Python strings.）。stuct模块提供了很简单的几个函数，下面写几个例子。 struct提供用format specifier方式对数据进行打包和解包（Packing and Unpacking）。例如: 123456789101112import structimport binasciivalues = (1, 'abc', 2.7)s = struct.Struct('I3sf')packed_data = s.pack(*values)unpacked_data = s.unpack(packed_data) print 'Original values:', valuesprint 'Format string :', s.formatprint 'Uses :', s.size, 'bytes'print 'Packed Value :', binascii.hexlify(packed_data)print 'Unpacked Type :', type(unpacked_data), ' Value:', unpacked_data 输出：12345Original values: (1, &apos;abc&apos;, 2.7) Format string : I3sf Uses : 12 bytes Packed Value : 0100000061626300cdcc2c40 Unpacked Type : &lt;type &apos;tuple&apos;&gt; Value: (1, &apos;abc&apos;, 2.700000047683716) 代码中，首先定义了一个元组数据，包含int、string、float三种数据类型，然后定义了struct对象，并制定了format‘I3sf’，I 表示int，3s表示三个字符长度的字符串，f 表示 float。最后通过struct的pack和unpack进行打包和解包。通过输出结果可以发现，value被pack之后，转化为了一段二进制字节串，而unpack可以把该字节串再转换回一个元组.但是值得注意的是对于float的精度发生了改变，这是由一些比如操作系统等客观因素所决定的。打包之后的数据所占用的字节数与C语言中的struct十分相似。 关于struct的更多的具体用法可参考https://docs.python.org/2/library/struct.htmlhttp://www.cnblogs.com/coser/archive/2011/12/17/2291160.html XML操作XML有两种方法：DOM和SAX。DOM会把整个XML读入内存，解析为树，因此占用内存大，解析慢，优点是可以任意遍历树的节点。SAX是流模式，边读边解析，占用内存小，解析快，缺点是我们需要自己处理事件。 SAX只允许读XML，而DOM则允许对XML文件进行读写操作。在只读的情况下，优先考虑SAX，因为DOM实在太占内存。 除了python自带的xml包可用于处理XML文件，第三发库如lxml也可以被用来处理XML文件。 python自带的xml包具体使用的实例代码可参考：http://www.tutorialspoint.com/python/python_xml_processing.htm 参考：http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中String的比较方式（== 和 equals）]]></title>
      <url>%2F2016%2F05%2F18%2FJava%E4%B8%ADString%E7%9A%84%E6%AF%94%E8%BE%83%E6%96%B9%E5%BC%8F%EF%BC%88%3D%3D%20%E5%92%8C%20equals%EF%BC%89%2F</url>
      <content type="text"><![CDATA[基本概念在Java中，String既可以作为一个对象来使用，又可以作为一个基本类型来使用。这里指的作为一个基本类型来使用只是指使用方法上的，比如String s = &quot;hello&quot;，它的使用方法如同基本类型int一样，比如int i = 1;，而作为一个对象来使用，则是指通过new关键字来创建一个新对象，比如String s = new String(&quot;Hello&quot;) Java中String比较的方法有两种：1）用”==”来比较。这种比较是比较两个String类型变量的引用是否相同(即是否指相同的内存地址)2）用Object对象的equals()方法来比较。String对象继承自Object，并且对equals()方法进行了重写。两个String对象通过equals()方法来进行比较时，也就是对String对象的实际内容进行比较。 实际例子String 作为对象时的比较123456789String s1 = new String("Hello");String s2 = new String("Hello");System.out.println(s1 == s2);System.out.println(s1.equals(s2));/*output*/falsetrue 两个String对象都是通过new创建出来的，而new关键字为创建的每个对象分配一块新的、独立的内存堆，因此当通过”==”来比较它们的引用是否相同时，将返回false；而通过equals()方法来比较时，则返回true，因为这两个对象所封装的字符串内容是完全相同的。 String作为基本类型时的比较123456789String s1 = "Hello";String s2 = "Hello";System.out.println(s1 == s2);System.out.println(s1.equals(s2));/*output*/truetrue 由于这两个String对象都是作为一个基本类型来使用的，而不是通过new关键字来创建的，因此虚拟机不会为这两个String对象分配新的内存堆，而是到String缓冲池中来寻找。 什么是String缓冲池？在Java中，由于String（final）是不可改变的，为了提高效率，不重复创建新的字符创，Java引用了String缓冲池的概念。 首先为s1寻找String缓冲池内是否有与”Hello”相同值的String对象存在，此时String缓冲没有相同值的String对象存在，所以虚拟机会在String缓冲池内创建此String对象，其动作就是new String(“Hello”);。然后把此String对象的引用赋值给s1。 接着为s2寻找String缓冲池内是否有与”Hello”相同值的String对象存在，此时虚拟机找到了一个与其相同值的String对象，这个String对象其实就是为s1所创建的String对象。既然找到了一个相同值的对象，那么虚拟机就不在为此创建一个新的String对象，而是直接把存在的String对象的引用赋值给s2。 这里既然s1和s2所引用的是同一个String对象，即自己等于自己，所以以上两种比较方法都返回ture。。 对象与基本类型的比较123456789String s1 = "Hello";String s2 = new String("Hello");System.out.println(s1 == s2);System.out.println(s1.equals(s2));/*output*/falsetrue 由于new关键字会申请新的内存空间，创建新的对象，因此不会去查找缓存池，即使缓存池中有”Hello”，因此两者的内存地址不是一样的，所以第一个输出为false，而两者的内容是一样的，输出为true。 将上面的代码稍作修改 12345678910String s1 = "Hello";String s2 = new String("Hello");s2 = s2.intern();System.out.println(s1 == s2);System.out.println(s1.equals(s2));/*output*/truetrue 上面的代码增加了一行s2 = s2.intern();其作用是从String缓冲池内取出一个与其值相同的String对象的引用赋值给s（假如有的话）。 这样做的原因是如果频繁地创建相同内容的对象，虚拟机分配许多新的内存堆，虽然它们的内容是完全相同的。由于String是final类，因此String对象在创建后不能改变。所以为了节省内存，可以使用String缓冲池，因为String缓冲池内不会存在相同内容的String对象。而intern()方法就是使用这种机制的途径。 在一个已实例化的String对象s上调用intern()方法后，虚拟机会在String缓冲池内寻找与此s对象存储内容相同的String对象，如果能找到，则返回对象在缓冲池中的地址，如果找不到，那么虚拟机在缓冲池中以s的内容新建一个对象并返回这个对象的地址。注意需要将s指向返回的缓冲池对象的地址，这样才能通过垃圾回收器回将原先那个通过new关键字所创建出的String对象回收。 因此可以解释上面的s1==s2返回结果为什么是true了，因为此时，两者的引用相同，均指向缓冲池的对象。 拼接字符串后的比较上面提到由于String是final类，因此String对象在创建后不能改变，那么像拼接字符串的操作如String s1=s2+s3;（s2、s3是已赋值的String）应该会产生一个新的对象，用新的地址空间存储。但是这句话也不完全对，详见下面的例子 12345678910111213String s1 = "a"; String s2 = "b"; String s3 = "ab"; String s4 = "a"+"b"; System.out.println("s3==s4? "+ (s3==s4)); String s5 = s1+s2; System.out.println("s3==s5? "+ (s3==s5)); final String s6 = "a" ; final String s7 = "b" ; String s8 = s6 + s7; System.out.println("s3==s8? "+ (s3==s8)); 输出如下：123s3==s4? trues3==s5? falses3==s8? true s4由”a”、”b”两个常量拼接而成，本来按照上面的说法应该会生成新的内存空间，但是因为”a”、”b”为两个为常量，不可变，在编译期间编译器会把s5=”a”+”b”优化为s5=”ab”。 s5由s1和s2拼接而成，由于两个变量的相加所以编译器无法优化， 在运行时，会有新的String地址空间的分配，而不是指向缓冲池中的“ab”。所以结果false。 s6虽让也是由两个变量拼接而成，但是这两个变量已经声明为final不可变的了，所以类似于s4，在编译期间编译器也进行了优化确定了s8的值。 参考：http://blog.csdn.net/wangdong20/article/details/8566217http://www.itxxz.com/a/tea/2014/0814/208.htmlhttp://renxiangzyq.iteye.com/blog/549554]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(51)--回溯法解决N皇后问题]]></title>
      <url>%2F2016%2F05%2F16%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(51)--%E5%9B%9E%E6%BA%AF%E6%B3%95%E8%A7%A3%E5%86%B3N%E7%9A%87%E5%90%8E%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[原题如下： The n-queens puzzle is the problem of placing n queens on an n×n chessboard such that no two queens attack each other. Given an integer n, return all distinct solutions to the n-queens puzzle. Each solution contains a distinct board configuration of the n-queens’ placement, where ‘Q’ and ‘.’ both indicate a queen and an empty space respectively. For example,There exist two distinct solutions to the 4-queens puzzle:1234567891011[ [&quot;.Q..&quot;, // Solution 1 &quot;...Q&quot;, &quot;Q...&quot;, &quot;..Q.&quot;], [&quot;..Q.&quot;, // Solution 2 &quot;Q...&quot;, &quot;...Q&quot;, &quot;.Q..&quot;]] 经典的n皇后问题，通过回溯法解决。 关键的地方有以下几点：1）由于题目本身的特点，每一行只能放置一个皇后，因此每行只要找到第一个合适的位置便可跳到下一行找下一个皇后的合适位置。2）由于遍历的顺序是从坐到又从上到下的，因此判断当前位置能否放置皇后时只需要判断当前位置上方的列、左上斜边、右上斜边是否有皇后即可。 实现的代码如下所示：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution(object): def solveNQueens(self, n): """ :type n: int :rtype: List[List[str]] """ solutions = [] board = [['.' for i in range(n)] for j in range(n)] #生成n*n的原始矩阵 self.helper(board, solutions,0 ) return solutions def helper(self,board,solutions, row): n = len(board) if row==n: tmp=[] for i in board: tmp.append(reduce(lambda x,y:x+y,i)) # reduce函数的作用是将['Q','.','.','.']转为'Q...' solutions.append(tmp) return for i in range(n): if self.is_valid(board,row,i): board[row][i] = 'Q' self.helper(board,solutions,row+1) board[row][i] = '.' def is_valid(self,board,i,j): n = len(board) # check column for k in xrange(n): if board[k][j] == 'Q': return False # check diagonal，只需要检查上面的两条边 ri = i rj = j while 0&lt;=i&lt;=n-1 and 0&lt;=j&lt;=n-1: if board[i][j]=='Q': return False i-=1 j-=1 i = ri j = rj while 0&lt;=i&lt;=n-1 and 0&lt;=j&lt;=n-1: if board[i][j]=='Q': return False i-=1 j+=1 return True 上面的代码虽然结果正确，但是判断当前位置能否放置皇后的方法效率较低，判断的一次的平均时间复杂度为O(n)，n为棋盘的边的大小，下面主要介绍一种判断当前位置能否放置皇后的时间复杂度为O(1)的方法。 通过观察可发现，在同一条左斜边上的每个点的行值加上列值的结果相等，而在同一条右斜边上的每个点的行值减去列值的结果相等。如下图所示就是左斜线每个点的行值加上列值的情况： 右斜线每个点的行值减去列值的情况同理。 由于这些结果大小是连续的，因此我们可以使用数组的下标代表某一斜边，用该下标的对应的数组值（0或1）表示该斜边上是否有皇后。设棋盘的大小为n*n。则左斜边的范围为[0,2n-2],长度为2n+1；右斜边的范围为[-n+1,n-1],由于数组的下标不能为负，所以每个下标加上n-1，将范围变为[0,2n-2],长度同样为2n-1。 实现的代码如下所示1234567891011121314151617181920212223242526272829303132class Solution(object): def solveNQueens(self, n): """ :type n: int :rtype: List[List[str]] """ solutions = [] board = [['.' for i in range(n)] for j in range(n)] #生成n*n的原始矩阵 col = [0 for i in range(n)] left_dia = [0 for i in range(2*n-1)] right_dia = [0 for i in range(2*n-1)] self.helper(board, solutions,0, col,left_dia,right_dia) return solutions def helper(self,board,solutions,row,col,left_dia,right_dia): n = len(board) if row==n: tmp=[] for i in board: tmp.append(reduce(lambda x,y:x+y,i)) # reduce函数的作用是将['Q','.','.','.']转为'Q...' solutions.append(tmp) return for i in range(n): if col[i]==0 and left_dia[row+i]==0 and right_dia[i-row+n-1]==0: board[row][i] = 'Q' col[i],left_dia[row+i],right_dia[i-row+n-1]=1,1,1 self.helper(board,solutions,row+1,col,left_dia,right_dia) board[row][i] = '.' col[i],left_dia[row+i],right_dia[i-row+n-1]=0,0,0]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java内置的数据类型]]></title>
      <url>%2F2016%2F05%2F15%2FJava%E5%86%85%E7%BD%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
      <content type="text"><![CDATA[Java可分为两大数据类型: 内置数据类型（Primitive） 引用数据类型（Reference） 两者的分类及区别如下所示： 从上图可知，内置数据类型有八种基本类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种布尔型。上图直接将字符类型char归入到整数类型中，原因是两者可以进行相互的转换。 整数类型byte byte数据类型是8位、有符号的，以二进制补码表示的整数； 最小值是-128（-2^7）； 最大值是127（2^7-1）； 默认值是0； byte类型用在大型数组中节约空间，主要代替整数，因为byte变量占用的空间只有int类型的四分之一； 例子：byte a = 100，byte b = -50。 short short数据类型是16位、有符号的以二进制补码表示的整数 最小值是-32768（-2^15）； 最大值是32767（2^15 - 1）； short数据类型也可以像byte那样节省空间。一个short变量是int型变量所占空间的二分之一； 默认值是0； 例子：short s = 1000，short r = -20000。 int int数据类型是32位、有符号的以二进制补码表示的整数； 最小值是-2,147,483,648（-2^31）； 最大值是2,147,485,647（2^31 - 1）； 一般地整型变量默认为int类型； 默认值是0； 例子：int a = 100000, int b = -200000。 long long数据类型是64位、有符号的以二进制补码表示的整数； 最小值是-9,223,372,036,854,775,808（-2^63）； 最大值是9,223,372,036,854,775,807（2^63 -1）； 这种类型主要使用在需要比较大整数的系统上； 默认值是0L； 例子： long a = 100000L，int b = -200000L。 小结可见整数类型的最大区别在于其表示范围和占用的内存大小不同。在使用时根据实际的使用场景选择合适的数据类型。 可将表示范围小的数值复制给表示范围大的数，但是反过来不行，因为大数的值可能会超出小的数的表示范围。如下所示12345int a = 100;short b = 3;byte c = a; // 错误long d = b; // 正确 浮点数float float数据类型是单精度（single-precision）、32位（4字节）、符合IEEE 754标准的浮点数； float在储存大型浮点数组的时候可节省内存空间； 默认值是0.0f； 浮点数不能用来表示精确的值，如货币； 例子：float f1 = 234.5f。 double double数据类型是双精度（double-precision）、64位（8字节）、符合IEEE 754标准的浮点数； 浮点数的默认类型为double类型，见下面的例子 12345/*下面的表达式会导致编译器会报错，因为默认0.1是double类型，而a是一个float类型，这样赋值会导致精度的损失*/float a=0.1; //有两种方法，一是将float改为double，二是将0.1改成0.1f double类型同样不能表示精确的值，如货币； 默认值是0.0f； 例子：double d1 = 123.4。 小结float是单精度数字，而double是双精度数字。两者的主要区别如下： 存储 float由32 bit存储，其中1位符号位，8位指数位，23位尾数位。double由64 bit存储，其中1位符号位，11位指数位，52位尾数位。 数值范围 float的指数范围为-128~+127，而double的指数范围为-1024~+1023，并且指数位是按补码的形式来划分的。 其中负指数决定了浮点数所能表达的绝对值最小的非零数；而正指数决定了浮点数所能表达的绝对值最大的数，也即决定了浮点数的取值范围。 float的范围为-2^128 ~ +2^127，也即-3.40E+38 ~ +3.40E+38；double的范围为-2^1024 ~ +2^1023，也即-1.79E+308 ~ +1.79E+308。 精度 float和double的精度是由尾数的位数来决定的。浮点数在内存中是按科学计数法来存储的，其整数部分始终是一个隐含着的“1”，由于它是不变的，故不能对精度造成影响。 float：2^23 = 8388608，一共七位，由于最左为1的一位省略了，这意味着最多能表示8位数： 2*8388608 = 16777216。有8位有效数字，但绝对能保证的为7位，也即float的精度为7~8位有效数字 double：2^52 = 4503599627370496，一共16位，同理，double的精度为16~17位。 之所以不能用f1==f2来判断两个数相等，是因为虽然f1和f2在可能是两个不同的数字，但是受到浮点数表示精度的限制，有可能会错误的判断两个数相等！ boolean boolean数据类型表示一位的信息，大小不确定 只有两个取值：true和false； 这种类型只作为一种标志来记录true/false情况； 默认值是false； 例子：boolean one = true。 char char类型是一个单一的16位（2字节）Unicode字符； 最小值是’\u0000’（即为0）； 最大值是’\uffff’（即为65,535）； char数据类型可以储存任何字符，包括汉字； 例子：char letter = ‘A’。(只能用单引号，不能用双引号) 参考：https://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.htmlhttp://www.runoob.com/java/java-basic-datatypes.htmlhttp://blog.csdn.net/zq602316498/article/details/41148063http://blog.csdn.net/abing37/article/details/5332798]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(48)--矩阵的旋转]]></title>
      <url>%2F2016%2F05%2F12%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(48)--%E7%9F%A9%E9%98%B5%E7%9A%84%E6%97%8B%E8%BD%AC%2F</url>
      <content type="text"><![CDATA[原题如下:You are given an n x n 2D matrix representing an image. Rotate the image by 90 degrees (clockwise). Follow up:Could you do this in-place?题目要求将图片顺时针翻转90度。通过用一个额外的图片矩阵来存储这个图片能够轻松完成，但是题目最后说了Could you do this in-place?，意思是能否通过O(1)的空间复杂度完成这项任务。 这就需要一点技巧了，下面以一个例子说明如何不借助额外的图片矩阵完成顺时针90度旋转。1231 2 3 7 8 9 7 4 14 5 6 --&gt; 4 5 6 --&gt; 8 5 27 8 9 1 2 3 9 6 3 首先将图片的行上下对称交换，假如有奇数行，中间的行不用动；假如有偶数行，则全部都要对称交换。然后将矩阵转置即可。 具体实现代码如下：12345678910111213class Solution(object): def rotate(self, matrix): """ :type matrix: List[List[int]] :rtype: void Do not return anything, modify matrix in-place instead. """ n = len(matrix) for i in xrange(n/2): matrix[i], matrix[n-1-i] = matrix[n-1-i], matrix[i] for i in xrange(n): for j in xrange(i+1,n): matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j] 参考上面的方法，同理可以推出逆时针旋转90度和旋转180度的实现方法。 逆时针旋转90度 将矩阵的列左右对称交换，然后转置。例子如下所示 1231 2 3 3 2 1 3 6 94 5 6 --&gt; 6 5 4 --&gt; 2 5 87 8 9 9 8 7 1 4 7 旋转180度上下对称交换，然后左右对称交换。例子如下所示：1231 2 3 7 8 9 9 8 74 5 6 --&gt; 4 5 6 --&gt; 6 5 47 8 9 1 2 3 3 2 1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(46，47)--排列]]></title>
      <url>%2F2016%2F05%2F06%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(46%EF%BC%8C47)--%E6%8E%92%E5%88%97%2F</url>
      <content type="text"><![CDATA[46. Permutations原题如下： Given a collection of distinct numbers, return all possible permutations.For example,[1,2,3] have the following permutations:[1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], and [3,2,1]. 题目要求列出给定的不重复数字的所有排列结果。解题方法有两种。下面分别讲述。 方法一第一种方法每次取一个数字，将数字插入现有的排列中形成新的排列，然后重复上面的过程直到所有数字都被取完。 实现代码如下： 1234567891011121314class Solution(object): def permute(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ result = [[]] for num in nums: new_result = [] for seq in result: for i in xrange(len(seq)+1): new_result.append(seq[:i]+[num]+seq[i:]) result = new_result return result 方法二第二种方法采用回溯法。将当前数字与后面的数字逐一交换，当与某一数字交换后，然后移动到下一个数字重复上面的操作。实现具体代码如下 123456789101112131415161718192021class Solution(object): def permute(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ result = [] self.helper(nums,0,result) return result def helper(self, nums, begin, result): n = len(nums) if begin == n: tmp = nums[:] result.append(tmp) return for i in xrange(begin,n): nums[begin],nums[i] = nums[i], nums[begin] self.helper(nums,begin+1,result) nums[begin],nums[i] = nums[i], nums[begin] 47. Permutations II原题如下： Given a collection of numbers that might contain duplicates, return all possible unique permutations. For example,[1,1,2] have the following unique permutations:[1,1,2], [1,2,1], and [2,1,1]. 这道题目的要求与上一道一样，只是在上面的基础上添加了存在重复数字的约束条件。上面提到回溯法在这里不适用了（即使判定交换的元素是否相等），比如说对于[1,2,3,3]就不正确了。 采用方法一的直接插入法，从后往前插，遇到与当前要插入数字相同的数字时就终止当前排列的插入，进行下一个排列的插入。实现的具体代码如下： 1234567891011121314151617class Solution(object): def permuteUnique(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ result = [[]] for num in nums: new_result = [] for seq in result: n = len(seq) for i in xrange(n,-1,-1): if i &lt; n and seq[i] == num: break new_result.append(seq[:i]+[num]+seq[i:]) result = new_result return result]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(42)--柱状图的储水量]]></title>
      <url>%2F2016%2F05%2F01%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(42)--%E6%9F%B1%E7%8A%B6%E5%9B%BE%E7%9A%84%E5%82%A8%E6%B0%B4%E9%87%8F%2F</url>
      <content type="text"><![CDATA[原题如下 Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining.For example,Given [0,1,0,2,1,0,1,3,2,1,2,1], return 6.The above elevation map is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this case, 6 units of rain water (blue section) are being trapped. Thanks Marcos for contributing this image! 从题目给出的图可以比较清楚知道，题目给出的数组中的每个数字代表一根柱子的高度，将数组中的数字全部表示为特定高度的柱子后，求这些柱子组成的容器能够容纳的水量。 解题思路：可以分别求出每根柱子的储水量，然后将每根柱子的储水量加起来即为总的储水量。而每根柱子的储水量可以通过下面方法求：找到这根柱子左边所有柱子中最高的那根，记为A；找到这根柱子右边所有柱子中最高的那根，记为B；min(A,B)-r即为这根柱子的储水量，r为这根柱子的高度。 实现代码如下：123456789101112131415161718192021222324252627class Solution(object): def trap(self, height): """ :type height: List[int] :rtype: int """ if len(height)==0: return 0 leftMax = [] # leftMax Array max = height[0] for i in range(len(height)): if height[i] &gt; max : max = height[i] leftMax.append(max) sum = 0 rightMax = height[len(height)-1] for i in reversed(range(1,len(height)-1)): miniHeight = min(leftMax[i-1],rightMax) if miniHeight &gt; height[i]: sum += miniHeight - height[i] if height[i] &gt; rightMax: rightMax = height[i] return sum]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(41)--第一个缺失的正整数]]></title>
      <url>%2F2016%2F04%2F20%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(41)--%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%BC%BA%E5%A4%B1%E7%9A%84%E6%AD%A3%E6%95%B4%E6%95%B0%2F</url>
      <content type="text"><![CDATA[原题如下： Given an unsorted integer array, find the first missing positive integer.For example,Given [1,2,0] return 3,and [3,4,-1,1] return 2. Your algorithm should run in O(n) time and uses constant space. 题目要求找出第一个缺失的正整数，难点在于时间复杂度要控制在O(n)，也就是说不能对数组排序，空间复杂度要控制在O(1)，也就是说不能开一个新的数组去存储这些数字。 但是这道题目也有它的特点，假如给定的数组的长度为n，那么缺失的第一个正数肯定在[1,n+1]内，根据这个特点，可以将范围为[1,n]的数字移动到与其数值相等的下标的位置，对所有的数这样操作后从头遍历一遍数组，找出的第一个 i+1 != a[i]的i,那么i+1就是第一个缺失的正整数。 12345678910111213141516171819202122class Solution(object): def firstMissingPositive(self, nums): """ :type nums: List[int] :rtype: int """ n = len(nums) for i in xrange(n): while 0&lt;nums[i]&lt;=n and (not nums[nums[i]-1]==nums[i]): tmp = nums[i] nums[i] = nums[tmp-1] nums[tmp-1] = tmp # nums[i],nums[nums[i]-1] = nums[nums[i]-1],nums[i]错误 for i in xrange(n): if nums[i] == i+1: continue else: return i+1 return n+1 # all match]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(39,40)--数字集合中找特定和]]></title>
      <url>%2F2016%2F04%2F16%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(39%2C40)--%E6%95%B0%E5%AD%97%E9%9B%86%E5%90%88%E4%B8%AD%E6%89%BE%E7%89%B9%E5%AE%9A%E5%92%8C%2F</url>
      <content type="text"><![CDATA[这两道题目39. Combination Sum和40. Combination Sum II均要求从给定的一个数字集合中找出若干个数字的和等于某个给定的数。解决方法有两种，第一种是回溯法，第二种是动态规划。下面分别讲述。 39. Combination Sum原题如下： Given a set of candidate numbers (C) and a target number (T), find all unique combinations in C where the candidate numbers sums to T.The same repeated number may be chosen from C unlimited number of times. Note:All numbers (including target) will be positive integers.Elements in a combination (a1, a2, … , ak) must be in non-descending order. (ie, a1 ≤ a2 ≤ … ≤ ak).The solution set must not contain duplicate combinations.For example, given candidate set 2,3,6,7 and target 7,A solution set is:[7][2, 2, 3] 回溯法回溯法的的过程如下：先对数组从小到大排序。然后从第一个数字开始往后加，每加上一个数字就保留当前的状态，再加上下一个数字的时候假如和超过了给定的数字，就回到上一状态，上一个状态就跳过这个数字继续往后加。注意当前数字可以重复使用。 实现方法如下：123456789101112131415class Solution(object): def combinationSum(self, candidates, target): res = [] candidates.sort() self.dfs(candidates, target, 0, [], res) return res def dfs(self, nums, target, index, path, res): if target == 0: res.append(path) return for i in xrange(index, len(nums)): if target-nums[i] &lt; 0: return # backtracking self.dfs(nums, target-nums[i], i, path+[nums[i]], res) 动态规划法动态规划利用一个list存储每个数字可能的组合，dp[i]表示和为i的数字的可能的组合，那么下一个dp[i+1]=dp[i]+dp[1]=dp[i-1]+dp2……上面的加号表示对这两个可能的数字的组合进行组合。实现代码如下： 123456789101112131415161718class Solution(object): def combinationSum(self, candidates, target): dp = &#123;&#125; dp[0] = [] for i in xrange(1,target+1): dp [i] = [] if i in candidates: dp[i].append([i]) for j in xrange(1,i/2+1): if len(dp[j]) ==0 or len(dp[i-j])==0: continue for m in dp[j]: for k in dp[i-j]: tmp = m+k tmp.sort() if tmp not in dp[i]: dp[i].append(tmp) return dp[target] 40. Combination Sum II原题如下： Given a collection of candidate numbers (C) and a target number (T), find all unique combinations in C where the candidate numbers sums to T.Each number in C may only be used once in the combination. Note:All numbers (including target) will be positive integers.Elements in a combination (a1, a2, … , ak) must be in non-descending order. (ie, a1 ≤ a2 ≤ … ≤ ak).The solution set must not contain duplicate combinations.For example, given candidate set 10,1,2,7,6,1,5 and target 8,A solution set is:[1, 7][1, 2, 5][2, 6][1, 1, 6] 本题与前一题Combination Sum非常相似，只是附加了每个数字只能使用一次的要求。需要注意给出的candidate numbers中会有重复的数字。 解决方法与前一题Combination Sum也类似，只是在这个过程中元素不能被重复使用。 回溯法实现代码如下：12345678910111213141516class Solution(object): def combinationSum2(self, candidates, target): candidates.sort() result = [] self.dfs(candidates,target,0 ,[],result) return result def dfs(self, nums, tar, index, tmp, res): if tar == 0 : if tmp not in res: res.append(tmp) else: for i in range(index,len(nums)): if tar-nums[i]&lt;0: return self.dfs(nums, tar-nums[i], i+1, tmp+[nums[i]], res) 动态规划法需要先获取原来的candidates中各个数字的个数，然后每次遇到和等于目标数字的list时都要判断这个list中的各个数字个数是否超过给定的candidates中的标准。实现代码如下：123456789101112131415161718192021222324252627282930313233343536class Solution(object): def combinationSum2(self, candidates, target): can_dict = &#123;&#125; for candidate in candidates: if candidate not in can_dict: can_dict[candidate] = 0 can_dict[candidate] += 1 dp = &#123;&#125; dp[0] = [] for i in xrange(1,target+1): dp[i] = [] if i in candidates: dp[i].append([i]) for j in xrange(1,i/2+1): if len(dp[j])==0 or len(dp[i-j])==0: continue for m in dp[j]: for n in dp[i-j]: tmp_dict = &#123;&#125; flag = 0 tmp = m+n for t in tmp: if t not in tmp_dict: tmp_dict[t] = 0 tmp_dict[t] += 1 for te in tmp_dict: if tmp_dict[te] &gt; can_dict[te]: flag = 1 break if flag == 0: tmp.sort() if tmp not in dp[i]: dp[i].append(tmp) return dp[target]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(37)--回溯法解决数独问题]]></title>
      <url>%2F2016%2F04%2F15%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(37)--%E5%9B%9E%E6%BA%AF%E6%B3%95%E8%A7%A3%E5%86%B3%E6%95%B0%E7%8B%AC%2F</url>
      <content type="text"><![CDATA[原题如下： Write a program to solve a Sudoku puzzle by filling the empty cells. Empty cells are indicated by the character ‘.’.You may assume that there will be only one unique solution. A sudoku puzzle… 题目要求解决一个给定的数独，且每个数独的答案唯一。 解题思路如下：采用回溯法，每插入一个数之前检查插入这个数之后是否有效。假如有效就插入并且保留当前插入了这个数的状态。以保证后面插入的数无效的时候能够回溯到当前这个状态，修改当前状态插入的数。实现这种状态的保存与回溯可以通过递归实现。 实现代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution(object): def solveSudoku(self, board): """ :type board: List[List[str]] :rtype: void Do not return anything, modify board in-place instead. """ self.helper(board) def helper(self, board): num = len(board) for i in range(num): for j in range(num): if board[i][j] == '.': for t in range(1,10): c = str(t) if self.is_valid(board, c, i, j): board[i][j] = c if self.helper(board): return True else: board[i][j] = '.' return False # 当前插入1~9均无效，回溯到前一状态 return True # 插入最后一个数成功时需要返回True,从而将前面的状态确定下来 def is_valid(self, board, c, row , col): num = len(board) # check row for m in range(num): if board[row][m] == c: return False # check column for n in range(num): if board[n][col] == c: return False # check block row_start = (row/3)*3 column_start = (col/3)*3 for i in range(row_start,row_start+3): for j in range(column_start,column_start+3): if board[i][j] == c: return False return True]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(33,81,153,154)--二分搜索找旋转数组特定值]]></title>
      <url>%2F2016%2F04%2F08%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(33)--%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%89%BE%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%89%B9%E5%AE%9A%E5%80%BC%2F</url>
      <content type="text"><![CDATA[本文主要讲述如何在一个Rotated Sorted Array中找到特定的值，Rotated Sorted Array指旋转了的数组，如4 5 6 7 0 1 2就是0 1 2 4 5 6 7的一个旋转数组。正常情况下遍历一遍即可，但是这样的时间复杂度为$O(n)$,但是本文主要讲述通过二分查找将时间复杂度降到$O(log_2n)$。 本文主要以LeetCode上的四道题目为例讲解,分别是 33. Search in Rotated Sorted Array81. Search in Rotated Sorted Array II153. Find Minimum in Rotated Sorted Array154. Find Minimum in Rotated Sorted Array II 其中33、81是在Rotated Sorted Array找出给定的值，不同的地方在于33中没有重复元素，而81中有重复的元素；153、154则是在Rotated Sorted Array中找到最小值，区别也是一个有重复元素而另一个没有。 33. Search in Rotated Sorted Array对于一个Rotated Sorted Array，每次的binary search查找的中间元素都会将原来的list分为两个lists,其中的一个list肯定是有序的，可以判断目标元素是否在这个有序的list中，如果在这个list则在这个list内进行二分查找，否则在另外一个list内找。 这种方法时间复杂度为$O(log_2n)$。实现代码如下： 123456789101112131415161718192021222324252627282930313233class Solution(object): def search(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if len(nums) == 0: return -1 left = 0 right = len(nums) - 1 while left &lt; right: mid = (left + right)/2 if nums[mid] == target: return mid if nums[left] &lt;= nums[mid]: if nums[left]&lt;=target&lt;=nums[mid]: right = mid - 1 # mid要减去1，否则两个元素的时候会导致死循环 else: left = mid+1 if nums[mid] &lt;= nums[right]: if nums[mid]&lt;=target&lt;=nums[right]: left = mid + 1 else: right = mid - 1 if nums[left] == target: return left else: return -1 81. Search in Rotated Sorted Array II这个问题的解决思路同上面的类似，但是因为有可能存在重复的元素，所以每次 binary search 后分成的两个lists中不一定存在一个有序的 list， 这时候就需要两个list都遍历了，所以这种方法最差情况下的时间复杂度为O(n)。 实现代码如下：12345678910111213141516171819202122232425262728293031class Solution(object): def search(self, nums, target): """ :type nums: List[int] :type target: int :rtype: bool """ if len(nums) == 0: return False n = len(nums) if self.helper(nums, 0, n-1,target): return True else: return False def helper(self,nums,left,right,target): if left &gt; right: return False if left == right: return nums[left] == target else: mid = (left+right)/2 if nums[mid] == target: return True elif nums[left]&lt;=target&lt;nums[mid]: return self.helper(nums,left,mid-1,target) elif nums[mid]&lt;target&lt;=nums[right]: return self.helper(nums,mid+1,right,target) else: return self.helper(nums,mid+1,right,target) or self.helper(nums,left,mid-1,target) 153. Find Minimum in Rotated Sorted Array本题需要求 Rotated Sorted Array中最小的元素，且Rotated Sorted Array中没有重复元素。关键点在于每次将nums[(left+right)/2]与nums[right]比较，假如nums[(left+right)/2]&gt;nums[right],则最小值在nums[(left+right)/2:right]中，反之在另一半元素中。 实现代码如下：123456789101112131415class Solution(object): def findMin(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) == 0: return None left, right = 0, len(nums)-1 while left &lt; right: mid = (left+right)/2 if nums[right] &lt; nums[mid]: left = mid + 1 else: right = mid return nums[left] 通过找到最小元素，也可以解决上面提到的问题33. Search in Rotated Sorted Array，就是先用二分搜索找到最小元素的下标，最小元素的下标会将原来的list分为两个sorted list，判断target在哪个sorted list然后对这个sorted list进行二分查找即可。 这种方法对array进行了两次二分查找 实现代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution(object): def search(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if len(nums) == 0: return -1 left = 0 right = len(nums)-1 while left&lt;right: mid = (left+ right)/2 if nums[mid]&gt;nums[right]: left = mid + 1 else: right = mid minIndex = left if minIndex == 0: if nums[minIndex]&lt;=target&lt;= nums[len(nums)-1]: return self.binarySearch(minIndex,len(nums)-1,nums,target) else: return -1 else: if nums[0]&lt;= target&lt;=nums[minIndex-1]: return self.binarySearch(0,minIndex-1,nums,target) elif nums[minIndex]&lt;= target&lt;=nums[len(nums)-1]: return self.binarySearch(minIndex,len(nums)-1,nums,target) else: return -1 # 对一个sorted list的经典二分查找 def binarySearch(self,left,right,nums,target): while left &lt; right: mid = (left + right)/2 if nums[mid] == target: return mid elif nums[mid]&gt;target: right = mid -1 else: left = mid+1 if nums[left] == target: return left else: return -1 154. Find Minimum in Rotated Sorted Array II这个问题跟153. Find Minimum in Rotated Sorted Array不同的地方是Rotated Sorted Array中可能存在重复的元素，虽然也可以通过二分搜索解决，但是最差的情况下的时间复杂度也是O(n),最差的情况下就是nums[left]=nums[mid]=nums[right],这是无法确定最小元素在那一边，因此两边都要搜索。下面给出两种实现方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243# method 1class Solution(object): def findMin(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) == 0: return None left, right = 0, len(nums)-1 while left &lt; right: mid = (left+right)/2 if nums[right] &lt; nums[mid]: left = mid + 1 elif nums[right] &gt; nums[mid]: right = mid else: if nums[mid] == nums[left]: if sum(nums[left:mid+1]) == nums[mid] * (mid-left+1): left = mid + 1 else: right = mid else: right = mid return nums[left] # method 2class Solution(object): def findMin(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) == 0: return None left, right = 0, len(nums)-1 while left &lt; right: mid = (left+right)/2 if nums[right] &lt; nums[mid]: left = mid + 1 elif nums[right] &gt; nums[mid]: right = mid else: right -= 1 return nums[left]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(32)--最长合法的子括号串]]></title>
      <url>%2F2016%2F04%2F06%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(32)--%E6%9C%80%E9%95%BF%E5%90%88%E6%B3%95%E7%9A%84%E5%AD%90%E6%8B%AC%E5%8F%B7%E4%B8%B2%2F</url>
      <content type="text"><![CDATA[原题如下： Given a string containing just the characters ‘(‘ and ‘)’, find the length of the longest valid (well-formed) parentheses substring.For “(()”, the longest valid parentheses substring is “()”, which has length = 2. Another example is “)()())”, where the longest valid parentheses substring is “()()”, which has length = 4. 题目的要求从给出的包含括号的string中找到最长的合法string。 最暴力的方法就是从长到短遍历string中的所有可能的subString，再判断subString是否合法，这种方法的时间复杂度是$O(n^3)$，提交时TLE。 两种比较巧妙的方法的方法的时间复杂度都是O(n)，一种是动态规划，一种是利用栈存储不合法的括号的位置。 下面分别列出这三种方法 方法一，暴力枚举，时间复杂度$O(n^3)$虽然容易理解，答案也正确，但是提交超时1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution(object): def longestValidParentheses(self, s): """ :type s: str :rtype: int """ if len(s)&lt;2: return 0 left = 0 right = 0 for i in range(len(s)): if s[i] == '(': left+=1 elif s[i] == ')': right+=1 m = min(left,right) cut = len(s)-2*m # 从长到短遍历 while cut&lt;=len(s)-2: subLen = len(s) - cut for i in range(cut+1): subStr = s[i:i+subLen] if subStr[0] == ')': continue if self.isValid(subStr): return subLen cut +=2 # 判断子字符串是否有效 def isValid(self,s): stack = [] for i in range(len(s)): if s[i] == '(': stack.append('(') elif s[i] == ')': if len(stack)&gt;0: stack.pop(len(stack)-1) else: break if len(stack)==0 and i==len(s)-1: return True else: return False 方法二，动态规划，时间复杂度$O(n)$DP会先建立一个longest列表，其中longest[i] 代表以 s[i] 结尾的最长合法串的长度。这样便有了以下判断条件： 假如 s[i]=( ，那么longest[i]=0 假如 s[i]=) &amp;&amp; s[i-1]=( , 那么longest[i]=longest[i-2]+2 假如 s[i]=) &amp;&amp; s[i-1]=) &amp;&amp; s[i-longest[i-1]-1]==( ,那么longest[i]=longest[i-longest[i-1]-2]+longest[i-1]+2 假如上面情况都不符合那么，longest[i] = 0 实现代码如下,提交时能够AC：1234567891011121314151617181920class Solution(object): def longestValidParentheses(self, s): """ :type s: str :rtype: int """ longest = [] longest.append(0) for i in range(1,len(s)): if s[i] == '(': longest.append(0) else: if i-1&gt;=0 and s[i-1]=='(': longest.append(longest[i-2]+2) elif i-1&gt;=0 and s[i-1]==')' and i-longest[i-1]-1 &gt;=0 and s[i-longest[i-1]-1]=='(': tmp = longest[i-longest[i-1]-2] if i-longest[i-1]-2&gt;=0 else 0 longest.append(longest[i-1]+2+tmp) else: longest.append(0) return max(longest) 方法三，通过栈存储不合法的括号位置，时间复杂度$O(n)$这种方法通过栈来存储不合法的括号的位置。 具体流程是先遍历字符串s，遇到左括号就将位置其入栈，遇到右括号就检查栈顶是否有左括号号，有的话就弹出这个左括号，否则将右括号的位置入栈。最后得到的栈中的元素就是原来str中不合法的括号的位置，那么栈中相邻位置的元素的差值就是一个合法的括号串的长度，找到其中最大值即可。 需要注意的是最后遍历栈找最长合法串时需要在最左边和最右边分别加上-1和len(s)(s就是原字符串的长度)，目的是为了当原字符串最左边或最右边的括号均为合法时能够被计算长度。 实现代码如下：123456789101112131415161718192021222324252627class Solution(object): def longestValidParentheses(self, s): """ :type s: str :rtype: int """ maxLen = 0 stack=[] for i in range(len(s)): if s[i] == ')' and len(stack)&gt;0 and s[stack[-1]]=='(': stack.pop(len(stack)-1) maxLen = 2 else: stack.append(i) # 全匹配 if len(stack) == 0: return len(s) # 不在不匹配的要在头尾插入元素来计算头或尾匹配的括号 stack.append(len(s)) stack.insert(0,-1) j = len(stack)-1 while j&gt;0: maxLen = max(maxLen,stack[j]-stack[j-1]-1) j -= 1 return maxLen 参考：【1】https://leetcode.com/discuss/7609/my-o-n-solution-using-a-stack【2】https://leetcode.com/discuss/8092/my-dp-o-n-solution-without-using-stack]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(31)--数字排列的下一项]]></title>
      <url>%2F2016%2F04%2F04%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(31)--%E6%95%B0%E5%AD%97%E6%8E%92%E5%88%97%E7%9A%84%E4%B8%8B%E4%B8%80%E9%A1%B9%2F</url>
      <content type="text"><![CDATA[原题如下： Implement next permutation, which rearranges numbers into the lexicographically next greater permutation of numbers.If such arrangement is not possible, it must rearrange it as the lowest possible order (ie, sorted in ascending order). The replacement must be in-place, do not allocate extra memory. Here are some examples. Inputs are in the left-hand column and its corresponding outputs are in the right-hand column.1,2,3 → 1,3,23,2,1 → 1,2,31,1,5 → 1,5,1 题目的意思就是对给出的数字list进行排列组合并从小到大排序，找出给出的list组成的数字的下一个数。 实现的时候当然不会弄得像上面说的这么复杂，只需要对给出的list从后往前遍历，找到第一个当前数字(记为i)比前一数字(记为j)大的那个数字，然后从这个位置往后的数字中找到一个&gt;j&amp;&amp;&lt;=i的最小的数字，交换两者后再对i后面的数字排序即可，说起来比较拗口。下面举个简单的例子。 假如给出的list是[1,2,5,4,3],找到的i=5，j=2,然后将i后面大于j且小于等于i的最小数字与j交换，这里是3，也就是交换后的list是[1,3,5,4,2],然后对i后面的数字进行排序，排序后为[1,3,2,4,5]，也是我们得到的结果。 实现代码如下：123456789101112131415161718192021222324252627282930#encoding:utf-8class Solution(object): def nextPermutation(self, nums): """ :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. """ i = len(nums)-1 flag = 0 while i &gt; 0: if nums[i] &gt; nums[i-1]: # 往后找大于nums[i-1]且小于nums[i]的数与nums[i-1]交换 j = len(nums)-1 while j&gt;i: if nums[i-1]&lt;nums[j]&lt;nums[i]: nums[j],nums[i-1] = nums[i-1],nums[j] flag = 1 break j-=1 # 找不到这样的数 if flag == 0: nums[i],nums[i-1] = nums[i-1],nums[i] # 交换后对i后面的元素进行排序 nums[i:] = sorted(nums[i:]) return else: i-=1 nums.sort() # 重新从小到大排序]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(30)--双指针找拼接子字符串]]></title>
      <url>%2F2016%2F04%2F04%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(30)--%E5%8F%8C%E6%8C%87%E9%92%88%E6%89%BE%E6%8B%BC%E6%8E%A5%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
      <content type="text"><![CDATA[原题如下： You are given a string, s, and a list of words, words, that are all of the same length. Find all starting indices of substring(s) in s that is a concatenation of each word in words exactly once and without any intervening characters.For example, given:s: “barfoothefoobarman”words: [“foo”, “bar”] You should return the indices: [0,9].(order does not matter). 题目不难理解，实现也不难，关键是时间复杂度的控制。下面会讲述两种方法，第一种方法时间复杂度$O((n-k)*k)$,其中n为s的长度，k为所有的words拼接起来后的长度，这种方法提交时在TLE和AC间徘徊（即偶尔AC，偶尔TLE）。第二种方法时间复杂度为O(n)，提交的时候能够AC。 方法一思路设wordLen为给出的words列表中每个词的长度，wordNum为给出的words列表的长度，n为给出的s的长度。 那么可以从头开始遍历s直到n-wordLen*wordNum+1，每次遍历用一个字典存储遍历中遇到的属于words的词语，并检查两者对应的词语的数量关系。遇到不属于words的词语或者字典中的某一词语的数量大于words中该词语的数量则跳出执行下一次的遍历 这种方法的时间复杂度$O((n-k)*k)$，n为s的长度。提交时偶尔TLE，偶尔AC，AC时显示的时间是876ms。 实现代码：12345678910111213141516171819202122232425262728293031323334353637383940class Solution(object): def findSubstring(self, s, words): """ :type s: str :type words: List[str] :rtype: List[int] """ # 注意words中的词可能会重复 wordDict = &#123;&#125; for word in words: if word in wordDict: wordDict[word] += 1 else: wordDict[word] = 1 wordLen = len(words[0]) result = [] for i in range(len(s)-wordLen*wordNum+1): tmp = s[i:i+wordLen] tmpDict = &#123;&#125; # 存储words的临时list if tmp in words: head = i while tmp in words and i &lt; len(s)-wordLen+1: if tmp in tmpDict: tmpDict[tmp] += 1 else: tmpDict[tmp] = 1 if tmpDict[tmp] &gt; wordDict[tmp]: tmpDict[tmp] -= 1 break i += wordLen tmp = s[i:i+wordLen] if tmpDict == wordDict: result.append(head) else: continue return result 方法二思路这种方法参考了这里,主要思想是利用了双指针围着当前符合words（包括数量和种类）中的连续子字符串，同时计算围着的子字符串中word的数量。 巧妙的地方在于双指针当前包围的子字符串中如果含有某个word的数量大于words中这个word的数量时，移动左指针直到把这个word的数量减少。 这种方法的时间复杂度为O(n),提交时AC的时间约100ms 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class Solution(object): def findSubstring(self, s, words): """ :type s: str :type words: List[str] :rtype: List[int] """ # 注意words中的词可能会重复 wordDict = &#123;&#125; for word in words: if word in wordDict: wordDict[word] += 1 else: wordDict[word] = 1 wordLen = len(words[0]) wordNum = len(words) wordSet = set(words) sLen = len(s) result = [] for i in range(wordLen): left = i tmpDict = &#123;&#125; count = 0 j=i while j &lt; sLen-wordLen+1: tmp = s[j:j+wordLen] j += wordLen if tmp in wordSet: if tmp in tmpDict: tmpDict[tmp]+=1 else: tmpDict[tmp]=1 if tmpDict[tmp]&lt;=wordDict[tmp]: count+=1 else: # 某个词的数量比wordDict中规定的要多了，往右移动左指针 while tmpDict[tmp] &gt; wordDict[tmp]: t = s[left:left+wordLen] left += wordLen tmpDict[t] -= 1 if tmpDict[t] &lt; wordDict[t]: count -= 1 # 仅去掉最左边的一个word，再往右找 if count == wordNum: result.append(left) t = s[left:left+wordLen] tmpDict[t] -= 1 left += wordLen count -= 1 # 当前的词不在wordDict else: tmpDict = &#123;&#125; count = 0 left = j return result]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中Hashtable与HashMap的区别]]></title>
      <url>%2F2016%2F04%2F03%2FJava%E4%B8%ADHashtable%E4%B8%8EHashMap%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[据说这是面试中被问频率非常高的一个问题，下面做简单的记录： 相同点 父类都是Map类 都是用来存储kv对的 存取kv的方法名一样（前提是均使用iterator来遍历） 不同点线程安全Hashtable是线程安全的，HashMap不是线程安全的。什么是线程安全？简单就是在多线程的情况下变量的值能否正确地被每个线程修改。因为多线程同时修改的时候有可能发生冲突，如同时修改一个变量等操作。 所以在多线程的情况下，要使用Hashtable。 效率Hashtable的效率比HashMap的效率要低。表现为相同的数据下HashMap比Hashtable使用的内存更多而且更慢。这个可以算是线程安全所付出的代价，因为要保证线程间的同步，需要额外的维护变量不能同时被修改。 所以单线程的时候，建议使用HashMap，效率较高 遍历的方法两者均可通过java.util.Iterator来遍历，除此之外Hashtable还可以通过java.util.Enumeration来遍历。关于两者的区别可以看这篇文章 空键值Hashtable不允许null的键值，HashMap则允许一个 null 键和若干null的value。 使用建议单线程的情况下使用HashMap，效率较高，多线程的情况下使用Hashtable保证线程间的同步。 此外，Vector和ArrayList的一个重要区别也是上面提到的线程安全问题，其中Vector是线程安全的，而ArrayList不是线程安全的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中Iterator和Enumeration的区别]]></title>
      <url>%2F2016%2F04%2F03%2FJava%E4%B8%ADIterator%E5%92%8CEnumeration%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[Java中的java.util.Iterator和java.util.Enumeration均可用来遍历Java中的集合框架（list,map,set等）。但是两者也有一些区别，主要表现为: 并非所有的collection都支持Enumeration的遍历，但是都支持Iterator的遍历。如Hashtable支持但是HashMap不支持。但是两者都支持Iterator的遍历。 Iterator 提供了remove()方法可以在遍历的同时删除集合中的元素，但是Enumeration没有这个方法，对集合中的元素只读 两者的方法名不同，具体见下面的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * compare Iterator with Enumeration */import java.util.*;public class HashTableAndHashMap &#123; public static void main(String[] args) &#123; //init the map Hashtable&lt;String,String&gt; ht = new Hashtable&lt;String,String&gt;(); Map&lt;String,String&gt; hm = new HashMap&lt;String,String&gt;(); for(int i=0;i&lt;5;i++) &#123; ht.put(Integer.toString(i), Integer.toString(i)); hm.put(Integer.toString(i), Integer.toString(i)); &#125; //different ways of iterating the values Iterator&lt;Map.Entry&lt;String, String&gt;&gt; it = hm.entrySet().iterator(); System.out.println("traverse hashmap"); while (it.hasNext()) &#123; String key = it.next().getKey(); String value = hm.get(key); if (key.equals("2")) &#123; it.remove(); System.out.println(key+" is removed"); continue; &#125; System.out.println(key+":"+value); &#125; System.out.println("hashmap final size:"+hm.size()+"\n"); System.out.println("traverse hashtable"); Enumeration&lt;String&gt; em = ht.keys(); while(em.hasMoreElements()) &#123; String key = em.nextElement(); String value = ht.get(key); if (key.equals("2")) &#123; //em不提供remove的方法，但是可以通过ht.remove(key)删除 System.out.println(key+" is removed"); continue; &#125; System.out.println(key+":"+value); &#125; &#125;&#125; 从上面的代码可以看到，两者遍历的方法名不同： 名称 是否还有元素 找到下一元素 Iterator hasNext() next() Enumeration hasMoreElements() nextElement() 而且虽然em遍历的时候不可以通过自己删除某个元素，但是可以通过collection自身删除，见上面的ht.remove(),而通过Iterator遍历的时候不可以通过collection自身删除，如上面假如用了hm.remove()会抛出ConcurrentModificationException。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(29)--通过加法完成除法]]></title>
      <url>%2F2016%2F04%2F01%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(29)--%E9%80%9A%E8%BF%87%E5%8A%A0%E6%B3%95%E5%AE%8C%E6%88%90%E9%99%A4%E6%B3%95%2F</url>
      <content type="text"><![CDATA[原题如下： Divide two integers without using multiplication, division and mod operator. If it is overflow, return MAX_INT. 题目要求返回一个数除以另外一个数的结果，并且要求不能用乘除或取模，意思就是只能用加减，用加减也不难，先初始化sum=0，每次sum加上除数的大小并与被除数比较，直到sum大于等于被除数即可。但是这样的时间复杂度是$\Theta(n)$，提交时会超时。 解决思路是采用二分搜索，或者说变形的二分搜索。思路类似于上面所说的，但是sum每次加上自己的大小，也相当于sum=sum*2,这样的时间复杂度是$\Theta(log_2n)$,提交时能够AC 实现的代码如下：1234567891011121314151617181920212223242526272829class Solution(object): def divide(self, dividend, divisor): """ :type dividend: int :type divisor: int :rtype: int """ MAX_INT = pow(2,31)-1 if divisor == 0: return MAX_INT flag = 1 if (dividend &gt; 0 and divisor&lt;0 ) or (dividend&lt;0 and divisor &gt;0): flag = -1 a = abs(dividend) b = abs(divisor) res = 0 while a &gt;= b : sum = b count =1 while a &gt;= sum+sum: sum += sum count += count a-=sum res+=count if res*flag &gt; MAX_INT: return MAX_INT else: return res*flag]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(27)--双指针找数组所有特定元素]]></title>
      <url>%2F2016%2F03%2F31%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(27)--%E5%8F%8C%E6%8C%87%E9%92%88%E6%89%BE%E6%95%B0%E7%BB%84%E6%89%80%E6%9C%89%E7%89%B9%E5%AE%9A%E5%85%83%E7%B4%A0%2F</url>
      <content type="text"><![CDATA[原题如下： Given an array and a value, remove all instances of that value in place and return the new length. Do not allocate extra space for another array, you must do this in place with constant memory.The order of elements can be changed. It doesn’t matter what you leave beyond the new length. Example:Given input array nums = [3,2,2,3], val = 3 Your function should return length = 2, with the first two elements of nums being 2. 实现思路：双指针遍历，指针1从前往后遍历，指针2从后往前遍历。指针2先往前找到与val值不同的元素，然后停止；接着指针1开始往后找到与val相同的元素，然后指针1和指针2的元素交换，这样便把与val值相同的元素全部扔到了数组末尾。 需要注意的是，虽然该数组提供了删除元素的操作，但是删除一个元素的平均时间复杂度是$O(n)$,总的时间复杂度为$O(n^2)$ 还需要注意的是一个溢出的问题，对于一般的编译型语言如Java、C++等int是4个字节的，所以int的范围是$-2^{31}$ ~ $2^{31}-1$,但是在python中int在32位系统上占四个字节，在64为系统上占8个字节，可通过sys.maxint查看，而在本题中默认就是4个字节，如果溢出时输出sys.maxint会报错。 实现代码如下:1234567891011121314151617181920212223242526class Solution(object): def removeElement(self, nums, val): """ :type nums: List[int] :type val: int :rtype: int """ if len(nums) == 0: return 0 l = 0 r = len(nums)-1 while l&lt;r: if nums[r] == val: r-=1 continue if nums[l] == val: nums[l],nums[r] = nums[r],nums[l] l += 1 r -= 1 else: l += 1 if l == r: # 有可能l&gt;r if not nums[l] == val: l+=1 return l]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(26)--消除有序数组中重复值(常数空间)]]></title>
      <url>%2F2016%2F03%2F31%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(26)--%E6%B6%88%E9%99%A4%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E5%80%BC(%E5%B8%B8%E6%95%B0%E7%A9%BA%E9%97%B4)%2F</url>
      <content type="text"><![CDATA[原题如下： Given a sorted array, remove the duplicates in place such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this in place with constant memory.For example,Given input array nums = [1,1,2], Your function should return length = 2, with the first two elements of nums being 1 and 2 respectively. It doesn’t matter what you leave beyond the new length. 题目不难理解，但是要注意的是除了返回length以外，对原来数组的元素也要处理，因为LeetCode的评测是根据你给出的length去读取原来数组的前length个元素。 注意题目要求的空间复杂度是常数，这意味着不能新建一个数组来存放不重复元素。 实现思路是双指针，就是先用一个整数count来维护已经选出的不重复的元素的个数，同时count也作为数组下标。然后另外一个指针遍历数组，找到与当前数组下标为count的元素不重复的元素，交换两者即可。 因为数组已经排列了，所以后面的元素肯定会比前面的要大。所以假如给出的数组是无序的同时要实现这个功能，那么可以先排序再使用上面的方法。 实现代码如下：123456789101112131415class Solution(object): def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) &lt;= 1: return len(nums) count = 0 for i in range(1,len(nums)): if not nums[i] == nums[count]: count+=1 nums[count]=nums[i] return count+1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(23)--合并k个有序数组]]></title>
      <url>%2F2016%2F03%2F29%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(23)--%E5%90%88%E5%B9%B6k%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%2F</url>
      <content type="text"><![CDATA[原题如下： Merge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity. 题目不难理解，就是将k个排好序的链表合并成一个，可以说是merge two sorted lists的升级版。一开始想的方法超时，后来参考了网上的两种方法并通过python实现后能够AC，下面分别讲述这三种方法。 方法一：线性合并（TLE）一开始想到的方法就是基于merge two sorted lists逐个合并list，就是先将两个list合成一个，然后将这个合并好的list再和一个未合并的list进行merge操作，这样总共会合并k-1次，时间复杂度为$O(2n+3n+….+kn) = O(nk^2)$（设n为链表的平均长度）。 实现代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): def mergeKLists(self, lists): """ :type lists: List[ListNode] :rtype: ListNode """ k = len(lists) if k == 0: return None mergedList = lists[0] for i in range(1,k): mergedList = self.mergeTwoLists(mergedList,lists[i]) return mergedList def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if l1 == None: return l2 if l2 == None: return l1 dummy = ListNode(0) nextNode = dummy while l1 and l2: if l1.val &gt; l2.val: nextNode.next = l2 nextNode = nextNode.next l2 = l2.next else: nextNode.next = l1 nextNode = nextNode.next l1 = l1.next if l1: nextNode.next = l1 if l2: nextNode.next = l2 return dummy.next 方法二：归并合并（AC）这种方法类似于归并排序，先将当前需要排序的list对半分，重复这个步骤直到对半分出的list的数量为1，在进行merge，这时实际进行的是merge two sorted list。 这种方法采用的思想跟第一种一样，都是分治法，先处理好局部，再合并成一个整体。但是与第一种方法在于这种方法在给出的lists的数量很大时需要进行merge的操作小于方法一。 这里有两个需要注意是当lists的数量很大（也就是k很大）是merge的操作才会比方法一要少。方法一无论k的大小merge的次数为k-1，而方法二merge的次数为 $$\sum_{i=0}^m 2^i (m=log_2k-1)$$ 可以通过下面的程序验证当k很大时，这两种方法merge的次数不同123456789import mathk = 100000m = int(math.log(k,2))sum = 0for i in range(m): sum+=math.pow(2,i)print 'merge times for method 1 when k=%s: %s'%(k,k-1)print 'merge times for method 2 when k=%s: %s'%(k,sum) 根据主定理分析，这种方法的时间复杂度是O(nklog(nk)),下面是方法二实现的具体代码1234567891011121314151617181920212223242526272829303132333435363738394041class Solution(object): def mergeKLists(self, lists): """ :type lists: List[ListNode] :rtype: ListNode """ k = len(lists) if k == 0: return None return self.helper(lists,0,len(lists)-1) def helper(self,lists,l,r): if l&lt;r: m = (r-l)/2 return self.mergeTwoLists(self.helper(lists,l,l+m),self.helper(lists,l+m+1,r)) else: return lists[l] def mergeTwoLists(self,l1,l2): if l1 == None: return l2 if l2 == None: return l1 dummy = ListNode(0) curr = dummy while l1 and l2: if l1.val &lt; l2.val: curr.next = l1 curr = curr.next l1 = l1.next else: curr.next = l2 curr = curr.next l2 = l2.next if l1: curr.next = l1 if l2: curr.next = l2 return dummy.next 方法三：基于堆排序的归并（AC）第三种方法非常巧妙，先建立一个大小为k的堆（k就是链表数量），堆中的一个元素代表一个链表当前的最小元素，每次取堆顶的最小元素放到结果中，然后读取该元素的下一个元素放入堆中，重新维护好。 因为每个链表是有序的，每次又是去当前k个元素中最小的，所以当所有链表都读完时结束，这个时候所有元素按从小到大放在结果链表中。时间复杂度是O(nklogk)。 实现代码如下,建堆的方法有两种，下面一并给出：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class Solution(object): def mergeKLists(self, lists): """ :type lists: List[ListNode] :rtype: ListNode """ k = len(lists) if k == 0: return None listsHeap = [] listsHeap.append(0) # 使堆中的元素从1开始 for i in range(k): if lists[i] == None: # avoid empty list continue listsHeap.append(lists[i]) dummy = ListNode(0) curr = dummy # 初始化堆有两种方法 '''方法一 j = len(listsHeap) - 1 while j &gt; 1: if listsHeap[j].val&lt;listsHeap[j/2].val: listsHeap[j],listsHeap[j/2] = listsHeap[j/2],listsHeap[j] self.siftDown(listsHeap,j) #必须，否则初始建的堆会有问题 j-=1 ''' # 方法二 leafParent = (len(listsHeap)-1)/2 for i in range(leafParent,0,-1): siftDown(listsHeap,i) # 取堆顶元素并调整堆 while len(listsHeap) &gt; 1: curr.next = listsHeap[1] curr = curr.next if listsHeap[1].next == None: # 将空的列表移到最后并删除 listsHeap[1] = listsHeap[len(listsHeap)-1] del(listsHeap[len(listsHeap)-1]) else: listsHeap[1] = listsHeap[1].next self.siftDown(listsHeap,1) return dummy.next def siftDown(self,listsHeap,i): while i*2+1 &lt;= len(listsHeap): if i*2+1 &lt; len(listsHeap): if listsHeap[i].val &gt; min(listsHeap[i*2].val,listsHeap[i*2+1].val): if listsHeap[i*2].val &lt; listsHeap[i*2+1].val: listsHeap[i],listsHeap[i*2] = listsHeap[i*2],listsHeap[i] i = i*2 else: listsHeap[i],listsHeap[i*2+1] = listsHeap[i*2+1],listsHeap[i] i = i*2+1 else: return elif i*2+1 == len(listsHeap): if listsHeap[i*2].val &lt; listsHeap[i].val: listsHeap[i],listsHeap[i*2] = listsHeap[i*2],listsHeap[i] i = i*2 return 参考：http://blog.csdn.net/linhuanmars/article/details/19899259]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(22)--生成所有合法的嵌套括号]]></title>
      <url>%2F2016%2F03%2F29%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(22)--%E7%94%9F%E6%88%90%E6%89%80%E6%9C%89%E5%90%88%E6%B3%95%E7%9A%84%E5%B5%8C%E5%A5%97%E6%8B%AC%E5%8F%B7%2F</url>
      <content type="text"><![CDATA[原题如下： Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.For example, given n = 3, a solution set is:“((()))”, “(()())”, “(())()”, “()(())”, “()()()” 题目要求生成给定数量的所有合法嵌套括号，最直观的想法是通过穷举得到所有的组合结果，再判断每个是否合法，但是这样的时间复杂度太大，会导致TLE。 既然上面的方法会导致TLE, 那么能不能在构造的过程中判断目前所构造的嵌套括号是否合法？ 答案是可以的，只需要遵循以下的两条规则即可:1.只要”(“的数量没有超过n，都可以插入”(“。2.而可以插入”)”的前提则是当前的”(“数量必须要多余当前的”)”数量。 通过回溯法能够实现这个判断并构造出所有合法的嵌套括号，实现代码如下：123456789101112131415161718class Solution(object): def generateParenthesis(self, n): """ :type n: int :rtype: List[str] """ res = [] self.helper(n,n,'',res) return res def helper(self,left,right,item,res): if left==0 and right==0: res.append(item) return if left&gt;0: self.helper(left-1,right,item+'(',res) if right&gt;left: self.helper(left,right-1,item+')',res)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搜狗、百度、QQ输入法的词库爬虫]]></title>
      <url>%2F2016%2F03%2F27%2F%E6%90%9C%E7%8B%97%E3%80%81%E7%99%BE%E5%BA%A6%E3%80%81QQ%E8%BE%93%E5%85%A5%E6%B3%95%E7%9A%84%E8%AF%8D%E5%BA%93%E7%88%AC%E8%99%AB%2F</url>
      <content type="text"><![CDATA[本文主要讲述了通过 python 实现的用于下载搜狗、百度、QQ三个输入法的词库的爬虫的实现原理。主要利用了python自带的urllib2、Queue、re、threading模块，并分别通过单线程和多线程实现。最后会给出完整的源码地址。 原理及注意事项爬取每个输入法的实现原理都一样，步骤如下： （1）获取输入法词库的分类（2）下载各个分类并按分类在本地存储 其中（1）实现的关键点是正则表达式提取网页中的词库分类，（2）实现的关键点是通过广度优先搜索（bfs）来遍历某一类别的所有页面。 正则表达式提取词库分类步骤（1）通过解释词库分类的网页源码来获取具体分类（以搜狗输入法为例，词库分类的网页为http://pinyin.sogou.com/dict/ )。先人工观察网页源码的结构，一般同一级的分类的在源码中的 href(超链接)的结构都是一样的，仅仅是id或名字不同，这就可以通过正则表达式来提取网页中的分类，并用嵌套的字典来存储多级分类。 BFS遍历某一分类的所有页面步骤（1）提供了词库的分类，步骤（2）需要考虑的就是怎么下载某一类词库。因为词库文件的数量较多，所以即使是某一类的词库往往也会分多个页面来展示（常见的如通过点击上一页、下一页跳转），所以要完整下载这一类的词库必须遍历这一类词库的所有页面。 这里采用BFS来实现，实现步骤如下：1）先通过正则表达式分析当前访问页面的源码，获取当前页面可以跳转到的其他页面的url，然后将这些URL放入到队列中作为待访问的URL2）通过正则表达式获取分析当前访问的页面的源码，获取可以下载的词库文件的url存入一个临时的列表（List）中，并开始逐一下载各个文件3）将当前页面url放入到一个集合（set）中，标记为已访问的url，防止下一次重复访问4）从队列中取出下一个url，到步骤3）的集合中检查url是否被访问，如果被访问过则继续取下一个，否则将这个url设为当前url并回到步骤1） 按照上面的步骤一直循环执行直到队列为空，这样便可下载了某一类的词库。如法炮制，便可下载所有类的词库了。 这里需要注意的是，在下载词库文件的时候有可能会出现防盗链的问题。简单来说就是下载词库文件的http请求头中的来源（Referer）不属于本站的就拒绝下载，返回403 forbidden。这种设计就是为了防止爬虫的整站下载，在爬取过程中发现搜狗输入法中有防盗链，而百度、QQ输入法则没有。解决方法也很简单，就是在下载文件是构造一个http请求头（headers），设置里面的Refererr为该站的任一url即可。 单线程下载单线程下载的注意事项上面基本提到了，下面贴出QQ输入法单线程下载的关键代码，注意这个代码直接执行会报错，可执行的完整代码见文末。1234567891011121314151617181920212223242526272829303132333435363738# 下载某一类别的关键代码，具体代码见文末链接 queue =Queue.Queue() # 存放待访问url visited = set() # 已经访问过的url downloaded = set() # 已经下载过的文件 firstURL = '' # url入口 queue.put(firstURL) # bfs遍历直至队列为空 while not queue.empty(): currentURL = queue.get() if currentURL in visited: continue else: visited.add(currentURL) try: response = urllib2.urlopen(currentURL) except : # 找到链接到其他页面的连接 data = response.read() pagePattern = re.compile('&amp;page=(\d+)"') pageList = re.findall(pagePattern,data) for i in pageList: pageURL = smallCateURL+'&amp;page='+i queue.put(pageURL) # 下载当前页面存在的文件 filePattern = re.compile('&lt;a href="/dict_detail\?dict_id=(\d+)"&gt;(.*?)&lt;/a&gt;') fileList = re.findall(filePattern,data) for id, name in fileList: fileURL = 'http://dict.qq.pinyin.cn/download?dict_id='+id filePath = cateDir.decode('utf8')+'/'+name.decode('gbk')+'.qpyd' # 文件已存在则不下载 if fileURL in downloaded: continue else: downloaded.add(fileURL) downloadSingleFile(fileURL) #下载这个文件 多线程下载单线程虽然能够下载词库文件，但是消耗的时间过长，这时候可通过多线程来下载。关于python多线程实现以及python多线程是否能够提高效率可以参考这篇文章。因为爬虫属于IO密集型的任务，所以可以通过多线程来提高下载效率。实测多线程开到10个的时候，下载速度比原来快了7倍。 多线程下载实现的思路与单线程的类似，只是在下载某一类别的词库时采用多线程完成，这就需要注意下面几个方面： 线程完成下载的时间不一样，早完成的线程需要等到最后一个线程下载完成才能一起开始下一个类别的下载,可通过python的Queue模块中的join()方法和task_done()方法实现 需要用线程锁保护可能会被修改的变量，采用python的队列数据结构（Queue）则不用，因为python的Queue模块已经实现了这个锁的功能，具体参见Queue — A synchronized queue class 关于队列模块中的join()方法和task_done()方法，官方文档说明如下： If a join() is currently blocking, it will resume when all items have been processed (meaning that a task_done() call was received for every item that had been put() into the queue). 也就是说Queue调用了join方法后，必须要收到每个取出的item（这里为url）返回的task_done()消息才会继续执行，否则会一直block等待，这就实现了我们说到的早完成的线程需要等到最后一个线程下载完成才能一起开始下一个类别的下载。 实现的关键代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 构建自己的线程类queue =Queue.Queue() # 存放待访问urlvisited = set() # 已经访问过的urldownloaded = set() # 已经下载过的文件class downloadThread(threading.Thread): # 重写run函数 def run(self): while True: if queue.empty(): # 防止一开始队列内容太少导致后创建的线程退出 continue currentURL = queue.get() # 查看url是否被访问过，需要用锁保护 threadingLock.acquire() try: if currentURL in visited: queue.task_done() # 不可少，否则queue调用了join()会一直block下去 continue else: visited.add(currentURL) finally: threadingLock.release() # 解析当前页面 try: response = urllib2.urlopen(currentURL) except : # 找到链接到其他页面的连接 data = response.read() pageList = re.findall(pagePattern,data) for i in pageList: pageURL = smallCateURL+'&amp;page='+i queue.put(pageURL) # 下载当前页面存在的文件 fileList = re.findall(filePattern,data) for id, name in fileList: fileURL = 'http://dict.qq.pinyin.cn/download?dict_id='+id filePath = downloadDir.decode('utf8')+'/'+name.decode('gbk')+'.qpyd' # 检查文件是否被下载 threadingLock.acquire() try: if fileURL in downloaded: continue else: downloaded.add(fileURL) finally: threadingLock.release() downloadSingleFile.downloadSingleFile(fileURL, filePath, logFile) #告诉queue当前任务已完成，否则因为queue调用了join，会一直block下去 queue.task_done() 最后，下载三个输入法的词库的python源码已放到github上，链接https://github.com/WuLC/ThesaurusSpider 文章为博主个人总结，如有错误，欢迎交流指正]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python中的多线程]]></title>
      <url>%2F2016%2F03%2F26%2Fpython%E4%B8%AD%E7%9A%84%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[本文主要讲述了python中多线程的使用、线程锁以及多线程在python中是否能够提高效率。 多线程的概念进程的相信大家都听说过，而线程可以理解为比进程更小一级的概念，一个进程内至少有一个线程,如果有多个线程，那么他们就共享进程的资源，共同完成进程的任务。 使用多线程一般有两个不同的目的：一是把程序细分成几个功能相对独立的模块，防止其中一个功能模块阻塞导致整个程序假死（GUI程序是典型）另一个就是提高运行效率，比如多个核同时跑，或者单核里面，某个线程进行IO操作时，另一个线程可以同时执行。具体可以参考这篇文章 相比进程，线程有以下优点 创建和销毁的代价比进程要小得多，尤其是在windows下，可以参考这个回答。而且线程间彼此切换所需的时间也远远小于进程间切换所需要的时间 线程间方便的通信机制。对不同进程来说，它们具有独立的数据空间，要进行数据的传递只能通过通信的方式进行。而由于同一进程下的线程之间共享数据空间，降低了通信的开销。 除了优点， 线程间方便的通信机制源于线程间数据的共享，同时也带来了其他问题，如需要保护变量不能同时被两个线程所修改，这也需要一定的开销，而且需要开发者处理好这个调度。 python中的多线程python中提供了两个模块实现多线程，分别是thread和threading，thread是比较低级的模块,而threading在其基础上封装了其他许多高级特性，故本文主要讲述threading模块的使用，若要了解thread模块的使用，请参考官方文档。 创建进程有两种方式，分别是继承threading.Thread类创建自己的线程子类和将需要线程执行的函数传入线程构造函数中。下面分别讲述 继承threading.Thread类继承threading.Thread类只能重写（override）__init__函数和run()函数，__init__函数就是构造函数，run()函数就是创建线程后线程需要执行的任务。下面是一个简单的demo12345678910111213141516171819202122232425# encoding:utf-8import threadingimport timeimport randomclass sleepThread(threading.Thread): def __init__(self): threading.Thread.__init__(self) print self.name+ ' is created!' def run(self): randomTime = random.randint(1,9) # 生成 1~9的随机整数 time.sleep(randomTime) print self.name+ ' slept for '+str(randomTime)+' seconds' if __name__ == '__main__': threads = [] for i in range(5): # 创建5个进程 th = sleepThread() threads.append(th) th.start() for t in threads: t.join() print 'all threads finished' 在上面的例子中，我们编写了自己的线程类sleepThread,然后创建了5个线程，用start()启动了各个线程，start()实际上是执行了线程类的run()函数。这时输出如下所示： 其中，默认线程的名称是Thread-i，i就是创建的第i个线程。join()函数的作用是等待线程执行完成再执行下面任务,实际的应用场景比如说进程要合并多个线程的处理结果，那么这时候join()函数就必不可少了。假如没有join()函数，即主函数改成下面的样子。123456if __name__ == '__main__': for i in range(5): # 创建5个进程 th = sleepThread() th.start() print 'all threads finished' 那么输出就像下面所示： 那为什么不在thread.start()后执行join()呢？即主函数改成以下样子。1234567 if __name__ == '__main__': for i in range(5): # 创建5个进程 th = sleepThread() th.start() th.join() print 'all threads finished' 这样输出的结果就像下面一样： 原因是线程join()后会阻塞后面线程的创建，导致线程无法并行，这样多线程就没有意义了。 将需要线程执行的函数传入线程构造函数中上面是线程的一种创建方式，实现上面相同功能的另外一种创建方式如下：123456789101112131415def sleepThread(threadName): randomTime = random.randint(1,9) # 生成 1~9的随机整数 time.sleep(randomTime) print threadName+ ' slept for '+str(randomTime)+' seconds' if __name__ == '__main__': threads = [] for i in range(5): th = threading.Thread(target=sleepThread,args=('Thread-'+str(i),)) threads.append(th) th.start() for t in threads: t.join() print 'all threads finished' 利用了threading.Thread自身的构造函数，传入的target参数作为线程的run函数,args参数则为传入的run函数的参数。 输出结果如下所示： 线程还有比较常用的方法比如说setdaemon(True),字面上的意思是设为守护线程，但是这个守护线程跟守护进程有很大的区别，实际上setdaemon(True)的作用是保证主线程（就是任何进程最开始的那个线程）退出时，派生出来的线程也必须退出。详细例子见http://stackoverflow.com/questions/5127401/setdaemon-function-in-thread 线程锁因为多线程共享一个进程内的资源，所以多个线程同时修改同一个变量时会发生冲突。这时候就需要线程锁了。比如说下面这段代码;1234567891011121314151617181920count = 10def modifyThread(num): global count for i in range(1000): count -= num count += numif __name__ == '__main__': threads = [] print 'before modifying, count=%s '%count for i in range(5): th = threading.Thread(target=modifyThread,args=(i,)) threads.append(th) th.start() for t in threads: t.join() print 'after modifying, count=%s '%count 执行的的时候每次输出结果都不一样，例如下图： 这是因为count是被多个线程同时修改了，解决方法就是利用线程锁threading.Lock(),每次需要修改count时先获取线程锁，修改完再释放。实例代码如下所示：1234567891011121314151617181920212223242526count = 10def modifyThread(num): global count threadLock.acquire() try: for i in range(1000): count -= num count += num finally: threadLock.release() if __name__ == '__main__': threads = [] threadLock = threading.Lock() print 'before modifying, count=%s '%count for i in range(5): th = threading.Thread(target=modifyThread,args=(i,)) threads.append(th) th.start() for t in threads: t.join() print 'after modifying, count=%s '%count 当多个线程同时执行threadLock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。 在其中一个线程获取了线程锁（threadLock.acquire()）后，其他线程便无法修改count，但是修改完后一定要记得释放线程锁（threadLock.release()），否则其他线程会一直处于blocked的状态，上面采用了try-finally保证锁一定被释放。除了try-finally,还可通过 with 语句实现锁的自动获取和释放, 也就是说上面的 modifyThread 函数可以写成下面的形式 123456def modifyThread(num): global count with threadLock: for i in range(1000): count -= num count += num 通过加锁的方法修改 count, 最终得到的count的值不变。 线程锁(Lock)是线程同步的一种方式，除此之外，还有RLocks, Semaphores, Condition, Events 和 Queues，具体可参考官方文档和Python threads synchronization: Locks, RLocks, Semaphores, Conditions, Events and Queues 多线程是否提高了效率常常会听到有人说，因为python多线程只能使用一个核，所以多线程实际上并没有提高效率。这句话可以说一半正确，一半不正确。原因如下： python多线程只能使用一个核这句话针对部分python解析器如CPython等是正确的，而且是相对与Java、C++那些一个线程就可以占一个核的程序而言。python的官方文档描述如下： In CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe 原因是python的解析器（如CPython）因为内存管理问题设计了一个GIL（全局解析锁），GIL保证了任何时候都只能有一个线程执行其字节码。这就限制了同一进程内同一时间只能有一个线程在执行其字节码，也就是说无论一个进程无论创建多少线程都只能使用一个核。 而且，这个GIL也只在CPython等解释器有，其他的如 Jython 或 IronPython 中没有GIL，多线程可以利用多个核。另外，即使是CPython解释器，也可通过多进程来达到利用多个核的目的。 那第二句话多线程实际上并没有提高效率是否正确？可以说也是部分正确，实际上针对CPU密集型的 python 进程，多线程没有提高效率，而针对IO密集型的 python 进程会提高效率。 从上面的解释我们知道，GIL是限制了多线程并发执行的一个关键因素，而GIL仅仅是限制了同一时间同一进程只能有一个线程执行字节码，执行字节码是在CPU中的，对于CPU密集型的多线程，会一直占据着CPU导致其效果跟单线程一样。 而对于IO密集型的多线程，线程的执行时间会较多地消耗在IO上，因而CPU可供多线程轮流使用。比如说我曾用python爬取几个输入法的词库的，多线程比单线程要快了好几倍，原因就是爬虫属于IO密集型的任务，线程执行字节码所需的时间很短，而把大部分时间放在了下载和存储在本地上，线程执行完字节码后会释放GIL，从而其他线程也能够执行其字节码。从而在总体上提高了下载效率。 文章为博主个人理解总结，如有错误，欢迎指出交流。 参考： threading — Higher-level threading interfaceGlobalInterpreterLock多线程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(19)--从后往前删除链表第n个元素]]></title>
      <url>%2F2016%2F03%2F10%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(19)--%E4%BB%8E%E5%90%8E%E5%BE%80%E5%89%8D%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%AC%ACn%E4%B8%AA%E5%85%83%E7%B4%A0%2F</url>
      <content type="text"><![CDATA[原题如下： Given a linked list, remove the nth node from the end of list and return its head. For example, Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2. After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5. Note:Given n will always be valid.Try to do this in one pass. 题目比较容易理解，就是删除从后往前数的第n个节点。一开始想到的方法就是先找出链表元素总数num，再删除第num-n+1个元素即可.实现方法如下：12345678910111213141516171819202122232425class Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ nextNode = head.next num = 1 while not nextNode == None: num +=1 nextNode = nextNode.next delete = num-n+1 if delete == 1: return head.next else: currNode = head i = 1 while i&lt;delete-1: currNode = currNode.next i+=1 currNode.next = currNode.next.next return head 这种方法虽然能AC，但是题目提示了Try to do this in one pass,也就是这道题目存在只需遍历一遍的解法。在网上查找后发现可以先让一个指针p1从head向后移动n个node，然后另外一个指针p2此时从head出发和p1一起移动，当p1移动到最后一个节点时，p2后面的一个元素即为需要删除的元素。实现的代码如下：12345678910111213141516171819class Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ p1=head i = 1 while i&lt;=n: p1 = p1.next i += 1 p2 = head while not p1.next==None: p1 = p1.next p2 = p2.next p2.next =p2.next.next return head 但是在提交时出错，排查后发现上述代码删除的node刚好是head时会出错，因为是从1开始往后n步，当删除的node是head是，n是所有节点的数目，而此时会导致p1为None，然后p1.next自然会报错。 解决方法是增加一个无意义的node指向head，从这个node开始遍历即可。 实现代码如下：1234567891011121314151617class Solution: def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ dummy=ListNode(0) dummy.next=head p1=p2=dummy for i in range(n): p1=p1.next while p1.next: p1=p1.next p2=p2.next p2.next=p2.next.next return dummy.next 除了上面的解决方法，一开始还想过通过将链表反转再删除第n个node即可，但是题目意思是删除这个node不能改变其他node原来的排序。虽然不符合题意，但是这里一并附上代码记录链表反转的一种方法： 123456789101112131415161718192021222324252627class Solution(object): def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ preNode = head nextNode = head.next tmp = None while not nextNode == None: preNode.next = tmp tmp = preNode preNode = nextNode nextNode = nextNode.next preNode.next = tmp head = preNode if n==1: return head.next else: i = 1 while i &lt; n-1: nextNode = nextNode.next i += 1 nextNode.next = nextNode.next.next return head]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告(167, 1, 15, 16, 18)--双指针解决ksum问题]]></title>
      <url>%2F2016%2F03%2F08%2FLeetCode%20%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(167%2C%201%2C%2015%2C%2016%2C%2018)--%E5%8F%8C%E6%8C%87%E9%92%88%E8%A7%A3%E5%86%B3ksum%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[ksum 问题是一类问题, 要求从给定的数字中找到k个数，使得这k个数的和等于特定值。题目不难，直观的方法的时间复杂度为 $O(n^k)$, $n$ 为给定的数字的个数， 关键在于时间复杂度的控制, 本文主要讲述通过双指针将这类问题的时间复杂度降为 $O(n^{k-1})$。 下面将以 LeetCode 上的几道题目为例讲述这类问题：167. Two Sum II - Input array is sorted1. Two Sum15. 3Sum16. 3Sum Closest18. 4Sum 之所以从 167. Two Sum II - Input array is sorted 开始讲述,是因为解决这个题目的思路就是解决这类问题的核心思想。 167. Two Sum II - Input array is sorted 这道题目的解决思路如下：1）初始化双指针，left指针在最左端，right指针在最右端2）计算left指针和right指针指向的两个数的和，如果大于目标和，右指针往左移动一步；如果小于目标和，左指针往右移动一步；如果等于目标和则返回3) 重复 2）的操作直到两个指针相遇 上面的方法需要的前提条件是数字列表是有序的。 实现的python代码如下所示： 12345678910111213141516class Solution(object): def twoSum(self, numbers, target): """ :type numbers: List[int] :type target: int :rtype: List[int] """ left, right = 0, len(numbers) - 1 while left &lt; right: tmp = numbers[left] + numbers[right] if tmp &gt; target: right -= 1 elif tmp &lt; target: left += 1 else: return [left+1, right+1] 通过上面的方法的思想可以解决这一类问题。 对于题目 1. Two Sum ，因为原来的数组数无序的，因此需要对数组先排序，而且由于要求返回的是下标而不是数字，因此也需要一个 HashTable 来存储原来数字的下标。时间复杂度为$(nlgn)$ AC的python代码如下：1234567891011121314151617181920212223# O(nlgn) time, O(n) space, ACclass Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ inx = &#123;&#125; for i in xrange(len(nums)): inx.setdefault(nums[i], []) inx[nums[i]].append(i) nums.sort() left, right = 0, len(nums) - 1 while left &lt; right: tmp = nums[left] + nums[right] if tmp &gt; target: right -= 1 elif tmp &lt; target: left += 1 else: return sorted([inx[nums[left]].pop(), inx[nums[right]].pop()]) 对于题目 15. 3Sum ,也是需要先要对无序的数组进行排序，然后固定第一个数的下标，再用上面的双指针方法找到另外两个数；由于需要返回的是数字而不是下标，因此不需要一个 HashTable 来存储这些数字的下标。时间复杂度为$O(n^2+nlgn)$,也就是$O(n^2)$。 AC的python代码如下所示： 12345678910111213141516171819202122232425262728class Solution(object): def threeSum(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ i = 0 result = [] nums.sort() while i&lt;len(nums)-2: if nums[i]&gt;0: break else: j = i+1 k = len(nums)-1 while j &lt; k: if nums[i]+nums[j]+nums[k]&gt;0: k-=1 elif nums[i]+nums[j]+nums[k]&lt;0: j+=1 else: tmp = [nums[i],nums[j],nums[k]] if tmp not in result: result.append(tmp) j+=1 k-=1 i+=1 return result 题目 16. 3Sum Closest 与 15. 3Sum 思路一致，只是要找到与目标和最接近的和。时间复杂度也是 $O(n^2)$ AC的python代码如下所示123456789101112131415161718192021222324252627282930313233class Solution(object): def threeSumClosest(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if len(nums) == 0: return 0 nums.sort() min = None for i in range(len(nums)-2): j = i+1 k = len(nums)-1 while j&lt;k: sum = nums[i]+nums[j]+nums[k] if sum &gt; target: gap = abs(sum -target) if min == None or min &gt; gap: min = gap result = sum k-=1 elif sum &lt; target: gap = abs(target - sum) if min==None or min &gt; gap: min = gap result = sum j+=1 else: result = sum return result return result 题目 18. 4Sum 的思路与上面的15. 3Sum， 只是一开始需要固定前两个数，然后通过双指针找到另外两个数，时间复杂度是$O(n^3)$。推广到 ksum 问题就是开始需要固定 k-2 两个数，然后通过双指针找到剩下的两个数。时间复杂度是$O(n^{k-1})$ AC的 python 代码如下： 1234567891011121314151617181920212223242526272829class Solution(object): def fourSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[List[int]] """ result =[] if len(nums) == 0: return result nums.sort() for i in range(len(nums)-3): for j in range(i+1,len(nums)-2): m = j +1 n = len(nums)-1 while m &lt; n: sum = nums[i] + nums[j] + nums[m] + nums[n] if sum&gt;target: n-=1 elif sum&lt;target: m+=1 else: tmp = [nums[i],nums[j],nums[m],nums[n]] if tmp not in result: result.append(tmp) m+=1 n-=1 return result]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(17)--电话数字组合成的不同字符串]]></title>
      <url>%2F2016%2F03%2F06%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(17)--%E7%94%B5%E8%AF%9D%E6%95%B0%E5%AD%97%E7%BB%84%E5%90%88%E6%88%90%E7%9A%84%E4%B8%8D%E5%90%8C%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
      <content type="text"><![CDATA[原题如下: Given a digit string, return all possible letter combinations that the number could represent.A mapping of digit to letters (just like on the telephone buttons) is given below. Input:Digit string “23”Output: [“ad”, “ae”, “af”, “bd”, “be”, “bf”, “cd”, “ce”, “cf”]. 题目很好理解，就是组合出所有可能结果即可。每个数字写一个for循环即可，但是在程序运行前我们无法知道到底输入有几个数字,下面给出两个方法解决这个问题。 方法一是回溯法，每次选择当前数字的一个字符，然后将下一个数字作为当前数字，直到所有的数字都遍历完即可。实现的代码如下所示： 123456789101112131415161718class Solution(object): numDict =&#123;'2':['a','b','c'],'3':['d','e','f'],'4':['g','h','i'],'5':['j','k','l'],'6':['m','n','o'],'7':['p','q','r','s'],'8':['t','u','v'],'9':['w','x','y','z']&#125; def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ result = [] self.helper(0, digits, '', result) return result def helper(self, index, digits, tmp, result): if index == len(digits): if tmp: # empty string result.append(tmp) return for char in Solution.numDict[digits[index]]: self.helper(index+1, digits, tmp+char, result) 除了回溯法，还可以通过顺序的合并，考虑每次对两个string list进行合并，合并后作为一个string list，再和后面的一个数字表示的string list合并，就这样一直循环下去直到最后一个数字 实现代码如下：12345678910111213141516171819202122232425#encoding:utf-8class Solution(object): def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ numDict =&#123;'2':['a','b','c'],'3':['d','e','f'],'4':['g','h','i'],'5':['j','k','l'],'6':['m','n','o'],'7':['p','q','r','s'],'8':['t','u','v'],'9':['w','x','y','z']&#125; result = [] if len(digits)==0: return result # 先将第一位对应的字符复制给result，防止调用函数combine2StrList时一开始result为空 result = numDict[digits[0]] for i in range(1,len(digits)): result = self.combine2StrList(result,numDict[digits[i]]) return result def combine2StrList(self,s1,s2): mix =[] for i in range(len(s1)): for j in range(len(s2)): mix.append(s1[i]+s2[j]) return mix 如有更好的解法，欢迎交流]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python连接mysql及其注意事项]]></title>
      <url>%2F2016%2F02%2F24%2Fpython%E8%BF%9E%E6%8E%A5mysql%E5%8F%8A%E5%85%B6%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
      <content type="text"><![CDATA[本文主要讲述利用 python 连接数据库的过程和部分注意事项。文章不会涉及到连接的原理，只是简单介绍连接的步骤，以及 mysql 不同的引擎连接的过程的细微区别。 基本上任何语言与数据库进行交互都需要引入外部的数据库驱动，在 python 操作mysql数据库中常用的就是MySQLdb这个驱动，后面也会以这个为例子进行讲解。 数据库的最常见的操作就是“增删查改”，实现这几个功能需要对应的SQL语句。而通过程序连接数据库实际上就是获得与数据库的连接，通过这个连接执行SQL命令，得到返回结果（如果有返回结果的话）。 因此，通过 python 操作 mysql 的步骤可以概括为下面3个步骤：1.获得连接2.获取游标3.通过游标执行SQL语句，获取返回结果（如果有） 获得连接获得连接的代码如下12345678910import MySQLdbHOST = XXXXPORT = XXXXUSER = XXXXPASSWD = XXXXDB = XXXXCHARSET = XXXXconn = MySQLdb.connect(host=HOST,port=PORT,user=USER,passwd=PASSWD,db=DB,charset=CHARSET) 一开始声明的几个常量表示要连接到哪台机器的哪个数据库以及采用的编码，注意除了PORT为整数类型，其余都为字符类型。 假如在某个工程中有多个地方需要操作数据库，可以将这个写成一个函数。如下所示： 123456def connectDB(): try: conn = MySQLdb.connect(passwd=PASSWD, host=HOST, user=USER, port=PORT, db=DB, charset='utf8') return conn except: return 0 可通过返回值是不是0判断是否建立了正常连接。 获取游标获得连接后，我们希望做的就是执行我们的 SQL 语句，但是在MySQLdb中，conn并不能执行此操作。需要通过游标（cursor）来执行命令并保存执行的结果，而游标可通过第一步得到的连接获取。代码如下： 1cursor = conn.cursor() 执行SQL语句，获得返回结果可以通过cursor.execute(SQL)来执行SQL语句，通过cursor.fetchall()获取返回的结果（针对select语句）这里获取返回结果针对的是查询（select）语句。 查询的代码代码如下1234567891011SQL = "select * from table"try: cursor.execute(SQL) resultSet = cursor.fetchall() for row in resultSet: for i in row: print iexcept: print 'error while querying' return 0 finally: 执行的SQL语句返回的结果集可以认为是一个嵌套的两级列表，数据库中每一条记录是一级列表中的一个元素。 插入新纪录的代码如下：123456SQL = 'insert into table(field) values(%s) ' %recordtry: cursor.execute(SQL)except: print 'error while querying' return 0 另外删除和更新的代码也类似，只是SQL语句不同而已。 这里需要注意的一点就是上面的代码有可能执行成功，但是数据库中不会更新记录。 原因是mysql的引擎问题，假如mysql的引擎是MyISAM，那么上面的代码就没问题，但是假如mysql的引擎是InnoDB，那么上面的插入新纪录的代码将不会执行成功。因为InnoDB的是支持事务处理的，执行更新的操作会在mysql事先分配的缓存中进行，只有提交后，修改才能生效。提交的操作也很简单,就是在cursor.execute()后加上这句：1conn.commit() 关于这个问题，MySQLdb官网说明如下： commit()If the database and the tables support transactions, this commits the current transaction; otherwise this method successfully does nothing.rollback()If the database and tables support transactions, this rolls back (cancels) the current transaction; otherwise a NotSupportedError is raised. 从说明中可以看到，commit()函数对于支持事务的引擎生效，对于不支持事务的引擎也不会报错，所以建议在代码中均使用。除了commit()函数外，对于支持事务的引擎还有一个rollback()函数用于执行失败后的回滚，但是这个只能在支持事务的引擎上使用。 最后，上个完整的代码1234567891011121314151617181920212223242526272829303132333435363738# encoding:utf-8# 将数据文件的数据导入到mysql数据库中import MySQLdbimport ReadDirFilesHOST = XXXXPORT = XXXXUSER = XXXXPASSWD = XXXXDB = XXXXCHARSET = XXXXdef connectDB(): try: conn = MySQLdb.connect(host=HOST, user=USER, passwd=PASSWD, port=PORT, db=DB, charset='utf8') return conn except: print 'error connecting the database' return 0def importData(): conn = connectDB() if conn == 0: return 0 cursor = conn.cursor() SQL = 'insert into `ad_log` values("%s","%s","%s","%s","%s","%s","%s","%s","%s","%s","%s","%s","%s","%s","%s","%s","%s","%s") ' %tuple # print SQL # 检查SQL语句是否正确 # 下面的代码是一个事务 try: cursor.execute(SQL) conn.commit() print 'successfully insert the record' except: conn.rollback() #引擎不支持事务时会报错 finally: cursor.close() conn.close()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Programming Collective Intelligence》读书笔记(2)--协同过滤]]></title>
      <url>%2F2016%2F02%2F22%2F%E3%80%8AProgramming%20Collective%20Intelligence%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0(2)--%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%2F</url>
      <content type="text"><![CDATA[《Programming Collective Intelligence》（中文名为《集体智慧编程》），是一本关于数据挖掘的书籍，每一章都会通过一个实际的例子来讲述某个机器学习算法，同时会涉及到数据的采集和处理等，是一本实践性很强的书籍。 本文是关于本书的第二章 Making Recommendations 的前半部分。主要讲述了寻找用户相似性和物品相似性的方法，并在这个基础上讲述如何为用户推荐物品。 推荐这个功能在很多网站或软件都有实现，如淘宝，当当，网易云音乐等。实现推荐的算法也许有很多。本文主要讲的是协同过滤。 下面分为这几部分来讲述：1.寻找用户相似性的几种方法2.基于用户相似性为用户推荐物品3.寻找物品相似性的方法 在讲述之前假定一下的数据集：12345678910# 电影评分数据集,用于后面的的测试# critics 字典里面每一项是一个用户对若干部电影的评分critics=&#123;'Lisa Rose': &#123;'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5,'The Night Listener': 3.0&#125;,'Gene Seymour': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0,'You, Me and Dupree': 3.5&#125;,'Michael Phillips': &#123;'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,'Superman Returns': 3.5, 'The Night Listener': 4.0&#125;,'Claudia Puig': &#123;'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, 'The Night Listener': 4.5, 'Superman Returns': 4.0,'You, Me and Dupree': 2.5&#125;,'Mick LaSalle': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,'You, Me and Dupree': 2.0&#125;,'Jack Matthews': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5&#125;,'Toby': &#123;'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0,'Superman Returns': 4.0&#125;&#125; 寻找用户相似性的几种方法欧几里得距离基于上面的数据集，最容易想到的第一种方法就是衡量他们所评的分数的距离。这个怎么理解呢？比如说A对两部电影的评分是1和2，B对两部电影的评分书3和4。那么他们评分的距离就是$\sqrt{(1-3)^2+(2-4)^2 }$。如果有多部电影则以此类推。这相当与把一个用户所有评分用多维空间中的一个点来表示，用户间的相似性用点间的距离来衡量，距离越小，相似度越高。原文称这种距离为欧几里得距离分数（Euclidean Distance score）。通过python实现也很简单：123456789# 通过欧几里得距离找用户相似性，为了使返回数值越大，表示相似性越高，对得出的距离取倒数即可，范围为（0,1）import mathdef similarUserWithEuclidean(scores,user1,user2): commom = [movie for movie in scores[user1] if movie in scores[user2]] if len(commom) == 0: #没有相同喜爱的电影 return 0 total = sum([math.pow(scores[user1][movie] - scores[user2][movie], 2) for movie in commom]) similarity=math.sqrt(total) return 1/(total+1) 上面实现的方法虽然简单，但是存在着分数膨胀(grade inflation)的问题,比如说A和B两个人的评判标准不一样，A比较苛刻，给三部电影打的分数是1、2、1，但是B要求不会那么高，给两部电影打分为4、5、4，如果用第一种方法那他们在二维空间中的距离将会比较大，从而被判为无相似性。那么他们真的是没有相似性吗？ 虽然他们在分数值上存在区别，但是两个人在每部电影的评分差上保持一致性，也可认为他们是相似用户。因为在对于好和不好的标准上每个人都会有自己的度量，也许对于A来说2就算是好的了，但是对于B来说好的标准是5。假如对于多部电影两者的评分趋势一致，那么可认为两者的品味也是差不多的，可以认为他们有相似性。通过皮尔逊相关系数（Pearson Correlation Coefficient）可以实现上面的功能。 皮尔逊相关系数皮尔逊相关系数取值范围为[-1,1],数值为正表示正相关，且越大表示相关性越强；数值为负则为负相关，越小则负相关性越强。计算公式如下： $$sim = \frac{\sum_{c \in I_{ij}}(R_{i,c} - \overline{R_i})(R_{j,c} - \overline{R_j})}{\sqrt{\sum_{c \in I_{ij}} (R_{i,c} - \overline{R_i})^2} \sqrt{\sum_{c \in I_{ij}} (R_{j,c} - \overline{R_j})^2} }$$ 公式中的符号意义如下： 符号 含义 $I_{ij}$ 用户 $i$ 和用户 $j$ 的公共评分集，也就是两者都有评分的物品的集合 $R_{i,c}$ 用户 $i$ 对物品c的评分 $R_{j,c}$ 用户 $j$ 对物品c的评分 $\overline {R_i}$ 用户 $i$ 对 $I_{ij}$ 中物品评分的均值 $\overline {R_j}$ 用户 $j$ 对 $I_{ij}$ 中物品评分的均值 皮尔逊系数的公式初看有点长，但是如果对概率论了解的同学可知，对于变量 $X$ 和 $Y$ ，上面的皮尔逊系数的表达式其实就是$$\frac{X和Y的协方差}{X的标准差*Y的标准差}$$，而这就是概率论中对相关系数的定义。实际上概率论中的相关系数就是皮尔逊提出的这个皮尔逊相关系数率(这里需要注意的是，对于不同测量尺度的变数，有不同的相关系数可用，而我们接触到的概率论教材里，大多是都是讲皮尔逊相关系数，实际上还有其他的相关系数)。 用图像直观表示皮尔逊相关系数如下所示，假设r为互相关系数： 且一般认为r值范围和相关性的对应关系如下，0表示两者无关： 相关性强弱 对应的r值 High correlation 0.5~1.0 or -0.5~ -1.0 Medium correlation 0.3 ~ 0.5 or -0.3~ -0.5 Low correlation 0.1~0.3 or -0.1~ -0.3 关于皮尔逊系数更详细的资料可参考该链接：http://www.statisticshowto.com/what-is-the-pearson-correlation-coefficient 实现该功能的代码如下：123456789101112131415# 通过 Pearson Correlation Coefficient 计算两个用户的相似性,数值绝对值越大相关性，正负表示正相关或负相关def similarUserWithPearson(scores,user1,user2): commom = [movie for movie in scores[user1] if movie in scores[user2]] if len(commom) == 0: #no common item of the two users return 0 average1 = float(sum(scores[user1][movie] for movie in scores[user1]))/len(scores[user1]) average2 = float(sum(scores[user2][movie] for movie in scores[user2]))/len(scores[user2]) # denominator multiply_sum = sum( (scores[user1][movie]-average1) * (scores[user2][movie]-average2) for movie in commom ) # member pow_sum_1 = sum( math.pow(scores[user1][movie]-average1, 2) for movie in commom ) pow_sum_2 = sum( math.pow(scores[user2][movie]-average2, 2) for movie in commom ) modified_cosine_similarity = float(multiply_sum)/math.sqrt(pow_sum_1*pow_sum_2) return modified_cosine_similarity 上面两种方法是书中提到的，并且计算皮尔逊相关系数的方法与原书有区别，但是结果是一样的。只是原书上对公式进行了进一步的简化。除此之外，还有一类比较常用的方法是通过余弦相似性（Cosine Similarity）判断用户相似性。原理也是用空间中的一个多维向量表示用户对所有电影的评分，用户间的相似性可以通过向量间的夹角表示，取值范围也是（-1,1）。但是因为这种方法也存在上面提到的分数膨胀(grade inflation)，在此基础上提出了修正的余弦相似度（Adjusted Cosine Similarity）。 修正的余弦相似性修正的余弦相似度跟上面提到的皮尔逊系数很相似，包括计算公式，区别在于皮尔逊系数是依据双方共同评分项进行计算，而修正的余弦相似度则对所有项都进行运算。比如说一部电影A有评分，B没有评分，那么计算皮尔逊系数时就不能采用这不电影作为计算的数据集，但是计算修正的余弦相似性则可以利用这个数据。 修正的余弦相似性的计算公式如下 $$sim = \frac{\sum_{c \in I_{ij}}(R_{i,c} - \overline{R_i})(R_{j,c} - \overline{R_j})}{\sqrt{\sum_{c \in I_i} (R_{i,c} - \overline{R_i})^2} \sqrt{\sum_{c \in I_j} (R_{j,c} - \overline{R_j})^2} }$$ 公式中的符号意义如下： 符号 含义 $I_{ij}$ 用户 $i$ 和用户 $j$ 的公共评分集，也就是均被两者评分的物品的集合 $I_i$ 被用户 $i$ 评分了的物品的集合 $I_j$ 被用户 $j$ 评分了的物品的集合 $R_{i,c}$ 用户 $i$ 对物品c的评分 $R_{j,c}$ 用户 $j$ 对物品c的评分 $\overline {R_i}$ 用户 $i$ 所有评分的均值 $\overline {R_j}$ 用户 $j$ 所有评分的均值 这条公式跟上面的皮尔逊相关系数很相似，皮尔逊相关系数的公式如下： $$sim = \frac{\sum_{c \in I_{ij}}(R_{i,c} - \overline{R_i})(R_{j,c} - \overline{R_j})}{\sqrt{\sum_{c \in I_{ij}} (R_{i,c} - \overline{R_i})^2} \sqrt{\sum_{c \in I_{ij}} (R_{j,c} - \overline{R_j})^2} }$$ 两者的区别在于分母上，皮尔逊系数的分母采用的评分集是两个用户的共同评分集（就是两个用户都对这个物品有评价），而修正的余弦系数则采用两个用户各自的评分集。 采用修正的余弦系数的实现代码如下： 1234567891011121314def user_similarity_on_modified_cosine(scores, user1, user2): commom = [movie for movie in scores[user1] if movie in scores[user2]] if len(commom) == 0: #no common item of the two users return 0 average1 = float(sum(scores[user1][movie] for movie in scores[user1]))/len(scores[user1]) average2 = float(sum(scores[user2][movie] for movie in scores[user2]))/len(scores[user2]) # denominator multiply_sum = sum( (scores[user1][movie]-average1) * (scores[user2][movie]-average2) for movie in commom ) # member pow_sum_1 = sum( math.pow(scores[user1][movie]-average1, 2) for movie in scores[user1] ) pow_sum_2 = sum( math.pow(scores[user2][movie]-average2, 2) for movie in scores[user2] ) modified_cosine_similarity = float(multiply_sum)/math.sqrt(pow_sum_1*pow_sum_2) return modified_cosine_similarity 计算最相似的前n个用户通过上面任意一种方法找出用户相似度后，可以根据相似度的大小排序，找出前n个最相似的用户，实现代码如下：12345678def findSimilarUsers(scores,user,similarFunction = similarUserWithPearson): similarUser = [(similarFunction(critics, user, otherUser), otherUser) for otherUser in scores if otherUser!=user] # 使用了dict会将含有kv对的列表封装成一个字典,无法利用列表自带的排序函数sort # similarDict = dict([(similarFunction(user,otherUser),otherUser) for otherUser in scores if otherUser!=user]) similarUser.sort() similarUser.reverse() #也可将上面两行改成:similarUser.sort(reverse = True) return similarUser # 返回相似度从高到低排序的一个列表 小结传统的计算相似度的方法就是上面提到的三种：余弦相似度、修正的余弦相似度和相关相似性（也就是计算皮尔逊系数）。下面是分别采用上面三种方法为测试数据集中的用户Lisa Rose找到前6个最相似用户的结果。完整的代码见文末 1234567891011121314151617181920212223# 欧几里得距离，第一个值是分数，第二个值是具体的用户(0.4444444444444444, &apos;Michael Phillips&apos;)(0.3333333333333333, &apos;Mick LaSalle&apos;)(0.2857142857142857, &apos;Claudia Puig&apos;)(0.2222222222222222, &apos;Toby&apos;)(0.21052631578947367, &apos;Jack Matthews&apos;)(0.14814814814814814, &apos;Gene Seymour&apos;)# 皮尔逊系数，第一个值是分数，第二个值是具体的用户(0.9345507010964664, &apos;Toby&apos;)(0.747017880833996, &apos;Jack Matthews&apos;)(0.5940885257860046, &apos;Mick LaSalle&apos;)(0.5477225575051661, &apos;Claudia Puig&apos;)(0.39605901719066977, &apos;Gene Seymour&apos;)(0.3872983346207417, &apos;Michael Phillips&apos;)# 修正的余弦相似度，第一个值是分数，第二个值是具体的用户(0.8093446482740976, &apos;Toby&apos;)(0.747017880833996, &apos;Jack Matthews&apos;)(0.5940885257860046, &apos;Mick LaSalle&apos;)(0.4743416490252569, &apos;Claudia Puig&apos;)(0.39605901719066977, &apos;Gene Seymour&apos;)(0.33541019662496846, &apos;Michael Phillips&apos;) 从结果可以看到，修正的余弦相似度和皮尔逊系数大小虽然不一样，但是预测的整体用户相似度分布一致。 为用户推荐物品经过第一步找出相似用户后，可以直接对相似用户之间进行物品推荐，如A和B相似，且B有部分喜欢的电影A没看过，就可以为A推荐这部分电影。 为了使得推荐结果更具有代表性，需要考虑多几位相似用户，同时也是为了有更多可推荐的选择，因为可能B看过而A没看过的电影也不多。 实现思路如下：为A找出了若干位相似用户，每个用户都有一个相似度系数（就是上面的皮尔森系数），找出他们看过而A没看过电影集合T。将每个用户的相似度乘上用户对集合T中某部电影的评分便可作为这部电影的得分，得分越高，代表推荐性越强。这乍一看还比较合理。但是一分析也存在一个问题，就是假如一部电影实际上都符合这一批用户的，但是看过的人很少，其他一些不太符合的电影因为看过的人多而的得分高。这就导致了误判。怎么解决呢？ 这里需要引入一个因子来调和这种不平衡性，使得结果能够合理，既然这种不平衡是由于评分的人数不同而引起的，那么就可以从这里着手。将上面相加得到的总分除以对这部电影有评分的所有用户的相似性。 例如下图就是为上面Toby推荐的结果： 实现的代码如下：123456789101112131415161718192021222324252627# 给用户推荐物品，物品的评分采用与其他用户的相似度乘上其他用户的实际评分def recommendItem(scores,user): userSimilarity = findSimilarUsers(scores, user) swapUserSimilarity = &#123;v:k for k, v in userSimilarity&#125; # 交换键值，将存储kv对的列表转换为字典，交换后为无序 allMovies = [] for (k,v) in critics.items(): for movie in v: if movie not in allMovies: allMovies.append(movie) itemScore = [] for movie in allMovies: scoreSum = 0 similaritySum = 0 for similarity, otherUser in userSimilarity: if critics[otherUser].has_key(movie): scoreSum += critics[otherUser][movie] * similarity similaritySum += swapUserSimilarity[otherUser] itemScore.append((scoreSum/similaritySum, movie)) itemScore.sort(reverse=True) recommend_movies = [] # 为user推荐的电影 print 'all movies ranking:' for i,j in itemScore: print i,j if j not in critics[user]: recommend_movies.append(j) print 'recommended movies for %s:%s' %(user, recommend_movies) 寻找物品间的相似性上面的推荐方法是通过寻找相似用户进行推荐，是一种“A喜欢这个物品，跟他相似的B也可能喜欢这件物品”思想；除此之外，还有一种思想就是“A喜欢物品1，也可能喜欢相似的物品2”。在这里判断1和2的相似性就成了关键，也就是如何寻找物品间的相似性。 乍一看会没有思路，但是实际上很简单，只需要对原始数据做个简单的转换。上面一开始给出的数据集是一个用户对多部电影的评价，现在把换成同一部电影不同用户的评价即可.123456将'Lisa Rose': &#123;'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5&#125;,'Gene Seymour': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5&#125;&#125;转换为&#123;'Lady in the Water':&#123;'Lisa Rose':2.5,'Gene Seymour':3.0&#125;,'Snakes on a Plane':&#123;'Lisa Rose':3.5,'Gene Seymour':3.5&#125;&#125; etc.. 这样上面求用户相似性的方法是不是同样可以用来求物品间的相似性了？这种基于物品相似性的方法称为基于物品的协同过滤，而上面基于用户相似性的方法则称为基于用户的协同过滤。 文中完整代码见这里 文章为笔者总结，如有错误，烦请指出]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop2.6.0安装注意事项]]></title>
      <url>%2F2016%2F02%2F20%2FHadoop2-6-0%E5%AE%89%E8%A3%85%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
      <content type="text"><![CDATA[本文为在centos上安装hadoop 2.6.0 的一些需要注意的地方。过程不会很详细，如需详细配置过程，可看后面的参考链接。 基本注意事项改主机名修改/etc/hosts文件。每台机器都要，目的是为了每台机器都能够通过主机名访问其他机器 java环境每台机器都要。**为了方便可直接安装open-jdk(1.7.0及以上)，要安装下面两个包。 java-1.8.0-openjdk java-1.8.0-openjdk-devel # 运行 jps 命令需要的 同时添加 JAVA_HOME 环境变量，如果是yum直接安装，JAVA_HOME 的值应该是/usr/lib/jvm/java-1.8.0 ssh环境要求无需密码登录。如果是伪分布式,要求能够无密码登录本机。如果是完全分布式，要求master能够无密码登录所有的slaves。 下载hadoop编译好的文件，解压并做软链接到/usr/local/hadoop添加hadoop相关的环境变量12345678export HADOOP_HOME=/usr/local/hadoopexport HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 伪分布式安装的配置需要修改core-site.xml和hdfs-site.xml两个文件,core-site.xml中的localhost可改为本机主机名，但是hosts文件要有对应关系1234567891011121314151617181920212223242526272829#core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;#hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 初次启动需要将namenode格式化。 hadf namenode -format 启动map-reduce start-dfs.sh 注意：若出现提示 “WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where # applicable”，该 WARN 提示可以忽略，不会影响 Hadoop 正常运行原因:http://stackoverflow.com/questions/19943766/hadoop-unable-to-load-native-hadoop-library-for-your-platform-warning 此时可访问http://localhost:50070，查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。 启动YARN（伪分布式下可选）。Yarn是从 MapReduce 中分离出来的，负责资源管理与任务调度。YARN 运行于 MapReduce之上，提供了高可用性、高扩展性， 需要修改配置文件 mapred-site.xml 和 yarn-site.xml 。修改配置文件 mapred-site.xml，需要将 mapred-site.xml.template重命名为mapred-site.xml，不用yarn时做相反操作，因为 mapred-site.xml 存在，而未开启 YARN 的情况下，运行程序会提示 “Retrying connect to server: 0.0.0.0/0.0.0.0:8032” 的错误，123456789101112131415#mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;#yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动yarn,先要start-dfs.sh,然后执行下面命令 ./sbin/start-yarn.sh # 启动YARN ./sbin/mr-jobhistory-daemon.sh start historyserver # 开启历史服务器，才能在Web中查看任务运行情况 访问localhoost:8088便可以看到启动yarn后开启的界面。 YARN 主要是为集群提供更好的资源管理与任务调度，然而这在单机上体现不出价值，反而会使程序跑得稍慢些。因此在单机上是否开启 YARN 就看实际情况了。 完全分布式配置master和所有的slave的时间要同步，可通过ntp实现 yum -y install ntp &amp;&amp; ntpdate time.nist.gov 可选的时间服务器：123456time.nist.govtime.nuri.net0.asia.pool.ntp.org1.asia.pool.ntp.org2.asia.pool.ntp.org3.asia.pool.ntp.org master上修改五个配置文件:salves、core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#slaves #datanodes的主机名，一行一个#core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master主机名:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;#hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; #datanode的数量 &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;Master:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;#mapred.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master主机名:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master主机名:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;#yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;Master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 将master的hadoop文件夹复制到salve（也是/usr/local/hadoop目录），然后在master启动即可： start-dfs.sh start-yarn.sh mr-jobhistory-daemon.sh start historyserver]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Maven的安装、配置及使用入门]]></title>
      <url>%2F2016%2F02%2F18%2FMaven%E7%9A%84%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8%2F</url>
      <content type="text"><![CDATA[这是一篇关于maven入门的相当好的文章，文章有点长，但是非常值得看。原文链接 Maven简介何为MavenMaven这个词可以翻译为“知识的积累”，也可以翻译为“专家”或“内行”。本书将介绍Maven这一跨平台的项目管理工具。作为Apache组织中的一个颇为成功的开源项目，Maven主要服务于基于Java平 台的项目构建、依赖管理和项目信息管理。无论是小型的开源类库项目，还是大型的企业级应用；无论是传统的瀑布式开发，还是流行的敏捷模式，Maven都能 大显身手。 何为构建不管你是否意识到，构建（build）是每一位程序员每天都在做的工作。早上来到公司，我们做的第一件事情就是从源码库签出最新的源码，然后进行单元测试，如果发现失败的测试，会找相关的同事一起调试，修复错误代码。接着回到自己的工作上来，编写自己的单元测试及产品代码，我们会感激IDE随时报出的编译错误提示。 忙到午饭时间，代码编写得差不多了，测试也通过了，开心地享用午餐，然后休息。下午先在昏昏沉沉中开了个例会，会议结束后喝杯咖啡继续工作。刚才在会上经理要求看测试报告，于是找了相关工具集成进IDE，生成了像 模像样的测试覆盖率报告，接着发了一封电子邮件给经理，松了口气。谁料QA小组又发过来了几个bug，没办法，先本地重现再说，于是熟练地用IDE生成了一个WAR包，部署到Web容器下，启动容器。看到熟悉的界面了，遵循bug报告，一步步重现了bug……快下班的时候，bug修好了，提交代码，通知 QA小组，在愉快中结束了一天的工作。 仔细总结一下，我们会发现，除了编写源代码，我们每天有相当一部分时间花在了编译、运行单元测试、生成文档、打包和部署等烦琐且不起眼的工作上，这就是构建。如果我们现在还手工这样做，那成本也太高了，于是有人用软件的方法让这一系列工作完全自动化，使得软件的构建可以像全自动流水线一样，只需要一条简单的命令，所有烦琐的步骤都能够自动完成，很快就能得到最终结果。 Maven是优秀的构建工具前面介绍了Maven的用途之一是服务于构建，它是一个异常强大的构建工具，能够帮我们自动化构建过程，从清理、编译、测试到生成报告，再到打包和部署。我们不需要也不应该一遍又一遍地输入命令，一次又一次地点击鼠标，我们要做的是使用Maven配置好项目，然后输入简单的命令(如mvn clean install)，Maven会帮我们处理那些烦琐的任务。 Maven是跨平台的，这意味着无论是在Windows上，还是在Linux或者Mac上，都可以使用同样的命令。 我们一直在不停地寻找避免重复的方法。设计的重复、编码的重复、文档的重复，当然还有构建的重复。Maven最大化地消除了构建的重复，抽象了构建生命周期，并且为绝大部分的构建任务提供了已实现的插件，我们不再需要定义过程，甚至不需要再去实现这些过程中的一些任务。最简单的例子是测试，我们没必要告诉Maven去测试，更不需要告诉Maven如何运行测试，只需要遵循Maven的约定编写好测试用例，当我们运行构建的时候，这些测试便会自动运行。 想象一下，Maven抽象了一个完整的构建生命周期模型，这个模型吸取了大量其他的构建脚本和构建工具的优点，总结了大量项目的实际需求。如果遵循这个模型，可以避免很多不必要的错误，可以直接使用大量成熟的Maven插件来完成我们的任务（很多时候我们可能都不知道自己在使用Maven插件）。此外，如果有非常特殊的需求，我们也可以轻松实现自己的插件。 Maven还有一个优点，它能帮助我们标准化构建过程。在Maven之前，十个项目可能有十种构建方式；有了Maven之后，所有项目的构建命令都是简单一致的，这极大地避免了不必要的学习成本，而且有利于促进项目团队的标准化。 综上所述，Maven作为一个构建工具，不仅能帮我们自动化构建，还能够抽象构建过程，提供构建任务实现；它跨平台，对外提供了一致的操作接口，这一切足以使它成为优秀的、流行的构建工具。 Maven不仅仅是构建工具Java不仅是一门编程语言，还是一个平台，通过JRuby和Jython，我们可以在Java平台上编写和运行Ruby和Python程序。我们也应该认识到，Maven不仅是构建工具，还是一个依赖管理工具和项目信息管理工具。 它提供了中央仓库，能帮我们自动下载构件。 在这个开源的年代里，几乎任何Java应用都会借用一些第三方的开源类库，这些类库都可通过依赖的方式引入到项目中来。随着依赖的增多，版本不一致、版本冲突、依赖臃肿等问题都会接踵而来。手工解决这些问题是十分枯燥的，幸运的是Maven提供了一个优秀的解决方案，它通过一个坐标系统准确地定位每一个构件（artifact），也就是通过一组坐标Maven能够找到任何一个 Java类库（如jar文件）。Maven给这个类库世界引入了经纬，让它们变得有秩序，于是我们可以借助它来有序地管理依赖，轻松地解决那些繁杂的依赖问题。 Maven还能帮助我们管理原本分散在项目中各个角落的项目信息，包括项目描述、开发者列表、版本控制系统地址、许可证、缺陷管理系统地址等。这些微小的变化看起来很琐碎，并不起眼，但却在不知不觉中为我们节省了大量寻找信息的时间。除了直接的项目信息，通过Maven自动生成的站点，以及一些已有的插件，我们还能够轻松获得项目文档、测试报告、静态分析报告、源码版本日志报告等 非常具有价值的项目信息。 Maven还为全世界的Java开发者提供了一个免费的中央仓库，在其中几乎可以找到任何的流行开源类库。通过一些Maven的衍生工具（如Nexus），我们还能对其进行快速地搜索。只要定位了坐标，Maven就能够帮我们自动下载，省去了手工劳动。 使用Maven还能享受一个额外的好处，即Maven对于项目目录结构、测试用 例命名方式等内容都有既定的规则，只要遵循了这些成熟的规则，用户在项目间切换的时候就免去了额外的学习成本，可以说是约定优于配置 （Convention Over Configuration）。 为什么需要MavenMaven不是Java领域唯一的构建管理的解决方案。本节将通过一些简单的例子解释Maven的必要性，并介绍其他构建解决方案，如IDE、Make和Ant，并将它们与Maven进行比较。 组装PC和品牌PC笔者初中时开始接触计算机，到了高中时更是梦寐以求希望拥有一台自己的计算机。我的第一台计算机是赛扬733的，选购是一个漫长的过程，我先阅读了大量的杂志以了解各类配件的优劣，CPU、内存、主板、显卡，甚至声卡，我都仔细地挑选，后来还跑了很多商家，调货、讨价还价，组装好后自己装操作系统和驱动程序……虽然这花费了我大量时间，但我很享受这个过程。可是事实证明，装出来的机器稳定性不怎么好。 一年前我需要配一台工作站，这时候我已经没有太多时间去研究电脑配件了。我选择了某知名PC供应商的在线商店，大概浏览了一下主流的机型，选择了我需要的配置，然后下单、付款。接着PC供应商帮我组装电脑、安装操作系统和驱动程序。一周后，物流公司将电脑送到我的家里，我接上显示器、电源、鼠标和键盘就能直接使用了。这为我节省了大量时间，而且这台电脑十分稳定，商家在把电脑发送给我之前已经进行了很好的测试。对了，我还能享受两年的售后服务。 使用脚本建立高度自定义的构建系统就像买组装PC，耗时费力，结果也不一定很好。当然，你可以享受从无到有的乐趣，但恐怕实际项目中无法给你那么多时间。使用Maven就像购买品牌PC，省时省力，并能得到成熟的构建系统，还能得到来自于Maven社区的大量支持。唯一与购买品牌PC不同的是，Maven是开源的，你无须为此付费。如果有兴趣，你还能去了解Maven是如何工作的，而我们无法知道那些PC巨头的商业秘密。 IDE不是万能的当然，我们无法否认优秀的IDE能大大提高开发效率。当前主流的IDE如Eclipse和NetBeans等都提供了强大的文本编辑、调试甚至重构功能。虽然使用简单的文本编辑器和命令行也能完成绝大部分开发工作，但很少有人愿意那样做。然而，IDE是有其天生缺陷的： IDE依赖大量的手工操作。编译、测试、代码生成等工作都是相互独立的，很难一键完成所有工作。手工劳动往往意味着低效，意味着容易出错。 很难在项目中统一所有的IDE配置，每个人都有自己的喜好。也正是由于这个原因，一个在机器A上可以成功运行的任务，到了机器B的IDE中可能就会失败。 我们应该合理利用IDE，而不是过多地依赖它。对于构建这样的任务，在IDE中一次次地点击鼠标是愚蠢的行为。Maven是这方面的专家，而且主流IDE都集成了Maven，我们可以在IDE中方便地运行Maven执行构建。 MakeMake也许是最早的构建工具，它由Stuart Feldman于1977年在Bell实验室创建。Stuart Feldman也因此于2003年获得了ACM国际计算机组织颁发的软件系统奖。目前Make有很多衍生实现，包括最流行的GNU Make和BSD Make，还有Windows平台的Microsoft nmake等。 Make由一个名为Makefile的脚本文件驱动，该文件使用Make自己定义的语法格式。其基本组成部分为一系列规则（Rules），而每一条规则又包括目标（Target）、依赖（Prerequisite）和命令（Command）。Makefile的基本结构如下： 1234TARGET… : PREREQUISITE…COMMAND … … Make通过一系列目标和依赖将整个构建过程串联起来，同时利用本地命令完成每个目标的实际行为。Make的强大之处在于它可以利用所有系统的本地命令，尤其是UNIX/Linux系统，丰富的功能、强大的命令能够帮助Make快速高效地完成任务。 但是，Make将自己和操作系统绑定在一起了。也就是说，使用Make，就不能实现（至少很难）跨平台的构建，这对于Java来说是非常不友好的。此外，Makefile的语法也成问题，很多人抱怨Make构建失败的原因往往是一个难以发现的空格或Tab使用错误。 AntAnt不是指蚂蚁，而是意指“另一个整洁的工具”（Another Neat Tool），它最早用来构建著名的Tomcat，其作者James Duncan Davidson创作它的动机就是因为受不了Makefile的语法格式。我们可以将Ant看成是一个Java版本的Make，也正因为使用了Java，Ant是跨平台的。此外，Ant使用XML定义构建脚本，相对于Makefile来说，这也更加友好。 与Make类似，Ant有一个构建脚本build.xml，如下所示：123456789101112131415&lt;?xml version="1.0"?&gt;&lt;project name="Hello" default="compile"&gt; &lt;target name="compile" description="compile the Java source code to class files"&gt; &lt;mkdir dir="classes"/&gt; &lt;javac srcdir="." destdir="classes"/&gt; &lt;/target&gt; &lt;target name="jar" depends="compile" description="create a Jar file "&gt; &lt;jar destfile="hello.jar"&gt; &lt;fileset dir="classes" includes="**/*.class"/&gt; &lt;manifest&gt; &lt;attribute name="Main.Class" value="HelloProgram"/&gt; &lt;/manifest&gt; &lt;/jar&gt; &lt;/target&gt;&lt;/project&gt; build.xml的基本结构也是目标（target）、依赖（depends），以及实现目标的任务。比如在上面的脚本中，jar目标用来创建应用程序jar文件，该目标依赖于compile目标，后者执行的任务是创建一个名为classes的文件夹，编译当前目录的java文件至classes目录。compile目标完成后，jar目标再执行自己的任务。Ant有大量内置的用Java实现的任务，这保证了其跨平台的特质，同时，Ant也有特殊的任务exec来执行本地命令。 和Make一样，Ant也都是过程式的，开发者显式地指定每一个目标，以及完成该目标所需要执行的任务。针对每一个项目，开发者都需要重新编写这一过程，这里其实隐含着很大的重复。 Maven是声明式的，项目构建过程和过程各个阶段所需的工作都由插件实现，并且大部分插件都是现成的，开发者只需要声明项目的基本元素，Maven就执行内置的、完整的构建过程。这在很大程度上消除了重复。 Ant是没有依赖管理的，所以很长一段时间Ant用户都不得不手工管理依赖，这是一个令人头疼的问题。幸运的是，Ant用户现在可以借助Ivy管理依赖。而对于Maven用户来说，依赖管理是理所当然的，Maven不仅内置了依赖管理，更有一个可能拥有全世界最多Java开源软件包的中央仓库，Maven用户无须进行任何配置就可以直接享用。 不重复发明轮子【该小节内容整理自网友Arthas最早在Maven中文MSN的群内的讨论，在此表示感谢】小张是一家小型民营软件公司的程序员，他所在的公司要开发一个新的Web项目。经过协商，决定使用Spring、iBatis和Tapstry。jar包去哪里找呢？公司里估计没有人能把Spring、iBatis和Tapstry所使用的jar包一个不少地找出来。大家的做法是，先到Spring的站点上去找一个spring.with.dependencies，然后去iBatis的网站上把所有列出来的jar包下载下来，对Tapstry、Apache commons等执行同样的操作。项目还没有开始，WEB.INF/lib下已经有近百个jar包了，带版本号的、不带版本号的、有用的、没用的、相冲突的，怎一个“乱”字了得！ 在项目开发过程中，小张不时地发现版本错误和版本冲突问题，他只能硬着头皮逐一解决。项目开发到一半，经理发现最终部署的应用的体积实在太大了，要求小张去掉一些没用的jar包，于是小张只能加班加点地一个个删…… 小张隐隐地觉得这些依赖需要一个框架或者系统来进行管理。 小张喜欢学习流行的技术，前几年Ant十分流行，他学了，并成为了公司这方面的专家。小张知道，Ant打包，无非就是创建目录，复制文件，编译源代码，使用一堆任务，如copydir、fileset、classpath、ref、target，然后再jar、zip、war，打包就成功了。 项目经理发话了：“兄弟们，新项目来了，小张，你来写Ant脚本！” “是，保证完成任务！”接着，小张继续创建一个新的XML文件。target clean; target compile; target jar; …… 不知道他是否想过，在他写的这么多的Ant脚本中，有多少是重复劳动，有多少代码会在一个又一个项目中重现。既然都差不多，有些甚至完全相同，为什么每次都要重新编写？ 终于有一天，小张意识到了这个问题，想复用Ant脚本，于是在开会时他说：“以后就都用我这个规范的Ant脚本吧，新的项目只要遵循我定义的目录结构就可以了。”经理听后觉得很有道理：“嗯，确实是个进步。” 这时新来的研究生发言了：“经理，用Maven吧，这个在开源社区很流行，比Ant更方便。”小张一听很惊讶，Maven真比自己的“规范化Ant”强大？其实他不知道自己只是在重新发明轮子，Maven已经有一大把现成的插件，全世界都在用，你自己不用写任何代码！ Maven的安装和配置前面介绍了Maven是什么，以及为什么要使用Maven，我们将从本章实际开始实际接触Maven。本章首先将介绍如何在主流的操作系统下安装Maven，并详细解释Maven的安装文件；其次还会介绍如何在主流的IDE中集成Maven，以及Maven安装的最佳实践。 在Windows上安装Maven检查JDK安装在安装Maven之前，首先要确认你已经正确安装了JDK。Maven可以运行在JDK 1.4及以上的版本上。本书的所有样例都基于JDK 5及以上版本。打开Windows的命令行，运行如下的命令来检查你的Java安装： C:\Users\Juven Xu&gt;echo %JAVA_HOME% C:\Users\Juven Xu&gt;java -version 结果如下图所示： 上述命令首先检查环境变量JAVA_HOME是否指向了正确的JDK目录，接着尝试运行java命令。如果Windows无法执行java命令，或者无法找到JAVA_HOME环境变量。你就需要检查Java是否安装了，或者环境变量是否设置正确。关于环境变量的设置，请参考2.1.3节。 下载Maven请访问Maven的下载页面：http://maven.apache.org/download.html，其中包含针对不同平台的各种版本的Maven下载文件。对于首次接触Maven的读者来说，推荐使用Maven3.0，因此下载apache-maven-3.0-bin.zip。当然，如果你对Maven的源代码感兴趣并想自己构建Maven，还可以下载apache-maven-3.0 -src.zip。该下载页面还提供了md5校验和（checksum）文件和asc数字签名文件，可以用来检验Maven分发包的正确性和安全性。 在本书编写的时候，Maven 2的最新版本是2.2.1，Maven 3基本完全兼容Maven 2，而且较之于Maven 2它性能更好，还有不少功能的改进，如果你之前一直使用Maven 2，现在正犹豫是否要升级，那就大可不必担心了，快点尝试下Maven 3吧！ 本地安装将安装文件解压到你指定的目录中，如： D:\bin&gt;jar xvf &quot;C:\Users\Juven Xu\Downloads\apache-maven-3.0--bin.zip&quot; 这里的Maven安装目录是D:\bin\apache-maven-3.0，接着需要设置环境变量，将Maven安装配置到操作系统环境中。 打开系统属性面板（桌面上右键单击“我的电脑”→“属性”），点击高级系统设置，再点击环境变量，在系统变量中新建一个变量，变量名为M2_HOME，变量值为Maven的安装目录D:\bin\apache-maven-3.0。点击确定，接着在系统变量中找到一个名为Path的变量，在变量值的末尾加上%M2_HOME%\bin;，注意多个值之间需要有分号隔开，然后点击确定。至此，环境变量设置完成 现在打开一个新的cmd窗口（这里强调新的窗口是因为新的环境变量配置需要新的cmd窗口才能生效），运行如下命令检查Maven的安装情况： C:\Users\Juven Xu&gt;echo %M2_HOME% C:\Users\Juven Xu&gt;mvn -v 第一条命令echo %M2_HOME%用来检查环境变量M2_HOME是否指向了正确的Maven安装目录；而mvn –version执行了第一条Maven命令，以检查Windows是否能够找到正确的mvn执行脚本。 升级MavenMaven还比较年轻，更新比较频繁，因此用户往往会需要更新Maven安装以获得更多更酷的新特性，以及避免一些旧的bug。 在Windows上更新Maven非常简便，只需要下载新的Maven安装文件，解压至本地目录，然后更新M2_HOME环境变量便可。例如，假设Maven推出了新版本3.1，我们将其下载然后解压至目录D:\bin\apache-maven-3.1，接着遵照前一节描述的步骤编辑环境变量M2_HOME，更改其值为D:\bin\apache-maven-3.1。至此，更新就完成了。同理，如果你需要使用某一个旧版本的Maven，也只需要编辑M2_HOME环境变量指向旧版本的安装目录。 在基于Unix的系统上安装MavenMaven是跨平台的，它可以在任何一种主流的操作系统上运行，本节将介绍如何在基于Unix的系统（包括Linux、Mac OS以及FreeBSD等）上安装Maven。 下载和安装首先，与在Windows上安装Maven一样，需要检查JAVA_HOME环境变量以及Java命令，细节不再赘述，命令如下： juven@juven-ubuntu:~$ echo $JAVA_HOME juven@juven-ubuntu:~$ java –version 接着到http://maven.apache.org/download.html 下载Maven安装文件，如apache-maven-3.0-bin.tar.gz，然后解压到本地目录： juven@juven-ubuntu:bin$ tar -xvzf apache-maven-3.0-bin.tar.gz 现在已经创建好了一个Maven安装目录apache-maven-3.0，虽然直接使用该目录配置环境变量之后就能使用Maven了，但这里我更推荐做法是，在安装目录旁平行地创建一个符号链接，以方便日后的升级： 12345juven@juven-ubuntu:bin$ ln -s apache-maven-3.0 apache-mavenjuven@juven-ubuntu:bin$ ls -ltotal 4lrwxrwxrwx 1 juven juven 18 2009-09-20 15:43 apache-maven -&gt; apache-maven-3.0drwxr-xr-x 6 juven juven 4096 2009-09-20 15:39 apache-maven-3.0 接下来，我们需要设置M2_HOME环境变量指向符号链接apache-maven-，并且把Maven安装目录下的bin/文件夹添加到系统环境变量PATH中去：12juven@juven-ubuntu:bin$ export M2_HOME=/home/juven/bin/apache-mavenjuven@juven-ubuntu:bin$ export PATH=$PATH:$M2_HOME/bin 一般来说，需要将这两行命令加入到系统的登录shell脚本中去，以我现在的Ubuntu 8.10为例，编辑~/.bashrc文件，添加这两行命令。这样，每次启动一个终端，这些配置就能自动执行。 至此，安装完成，我们可以运行以下命令检查Maven安装： juven@juven-ubuntu:bin$ echo $M2_HOME juven@juven-ubuntu:bin$ mvn –version 升级Maven在基于Unix的系统上，可以利用符号链接这一工具来简化Maven的升级，不必像在Windows上那样，每次升级都必须更新环境变量。 前一小节中我们提到，解压Maven安装包到本地之后，平行地创建一个符号链接，然后在配置环境变量时引用该符号链接，这样做是为了方便升级。现在，假设我们需要升级到新的Maven 3.1版本，同理，将安装包解压到与前一版本平行的目录下，然后更新符号链接指向3.1版的目录便可： juven@juven-ubuntu:bin$ rm apache-maven juven@juven-ubuntu:bin$ ln -s apache-maven-3.1/ apache-maven juven@juven-ubuntu:bin$ ls -l total 8 lrwxrwxrwx 1 juven juven 17 2009-09-20 16:13 apache-maven -&gt; apache-maven-3.1 / drwxr-xr-x 6 juven juven 4096 2009-09-20 15:39 apache-maven-3.0drwxr-xr-x 2 juven juven 4096 2009-09-20 16:09 apache-maven-3.1 同理，可以很方便地切换到Maven的任意一个版本。现在升级完成了，可以运行mvn -v进行检查。 安装目录分析本章前面的内容讲述了如何在各种操作系统中安装和升级Maven。现在我们来仔细分析一下Maven的安装文件。 M2_HOME前面我们讲到设置M2_HOME环境变量指向Maven的安装目录，本书之后所有使用M2_HOME的地方都指代了该安装目录，让我们看一下该目录的结构和内容： 1234567binbootconflibLICENSE.txtNOTICE.txtREADME.txt Bin： 该目录包含了mvn运行的脚本，这些脚本用来配置Java命令，准备好classpath和相关的Java系统属性，然后执行Java命令。其中mvn是基于UNIX平台的shell脚本，mvn.bat是基于Windows平台的bat脚本。在命令行输入任何一条mvn命令时，实际上就是在调用这些脚本。该目录还包含了mvnDebug和mvnDebug.bat两个文件，同样，前者是UNIX平台的shell脚本，后者是windows的bat脚本。那么mvn和mvnDebug有什么区别和关系呢？打开文件我们就可以看到，两者基本是一样的，只是mvnDebug多了一条MAVEN_DEBUG_OPTS配置，作用就是在运行Maven时开启debug，以便调试Maven本身。此外，该目录还包含m2.conf文件，这是classworlds的配置文件，稍微会介绍classworlds。 Boot： 该目录只包含一个文件，以maven 3.0为例，该文件为plexus-classworlds-2.2.3.jar。plexus-classworlds是一个类加载器框架，相对于默认的java类加载器，它提供了更丰富的语法以方便配置，Maven使用该框架加载自己的类库。更多关于classworlds的信息请参考http://classworlds.codehaus.org/。对于一般的Maven用户来说，不必关心该文件。 Conf： 该目录包含了一个非常重要的文件settings.xml。直接修改该文件，就能在机器上全局地定制Maven的行为。一般情况下，我们更偏向于复制该文件至~/.m2/目录下（这里~表示用户目录），然后修改该文件，在用户范围定制Maven的行为。本书的后面将会多次提到该settings.xml，并逐步分析其中的各个元素。 Lib： 该目录包含了所有Maven运行时需要的Java类库，Maven本身是分模块开发的，因此用户能看到诸如mavn-core-3.0.jar、maven-model-3.0.jar之类的文件，此外这里还包含一些Maven用到的第三方依赖如common-cli-1.2.jar、google-collection-1.0.jar等等。（对于Maven 2来说，该目录只包含一个如maven-2.2.1-uber.jar的文件原本各为独立JAR文件的Maven模块和第三方类库都被拆解后重新合并到了这个JAR文件中）。可以说，这个lib目录就是真正的Maven。关于该文件，还有一点值得一提的是，用户可以在这个目录中找到Maven内置的超级POM，这一点在8.5小节详细解释。其他： LICENSE.txt记录了Maven使用的软件许可证Apache License Version 2.0； NOTICE.txt记录了Maven包含的第三方软件；而README.txt则包含了Maven的简要介绍，包括安装需求及如何安装的简要指令等等。 ~/.m2在讲述该小节之前，我们先运行一条简单的命令：mvn help:system。该命令会打印出所有的Java系统属性和环境变量，这些信息对我们日常的编程工作很有帮助。这里暂不解释help:system涉及的语法，运行这条命令的目的是为了让Maven执行一个真正的任务。我们可以从命令行输出看到Maven会下载maven-help-plugin，包括pom文件和jar文件。这些文件都被下载到了Maven本地仓库中。 现在打开用户目录，比如当前的用户目录是C:\Users\Juven Xu\，你可以在Vista和Windows7中找到类似的用户目录。如果是更早版本的Windows，该目录应该类似于C:\Document and Settings\Juven Xu\。在基于Unix的系统上，直接输入cd 回车，就可以转到用户目录。为了方便，本书统一使用符号 ~ 指代用户目录。 在用户目录下，我们可以发现.m2文件夹。默认情况下，该文件夹下放置了Maven本地仓库.m2/repository。所有的Maven构件（artifact）都被存储到该仓库中，以方便重用。我们可以到~/.m2/repository/org/apache/maven/plugins/maven-help-plugins/目录下找到刚才下载的maven-help-plugin的pom文件和jar文件。Maven根据一套规则来确定任何一个构件在仓库中的位置，这一点本书第6章将会详细阐述。由于Maven仓库是通过简单文件系统透明地展示给Maven用户的，有些时候可以绕过Maven直接查看或修改仓库文件，在遇到疑难问题时，这往往十分有用。 默认情况下，~/.m2目录下除了repository仓库之外就没有其他目录和文件了，不过大多数Maven用户需要复制M2_HOME/conf/settings.xml文件到~/.m2/settings.xml。这是一条最佳实践，我们将在本章最后一小节详细解释。 设置HTTP代理有时候你所在的公司由于安全因素考虑，要求你使用通过安全认证的代理访问因特网。这种情况下，就需要为Maven配置HTTP代理，才能让它正常访问外部仓库，以下载所需要的资源。 首先确认自己无法直接访问公共的Maven中央仓库，直接运行命令ping repo1.maven.org可以检查网络。如果真的需要代理，先检查一下代理服务器是否畅通，比如现在有一个IP地址为218.14.227.197，端口为3128的代理服务，我们可以运行telnet 218.14.227.197 3128来检测该地址的该端口是否畅通。如果得到出错信息，需要先获取正确的代理服务信息；如果telnet连接正确，则输入ctrl+]，然后q，回车，退出即可。 检查完毕之后，编辑~/.m2/settings.xml文件（如果没有该文件，则复制$M2_HOME/conf/settings.xml）。添加代理配置如下： 123456789101112131415161718&lt;settings&gt;… &lt;proxies&gt; &lt;proxy&gt; &lt;id&gt;my-proxy&lt;/id&gt; &lt;active&gt;true&lt;/active&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;host&gt;218.14.227.197&lt;/host&gt; &lt;port&gt;3128&lt;/port&gt; &lt;!-- &lt;username&gt;***&lt;/username&gt; &lt;password&gt;***&lt;/password&gt; &lt;nonProxyHosts&gt;repository.mycom.com|*.google.com&lt;/nonProxyHosts&gt; --&gt; &lt;/proxy&gt; &lt;/proxies&gt; …&lt;/settings&gt; 这段配置十分简单，proxies下可以有多个proxy元素，如果你声明了多个proxy元素，则默认情况下第一个被激活的proxy会生效。这里声明了一个id为my-proxy的代理，active的值为true表示激活该代理，protocol表示使用的代理协议，这里是http。当然，最重要的是指定正确的主机名（host元素）和端口（port元素）。上述XML配置中我注释掉了username、password、nonProxyHost几个元素，当你的代理服务需要认证时，就需要配置username和password。nonProxyHost元素用来指定哪些主机名不需要代理，可以使用 | 符号来分隔多个主机名。此外，该配置也支持通配符，如*.google.com表示所有以google.com结尾的域名访问都不要通过代理。 安装m2eclipseEclipse是一款非常优秀的IDE。除了基本的语法标亮、代码补齐、XML编辑等基本功能外，最新版的Eclipse还能很好地支持重构，并且集成了JUnit、CVS、Mylyn等各种流行工具。可惜Eclipse默认没有集成对Maven的支持。幸运的是，由Maven之父Jason Van Zyl创立的Sonatype公司建立了m2eclipse项目，这是Eclipse下的一款十分强大的Maven插件，可以访问http://m2eclipse.sonatype.org/ 了解更多该项目的信息。 本小节将先介绍如何安装m2eclipse插件，本书后续的章节会逐步介绍m2eclipse插件的使用。 现在我以Eclipse 3.6为例逐步讲解m2eclipse的安装。启动Eclipse之后，在菜单栏中选择Help，然后选择Install New Software…，接着你会看到一个Install对话框，点击Work with:字段边上的Add按钮，你会得到一个新的Add Repository对话框，在Name字段中输入m2e，Location字段中输入http://m2eclipse.sonatype.org/sites/m2e，然后点击OK。Eclipse会下载m2eclipse安装站点上的资源信息。等待资源载入完成之后，我们再将其全部展开，就能看到下图所示的界面： 如图显示了m2eclipse的核心模块Maven Integration for Eclipse (Required)，选择后点击Next &gt;，Eclipse会自动计算模块间依赖，然后给出一个将被安装的模块列表，确认无误后，继续点击Next &gt;，这时我们会看到许可证信息，m2eclipse使用的开源许可证是Eclipse Public License v1.0，选择I accept the terms of the license agreements，然后点击Finish，接着就耐心等待Eclipse下载安装这些模块，如下图所示： 除了核心组件之外，m2eclipse还提供了一组额外组件，主要是为了方便与其它工具如Subversion进行集成，这些组件的安装地址为http://m2eclipse.sonatype.org/sites/m2e-extras。使用前面类似的安装方法，我们可以看到如下图的组件列表： 下面简单解释一下这些组件的用途： 1.重要的 Maven SCM handler for Subclipse(Optional）：Subversion是非常流行的版本管理工具，该模块能够帮助我们直接从Subversion服务器签出Maven项目，不过前提是需要首先安装Subclipse（http://subclipse.tigris.org/）。 Maven SCM Integration (Optional）：Eclipse环境中Maven与SCM集成核心的模块，它利用各种SCM工具如SVN实现Maven项目的签出和具体化等操作。 2. 不重要的 Maven issue tracking configurator for Mylyn 3.x (Optional）：该模块能够帮助我们使用POM中的缺陷跟踪系统信息连接Mylyn至服务器。 Maven SCM handler for Team/CVS (Optional）：该模块帮助我们从CVS服务器签出Maven项目，如果你还在使用CVS，就需要安装它。 Maven Integration for WTP (Optional）：使用该模块可以让Eclipse自动读取POM信息并配置WTP项目。、 M2eclipse Extensions Development Support (Optional)：用来支持扩展m2eclipse，一般用户不会用到。 Project configurators for commonly used maven plugins (temporary)：一个临时的组件，用来支持一些Maven插件与Eclipse的集成，建议安装。读者可以根据自己的需要安装相应组件，具体步骤不再赘述。 待安装完毕后，重启Eclipse，现在让我们验证一下m2eclipse是否正确安装了。首先，点击菜单栏中的Help，然后选择About Eclipse，在弹出的对话框中，点击Installation Details按钮，会得到一个对话框，在Installed Software标签栏中，检查刚才我们选择的模块是否在这个列表中如果一切没问题，我们再检查一下Eclipse现在是否已经支持创建Maven项目，依次点击菜单栏中的File→New→Other，在弹出的对话框中，找到Maven一项，再将其展开，你应该能够看到如下所示的对话框： 如果一切正常，说明m2eclipse已经正确安装了。 最后，关于m2eclipse的安装，需要提醒的一点是，你可能会在使用m2eclipse时遇到类似这样的错误： 09-10-6 上午01时14分49秒: Eclipse is running in a JRE, but a JDK is requiredSome Maven plugins may not work when importing projects or updating source folders. 这是因为Eclipse默认是运行在JRE上的，而m2eclipse的一些功能要求使用JDK，解决方法是配置Eclipse安装目录的eclipse.ini文件，添加vm配置指向JDK，如： --launcher.XXMaxPermSize 256m -vm D:\java\jdk1.6.0_07\bin\javaw.exe -vmargs -Dosgi.requiredJavaVersion=1.5 -Xms128m -Xmx256m Maven安装最佳实践本节介绍一些在安装Maven过程中不是必须的，但十分有用的实践。 设置MAVEN_OPTS环境变量本章前面介绍Maven安装目录时我们了解到，运行mvn命令实际上是执行了Java命令，既然是运行Java，那么运行Java命令可用的参数当然也应该在运行mvn命令时可用。这个时候，MAVEN_OPTS环境变量就能派上用场。 我们通常需要设置MAVEN_OPTS的值为：-Xms128m -Xmx512m，因为Java默认的最大可用内存往往不能够满足Maven运行的需要，比如在项目较大时，使用Maven生成项目站点需要占用大量的内存，如果没有该配置，我们很容易得到java.lang.OutOfMemeoryError。因此，一开始就配置该变量是推荐的做法。 关于如何设置环境变量，请参考前面设置M2_HOME环境变量的做法，尽量不要直接修改mvn.bat或者mvn这两个Maven执行脚本文件。因为如果修改了脚本文件，升级Maven时你就不得不再次修改，一来麻烦，二来容易忘记。同理，我们应该尽可能地不去修改任何Maven安装目录下的文件。 配置用户范围settings.xmlMaven用户可以选择配置$M2_HOME/conf/settings.xml或者~/.m2/settings.xml。前者是全局范围的，整台机器上的所有用户都会直接受到该配置的影响，而后者是用户范围的，只有当前用户才会受到该配置的影响。 我们推荐使用用户范围的settings.xml，主要原因是为了避免无意识地影响到系统中的其他用户。当然，如果你有切实的需求，需要统一系统中所有用户的settings.xml配置，当然应该使用全局范围的settings.xml。 除了影响范围这一因素，配置用户范围settings.xml文件还便于Maven升级。直接修改conf目录下的settings.xml会导致Maven升级不便，每次升级到新版本的Maven，都需要复制settings.xml文件，如果使用~/.m2目录下的settings.xml，就不会影响到Maven安装文件，升级时就不需要触动settings.xml文件。 不要使用IDE内嵌的Maven无论是Eclipse还是NetBeans，当我们集成Maven时，都会安装上一个内嵌的Maven，这个内嵌的Maven通常会比较新，但不一定很稳定，而且往往也会和我们在命令行使用的Maven不是同一个版本。这里有会出现两个潜在的问题：首先，较新版本的Maven存在很多不稳定因素，容易造成一些难以理解的问题；其次，除了IDE，我们也经常还会使用命令行的Maven，如果版本不一致，容易造成构建行为的不一致，这是我们所不希望看到的。因此，我们应该在IDE中配置Maven插件时使用与命令行一致的Maven。 在m2eclipse环境中，点击菜单栏中的Windows，然后选择Preferences，在弹出的对话框中，展开左边的Maven项，选择Installation子项，在右边的面板中，我们能够看到有一个默认的Embedded Maven安装被选中了，点击Add…然后选择我们的Maven安装目录M2_HOME，添加完毕之后选择这一个外部的Maven，如下图所示： 小结本章详细介绍了在各种操作系统平台上安装Maven，并对Maven安装目录进行了深入的分析，在命令行的基础上，本章又进一步介绍了Maven与主流IDE Eclipse及NetBeans的集成，本章最后还介绍了一些与Maven安装相关的最佳实践。本书下一章会创建一个Hello World项目，带领读者配置和构建Maven项目。 Maven使用入门到目前为止，我们已经大概了解并安装好了Maven，现在，我们开始 创建一个最简单的Hello World项目。如果你是初次接触Maven，我建议你按照本章的内容一步步地编写代码并执行，可能你会碰到一些概念暂时难以理解，不用着急，记下这些疑 难点，相信本书的后续章节会帮你逐一解答。 就像Make的Makefile，Ant的build.xml一样，Maven项目的核心是pom.xml。POM（Project Object Model，项目对象模型）定义了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。 编写pom.xml文件现在我们先为Hello World项目编写一个最简单的pom.xml。 首先创建一个名为hello-world的文件夹（本书中各章的代码都会对应一个以ch开头的项目），打开该文件夹，新建一个名为pom.xml的文件，输入其内容如代码清单3-1： 代码清单3-1：Hello World的POM 1234567891011&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.juvenxu.mvnbook&lt;/groupId&gt; &lt;artifactId&gt;hello-world&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;Maven Hello World Project&lt;/name&gt; &lt;/project&gt; 代码的第一行是XML头，指定了该xml文档的版本和编码方式。紧接着是project元素，project是所有pom.xml的根元素，它还声明了一些POM相关的命名空间及xsd元素，虽然这些属性不是必须的，但使用这些属性能够让第三方工具（如IDE中的XML编辑器）帮助我们快速编辑POM。 根元素下的第一个子元素modelVersion指定了当前POM模型的版本，对于Maven2及Maven 3来说，它只能是4.0.0。 这段代码中最重要的是groupId，artifactId和version三行。这三个元素定义了一个项目基本的坐标，在Maven的世界，任何的jar、pom或者war都是以基于这些基本的坐标进行区分的。 groupId定义了项目属于哪个组，这个组往往和项目所在的组织或公司存在关联，譬如你在googlecode上建立了一个名为myapp的项目，那么groupId就应该是com.googlecode.myapp，如果你的公司是mycom，有一个项目为myapp，那么groupId就应该是com.mycom.myapp。本书中所有的代码都基于groupId com.juvenxu.mvnbook。 artifactId定义了当前Maven项目在组中唯一的ID，我们为这个Hello World项目定义artifactId为hello-world，本书其他章节代码会被分配其他的artifactId。而在前面的groupId为com.googlecode.myapp的例子中，你可能会为不同的子项目（模块）分配artifactId，如：myapp-util、myapp-domain、myapp-web等等。 version指定了Hello World项目当前的版本——1.0-SNAPSHOT。SNAPSHOT意为快照，说明该项目还处于开发中，是不稳定的版本。随着项目的发展，version会不断更新，如升级为1.0、1.1-SNAPSHOT、1.1、2.0等等。本书的6.5小节会详细介绍SNAPSHOT，第13章介绍如何使用Maven管理项目版本的升级发布。 最后一个name元素声明了一个对于用户更为友好的项目名称，虽然这不是必须的，但我还是推荐为每个POM声明name，以方便信息交流。 没有任何实际的Java代码，我们就能够定义一个Maven项目的POM，这体现了Maven的一大优点，它能让项目对象模型最大程度地与实际代码相独立，我们可以称之为解耦，或者正交性，这在很大程度上避免了Java代码和POM代码的相互影响。比如当项目需要升级版本时，只需要修改POM，而不需要更改Java代码；而在POM稳定之后，日常的Java代码开发工作基本不涉及POM的修改。 编写主代码项目主代码和测试代码不同，项目的主代码会被打包到最终的构件中（比如jar），而测试代码只在运行测试时用到，不会被打包。默认情况下，Maven假设项目主代码位于src/main/java目录，我们遵循Maven的约定，创建该目录，然后在该目录下创建文件com/juvenxu/mvnbook/helloworld/HelloWorld.java，其内容如代码清单3-2： 代码清单3-2：Hello World的主代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.juvenxu.mvnbook.helloworld; public class HelloWorld &#123; public String sayHello() &#123; return "Hello Maven"; &#125; public static void main(String[] args) &#123; System.out.print( new HelloWorld().sayHello()); &#125;&#125; ``` 这是一个简单的Java类，它有一个sayHello()方法，返回一个String。同时这个类还带有一个main方法，创建一个HelloWorld实例，调用sayHello()方法，并将结果输出到控制台。关于该Java代码有两点需要注意。首先，在95%以上的情况下，我们**应该把项目主代码放到src/main/java/目录下（遵循Maven的约定），而无须额外的配置，Maven会自动搜寻该目录找到项目主代码。**其次，该Java类的包名是com.juvenxu.mvnbook.helloworld，这与我们之前在POM中定义的groupId和artifactId相吻合。**一般来说，项目中Java类的包都应该基于项目的groupId和artifactId，这样更加清晰，更加符合逻辑，也方便搜索构件或者Java类。**代码编写完毕后，我们使用Maven进行编译，在项目根目录下运行命令 `mvn clean compile `，我们会得到如下输出： [INFO] Scanning for projects... [INFO] ------------------------------------------------------------------------ [INFO] Building Maven Hello World Project [INFO] task-segment: [clean, compile] [INFO] ------------------------------------------------------------------------ [INFO] [clean:clean &#123;execution: default-clean&#125;] [INFO] Deleting directory D:\code\hello-world\target [INFO] [resources:resources &#123;execution: default-resources&#125;] [INFO] skip non existing resourceDirectory D: \code\hello-world\src\main\resources [INFO] [compiler:compile &#123;execution: default-compile&#125;] [INFO] Compiling 1 source file to D: \code\hello-world\target\classes [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESSFUL [INFO] ------------------------------------------------------------------------ [INFO] Total time: 1 second [INFO] Finished at: Fri Oct 09 02:08:09 CST 2009 [INFO] Final Memory: 9M/16M [INFO] ------------------------------------------------------------------------ clean告诉Maven清理输出目录target，compile告诉Maven编译项目主代码，从输出中我们看到Maven首先执行了clean:clean任务，删除target/目录，**默认情况下Maven构建的所有输出都在target/目录中**；接着执行resources:resources任务（未定义项目资源，暂且略过）；最后执行compiler:compile任务，将项目主代码编译至target/classes目录(编译好的类为com/juvenxu/mvnbook/helloworld/HelloWorld.Class）。上文提到的clean:clean、resources:resources，以及compiler:compile对应了一些Maven插件及插件目标，比如clean:clean是clean插件的clean目标，compiler:compile是compiler插件的compile目标，后文会详细讲述Maven插件及其编写方法。至此，Maven在没有任何额外的配置的情况下就执行了项目的清理和编译任务，接下来，我们编写一些单元测试代码并让Maven执行自动化测试。### 编写测试代码 为了使项目结构保持清晰，主代码与测试代码应该分别位于独立的目录中。3.2节讲过Maven项目中默认的主代码目录是src/main/java，对应地，Maven项目中默认的测试代码目录是src/test/java。因此，在编写测试用例之前，我们先创建该目录。在Java世界中，由Kent Beck和Erich Gamma建立的**JUnit是事实上的单元测试标准。要使用JUnit，我们首先需要为Hello World项目添加一个JUnit依赖**，修改项目的POM如代码清单3-3：代码清单3-3：为Hello World的POM添加依赖 &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; 4.0.0 com.juvenxu.mvnbook hello-world 1.0-SNAPSHOT Maven Hello World Project junit junit 4.7 test 123456789101112131415161718192021222324代码中添加了dependencies元素，该元素下可以包含多个dependency元素以声明项目的依赖，这里我们添加了一个依赖——groupId是junit，artifactId是junit，version是4.7。前面我们提到groupId、artifactId和version是任何一个Maven项目最基本的坐标，JUnit也不例外，有了这段声明，Maven就能够自动下载junit-4.7.jar。也许你会问，Maven从哪里下载这个jar呢？**在Maven之前，我们可以去JUnit的官网下载分发包。而现在有了Maven，它会自动访问中央仓库（http://repo1.maven.org/maven2/），下载需要的文件。**读者也可以自己访问该仓库，打开路径junit/junit/4.7/，就能看到junit-4.7.pom和junit-4.7.jar。本书第6章会详细介绍Maven仓库及中央仓库。上述POM代码中还有一个值为test的元素scope，scope为依赖范围，若依赖范围为test则表示该依赖只对测试有效，换句话说，测试代码中的import JUnit代码是没有问题的，但是如果我们在主代码中用import JUnit代码，就会造成编译错误。如果不声明依赖范围，那么默认值就是compile，表示该依赖对主代码和测试代码都有效。配置了测试依赖，接着就可以编写测试类，回顾一下前面的HelloWorld类，现在我们要测试该类的sayHello()方法，检查其返回值是否为“Hello Maven”。在src/test/java目录下创建文件，其内容如代码清单3-4：代码清单3-4：Hello World的测试代码```javapackage com.juvenxu.mvnbook.helloworld; import static org.junit.Assert.assertEquals; import org.junit.Test; public class HelloWorldTest &#123; @Test public void testSayHello() &#123; HelloWorld helloWorld = new HelloWorld(); String result = helloWorld.sayHello(); assertEquals( &quot;Hello Maven&quot;, result ); &#125; &#125;一个典型的单元测试包含三个步骤：1.准备测试类及数据；2.执行要测试的行为；3.检查结果。上述样例中，我们首先初始化了一个要测试的HelloWorld实例，接着执行该实例的sayHello()方法并保存结果到result变量中，最后使用JUnit框架的Assert类检查结果是否为我们期望的”Hello Maven”。在JUnit 3中，约定所有需要执行测试的方法都以test开头，这里我们使用了JUnit 4，但我们仍然遵循这一约定，在JUnit 4中，需要执行的测试方法都应该以@Test进行标注。测试用例编写完毕之后就可以调用Maven执行测试，运行 mvn clean test ：1234567891011121314151617181920212223242526272829303132333435363738[INFO] Scanning for projects... [INFO] ------------------------------------------------------------------------ [INFO] Building Maven Hello World Project [INFO] task-segment: [clean, test] [INFO] ------------------------------------------------------------------------ [INFO] [clean:clean &#123;execution: default-clean&#125;] [INFO] Deleting directory D:\git-juven\mvnbook\code\hello-world\target [INFO] [resources:resources &#123;execution: default-resources&#125;] … Downloading: http://repo1.maven.org/maven2/junit/junit/4.7/junit-4.7.pom 1K downloaded (junit-4.7.pom) [INFO] [compiler:compile &#123;execution: default-compile&#125;] [INFO] Compiling 1 source file to D: \code\hello-world\target\classes [INFO] [resources:testResources &#123;execution: default-testResources&#125;] … Downloading: http://repo1.maven.org/maven2/junit/junit/4.7/junit-4.7.jar 226K downloaded (junit-4.7.jar) [INFO] [compiler:testCompile &#123;execution: default-testCompile&#125;] [INFO] Compiling 1 source file to D:\ code\hello-world\target\test-classes [INFO] ------------------------------------------------------------------------ [ERROR] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Compilation failure D:\code\hello-world\src\test\java\com\juvenxu\mvnbook\helloworld\HelloWorldTest.java:[8,5] -source 1.3 中不支持注释 （请使用 -source 5 或更高版本以启用注释） @Test [INFO] ------------------------------------------------------------------------ [INFO] For more information, run Maven with the -e switch … ``` 不幸的是构建失败了，不过我们先耐心分析一下这段输出（为了本书的简洁，一些不重要的信息我用省略号略去了）。命令行输入的是mvn clean test，而Maven实际执行的可不止这两个任务，还有clean:clean、resources:resources、compiler:compile、resources:testResources以及compiler:testCompile。暂时我们需要了解的是，在Maven执行测试（test）之前，它会先自动执行项目主资源处理，主代码编译，测试资源处理，测试代码编译等工作，这是Maven生命周期的一个特性，本书后续章节会详细解释Maven的生命周期。从输出中我们还看到：Maven从中央仓库下载了junit-4.7.pom和junit-4.7.jar这两个文件到本地仓库（~/.m2/repository）中，供所有Maven项目使用。构建在执行compiler:testCompile任务的时候失败了，Maven输出提示我们需要使用-source 5或更高版本以启动注释，也就是前面提到的JUnit 4的@Test注解。这是Maven初学者常常会遇到的一个问题。**由于历史原因，Maven的核心插件之一compiler插件默认只支持编译Java 1.3，因此我们需要配置该插件使其支持Java 5**，见代码清单3-5：代码清单3-5：配置maven-compiler-plugin支持Java 5… org.apache.maven.plugins maven-compiler-plugin 1.5 1.5 … 123456789101112131415161718192021222324该POM省略了除插件配置以外的其他部分，我们暂且不去关心插件配置的细节，只需要知道compiler插件支持Java 5的编译。现在再执行mvn clean test，输出如下： [INFO] Compiling 1 source file to D: \code\hello-world\target\test-classes [INFO] [surefire:test &#123;execution: default-test&#125;] [INFO] Surefire report directory: D:\code\hello-world\target\surefire-reports T E S T S Running com.juvenxu.mvnbook.helloworld.HelloWorldTest Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.055 sec Results : Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] [INFO] BUILD SUCCESSFUL [INFO] 我们看到compiler:testCompile任务执行成功了，测试代码通过编译之后在target/test-classes下生成了二进制文件，紧接着surefire:test任务运行测试，surefire是Maven世界中负责执行测试的插件，这里它运行测试用例HelloWorldTest，并且输出测试报告，显示一共运行了多少测试，失败了多少，出错了多少，跳过了多少。显然，我们的测试通过了——BUILD SUCCESSFUL。### 打包和运行 将项目进行编译、测试之后，下一个重要步骤就是打包（package）。Hello World的POM中没有指定打包类型，使用默认打包类型jar，我们可以简单地执行命令 `mvn clean package` 进行打包，可以看到如下输出： …Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] [jar:jar {execution: default-jar}][INFO] Building jar: D:\code\hello-world\target\hello-world-1.0-SNAPSHOT.jar[INFO] [INFO] BUILD SUCCESSFUL…12345678910111213141516171819类似地，Maven会在打包之前执行编译、测试等操作。这里我们看到jar:jar任务负责打包，实际上就是jar插件的jar目标将项目主代码打包成一个名为hello-world-1.0-SNAPSHOT.jar的文件，**该文件也位于target/输出目录中，它是根据artifact-version.jar规则进行命名的**，如有需要，我们还可以使用finalName来自定义该文件的名称，这里暂且不展开，本书后面会详细解释。至此，我们得到了项目的输出，如果有需要的话，就可以复制这个jar文件到其他项目的Classpath中从而使用HelloWorld类。但是，**如何才能让其他的Maven项目直接引用这个jar呢？**我们还需要一个安装的步骤，执行 `mvn clean install`： [INFO] [jar:jar &#123;execution: default-jar&#125;] [INFO] Building jar: D: \code\hello-world\target\hello-world-1.0-SNAPSHOT.jar [INFO] [install:install &#123;execution: default-install&#125;] [INFO] Installing D:\code\hello-world\target\hello-world-1.0-SNAPSHOT.jar to C:\Users\juven\.m2\repository\com\juvenxu\mvnbook\hello-world\1.0-SNAPSHOT\hello-world-1.0-SNAPSHOT.jar [INFO] [INFO] BUILD SUCCESSFUL 在打包之后，我们又执行了安装任务install:install，从输出我们看到该任务将项目输出的jar安装到了Maven本地仓库中，我们可以打开相应的文件夹看到Hello World项目的pom和jar。之前讲述JUnit的POM及jar的下载的时候，我们说只有构件被下载到本地仓库后，才能由所有Maven项目使用，这里是同样的道理，只有将Hello World的构件安装到本地仓库之后，其他Maven项目才能使用它。我们已经将体验了Maven最主要的命令：`mvn clean compile`、`mvn clean test`、`mvn clean package`、`mvn clean install`。**执行test之前是会先执行compile的，执行package之前是会先执行test的，而类似地，install之前会执行package**。我们可以在任何一个Maven项目中执行这些命令，而且我们已经清楚它们是用来做什么的。到目前为止，我们还没有运行Hello World项目，不要忘了HelloWorld类可是有一个main方法的。默认打包生成的jar是不能够直接运行的，因为带有main方法的类信息不会添加到manifest中(我们可以打开jar文件中的META-INF/MANIFEST.MF文件，将无法看到Main-Class一行)。为了生成可执行的jar文件，我们需要借助maven-shade-plugin，配置该插件如下： org.apache.maven.plugins maven-shade-plugin 1.2.1 package shade com.juvenxu.mvnbook.helloworld.HelloWorld 12345678910111213141516171819202122232425262728293031plugin元素在POM中的相对位置应该在`&lt;project&gt;&lt;build&gt;&lt;plugins&gt;`下面。我们配置了mainClass为com.juvenxu.mvnbook.helloworld.HelloWorld，项目在打包时会将该信息放到MANIFEST中。现在执行 mvn clean install ，待构建完成之后打开target/目录，我们可以看到hello-world-1.0-SNAPSHOT.jar和original-hello-world-1.0-SNAPSHOT.jar，前者是带有Main-Class信息的可运行jar，后者是原始的jar，打开hello-world-1.0-SNAPSHOT.jar的META-INF/MANIFEST.MF，可以看到它包含这样一行信息： Main-Class: com.juvenxu.mvnbook.helloworld.HelloWorld现在，我们在项目根目录中执行该jar文件： D: \code\hello-world&gt;java -jar target\hello-world-1.0-SNAPSHOT.jar Hello Maven控制台输出为 Hello Maven，这正是我们所期望的。本小节介绍了Hello World项目，侧重点是Maven而非Java代码本身，介绍了POM、Maven项目结构、以及如何编译、测试、打包，等等。### 使用Archetype生成项目骨架 Hello World项目中有一些Maven的约定：在项目的根目录中放置pom.xml，在src/main/java目录中放置项目的主代码，在src/test/java中放置项目的测试代码。我之所以一步一步地展示这些步骤，是为了能让可能是Maven初学者的你得到最实际的感受。我们称这些基本的目录结构和pom.xml文件内容称为项目的骨架，当你第一次创建项目骨架的时候，你还会饶有兴趣地去体会这些默认约定背后的思想，第二次，第三次，你也许还会满意自己的熟练程度，但第四、第五次做同样的事情，就会让程序员恼火了，为此**Maven提供了Archetype以帮助我们快速勾勒出项目骨架。**还是以Hello World为例，我们使用maven archetype来创建该项目的骨架，离开当前的Maven项目目录。如果是Maven 3，简单的运行： mvn archetype:generate如果是Maven 2，最好运行如下命令： mvn org.apache.maven.plugins:maven-archetype-plugin:2.0-alpha-5:generate很多资料会让你直接使用更为简单的 mvn archetype:generate 命令，但在Maven2中这是不安全的，因为该命令没有指定archetype插件的版本，于是Maven会自动去下载最新的版本，进而可能得到不稳定的SNAPSHOT版本，导致运行失败。然而在Maven 3中，即使用户没有指定版本，Maven也只会解析最新的稳定版本，因此这是安全的，具体内容见7.7小节。我们实际上是在运行插件**maven-archetype-plugin**，注意冒号的分隔，其格式为 groupId:artifactId:version:goal ，org.apache.maven.plugins 是maven官方插件的groupId，maven-archetype-plugin 是archetype插件的artifactId，2.0-alpha-5 是目前该插件最新的稳定版，generate是我们要使用的插件目标。紧接着我们会看到一段长长的输出，有很多可用的archetype供我们选择，包括著名的Appfuse项目的archetype，JPA项目的archetype等等。每一个archetype前面都会对应有一个编号，同时命令行会提示一个默认的编号，其对应的archetype为maven-archetype-quickstart，我们直接回车以选择该archetype，紧接着Maven会提示我们输入要创建项目的groupId、artifactId、 version、以及包名package，如下输入并确认： Define value for groupId: : com.juvenxu.mvnbookDefine value for artifactId: : hello-worldDefine value for version: 1.0-SNAPSHOT: :Define value for package: com.juvenxu.mvnbook: : com.juvenxu.mvnbook.helloworldConfirm properties configuration:groupId: com.juvenxu.mvnbookartifactId: hello-worldversion: 1.0-SNAPSHOTpackage: com.juvenxu.mvnbook.helloworld Y: : Y``` Archetype插件将根据我们提供的信息创建项目骨架。在当前目录下，Archetype插件会创建一个名为hello-world（我们定义的artifactId）的子目录，从中可以看到项目的基本结构：基本的pom.xml已经被创建，里面包含了必要的信息以及一个junit依赖；主代码目录src/main/java已经被创建，在该目录下还有一个Java类com.juvenxu.mvnbook.helloworld.App，注意这里使用到了我们刚才定义的包名，而这个类也仅仅只有一个简单的输出Hello World!的main方法；测试代码目录src/test/java也被创建好了，并且包含了一个测试用例com.juvenxu.mvnbook.helloworld.AppTest。 Archetype可以帮助我们迅速地构建起项目的骨架，在前面的例子中，我们完全可以在Archetype生成的骨架的基础上开发Hello World项目以节省我们大量时间。 此外，我们这里仅仅是看到了一个最简单的archetype，如果你有很多项目拥有类似的自定义项目结构以及配置文件，你完全可以一劳永逸地开发自己的archetype，然后在这些项目中使用自定义的archetype来快速生成项目骨架，本书后面的章节会详细阐述如何开发Maven Archetype。 m2eclipse简单使用介绍前面Hello World项目的时候，我们并没有涉及IDE，如此简单的一个项目，使用最简单的编辑器也能很快完成，但对于稍微大一些的项目来说，没有IDE就是不可想象的，本节我们先介绍m2eclipse的基本使用。 导入Maven项目第2章介绍了如何安装m2eclipse，现在，我们使用m2ecilpse导入Hello World项目。选择菜单项File，然后选择Import，我们会看到一个Import对话框，在该对话框中选择General目录下的Maven Projects，然后点击Next，就会出现Import Projects对话框，在该对话框中点击Browse…选择Hello World的根目录（即包含pom.xml文件的那个目录），这时对话框中的Projects:部分就会显示该目录包含的Maven项目，如下图所示： 点击Finish之后，m2ecilpse就会将该项目导入到当前的workspace中，导入完成之后，我们就可以在Package Explorer视图中看到如下图的项目结构： 我们看到主代码目录src/main/java和测试代码目录src/test/java成了Eclipse中的资源目录，包和类的结构也十分清晰，当然pom.xml永远在项目的根目录下，而从这个视图中我们甚至还能看到项目的依赖junit-4.7.jar，其实际的位置指向了Maven本地仓库（这里我自定义了Maven本地仓库地址为D:\java\repository，后续章节会介绍如何自定义本地仓库位置）。 创建Maven项目创建一个Maven项目也十分简单，选择菜单项File -&gt; New -&gt; Other，在弹出的对话框中选择Maven下的Maven Project，然后点击Next &gt;，在弹出的New Maven Project对话框中，我们使用默认的选项（不要选择Create a simple project选项，那样我们就能使用Maven Archetype），点击Next &gt;，此时m2eclipse会提示我们选择一个Archetype，我们选择maven-archetype-quickstart，再点击Next &gt;。由于m2eclipse实际上是在使用maven-archetype-plugin插件创建项目，因此这个步骤与上一节我们使用archetype创建项目骨架类似，输入groupId,、artifactId、version、package（暂时我们不考虑Properties），如下图所示：注意，为了不和前面已导入的Hello World项目产生冲突和混淆，我们使用不同的artifactId和package。OK，点击Finish，Maven项目就创建完成了，其结构与前一个已导入的Hello World项目基本一致。 运行mvn命令我们需要在命令行输入如mvn clean install之类的命令来执行maven构建，m2eclipse中也有对应的功能，在Maven项目或者pom.xml上右击，再选择Run As，就能看到如下的常见Maven命令，如下图所示：选择想要执行的Maven命令就能执行相应的构建，同时我们也能在Eclipse的console中看到构建输出。这里常见的一个问题是，默认选项中没有我们想要执行的Maven命令怎么办？比如，默认带有mvn test，但我们想执行mvn clean test，很简单，选择Maven buid…以自定义Maven运行命令，在弹出对话框中的Goals一项中输入我们想要执行的命令，如clean test，设置一下Name，点击Run即可。并且，下一次我们选择Maven build，或者使用快捷键Alt + Shift + X, M快速执行Maven构建的时候，上次的配置直接就能在历史记录中找到。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python中的类属性和实例属性]]></title>
      <url>%2F2016%2F02%2F14%2Fpython%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%B1%9E%E6%80%A7%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%B1%9E%E6%80%A7%2F</url>
      <content type="text"><![CDATA[面向对象语言中，一般会有“静态变量”，也就是给整个类共享的变量，如C++，java中static修饰的变量。但是在 python 中并没有 static 这个关键字，实现类似功能需要依靠python中的类属性和实例属性的语法特点。本文主要就是讲述这两种属性的区别。 在讲述之前，需要清楚下面两个事实：1）python 中类创建的对象叫实例2）类和实例均是对象，均有自己的对象属性,可通过__dict__查看 下面先看一个例子：1234567891011class TestAttribute: content = [] def addContent(self,x): self.content.append(x) if __name__ == '__main__': t1 = TestAttribute() t2 = TestAttribute() t1.addContent('t1') print 'object t1:', t1.content, t1.__dict__ print 'object t2:', t2.content, t2.__dict__ 输出结果：12object t1: [&apos;t1&apos;] &#123;&#125;object t2: [&apos;t1&apos;] &#123;&#125; 从例子可以看到，这时的 content 就相当于一个static 变量，被所有实例共享。注意后面那个花括号{}表示实例的所有对象属性，只是当前实例没有自己的属性。 那假如各个实例要有自己独立的变量的？也很简单，只需要在类的构造函数（也就是__init__函数)为变量赋值即可. 123456789101112131415class TestAttribute: content = [] def __init__(self): self.content=[] def addContent(self,x): self.content.append(x) if __name__ == '__main__': t1 = TestAttribute() t2 = TestAttribute() t1.addContent('t1') print 'object t1:', t1.content ,t1.__dict__ print 'object t2:', t2.content ,t2.__dict__ 输出的结果如下所示：12object t1: [&apos;t1&apos;] &#123;&apos;content&apos;:[&apos;t1&apos;]&#125;object t2: [] &#123;&apos;content&apos;:[]&#125; 从结果可知，现在的t1,t2的变量独立了。那原因是什么呢？ 原因是Python中对象属性的获取是按照从下到上的顺序来查找属性 怎么理解上面这句话呢？以上面代码为例，类和实例的关系如下所示：1234 TestAttribute ____|____ | |t1 t2 输出t1.content时，python 解析器会先查看对象 t1 中是否有content这个属性，有的话输出这个属性的值，没有的话就往上查找 TestAttribute 中是否有这个属性并输出。 在上面的例一中，因为t1和t2的属性均为空，所以输出 t1.content 和 t2.content时实际上是输出 TestAttribute 的属性。又因为 TestAttribute 的 content 属性被修改了 t1 修改了，所以最后输出的的 t1.content 和 t2.content 内容一致。 而在例二中，因为在构造函数中的为content复制的操作使得每个被创建的实例均有自己的content属性，所以 t1 修改 content 时查到自己有content的属性，就只会修改自己的 content。不影响t2 的 content 和 TestAttribute 的 content。这个可以从下面的例子看出，假如将例二的代码修改成如下所示：1234567891011121314151617class TestAttribute: content = [] def __init__(self): self.content=[] def addContent(self,x): self.content.append(x) if __name__ == '__main__': t1 = TestAttribute() t2 = TestAttribute() t1.addContent('t1') TestAttribute.content.append('tt') print 'object t1:', t1.content ,t1.__dict__ print 'object t2:', t2.content ,t2.__dict__ print 'class TestAttribute:', TestAttribute.content 那输出结果是：123object t1: [&apos;t1&apos;] &#123;&apos;content&apos;:[&apos;t1&apos;]&#125;object t2: [] &#123;&apos;content&apos;:[]&#125;class TestAttribute: [&apos;tt&apos;] 可以看到这三个对象的属性均独立。 那么如何为一个实例添加属性呢？ 答案是通过赋值号 = 给实例所需添加的属性赋值。 通过赋值号 = 给实例所需添加的属性赋值实际上是将这个属性指向了新的引用，也就是新的内存空间。 如在例一中没有通过赋值号为 content 赋值，所以这个属性并没有成为 t1 自己的属性，输出t1.__dict__ 为空。而在例二中的构造函数里面为每个实例的content均赋值，所以例二中的三个对象的content属性独立。通过下面的例子可以更深入说明这点： 12345678910class TestAttribute: num = 0if __name__ == '__main__': t1 = TestAttribute() t2 = TestAttribute() t1.num = 1 print 'object t1:', t1.num, t1.__dict__ print 'object t2:', t2.num, t2.__dict__ 输出结果：12object t1: 1 &#123;&apos;num&apos;: 1&#125;object t2: 0 &#123;&#125; 最后，小结如下：1.Python中的类和实例是两个完全独立的对象；2.Python中属性的获取是按照从下到上的顺序来查找属性；3.为实例添加属性的方法：通过赋值号 = 给实例所需添加的属性赋值]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[浏览器缓存机制]]></title>
      <url>%2F2016%2F02%2F12%2F%E6%B5%8F%E8%A7%88%E5%99%A8%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6%2F</url>
      <content type="text"><![CDATA[最近看到一篇比较好的关于浏览器缓存的文章，原文链接,原文内容如下 浏览器缓存机制，其实主要就是HTTP协议定义的缓存机制（如： Expires； Cache-control等）。但是也有非HTTP协议定义的缓存机制，如使用HTML Meta 标签，Web开发者可以在HTML页面的&lt; head &gt;节点中加入&lt; meta &gt;标签，代码如下： 1&lt;META HTTP-EQUIV="Pragma" CONTENT="no-cache"&gt; 上述代码的作用是告诉浏览器当前页面不被缓存，每次访问都需要去服务器拉取。使用上很简单，但只有部分浏览器可以支持，而且所有缓存代理服务器都不支持，因为代理不解析HTML内容本身。 下面我主要介绍HTTP协议定义的缓存机制。 Expires策略Expires是Web服务器响应消息头字段，在响应http请求时告诉浏览器在过期时间前浏览器可以直接从浏览器缓存取数据，而无需再次请求。 下面是宝宝PK项目中，浏览器拉取jquery.js web服务器的响应头： 【注：Date头域表示消息发送的时间，时间的描述格式由rfc822定义。例如，Date: Mon,31 Dec 2001 04:25:57GMT。】 Web服务器告诉浏览器在2012-11-28 03:30:01这个时间点之前，可以使用缓存文件。发送请求的时间是2012-11-28 03:25:01，即缓存5分钟。 不过 Expires 是HTTP 1.0的东西，现在默认浏览器均默认使用HTTP 1.1，所以它的作用基本忽略。 Cache-control策略（重点关注）Cache-Control与Expires的作用一致，都是指明当前资源的有效期，控制浏览器是否直接从浏览器缓存取数据还是重新发请求到服务器取数据。只不过Cache-Control的选择更多，设置更细致，如果同时设置的话，其优先级高于Expires。 http协议头Cache-Control ：值可以是public、private、no-cache、no- store、no-transform、must-revalidate、proxy-revalidate、max-age各个消息中的指令含义如下：|值|含义||:–|:–:||public|指示响应可被任何缓存区缓存||private|指示对于单个用户的整个或部分响应消息，不能被共享缓存处理。这允许服务器仅仅描述当用户的部分响应消息，此响应消息对于其他用户的请求无效||no-cache|指示请求或响应消息不能缓存||no-store|用于防止重要的信息被无意的发布。在请求消息中发送将使得请求和响应消息都不使用缓存||max-age|指示客户机可以接收生存期不大于指定时间（以秒为单位）的响应||min-fresh|指示客户机可以接收响应时间小于当前时间加上指定时间的响应||max-stale|指示客户机可以接收超出超时期间的响应消息。如果指定max-stale消息的值，那么客户机可以接收超出超时期指定值之内的响应消息| 还是上面那个请求，web服务器返回的Cache-Control头的值为max-age=300，即5分钟（和上面的Expires时间一致，这个不是必须的）。 Last-Modified/If-Modified-SinceLast-Modified/If-Modified-Since要配合Cache-Control使用。 Last-Modified：标示这个响应资源的最后修改时间。web服务器在响应请求时，告诉浏览器资源的最后修改时间。 If-Modified-Since：当资源过期时（使用Cache-Control标识的max-age），发现资源具有Last-Modified声明，则再次向web服务器请求时带上头 If-Modified-Since，表示请求时间。web服务器收到请求后发现有头If-Modified-Since 则与被请求资源的最后修改时间进行比对。 若最后修改时间较新，说明资源又被改动过，则响应整片资源内容（写在响应消息包体内），HTTP 200；若最后修改时间较旧，说明资源无新修改，则响应HTTP 304 (无需包体，节省浏览)，告知浏览器继续使用所保存的cache。 Etag/If-None-MatchEtag/If-None-Match也要配合Cache-Control使用。 Etag：web服务器响应请求时，告诉浏览器当前资源在服务器的唯一标识（生成规则由服务器决定）。Apache中，ETag的值，默认是对文件的索引节（INode），大小（Size）和最后修改时间（MTime）进行Hash后得到的。 If-None-Match：当资源过期时（使用Cache-Control标识的max-age），发现资源具有Etage声明，则再次向web服务器请求时带上头If-None-Match （Etag的值）。web服务器收到请求后发现有头If-None-Match 则与被请求资源的相应校验串进行比对，决定返回200或304。 既生Last-Modified何生Etag？你可能会觉得使用Last-Modified已经足以让浏览器知道本地的缓存副本是否足够新，为什么还需要Etag（实体标识）呢？HTTP1.1中Etag的出现主要是为了解决几个Last-Modified比较难解决的问题： Last-Modified标注的最后修改只能精确到秒级，如果某些文件在1秒钟以内，被修改多次的话，它将不能准确标注文件的修改时间 如果某些文件会被定期生成，当有时内容并没有任何变化，但Last-Modified却改变了，导致文件没法使用缓存 有可能存在服务器没有准确获取文件修改时间，或者与代理服务器时间不一致等情形 Etag是服务器自动生成或者由开发者生成的对应资源在服务器端的唯一标识符，能够更加准确的控制缓存。Last-Modified与ETag是可以一起使用的，服务器会优先验证ETag，一致的情况下，才会继续比对Last-Modified，最后才决定是否返回304。 用户行为与缓存浏览器缓存行为还有用户的行为有关！！！ 用户操作 Expires/Cache-Control Last-Modified/Etag 地址栏回车 有效 有效 页面链接跳转 有效 有效 新开窗口 有效 有效 前进、后退 有效 有效 F5刷新 无效 有效 Ctrl+F5刷新 无效 无效 总结浏览器第一次请求： 浏览器再次请求时：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python读取文件夹下所有文件的一种方法]]></title>
      <url>%2F2016%2F02%2F10%2Fpython%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
      <content type="text"><![CDATA[在数据挖掘中需要大量的数据，这些数据往往存储在数据库中或者文件中。存储数据库中比较好理解，可通过 程序数据库接口+SQL语句 获取。存储在文件中则往往有多个按日期命名的文件夹，数据以文本格式存储，且有特定的分割符。本文主要就是讲述如何通过python读取后一类的数据。 总体思路就是先获取给定目录下所有文件的绝对路径，包括给定目录下的子目录；然后再读取每个文件的内容。 首先要获取给定目录下的所有文件的绝对路径，python的os.listdir(dirPath)方法可以列出dirPath下的所有文件和文件夹。为了处理dirPath下的子目录，需要判断读出的一个对象a是否为文件夹，可以通过os.path.isdir(a)来判断读出的a是否为一个文件夹，如果是，则递归读出下面的文件。 实现代码如下：1234567891011121314151617181920212223# encoding:utf-8# 功能：读取传入的目录下所有的文件（包括该目录下的所有子目录）的绝对路径，并以列表形式返回所有文件的绝对路径# 要求传入的路径参数最后不能有斜杠,目的是为了递归时格式统一import osdef readDir(dirPath): if dirPath[-1] == '/': print u'文件夹路径末尾不能加/' return allFiles = [] if os.path.isdir(dirPath): fileList = os.listdir(dirPath) for f in fileList: f = dirPath+'/'+f if os.path.isdir(f): subFiles = readDir(f) allFiles = subFiles + allFiles #合并当前目录与子目录的所有文件路径 else: allFiles.append(f) return allFiles else: return 'Error,not a dir' 将上面的代码命名为ReadDirFiles.py,便于被下面调用。现在已经可以得到某一目录下所有文件的绝对路径，下面只需要读出这些文件里面的内容即可。 为了存储的便利性，用文件存储数据时往往一行存储一条记录，一条记录中不同字段以特定分隔符分开。下面的代码就是解决这类型的数据的 123456789101112131415161718# encoding:utf-8# 功能：解析文件中按行存放的数据，行内数据以SOH（\001）分割import ReadDirFilesdef readFile (filePath): with open(filePath) as f: lines = f.readlines() for line in lines: data = line.split('\001') #以列表形式返回分割了的行数据 for i in data: print i if __name__ == '__main__': dirPath = 'G:/20160107' fileList=ReadDirFiles.readDir(dirPath) for f in fileList: readFile(f) 上面有两个需要注意的地方： 1）建议采用 with open(filePath) as f方法打开文件，因为这种方法不需要显示调用f.close()来关闭文件。open函数可在第二个形参位置决定打开文件的模式，有读（a）、写（w）、追加（a）等，省略时默认为读。 2）读取文件的行时，有readline和readlines两种方法，前者每次读一行，后者一次把文件全部行读入内存，显然后者的效率比前者要高。只有当内存太小或文件过大，无法一次全部读入内存才建议采用第一种方法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[详解Java的TCP网络编程]]></title>
      <url>%2F2016%2F02%2F08%2F%E8%AF%A6%E8%A7%A3Java%E7%9A%84TCP%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[网络编程简介网络通讯的方式有TCP和UDP两种，其中TCP方式的网络通讯是指在通讯的过程中保持连接，有点类似于打电话，只需要拨打一次号码(建立一次网络连接)，就可以多次通话(多次传输数据)。这样方式在实际的网络编程中，由于传输可靠，类似于打电话，如果甲给乙打电话，乙说没有听清楚让甲重复一遍，直到乙听清楚为止，实际的网络传输也是这样，如果发送的一方发送的数据接收方觉得有问题，则网络底层会自动要求发送方重发，直到接收方收到为止。 在Java语言中，对于TCP方式的网络编程提供了良好的支持，在实际实现时，以java.net.Socket类代表客户端连接，以java.net.ServerSocket类代表服务器端连接。在进行网络编程时，底层网络通讯的细节已经实现了比较高的封装，所以在程序员实际编程时，只需要指定IP地址和端口号码就可以建立连接了。正是由于这种高度的封装，一方面简化了Java语言网络编程的难度，另外也使得使用Java语言进行网络编程时无法深入到网络的底层，所以使用Java语言进行网络底层系统编程很困难，具体点说，Java语言无法实现底层的网络嗅探以及获得IP包结构等信息。但是由于Java语言的网络编程比较简单，所以还是获得了广泛的使用。 在使用TCP方式进行网络编程时，需要按照前面介绍的网络编程的步骤进行，下面分别介绍一下在Java语言中客户端和服务器端的实现步骤。 客户端在网络编程中客户端需要做的事情包括：建立连接-&gt;发送并接受数据-&gt;关闭连接。 建立连接在客户端网络编程中，首先需要建立连接，在Java API中以java.net.Socket类的对象代表网络连接，所以建立客户端网络连接，也就是创建Socket类型的对象，该对象代表网络连接，示例如下：12Socket socket1 = new Socket(“192.168.1.103”,10000);Socket socket2 = new Socket(“www.sohu.com”,80); 上面的代码中，socket1实现的是连接到IP地址是192.168.1.103的计算机的10000号端口，而socket2实现的是连接到域名是www.sohu.com的计算机的80号端口，至于底层网络如何实现建立连接，对于程序员来说是完全透明的。如果建立连接时，本机网络不通，或服务器端程序未开启，则会抛出异常。 传输数据连接一旦建立，则完成了客户端编程的第一步，紧接着的步骤就是按照“请求-响应”模型进行网络数据交换，在Java语言中，数据传输功能由Java IO实现，也就是说需要发送的数据写入连接对象的输出流中，在发送完成以后从输入流中读取数据即可。示例代码如下： 12OutputStream os = socket1.getOutputStream(); //获得输出流InputStream is = socket1.getInputStream(); //获得输入流 上面的代码中，分别从socket1这个连接对象获得了输出流和输入流对象，在整个网络编程中，后续的数据交换就变成了IO操作，也就是遵循“请求-响应”模型的规定，先向输出流中写入数据，这些数据会被系统发送出去，然后在从输入流中读取服务器端的反馈信息，这样就完成了一次数据交换过程，当然这个数据交换过程可以多次进行。 这里获得的只是最基本的输出流和输入流对象，还可以根据前面学习到的IO知识，使用流的嵌套将这些获得到的基本流对象转换成需要的装饰流对象，从而方便数据的操作。 关闭连接最后当数据交换完成以后，关闭网络连接，释放网络连接占用的系统端口和内存等资源，完成网络操作，示例代码如下：1socket1.close(); 客户端简单案例这就是最基本的网络编程功能介绍。下面是一个简单的网络客户端程序示例，该程序的作用是向服务器端发送一个字符串“Hello”，并将服务器端的反馈显示到控制台，数据交换只进行一次，当数据交换进行完成以后关闭网络连接，程序结束。实现的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041import java.io.*;import java.net.*;/** * 简单的Socket客户端 * 功能为：发送字符串“Hello”到服务器端，并打印出服务器端的反馈 */public class SimpleSocketClient &#123; public static void main(String[] args) &#123; Socket socket = null; InputStream is = null; OutputStream os = null; //服务器端IP地址 String serverIP = "127.0.0.1"; //服务器端端口号 int port = 10000; //发送内容 String data = "Hello"; try &#123; //建立连接 socket = new Socket(serverIP,port); //发送数据 os = socket.getOutputStream(); os.write(data.getBytes()); //接收数据 is = socket.getInputStream(); byte[] b = new byte[1024]; int n = is.read(b); //输出反馈数据 System.out.println("服务器反馈：" + new String(b,0,n)); &#125; catch (Exception e) &#123; e.printStackTrace(); //打印异常信息 &#125;finally&#123; try &#123; //关闭流和连接 is.close(); os.close(); socket.close(); &#125; catch (Exception e2) &#123;&#125; &#125; &#125;&#125; 在该示例代码中建立了一个连接到IP地址为127.0.0.1，端口号码为10000的TCP类型的网络连接，然后获得连接的输出流对象，将需要发送的字符串“Hello”转换为byte数组写入到输出流中，由系统自动完成将输出流中的数据发送出去，如果需要强制发送，可以调用输出流对象中的flush方法实现。在数据发送出去以后，从连接对象的输入流中读取服务器端的反馈信息，读取时可以使用IO中的各种读取方法进行读取，这里使用最简单的方法进行读取，从输入流中读取到的内容就是服务器端的反馈，并将读取到的内容在客户端的控制台进行输出，最后依次关闭打开的流对象和网络连接对象。 这是一个简单的功能示例，在该示例中演示了TCP类型的网络客户端基本方法的使用，该代码只起演示目的，还无法达到实用的级别。 服务器端介绍完一个简单的客户端编程的示例，下面接着介绍一下TCP类型的服务器端的编写。 服务器端需要完成的事情包括：监听端口 -&gt; 获得客户端的连接 -&gt;连接后传送数据 -&gt; 关闭连接。 监听端口在服务器端程序编程中，由于服务器端实现的是被动等待连接，所以服务器端编程的第一个步骤是监听端口，也就是监听是否有客户端连接到达。实现服务器端监听的代码为： 1ServerSocket ss = new ServerSocket(10000); 该代码实现的功能是监听当前计算机的10000号端口，如果在执行该代码时，10000号端口已经被别的程序占用，那么将抛出异常。否则将实现监听。 获得客户端的连接服务器端编程的第二个步骤是获得连接。该步骤的作用是当有客户端连接到达时，建立一个和客户端连接对应的Socket连接对象，从而释放客户端连接对于服务器端端口的占用。实现功能就像公司的前台一样，当一个客户到达公司时，会告诉前台我找某某某，然后前台就通知某某某， 然后就可以继续接待其它客户了。通过获得连接，使得客户端的连接在服务器端获得了保持，另外使得服务器端的端口释放出来，可以继续等待其它的客户端连接。 实现获得连接的代码是： 1Socket socket = ss.accept(); 该代码实现的功能是获得当前连接到服务器端的客户端连接。需要说明的是accept和前面IO部分介绍的read方法一样，都是一个阻塞方法，也就是当无连接时，该方法将阻塞程序的执行，直到连接到达时才执行该行代码。另外获得的连接会在服务器端的该端口注册，这样以后就可以通过在服务器端的注册信息直接通信，而注册以后服务器端的端口就被释放出来，又可以继续接受其它的连接了。 连接获得以后，后续的编程就和客户端的网络编程类似了，这里获得的Socket类型的连接就和客户端的网络连接一样了，只是服务器端需要首先读取发送过来的数据，然后进行逻辑处理以后再发送给客户端，也就是交换数据的顺序和客户端交换数据的步骤刚好相反。这部分的内容和客户端很类似，所以就不重复了，如果还不熟悉，可以参看下面的示例代码。 关闭连接最后，在服务器端通信完成以后，关闭服务器端连接。实现的代码为：1ss.close(); 简单例子这就是基本的TCP类型的服务器端编程步骤。下面以一个简单的echo服务实现为例子，介绍综合使用示例。echo的意思就是“回声”，echo服务器端实现的功能就是将客户端发送的内容再原封不动的反馈给客户端。实现的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041import java.io.*;import java.net.*;/** * echo服务器 * 功能：将客户端发送的内容反馈给客户端 */public class SimpleSocketServer &#123; public static void main(String[] args) &#123; ServerSocket serverSocket = null; Socket socket = null; OutputStream os = null; InputStream is = null; //监听端口号 int port = 10000; try &#123; //建立连接 serverSocket = new ServerSocket(port); //获得连接 socket = serverSocket.accept(); //接收客户端发送内容 is = socket.getInputStream(); byte[] b = new byte[1024]; int n = is.read(b); //输出 System.out.println("客户端发送内容为：" + new String(b,0,n)); //向客户端发送反馈内容 os = socket.getOutputStream(); os.write(b, 0, n); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; try&#123; //关闭流和连接 os.close(); is.close(); socket.close(); serverSocket.close(); &#125;catch(Exception e)&#123;&#125; &#125; &#125;&#125; 在该示例代码中建立了一个监听当前计算机10000号端口的服务器端Socket连接，然后获得客户端发送过来的连接，如果有连接到达时，读取连接中发送过来的内容，并将发送的内容在控制台进行输出，输出完成以后将客户端发送的内容再反馈给客户端。最后关闭流和连接对象，结束程序。 这样，就以一个很简单的示例演示了TCP类型的网络编程在Java语言中的基本实现，这个示例只是演示了网络编程的基本步骤以及各个功能方法的基本使用，只是为网络编程打下了一个基础，下面将就几个问题来深入介绍网络编程深层次的一些知识。 深入介绍网络编程 为了一步一步的掌握网络编程，下面再研究网络编程中的两个基本问题，通过解决这两个问题将对网络编程的认识深入一层。 如何复用Socket连接？在前面的示例中，客户端中建立了一次连接，只发送一次数据就关闭了，这就相当于拨打电话时，电话打通了只对话一次就关闭了，其实更加常用的应该是拨通一次电话以后多次对话，这就是复用客户端连接。 那么如何实现建立一次连接，进行多次数据交换呢？其实很简单，建立连接以后，将数据交换的逻辑写到一个循环中就可以了。这样只要循环不结束则连接就不会被关闭。按照这种思路，可以改造一下上面的代码，让该程序可以在建立连接一次以后，发送三次数据，当然这里的次数也可以是多次，示例代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.io.*;import java.net.*;/** * 复用连接的Socket客户端 * 功能为：发送字符串“Hello”到服务器端，并打印出服务器端的反馈 */public class MulSocketClient &#123; public static void main(String[] args) &#123; Socket socket = null; InputStream is = null; OutputStream os = null; //服务器端IP地址 String serverIP = "127.0.0.1"; //服务器端端口号 int port = 10000; //发送内容 String data[] =&#123;"First","Second","Third"&#125;; try &#123; //建立连接 socket = new Socket(serverIP,port); //初始化流 os = socket.getOutputStream(); is = socket.getInputStream(); byte[] b = new byte[1024]; for(int i = 0;i &lt; data.length;i++)&#123; //发送数据 os.write(data[i].getBytes()); //接收数据 int n = is.read(b); //输出反馈数据 System.out.println("服务器反馈：" + new String(b,0,n)); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); //打印异常信息 &#125;finally&#123; try &#123; //关闭流和连接 is.close(); os.close(); socket.close(); &#125; catch (Exception e2) &#123;&#125; &#125; &#125;&#125; 该示例程序和前面的代码相比，将数据交换部分的逻辑写在一个for循环的内容，这样就可以建立一次连接，依次将data数组中的数据按照顺序发送给服务器端了。 如果还是使用前面示例代码中的服务器端程序运行该程序，则该程序的结果是：1234567java.net.SocketException: Software caused connection abort: recv failed at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.read(SocketInputStream.java:129)at java.net.SocketInputStream.read(SocketInputStream.java:90)at tcp.MulSocketClient.main(MulSocketClient.java:30)服务器反馈：First 显然，客户端在实际运行时出现了异常，出现异常的原因是什么呢？如果仔细阅读前面的代码，应该还记得前面示例代码中的服务器端是对话一次数据以后就关闭了连接，如果服务器端程序关闭了，客户端继续发送数据肯定会出现异常，这就是出现该问题的原因。按照客户端实现的逻辑，也可以复用服务器端的连接，实现的原理也是将服务器端的数据交换逻辑写在循环中即可，按照该种思路改造以后的服务器端代码为： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.io.*;import java.net.*;/** * 复用连接的echo服务器 * 功能：将客户端发送的内容反馈给客户端 */public class MulSocketServer &#123; public static void main(String[] args) &#123; ServerSocket serverSocket = null; Socket socket = null; OutputStream os = null; InputStream is = null; //监听端口号 int port = 10000; try &#123; //建立连接 serverSocket = new ServerSocket(port); System.out.println("服务器已启动："); //获得连接 socket = serverSocket.accept(); //初始化流 is = socket.getInputStream(); os = socket.getOutputStream(); byte[] b = new byte[1024]; for(int i = 0;i &lt; 3;i++)&#123; int n = is.read(b); //输出 System.out.println("客户端发送内容为：" + new String(b,0,n)); //向客户端发送反馈内容 os.write(b, 0, n); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; try&#123; //关闭流和连接 os.close(); is.close(); socket.close(); serverSocket.close(); &#125;catch(Exception e)&#123;&#125; &#125; &#125;&#125; 在该示例代码中，也将数据发送和接收的逻辑写在了一个for循环内部，只是在实现时硬性的将循环次数规定成了3次，这样代码虽然比较简单，但是通用性比较差。 以该服务器端代码实现为基础运行前面的客户端程序时，客户端的输出为： 123服务器反馈：First服务器反馈：Second服务器反馈：Third 服务器端程序的输出结果为：1234服务器已启动：客户端发送内容为：First客户端发送内容为：Second客户端发送内容为：Third 在该程序中，比较明显的体现出了“请求-响应”模型，也就是在客户端发起连接以后，首先发送字符串“First”给服务器端，服务器端输出客户端发送的内容“First”，然后将客户端发送的内容再反馈给客户端，这样客户端也输出服务器反馈“First”，这样就完成了客户端和服务器端的一次对话，紧接着客户端发送“Second”给服务器端，服务端输出“Second”，然后将“Second”再反馈给客户端，客户端再输出“Second”，从而完成第二次会话，第三次会话的过程和这个一样。在这个过程中，每次都是客户端程序首先发送数据给服务器端，服务器接收数据以后，将结果反馈给客户端，客户端接收到服务器端的反馈，从而完成一次通讯过程。 在该示例中，虽然解决了多次发送的问题，但是客户端和服务器端的次数控制还不够灵活，如果客户端的次数不固定怎么办呢？是否可以使用某个特殊的字符串，例如quit，表示客户端退出呢,这就涉及到网络协议的内容了，会在后续的网络应用示例部分详细介绍。下面开始介绍另外一个网络编程的突出问题。 如何使服务器端支持多个客户端同时工作？前面介绍的服务器端程序，只是实现了概念上的服务器端，离实际的服务器端程序结构距离还很遥远，如果需要让服务器端能够实际使用，那么最需要解决的问题就是——如何支持多个客户端同时工作。 一个服务器端一般都需要同时为多个客户端提供通讯，如果需要同时支持多个客户端，则必须使用前面介绍的线程的概念。简单来说，也就是当服务器端接收到一个连接时，启动一个专门的线程处理和该客户端的通讯。 按照这个思路改写的服务端示例程序将由两个部分组成，MulThreadSocketServer类实现服务器端控制，实现接收客户端连接，然后开启专门的逻辑线程处理该连接，LogicThread类实现对于一个客户端连接的逻辑处理，将处理的逻辑放置在该类的run方法中。该示例的代码实现为： 12345678910111213141516171819202122232425262728293031import java.net.ServerSocket;import java.net.Socket;/** * 支持多客户端的服务器端实现 */public class MulThreadSocketServer &#123; public static void main(String[] args) &#123; ServerSocket serverSocket = null; Socket socket = null; //监听端口号 int port = 10000; try &#123; //建立连接 serverSocket = new ServerSocket(port); System.out.println("服务器已启动："); while(true)&#123; //获得连接 socket = serverSocket.accept(); //启动线程 new LogicThread(socket); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; try&#123; //关闭连接 serverSocket.close(); &#125;catch(Exception e)&#123;&#125; &#125; &#125;&#125; 在该示例代码中，实现了一个while形式的死循环，由于accept方法是阻塞方法，所以当客户端连接未到达时，将阻塞该程序的执行，当客户端到达时接收该连接，并启动一个新的LogicThread线程处理该连接，然后按照循环的执行流程，继续等待下一个客户端连接。这样当任何一个客户端连接到达时，都开启一个专门的线程处理，通过多个线程支持多个客户端同时处理。 下面再看一下LogicThread线程类的源代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.io.*;import java.net.*;/** * 服务器端逻辑线程 */public class LogicThread extends Thread &#123; Socket socket; InputStream is; OutputStream os; public LogicThread(Socket socket)&#123; this.socket = socket; start(); //启动线程 &#125; public void run()&#123; byte[] b = new byte[1024]; try&#123; //初始化流 os = socket.getOutputStream(); is = socket.getInputStream(); for(int i = 0;i &lt; 3;i++)&#123; //读取数据 int n = is.read(b); //逻辑处理 byte[] response = logic(b,0,n); //反馈数据 os.write(response); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; close(); &#125; &#125; /** * 关闭流和连接 */ private void close()&#123; try&#123; //关闭流和连接 os.close(); is.close(); socket.close(); &#125;catch(Exception e)&#123;&#125; &#125; /** * 逻辑处理方法,实现echo逻辑 * @param b 客户端发送数据缓冲区 * @param off 起始下标 * @param len 有效数据长度 * @return */ private byte[] logic(byte[] b,int off,int len)&#123; byte[] response = new byte[len]; //将有效数据拷贝到数组response中 System.arraycopy(b, 0, response, 0, len); return response; &#125;&#125; 在该示例代码中，每次使用一个连接对象构造该线程，该连接对象就是该线程需要处理的连接，在线程构造完成以后，该线程就被启动起来了，然后在run方法内部对客户端连接进行处理，数据交换的逻辑和前面的示例代码一致，只是这里将接收到客户端发送过来的数据并进行处理的逻辑封装成了logic方法，按照前面介绍的IO编程的内容，客户端发送过来的内容存储在数组b的起始下标为0，长度为n个中，这些数据是客户端发送过来的有效数据，将有效的数据传递给logic方法，logic方法实现的是echo服务的逻辑，也就是将客户端发送的有效数据形成以后新的response数组，并作为返回值反馈。 在线程中将logic方法的返回值反馈给客户端，这样就完成了服务器端的逻辑处理模拟，其他的实现和前面的介绍类似，这里就不在重复了。 这里的示例还只是基础的服务器端实现，在实际的服务器端实现中，由于硬件和端口数的限制，所以不能无限制的创建线程对象，而且频繁的创建线程对象效率也比较低，所以程序中都实现了线程池来提高程序的执行效率。 这里简单介绍一下线程池的概念，线程池(Thread pool)是池技术的一种，就是在程序启动时首先把需要个数的线程对象创建好，例如创建5000个线程对象，然后当客户端连接到达时从池中取出一个已经创建完成的线程对象使用即可。当客户端连接关闭以后，将该线程对象重新放入到线程池中供其它的客户端重复使用，这样可以提高程序的执行速度，优化程序对于内存的占用等。 转载，作者不详，侵删]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(11)--双指针找最大储水容器]]></title>
      <url>%2F2016%2F02%2F05%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(11)--%E5%8F%8C%E6%8C%87%E9%92%88%E6%89%BE%E6%9C%80%E5%A4%A7%E5%82%A8%E6%B0%B4%E5%AE%B9%E5%99%A8%2F</url>
      <content type="text"><![CDATA[原题如下： Given n non-negative integers a1, a2, …, an, where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water. 这道题的不难理解，但是对时间复杂度有要求。简单地通过两个for循环寻找最大的蓄水量的时间复杂度为O(n^2),提交时提示超时。 蓄水量由container 的底（也就是两个下标之差）乘上其两端的边的最小值。假如left 为第一条竖直边的下标，right 为最后一条竖直边的下标，i,j为其中的两条边（设i &lt; j），那么： 要使任何S(i&gt;=left, j&lt;=right) &gt;= S(left,right)，由于j-i &lt;= right-left，必然要有min(ai,aj)&gt;=min(a(left),a(right))才行 因此可以从两边同时开始往中间逼近，每次只移动一步（左边的边或右边的边），而且两条边中最短的边进行移动。时间复杂度为O(n)，实现代码如下： 1234567891011121314151617181920# encoding:utf-8class Solution(object): def maxArea(self, height): """ :type height: List[int] :rtype: int """ left = 0 right = len(height) - 1 max = 0 while left &lt; right: current = (right - left)* min(height[left],height[right]) if current &gt;max: max = current if height[left] &gt; height[right]: right-=1 else: left+=1 return max]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(5)--最长回文子字符串]]></title>
      <url>%2F2016%2F01%2F24%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(5)--%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
      <content type="text"><![CDATA[原题如下： Given a string S, find the longest palindromic substring in S. You may assume that the maximum length of S is 1000, and there exists one unique longest palindromic substring. 就是要从给定的字符串中找出最大的回文子字符串。 下面介绍两种时间复杂度为 $O(n^2)$ 的方法，第一种是以每个字符为中心向两边拓展从而得到尽可能长的回文字符串，第二种是利用动态规划。 第一种方法的关键点是以每个字符为中心向两边拓展从而得到尽可能长的回文字符串。这里要注意的是回文字符串的长度可以是奇数也可以是偶数，所以要分两种情况讨论。 实现代码如下：1234567891011121314151617181920212223242526272829303132#encoding:utf-8class Solution(object): def longestPalindrome(self, s): """ :type s: str :rtype: str """ r=s[0] if len(s) &gt; 1: for i in range (1,len(s)): # oddResult begin=i end=i oddResult=self.getPalindoreStr(s,begin,end) if oddResult != None and len(oddResult)&gt; len(r): r=oddResult # enevResult begin=i-1 end=i evenResult=self.getPalindoreStr(s,begin,end) if evenResult != None and len(evenResult)&gt; len(r): r=evenResult return r #从给定的begin和end位置向两边扩展，得到回文字符串 def getPalindoreStr(self,s,begin,end): while ( begin&gt;=0 and end&lt;len(s) and s[begin]==s[end] ): begin-=1 end+=1 result=s[begin+1:end] if begin+1 &lt;= end else None return result 第二种方法采用的是动态规划，dp[i][j] 表示 s[i:j] 所形成的字符串是否为回文字符串，时间复杂度为 $O(n^2)$。代码如下 123456789101112131415161718class Solution(object): def longestPalindrome(self, s): """ :type s: str :rtype: str """ n = len(s) dp = [[False for j in xrange(n)] for i in xrange(n)] result = '' for i in reversed(xrange(n)): for j in xrange(i, n): if i == j: dp[i][j] = True elif s[i] == s[j]: dp[i][j] = dp[i+1][j-1] if i+1&lt;=j-1 else True if dp[i][j] and j-i+1 &gt; len(result): result = s[i:j+1] return result]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Programming Collective Intelligence》读书笔记(1)--梗概]]></title>
      <url>%2F2016%2F01%2F24%2F%E3%80%8AProgramming%20Collective%20Intelligence%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0(1)%2F</url>
      <content type="text"><![CDATA[《Programming Collective Intelligence》（中文名为《集体智慧编程》），是一本关于数据挖掘的书籍，每一章都会通过一个实际的例子来讲述某个机器学习算法，同时会涉及到数据的采集和处理等，是一本实践性很强的书籍。 本文是关于本书的第一章 Introduction to Collective Intelligence ,主要介绍了 collective intelligence 以及 machine learning 的一些概念。 什么是 collective intelligence ？根据单词直译过来就是“集体智慧”,引用原文的解释如下 Collecting answers from a large group of people lets you draw statistical conclusions about the group that no individual member would have known by themselves. Building new conclusions from independent contributors is really what collective intelligence is all about. 可以简单认为就是从一个群体中获取每个个体的信息，经过算法处理后得出能够描述这个群体的一些结论。常见的比如说问卷调查可以认为是一种集体智慧，超市从顾客的购物清单得出顾客的喜好进而调整货物的摆放也可以认为是一种集体智慧。 什么是 machine learning？机器学习，顾名思义，就是让机器具有学习的能力。具体的做法就是先用一些历史数据来训练一个模型，再利用模型去预测新的数据或趋势。训练模型的方法就是机器学习算法，根据实际的应用场景也可以分为多种，如直观的决策树算法、较为抽象的神经网络等。 机器学习也有其的局限性，主要体现在两个方面： 第一个方面是机器学习只能凭借其“见过的数据”（也就是用来训练这个模型的数据）来进行预测归纳，这导致了遇到了新的情况可能出现误判的情况。因此在机器学习中用来训练模型的数据集对模型的效果有很大影响。 第二个方面是大部分机器学习存在笼统归纳（overgeneralize）的问题,例如你收到朋友的一份邮件，里面很可能出现“购买”的字眼，而如果垃圾邮件过滤算法认为出现这个字眼即为垃圾邮件，那么便会把这封邮件归为垃圾邮件过滤掉。但是这种情况也存在解决方法，就是在拉结邮件过滤系统中将这位朋友的邮件均标记为合法邮件。这也说明了只要给机器学习算法更详细的信息进行学习，机器学习算法便能够变得更加精准。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Advanced Web Metrics with Google Analytics》读书笔记(2)]]></title>
      <url>%2F2016%2F01%2F22%2F%E3%80%8AAdvanced%20Web%20Metrics%20with%20Google%20Analytics%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%2F</url>
      <content type="text"><![CDATA[《 Advanced Web Metrics with Google Analytics 》是 Google 一位数据分析专家 Brian Clifton 出版的书，主要介绍了涉及网站分析的一些概念和方法以及如何利用 Google Analytics 进行网站分析。Google Analytics 是 Google 免费提供的一个用于网站分析的工具。 本系列文章是笔者在阅读本书过程中总结整理的一些笔记。 本文是本书中 Chapter 2 的阅读笔记，主要介绍了网站分析常用的两种途径以及网站分析的数据的准确性。 网站分析的两种手段页面标签（page tags）和日志文件（logfiles）是网站分析的两种常用手段。 页面标签一般是在html页面中嵌入js代码段，浏览页面时js代码会将浏览记录发送到远端服务器，远端服务器将对浏览页面进行统计，并提供可视 web 界面，GoogleAnalytics 就是属于这种手段。 日志文件则是利用 web 服务器软件（如Apache、Tomcat等）产生的日志文件进行统计，日志文件能够记录访问的ip和页面等信息；然后在本地服务器上统计并生成所需结果。 除了上面两种途径，还可通过网络流量情况、web服务器软件提供的api等方式进行数据的采集和统计。但是最常用的手段就是上面提到的两种。所以下面详细列出这两种方法的优点点和不足。 方法 Page Tags Logfiles Advantages (1)不受代理服务器和缓存服务器的影响 (2)能够实时处理客户端产生的数据(3)数据采集程序的升级以及数据的存储仅需服务商提供 (1)能够保留历史数据(2)不会受到防火墙影响(3)跟踪带宽及下载情况，区分完全下载和部分下载(4)能够跟踪网络爬虫的来源 Disadvantage (1)部署的不恰当或导致数据的丢失且无法重新找到(2)防火墙可能会拦截这种方法的流量(3)不能够获取带宽或者下载具体情况，即不能确定是否下载成功(4)不能跟踪网络爬虫的来源 (1)代理服务器和缓存服务器会影响数据准确性(2)需要维护服务器的程序正确性以及数据的存储等(3)网络爬虫会使得访问者的数量比实际的大 从上面的比较分析可以看到，两种方法的优点和缺点几乎是互补的，所以在实际中不需要局限于一种方法，而可以混合多种方法进行分析。 Cookies 在网站分析中的作用页面标签（page tags）是通过 cookies 来追踪访客的。cookies 是以文本方式存储在客户端本地的一系列 key-value 对，用来存储客户端的一些信息，每次客户端请求服务器端的资源时，服务器端都可以获取客户端对应的cookie，不同的 cookie 根据不同的域名来区分。 cookies 有几种分类： 如根据生存时间可以分为永久 cookies （persistent cookies）和 会话 cookies （session cookies），永久 cookies 指那些关闭浏览器后再重新打开时依然有效的cookies（前提是客户端不会清理cookies），而 会话 cookies 则指那些只在浏览网站期间有效的cookies，关闭浏览器后 cookies 会自动失效。 根据来源可以分为第一方 cookie （first-party cookie）和第三方 cookie （third-party cookie），第一方cookie指你访问的url 所属的域名在你的电脑上留下的cookie，我们之前已经提到过：不同的cookie通过不同的域名进行识别。第三方 cookie 则刚好相反，是你访问一个url后所获得的cookies中，那些不属于这个url所属域名的cookie，如一些网页中嵌入的一些广告可能会留下cookies，这些cookies不属于所访问的页面所属的域名，这个就可以算为第三方 cookie。两者的区别在于第一方 cookie 只能由设定这个cookie的域名所获取；而第三方 cookie 则允许列出可以获取这个cookie的所有域名。 cookies 在网站分析中的一些常见作用包括：能够判断访客是否第一次访问这个网站，每隔一定时间会有多少访客再次访问，每位访客具体的访问间隔时间等。 数据准确性无论是网页标签（page tags），还是日志文件（logfiles），任何一种手段均存在采集数据的不准确性问题。下面分别讲述这两种方法存在一些问题以及解决方法 日志文件中的数据不准确性动态分配IP动态分配IP会使得统计出来的访客人数比实际要大。现今为家庭提供网络服务的 ISP 一般都是分配动态IP,美国曾统计过（http://www.comscore.com/Press_Events/Presentations_Whitepapers/2007/Cookie_Deletion_Whitepape）一个家庭平均每个月会使用10.5个IP地址，这会导致通过日志文件统计出来的访客数量要大于实际的。因为日志文件是通过不同IP来区分不同的访客的，按照上面的情况，会将一个访客统计为10个访客，因为这个人在访问网站时IP会变化。这类问题可通过cookie 来解决。 缓存缓存会使得统计出来的访客人数比实际要小。缓存又可以分为客户端的缓存和服务器端的缓存。客户端的缓存指的是用户在访问网站后，用户使用的浏览器会缓存其访问过的某些页面，使得用户再次访问这个页面时浏览器可以从本地获取，从而加快其访问速度。但是这样服务器的日志文件就无法记录这次的访问信息了。服务器端的缓存的作用与客户算的类似，也是将一些常被访问的页面缓存起来，加快客户端的访问速度，我们常听到的CDN就是其中的一种。对于这种问题，貌似目前还没有比较好的解决方法。 网络爬虫网络爬虫会使得统计出来的访客人数比实际要大。网络爬虫在搜索引擎等领域使用得非常广泛，可以理解为通过程序获取页面信息。这会产生大量非实际访客访问的PV量，这就导致了统计出来的PV量比实际的要大。可以通过追踪爬虫的来源从而在日志中过滤掉这个爬虫的访问记录。但是因为爬虫的数量很多，往往难以完全过滤掉所有爬虫的访问记录。 网页标签中的数据不准确性代码部署不全因为网页标签（page tags）的方法是通过在网站的每个网页上嵌入一段 JS 代码实现，所以在一开始部署这段代码的时候有可能会存在部署不全的情况，就是没有在每个网页上部署这段代码。这种情况在一些较大的网站上普遍存在。 JS代码发生错误除了采集访问信息的js代码外，网页中不可避免会有完成其他功能的js代码，这些js代码假如发生了错误并且在网页源码中的位置处于采集访问信息的js代码前，会导致浏览器解析脚本引擎停滞工作，从而在下面的采集访问信息的js代码段将没有执行。 防火墙的阻挡因为网页标签（page tags）方法会将数据发送给指定的数据采集服务器，所以防火墙能够阻挡这一动作。除此之外，防火墙还能够阻挡或者自动删除cookies。 Cookies中的数据不准确性访客拒绝或删除cookies因为cookies是存储在访客本地的电脑的，故可以将已有的cookies删除掉，也可以在浏览器设置中距拒绝cookies。据调查显示，第一方cookies的接受率可达95%，而第三方cookies则常被防火墙或反病毒软件拦截。 访客有多台设备或共享一台设备这里的设备可以指电脑、平板、手机等，现今同一个人同时拥有这几种设备是很常见的事情，同时有些家庭也共用一台电脑，这就导致了下面可能存在的问题。 同一个访客有多台设备：用这些设备访问统一网页时均会生成cookies，这样统计时会将同一用户产生的三个cookies当做是不同用户产生的。多个访客共享同一台设备：这样访问一个同一个网站只会产生一个cookies，但是分析cookies时只会将这个当做一个用户，显然分析结果不合理。 解决这类问题可以设置用户登录这一步骤，从而在cookies中标记不同的用户区分cookies的来源。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Advanced Web Metrics with Google Analytics》读书笔记(1)]]></title>
      <url>%2F2016%2F01%2F20%2F%E3%80%8AAdvanced%20Web%20Metrics%20with%20Google%20Analytics%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
      <content type="text"><![CDATA[《Advanced Web Metrics with Google Analytics》是 Google 一位数据分析专家 Brian Clifton 出版的书，主要介绍了涉及网站分析的一些概念和方法以及如何利用 Google Analytics 进行网站分析。Google Analytics 是 Google 免费提供的一个用于网站分析的工具。本系列文章是笔者在阅读本书过程中总结整理的一些笔记。 本文是本书中 Chapter 1的阅读笔记，主要介绍了网站分析的一些概念以及为什么要进行网站分析。 为什么要进行网站分析在进行网站分析前需要考虑的一个问题是通过网站分析能够给你带来的价值，这种价值可以表现在商业上，如某家公司的网站是否能够给实际销量带来提升，也可以表现在个人上，如个人网站的访问量、影响力如何。 网站分析能够获取的信息一般的网站分析均能够获取下面的信息： 每天的访客人数 平均转化率（conversion rate），就是销量、注册量和下载量等 网站上访问最多的页面 访客的每次平均访问时间和访问的频率 访客的地理分布 假如你的网站是商业网站，还能够获取以下信息： 网站产生的盈利 网站的顾客来自哪里 网站最畅销的商品是那些 随着挖掘的深入还可以获取到以下的信息： 一个顾客对于网站的价值 如何量化一个网页的价值 老顾客和新顾客在使用网站的方式上是否有区别 访问量以及转化率如何被将顾客引导到本站的外站的影响 页面跳出率受何影响 站内搜索是否对转化率有利 平均需要多长时间才能让一位访客成为一位顾客 如何利用网站分析得到的结果 从前面的分析可知，网站的分析能够提供很多信息，这些繁杂的信息会让许多初学者感到无从下手。因此，在进行网站分析前一定要确定希望通过网站分析达到的目标。 目标根据网站具体类型有所区别，总的来说，目标就是希望访客在离开网页之前要完成的内容。如对于购物网站，目标可能是希望顾客购买了产品，对于个人网站而言，可能希望顾客能够对网站留言给出建议等。更进一步说，目标就是在访客好你的网站之间建立起的任何一种关系，而不是仅仅一个PV量，如留言，订阅网站RSS，下载了一个PDF文件等。 有了目标，便可以从复杂繁多的数据中找到所需要的数据。进而分析这些数据，并采取相应的措施。如最简单的购物网站需要判断那种物品销量较好，哪种较差，进而修改相应的进货量；便可商品的购买量、PV量等进行分析。 上面只是对本书的内容做了一个很笼统的总结，具体的实现方法、原理及注意事项会在后续文章介绍。 因为这本书主要就是讲述通过Google Analytics 进行网站分析，所以有必要了解一下Google Analytics 中的一些专用术语。下面是 Google Analytics 中一些常用的术语，更详细的可见 http://www.google.com/support/googleanalytics/bin/topic.py?topic=11285 术语 含义 Bounced Visitor 只在你的网页上浏览过一次的访客，这个数值当然越小越好 Google Analytics Tracking Code（GATC） Google Analytics 工具提供的一段js代码，用来追踪网站的访问情况 Goal Conversion 也可简称为 goal 或 conversion，表示希望网站达到的一个目标，如购买页面的一次访问量或一次下载 Funnel 表示达到Goal Conversion所需要经过的一个流程 Landing Page 网站的首页 Referrer 含有你的网站超链接的页面 Return On Investment(ROI) 检验网站分析效果的一种指标，计算公式：纯利润/支出 Session 也叫会话，指一个访客在网站停留的时间，一个会话在访客关闭页面后结束，也会在访客在一段时间内对网站无任何操作情况下结束 Site Search 网站的内部搜索功能]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[不要在最好的位置睡觉]]></title>
      <url>%2F2016%2F01%2F19%2F%E4%B8%8D%E8%A6%81%E5%9C%A8%E6%9C%80%E5%A5%BD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%9D%A1%E8%A7%89%2F</url>
      <content type="text"><![CDATA[转载，作者不详，侵删 有个小镇上来了一个马戏团， 他们在当地招募临时工，并提出以下几种不同待遇： 做三个小时工作送一张外场的票；做六个小时就可以进到内场看表演；做一整天，就可以得到最前排最中间最好位置的票。有一对穷人家的小兄弟，愿意干一整天，换一张最前排的票。于是，他们开始了辛苦的工作。 从太阳升起到落去，他们一刻不停地干活，中间只分吃了一个馒头。到下午的时候，兄弟俩都十分疲惫，但是看马戏的信念支撑着他们，希望得到最前排最中间的位置。 到了晚上，兄弟俩终于在艰辛劳动后达成目标，拿到入场观赏演出的票。 他们筋疲力尽地，坐在第一排最中间最好的位置，却满身尘土，还有满手土豆子一样大的水泡。 主持人出场的时候，大家都热烈地鼓掌，而这两个可怜的孩子，却在这掌声里，疲累沉沉地睡去……。 故事蛮让人心酸。可是，仔细想想，这不就是很多人的人生写照吗？ 这个世界很精彩，就像马戏团的演出一样。而我们每个人，都渴望能坐在最前排最中间的位置，看这场演出。 我们其实也一直接受着这样的鞭策；演出力求精彩，一定要努力努力再努力，争取坐在第一排最好的位置看演出。 然后我们就拼命干，干到身体疲惫、崩溃，终于得到那张美好的票了。 可是，到了这一刻，你已经老了；耳聋眼花、百病缠身。 虽然努力拿到了入场券，又拼命取得了最好的位置，却再没有精力和心思去欣赏这场精彩的演出了！ 回头看看，你是选择付出适当的努力，然后高高兴兴地在内场看演出，还是选择拼命，也要坐在第一排最中间，沉沉睡去呢？ 要知道，人生的目的，不是只为坐在一个多么好的位置，而拼命，是只要尽了心力后，可怡然尽情地欣赏每一场精彩的演出。 如果有一天你感觉累了，感觉不堪重负了，那么就停下来好好衡量一下，给自己的人生一个更准确的定位吧！ 记住，来这个世界，我们是为了看一场精彩的演出，而不是为了坐在最好的位置上，沉沉睡去。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过 python 发送邮件提醒网站的新评论]]></title>
      <url>%2F2016%2F01%2F17%2F%E9%80%9A%E8%BF%87%20python%20%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E6%8F%90%E9%86%92%E7%BD%91%E7%AB%99%E7%9A%84%E6%96%B0%E8%AF%84%E8%AE%BA%2F</url>
      <content type="text"><![CDATA[这篇文章是当时在新浪云上搭建博客的时候写的，后来因为新浪云收费了，把网站迁移到了github上。这里还是把文章贴出来，做个记录。 最近在写本站的评论提醒功能的时候，需要通过 python 发送邮件提醒具体哪些文章有了新评论，采用邮件的方式便于在特定时间处理所有的评论，比如说在第二天早上7点检查网站昨天是否有新的评论，假如有就会发送邮件显示那些有新评论的文章。 实现思路如下：先检查数据库中是否有新的评论，假如有新的评论就发送邮件，否则不发送。 如何检查新评论？在存储评论的表中有一个字段表示添加该评论的时间（datetime类型），利用这个字段能够在今天判断昨天是否有新的评论，现假设评论表为comment,表示评论添加时间的字段为created_date,采用的SQL语句如下： 1select * from comment where date(created_date) =( select date_sub(curdate(),interval 1 day) 假如查询的语句不为空，那么说明有了新的评论，可以将关于评论的具体内容以邮件形式发送给管理员。下面是python发送邮件的实现方法之一： PS：虽然新浪云提供了用于发送邮件的Mail服务，当时对于普通用户每天好像会限定发送的次数，而且最严重的问题是邮件的延迟非常严重，建议还是使用smtplib模块。 1234567891011121314151617181920212223242526#encoding:utf-8import smtplib #用于发送邮件import string #用于将要发送的内容格式化成邮件标准格式from email.mime.text import MIMETextdef send_email(content): #content为发送邮件的正文内容，这里只涉及到文本 HOST="smtp.XXX.com"#发送邮件smtp服务器 FROM="XXX@XXX.com" #发送邮件的邮箱 PASSWORD="" #发送邮件的邮箱的密码 TO="XXX@XXX.com" #接收邮件的邮箱 SUBJECT="New Comments on your Website" #邮件主题 msg=MIMEText(content,'plain','utf-8') #邮件内容及编码方式 msg['Subject']=SUBJECT msg['From']=FROM msg['To']=TO try: s=smtplib.SMTP() s.connect(HOST) s.login(FROM,PASSWORD) s.sendmail(FROM,[TO],msg.as_string()) s.quit() return True except EXception,e: return False 下面提到的几点有助于更好地理解上面的代码 通过免费提供的邮箱（如163邮箱，新浪邮箱）发送邮件的过程是先将邮件发送到对应邮箱的smtp服务器，然后由smtp服务器将邮件帮你发送到你要发送的邮箱。一般来说，邮箱对应的服务器都跟其名称有关，如163邮箱对应的是smtp服务器是smtp.163.com,新浪邮箱对应的smtp服务器是smtp.sina.com；其他的邮箱相应可以从设置中找到。 smtplib和MIMEText是python自带的库，不用另外安装，比较方便，其中smtplib负责发送邮件，MIMEText则负责将要发送的文本内容统一成邮件的标准模式，加入要加入一些附件，需要MIMEMultipart这个模块。 需要注意的是发送邮件的邮箱需要开启smtp服务,一般能够在邮箱的设置中开启。因为有些有些邮箱是不开启这个功能的，比如说新浪邮箱。 发送邮件的效果图如下所示： 能够发送邮件后，要考虑的问题就是如何自动化执行了，总不能每天手动执行一遍脚本吧，在Linux下可通过crontab来设置计划任务，同样新浪云也提供了类似的cron的服务，这样便可在每天的早上检查昨天是否有新评论在决定是否要发送邮件了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[炒股损失的不仅是钱,年轻人请远离股市]]></title>
      <url>%2F2016%2F01%2F17%2F%E7%82%92%E8%82%A1%E6%8D%9F%E5%A4%B1%E7%9A%84%E4%B8%8D%E4%BB%85%E6%98%AF%E9%92%B1-%E5%B9%B4%E8%BD%BB%E4%BA%BA%E8%AF%B7%E8%BF%9C%E7%A6%BB%E8%82%A1%E5%B8%82%2F</url>
      <content type="text"><![CDATA[转载，作者：李晓鹏，侵删 这篇文章本来是该几年前写的，奉劝大家不要去玩股票。因为那个时候我的《中国崛起的经济学分析》这本书刚刚出版，里面用“破坏性要素参与分配”的理论来分析了中国经济。在写作过程中我发现这个理论也可以顺便用来解释股票市场，让大家看清楚股票市场的本质。但当时的大盘指数才1980点，我怕写出来很多人会被我“忽悠”，把手里的股票“割肉”卖掉，回头会恨死我。所以就忍了。 我的观点很简单：股票市场不是年轻人应该去的地方。对年轻人来说，玩股票就跟爱上赌博一样，是在浪费生命。年轻人最大的资本是自己，一旦把自己有限的积蓄投入到股市中去，就会被行情的波动死死的抓住，然后在里面虚度光阴：原本应该学英语的时间，却拿来研究波浪理论；原本计划去听一场学术讲座的，却跑到证券公司去被各种股票大师洗脑；原本可以把本职工作做得更好一些，却敷衍了事然后偷偷打开行情软件看股票；原本可以在自己喜欢并擅长的领域取得成就的，却跟成千上万的“股民”一样天天守在电脑前面想着一夜暴富，沦为庸碌之辈。 总之，炒股，你损失的不仅是钱，最重要的是会耽误你自己最宝贵的财富——个人才干的价值提升。而后者才是你可靠而长远的财富来源。 我进入股市的时间是2006年7月，也就是我研究生毕业的时候。因为几个月前中国股市跌破了1000点，许小年吴敬琏等人炮轰中国股市还不如赌场，应该推倒重来。我却觉得被他们轰到1000点的中国股市的价值被严重低估，到处找人推销我的观点，但是没有人相信，所以就自己找人借了10万块钱去炒股。本来打算早点进去的，但是因为硕士论文的事情折腾了好久，答辩通过的第二天我就去银行开户了，这时大盘已经涨到了1500点了。 总之，就是这么误打误撞，不经意间，竟然闯进了中国股市历史上最大的一轮牛市之中。那个时候挣钱真好挣啊，不管买什么股票都在涨。每隔几天就能撞见一次涨停。但是我并不满足于跟着大盘涨，还想赚的更多。就开始买书来看，把什么《股票作手回忆录》《波浪理论》《巴菲特致股东的信》……等等有关股票投资的“名著”都翻出来看了一遍。 那个时候就有了一种幻想，觉得股票赚钱很容易，以后就可以靠这个吃饭了。也就懒得去找工作，此前还打算自己跟几个朋友一起创业的，那10万块钱既然被我用来炒股了，创业的事情也就不了了之。 2007年股市到6000点的时候，我大概赚了有20万。上证指数涨了300%，我挣了200%，没有跑过大盘，但是也很令人满意了。毕竟那是借来的10万块钱啊。我大概在4000点的时候，就已经把钱还给别人了，所以剩下的钱就都是纯利润。而且这一年的吃喝也都是从股市里出的。如果从这个时间点来看，我一年零三个月挣了20多万，对一个硕士毕业第一年的人来说是很不错了。 但是当大盘指数从6000点开始下跌的时候，我这过去一年多挣的钱就开始一点一点往回吐了。这时候我发现，自己曾经非常得意的那种“炒股技术”，没有一个是真正有用的，买什么股票都在跌。吐啊吐，吐啊吐，一直回吐到3600点，亏得还剩不到十万块钱了，终于受不了，斩仓出局，把剩下的钱全部转回到了银行卡上。退出股市。这个时候算下来，我接近两年的时间，实际上只挣了大约10万块钱。即使纯粹算一个收入帐，也变得非常不划算了。 更要命的是，这个时候我再去找工作，就变得非常困难。因为同年毕业的同学们，都已经工作了一年多快两年了。一般很好的大公司大企业会给应届毕业生提供一些很好的岗位，一旦不是应届生，就会对工作经验有要求。我既不是应届毕业生，又没有足够的工作经验。连续给一些大企业投了一轮简历，没有收到一个回复。那个时候真的好惨啊，别说面试什么的了，连面试通知都没有一个。只有一些莫名其妙的小公司给了我面试机会，我跑过去一看，好多都是骗子公司，有做传销的，还有忽悠人炒外汇的……当时觉得自己前途一片渺茫，本来是重点大学的本科、硕士毕业，混了两年变成这个鸟样，哎，有何面目再见人呢？ 这个时候没有办法，才想起来重新回学校去读书。还好我考试的功力还是在的，经过几个月的准备，终于考上了博士。后来突然就一帆风顺起来了，去了剑桥、去了哈佛、去麦肯锡、出版《中国崛起的经济学分析》……瞬间整个人都变了，从一个到传销公司找工作的无业游民，变成了一个集各种高大上于一身的学者。 这一段经历，让我刻骨铭心。我知道了什么叫做“珍惜生命，远离股市”。 人的一生，从20岁到30岁之间这一段时光——如果你不是富二代或者官二代的话，是比较难熬的。包括像任正非、柳传志、马云、刘强东这些白手起家的大牛人，他们在这个年龄也是生活比较黯淡的。因为这一段时间，从学校这个与世无争的世外桃源走进社会了，要自食其力了，但是资历、经验、关系网络什么的都不够，付出和收获很不成比例。不管是创业还是工作，其实都很难。你的个人期许和社会对你的承认程度，往往有很大的差距。 这段时期其实不是一个学习了很多年以后，开始收获的时期。应该是一个一半工作、一半学习的时期。就是说你即使去机关企业工作，你这种工作也带有学习的性质。所以机关企业并不会按照你的付出程度支付“足够”的报酬，因为你的工作能力各方面还很不成熟，他们同时也在为你提供一个学习进步的环境。严格来说只能叫做“半工半读”。通过一段很长时期的“半工半读”之后，等你对于本职工作十分擅长了，人际管理的资源网络也比较健全了，才能度过这一段考验期，进入一个比较好的发展时期。 如果不考虑家庭因素，同龄人之间在20岁到30岁这段时间并不会拉开很大的差距，都差不多，工资高点低点也就是那么一点，农民工和硕士毕业的收入差距不是很大。没有飞来横财的话，大家都过着一样平凡的日子，默默的为自己的理想奋斗着、努力着。但是过了30岁以后，差距就会拉大了，成功的人可能非常成功，变身土豪名流，而没有进步的人可能会原地踏步，还是原来那样的地位或收入。这种差距可以是天上地下的区别。 正因为如此，年轻人如果把时间放到炒股票上去，每天被行情的波动折腾得对本职工作心不在焉，最大的损失不是钱，而是耽误自己能力素质的积累。对于那些一无所有的年轻人来说，他们最值钱的东西是自己的学识和才干。 股票上挣的钱能够立竿见影的看到：行情最火爆的时候，一两个月就能翻一倍，刺激的很。但是时间长了你会发现，这个东西根本不挣钱、挣到的还会赔回去，得不偿失。 在工作学习上所花的时间，短期内很难挣到什么钱，你做工作努力一点、还是敷衍一点，上班时间是在有空就看看本职工作相关的书籍，还是偷偷摸摸的打开电脑看股票，每个月都是那么一点工资收入，并不会增加。但是，长远来看，平时的一点一点的积累，最终会在关键时刻让你脱颖而出，并因此而受益终身。这种回报，比股票市场上能够挣到的钱，不知道要多出多少倍，不知道要可靠多少倍。 这里边的利害计算，我们一定要想清楚。更何况，我们的理想，难道是一个每天坐在电脑前面看股票行情的人吗？ 我觉得，这不仅是20-30岁这个年龄段的年轻人的事情。任何年龄段的人，只要他还没有对自己的事业失去信心，还想着取得比现在更大的进步，那么就不应该去玩什么股票。股票，要么是专业人士玩，要么是退休了没事干的老爷爷老奶奶去玩，除此以外的其他人都不应该去玩。 股票市场这种制度的设计，本来就不是给普通人赚钱的。我在《中国崛起的经济学分析》里面提出一个理论，就是：一个人，要想稳定的获得任何形式的收入，都要对应一种这个人所能掌握的资源。他要么掌握生产性要素进行创造获利，要么掌握破坏性要素进行破坏获利。 比如，你精通计算机软件，那么就可以从编程中赚到工资；如果你精通企业管理和市场营销，你就可以成为一个成功的企业家，这些都是你利用自己的能力——也就是你能掌握的生产性要素参与社会分工，获得的合理的报酬。 还有一种相反的，就是你身强力壮，可以晚上找个偏僻的地方拦路抢劫，或者精通攀沿技术，晚上可以入室盗窃把别人的钱变成自己的……这些就是你个人掌握的破坏性资源，从别人手里掠夺财富来获得收入，这叫“破坏性要素参与分配”。 不管是生产性还是破坏性要素，你总得占有一样。如果你一样都不占有，纯粹就是去赌钱玩，那么最后一定是得到一个平局：一会儿输钱，一会儿赢钱，最后不输不赢。 但是在真正的赌场里边，输钱的总是赌徒，赚钱的总是庄家。这是因为庄家掌握了“破坏性要素”，就是可以通过操纵规则和作弊来掠夺赌徒的钱。 那么到了股市里，这个不停波动的市场里面，我们可以问问自己，我们掌握了什么资源可以参与股票市场的财富分配呢？有生产性要素吗？没有。那些能够经营企业上市的企业家才有。有破坏性要素吗？还是没有，那些操作股市的庄家才有。那这些我们普通的年轻人，凭什么能赚到比把钱存在银行吃利息更多的钱呢？相信什么波浪理论、什么趋势线，你能玩的过那些金融、数学专业毕业的研究团队吗？ 把这个道理想清楚，我们就不难发现，我们去股市赚钱，最好的结果，就是不亏本，能够不被别人掠夺，挣点跟银行利息加社会最低工资标准差不多的收入就到头了，有时候看起来一夜暴富，其实很快又会吐回去去；但大部分结果是，被那些掌握了“破坏性要素”的人掠夺，浪费了时间还亏了本。 不管是哪一种结果，我们的时间都耽误不起。有人会听信关于“价值投资”的谎言，说我就长期投资，学习巴菲特，放在那里不动，又能挣钱又不耽误本职工作。这种想法是同样行不通的：如果你只用很少的一点钱去炒股，比如收入的10%，这点长期“价值投资”的收益其实对你没有影响，那么为什么要花那个时间呢？如果你投入大钱去炒股，股票的波动会影响你的生活，那么你虽然心里想着价值投资，但还是会变成一个赌博一样，每天除了关注股市行情以外什么事情也做不好。而这就会毁掉你的生活。不管是那一种，你都应该远离股市。所以，与其“价值投资”，还不如把钱存银行，旱涝保收。 总之，最好的选择，就是把我们的钱安安稳稳的放在那里，别去想什么理财之类的事情。在我们年轻的时候，能够掌握的那一点点资金，根本不值得浪费时间去“理财”。最需要“理”的“财”，是自己的知识和能力。 如果我们真的要玩股票，学习巴菲特、索罗斯，那应该做的不是拿着自己的钱去玩，而是像他们一样，学一个金融或者数学的学位，然后进入金融行业工作，争取成为基金经理，然后拿着别人的钱去玩，赚了提成，亏了不管。这样，你就可以掌握“生产性要素”——也就是运用各种专业知识和组织专业研究队伍来进行价值判断，真的为企业发展融资，促进企业成长；或者掌握“破坏性要素”，就是具备影响股票走势的能力，玩死那些小散户。 如果你不打算这么干，那么，请远离股市，最好是碰都不要去碰一下；如果你已经进入了股市，最好是马上清仓，把所有的钱转回到银行卡里面。然后专注于自己的梦想，把它一点一点变成现实。 如果你还是对股市恋恋不舍，还做着一夜暴富的美梦，那么，你生活的价值，无非就是花一些时间在电脑屏幕上观察布朗运动罢了。 现在大盘正在蠢蠢欲动，很有可能再现2006年、2007年的盛况，又会有一大波还在校园里或者刚刚工作的年轻人会面对我当年面对的那种诱惑了。我写这篇文章，希望可以帮助一些看到它的人，不要再走一次我当年走过的弯路。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[命令行编译Java源文件]]></title>
      <url>%2F2016%2F01%2F13%2F%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%BC%96%E8%AF%91Java%E6%BA%90%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[众所周知，Java是一门编译型语言，需要编译成字节码才能在JVM上运行。常用的IDE如Eclipse等将编译、运行等步骤结合起来一起执行，只需要按下Run即可完成编译和运行的工作。但是实际上编译java程序的核心是JDK。本文主要讲述了只安装jdk通过命令行来编译运行Java程序。 JDK 与 JRE说到JDK和JRE的关系。可以简单认为JDK是用来编译java文件的，而jre是用来运行编译生成的class文件。JDK中包含JRE，在JDK的安装目录下有一个名为jre的目录，里面有两个文件夹bin和lib，在这里可以认为bin里的就是jvm，lib中则是jvm工作所需要的类库，而jvm和 lib和起来就称为jre。 另外在windows下安装jdk是会有选择是否安装单独的jre，如果选择安装，那么就会安装了两套jre，一套包含在jdk安装目录中，另外一套则是独立安装的jre。那么这两套jre有区别吗？ 答案是有的，因为jdk是作为开发使用的，而jre仅仅是作为运行java程序的环境。因此具备开发功能的jdk自己的jre下会同时有client性质的jvm和server性质的 jvm，而仅仅作为运行环境的jre下只需要client性质的jvm.dll就够了。 可根据需要编译的源文件的数目和位置将通过命令行编译运行java程序分成两大类。一类是只有单一的源文件，另外一种是有多个源文件。 单个源文件单个原文件是最简单的了，编译命令为：javac source.java运行的命令为java source上面没有考虑单个源文件中引入的jar包，假如引入了不在classpath环境变量中的jar包时，需要在编译时加上-classpath参数。即命令为：javac -classpath *.jar source.java 多个源文件编译多个源文件是指其中一个源文件引用了另外一个源文件里面的类。这里可以根据源文件是否在同一个包进行以下分类。 源文件在同一个包这时候只需要编译“最顶层”的类的源文件即可，比如说有以下三个类： 类E源文件如下12345public class E&#123; public E()&#123; System.out.println("this is E"); &#125;&#125; 类C源文件如下123456public class C&#123; public C()&#123; E e=new E(); System.out.println("this is C"); &#125;&#125; 类D源文件如下123456public class D&#123;public static void main(String [] args)&#123; C c=new C(); System.out.println("this is D"); &#125;&#125; 现在这三个源文件在同一个包，也就是同一个文件夹下，此时只需要编译类D的源文件即可，执行时也只需要执行类D的class文件。即依次执行下面命令 12javac D.javajava D 得到的结果如下： 123this is Ethis is Cthis is D 源文件不在同一个包这种情况下又可分为两种情况，详见下面例子 第一种情况：类B的源文件如下：12345678package tcp;public class B&#123;public B()&#123; System.out.println("this is B"); &#125;&#125; 类A的源文件如下：12345678import tcp.*;public class A&#123; public static void main(String [] args)&#123; B b=new B(); System.out.println("this is A"); &#125;&#125; 这时候需要建立一个名为tcp的目录,将类B的源文件放到tcp目录中，然后类A的源文件与tcp目录在同一个目录下，这时候只需要进入到类A所在目录下执行javac A.java即可生成A.class和B.class两个类文件，且B.class在tcp目录下生成。再执行命令java A便可得到以下结果： 12this is Bthis is A 第二种情况在第一种情况下类A并没有在任何包中，那么假如类A也在一个包中该怎么编译运行呢？ 在类A的源文件的开头添加多一行pacakge udp;即类A在包udp中，这时候通过命令行该怎么编译？ 首先建立tcp和udp两个文件夹且将他们放在同一目录，tcp目录中放置B.java,udp目录中放置A.java,编译的方法有两种： 第一种：进入到udp目录，执行命令javac -classpath .. A.java第二种：不进入tcp或udp，在tcp和udp所在的目录执行命令javac udp\A.java 第一种方法的-classpath参数指出了tcp包所在目录，..表示上一层目录，这里采用了相对路径，也可采用绝对路径。 第二种方法利用了jdk编译时遇到import的包时会先在当前目录寻找的特性。 执行A程序的步骤则只有一种方法，就是在tcp和udp目录下执行命令java udp.A]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java中String和byte[]的转换]]></title>
      <url>%2F2016%2F01%2F13%2FJava%E4%B8%ADString%E5%92%8Cbyte%5B%5D%E7%9A%84%E8%BD%AC%E6%8D%A2%2F</url>
      <content type="text"><![CDATA[在Java的网络编程中，通过socket发送和接收到的数据是byte[ ]类型的，而我们希望发送的消息一般是用String类型表示的，所以在Java的网络编程中，发送端需要将String类型转换为byte[ ]类型，接收端需要将byte[ ]类型转换为String类型。 String类型转换为byte[ ]类型代码如下：12String sendString="hello";byte[] sendByte=sendString.getBytes("UTF8"); 除了UTF8编码方式外，还可以采用”GBK”,”IOS-8859-1”等编码方式，这几种编码方式的差别在于表示同一个字符采用的字节数不同，而且为了中文显示不乱码，一般采用UTF8的编码方式。 byte[ ]类型转换为String类型代码如下： 1String receiveString=new String(receiveBytes,"UTF8"); receiveBytes为需要转为String的字节数组，”UTF8”的作用也是为了指定转换采用的编码方式，省略时会采用系统默认的编码方式。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java通过socket获取mysql中的记录遇到的问题]]></title>
      <url>%2F2016%2F01%2F12%2FJava%E9%80%9A%E8%BF%87socket%E8%8E%B7%E5%8F%96mysql%E4%B8%AD%E7%9A%84%E8%AE%B0%E5%BD%95%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[在实际应用中，经常会遇到这种需求，客户端发送关键字到服务器端，服务器端根据关键字查找数据库中的内容并发送给客户端。本文主要阐述根据传输过来的关键字在查找数据库时查找不到（实际上是数据库中是有数据的）的问题，以及解决方法。 先看Java与mysql交互的代码: 12345678910111213141516 String driver = "com.mysql.jdbc.Driver"; // 驱动程序名 String url = "mysql的地址"; // URL指向要访问的数据库 String user = "用户名"; // MySQL配置时的用户名 String password = "你的密码"; // MySQL配置时的密码Class.forName(driver);// 加载驱动程序Connection conn = DriverManager.getConnection(url, user, password);// 连接数据库if(!conn.isClosed()) System.out.println("成功连接到数据库!");PreparedStatement prepstmt = null; //用预编译对象执行sql语句并返回结果String sql = "select admoment,url1,url2 from captiontest where filmname=? order by admoment";//要执行的SQL语句,按时间查找广告出现的数据记录prepstmt=conn.prepareStatement(sql);prepstmt.setString(1,name); // 由于filmnam是String类所以这里是setString，其他同理为setxxx（xxx为类型）ResultSet rs=prepstmt.executeQuery(); //结果集 其中程序中的 name 为 String 类型，也就是客户端传过来的关键字，但是测试时的问题是即使数据库中存在相关的关键字信息，也不会输出相关信息。。 这个时候首先要做的就是将执行的SQL语句打印出来，因为很多时候得不到预期的结果都是因为是实际执行的SQL语句跟自己预期会执行的SQL语句不同。通过下面语句可以得出执行的SQL语句: System.out.println(prepstmt.toString());。 结果如下 1select admoment,url1,url2 from captiontest where filmname=&apos;爱情公寓4\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\&apos; order by admoment 怪不得找不到，sql语句都变成这个模样了，那中间的 \0 是从哪里来的？在网上查了后得出的原因如下：在 socket 传输自己流时，默认会有一段固定长度的 bufferstream 来接受传输的字节流，这段固定长度的默认值就是 \0 , 而那些 \0 就是 bufferstream 中没被传输过来的字节流覆盖的单元，解决方法也很简单，就是在服务器接受到客户端传输过来的字节流时，根据字节流有效长度来创建String 原来我的name是这样产生的，is是socket的输入流： 12int n = is.read(b);String name=new String(b,"UTF-8"); 没有指定长度，改为下面的产生方式就好了 12int n = is.read(b);String name=new String(b,0,n);//0和n是指定产生String用到的字节流的开头和结尾]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(4)--二分法查找两个有序数组的中位数]]></title>
      <url>%2F2016%2F01%2F10%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(4)--%E4%BA%8C%E5%88%86%E6%B3%95%E6%9F%A5%E6%89%BE%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0%2F</url>
      <content type="text"><![CDATA[原题如下： There are two sorted arrays nums1 and nums2 of size m and n respectively. Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)). 大意就是找出两个排好序的数组中所有数的中位数。 最直观的做法是先将两个数组按从大到小合并成一个数组，再找出中位数。这样相当于遍历了两个数组，其时间复杂度是O(m+n)。实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738class Solution(object): def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ index1=0 index2=0 mergedArr=[] while index1&lt;len(nums1) and index2&lt;len(nums2): if nums1[index1] &gt; nums2[index2]: mergedArr.append(nums2[index2]) index2+=1 elif nums1[index1] &lt; nums2[index2]: mergedArr.append(nums1[index1]) index1+=1 else: #equal mergedArr.append(nums1[index1]) mergedArr.append(nums2[index2]) index1+=1 index2+=1 if index1!=len(nums1): while index1&lt;len(nums1): mergedArr.append(nums1[index1]) index1+=1 if index2!=len(nums2): while index2&lt;len(nums2): mergedArr.append(nums2[index2]) index2+=1 mergedLen=len(mergedArr) if mergedLen %2 == 0: #even length return float(mergedArr[mergedLen/2]+mergedArr[mergedLen/2-1])/2 else: return float (mergedArr[(mergedLen-1)/2]) 尽管上面的时间复杂度是O(m+n)，但是实际的提交时也能AC。 虽然上面的方法也能够AC，但是根据题目的提示的O(log(m+n))时间复杂度，显然还有更好的解法。下面就来讨论一种时间复杂度为O(log(m+n))的算法。 提到时间复杂度为O(log(m+n))的算法，很容易想到的就是二分查找，所以现在要做的就是在两个排序数组中进行二分查找。 具体思路如下，可以将问题转化为在两个数组中找第K个大的数，先在两个数组中分别找出第k/2大的数，再比较这两个第k/2大的数，这里假设两个数组为A,B。那么比较结果会有下面几种情况： A[k/2]=B[k/2],那么第k大的数就是A[k/2] A[k/2]&gt;B[k/2],那么第k大的数肯定在A[0:k/2+1]和B[k/2:]中，这样就将原来的所有数的总和减少到一半了，再在这个范围里面找第k/2大的数即可，这样也达到了二分查找的区别了。 A[k/2] &lt; B[k/2]，那么第k大的数肯定在B[0:k/2+1]和A[k/2:]中,同理在这个范围找第k/2大的数就可以了。 上面思路的实现代码如下 123456789101112131415161718192021222324class Solution: def findMedianSortedArrays(self, A, B): n = len(A) + len(B) if n % 2 == 1: return self.findKth(A, B, n / 2 + 1) else: smaller = self.findKth(A, B, n / 2) bigger = self.findKth(A, B, n / 2 + 1) return (smaller + bigger) / 2.0 def findKth(self, A, B, k): if len(A) == 0: return B[k - 1] if len(B) == 0: return A[k - 1] if k == 1: return min(A[0], B[0]) a = A[k / 2 - 1] if len(A) &gt;= k / 2 else None b = B[k / 2 - 1] if len(B) &gt;= k / 2 else None if b is None or (a is not None and a &lt; b): return self.findKth(A[k / 2:], B[0:k/2+1], k - k / 2) return self.findKth(A[0:k/2+1], B[k / 2:], k - k / 2)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode解题报告(3)--双指针找最长不重复子串]]></title>
      <url>%2F2016%2F01%2F08%2FLeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A(3)--%E5%8F%8C%E6%8C%87%E9%92%88%E6%89%BE%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2%2F</url>
      <content type="text"><![CDATA[原题如下 Given a string, find the length of the longest substring without repeating characters. For example, the longest substring without repeating letters for “abcabcbb” is “abc”, which the length is 3. For “bbbbb” the longest substring is “b”, with the length of 1. 这道题是典型的找最长子字符串问题，就是要找出没有重复的最长子字符串。本文讲述了两种方法，一种时间复杂度为$O(n^2)$，另一种的时间复杂度为$O(n)$ 最容易想到的方法就是遍历整个字符串，找出以每个字符开头的最长子字符串，再从中找出最长的子字符串。这样的时间复杂度是$O(n^2)$，实现代码如下 123456789101112131415161718class Solution(object): def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ max=0 for i in range(len(s)): tmp=[] tmp.append(s[i]) for j in range(i+1,len(s)): if s[j] not in tmp: tmp.append(s[j]) else: if len(tmp)&gt;max: max=len(tmp) break return max 这种方法虽然易于理解，但是由于时间复杂度问题，运行超时。 现在再分析一下题目，是否有必要计算以每个字符开头的最长子字符串？ 答案是不必要的。比如说对于字符串abcdedfghi,从a开始往后进行判断，则遇到第二个d的时候会发现与前面的d重复了，判断停止，那么以a开头的最长子字符串为abcde，然后以一个新的字母开头找最长的子字符串。按照上面的方法，接下来会以b为开头找其最长子字符串。但是你会发现找到的最长子字符串肯定比以a开头的最长子字符串要短，原因是后面重复的元素位置没有变，而从b或c开始相当于把不重复的部分截断了。所以如果要找到可能更长的子字符串，必须要从与当前字符重复的字符往后的一个字符位置开始找，在本例中就是要从e开始找，才有可能找得到比以a开头的最长子字符串更长的子字符串。 这样，便可以通过找与当前字符重复的前一字符的位置来避免第二层的for循环，将时间复杂度缩短为$O(n)$ 通过双指针能够实现上面的功能，左右指针共同维护一个没有重复字符的子字符串，每当没有重复字符时，右指针一直往右移动；有重复字符时左指针往右移动直到左右指针之间没有重复字符。12345678910111213141516171819class Solution(object): def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ left, right, n = 0, 0, len(s) visited, result = set(), 0 while right &lt; n: if s[right] in visited: while s[right] in visited: visited.remove(s[left]) left += 1 visited.add(s[right]) else: visited.add(s[right]) result = max(right-left+1, result) right += 1 return result 上面的方法虽然能够AC,但是每次遇到重复元素时左指针往右一步一步地移动并删除元素的操作会比较慢，因此可以 通过 HashTable 存储每个元素的最大下标，遇到重复元素的时候判断前一重复元素位置的下标是否大于当前左指针位置，若是则更新左指针，否则不更新；同时更新HashTable 当前重复元素的下标。下面是实现的代码 123456789101112131415class Solution(object): def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ left, right,result= 0, 0, 0 indices = &#123;&#125; while right &lt; len(s): if s[right] in indices and indices[s[right]] &gt;= left: result = max(result, right - left) left = indices[s[right]] + 1 indices[s[right]] = right right += 1 return max(result, right - left)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[物理CPU、CPU核数、逻辑CPU、超线程]]></title>
      <url>%2F2016%2F01%2F06%2F%E7%89%A9%E7%90%86CPU%E3%80%81CPU%E6%A0%B8%E6%95%B0%E3%80%81%E9%80%BB%E8%BE%91CPU%E3%80%81%E8%B6%85%E7%BA%BF%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[由于经常会混淆这几个概念，所以特意借该文比较详细地记录这几个概念的区别以及在Linux下如何查看这几个参数。 基本概念 物理CPU：物理CPU就是插在主机上的真实的CPU硬件，在Linux下可以数不同的physical id 来确认主机的物理CPU个数。 核心数：物理CPU下一层概念就是核心数，我们常常会听说多核处理器，其中的核指的就是核心数。在Linux下可以通过cores来确认主机的物理CPU的核心数。 逻辑CPU：核心数下一层的概念是逻辑CPU，逻辑CPU跟超线程技术有联系，假如物理CPU不支持超线程的，那么逻辑CPU的数量等于核心数的数量；如果物理CPU支持超线程，那么逻辑CPU的数目是核心数数目的两倍。在Linux下可以通过 processors 的数目来确认逻辑CPU的数量。 超线程：超线程是英特尔开发出来的一项技术，使得单个处理器可以象两个逻辑处理器那样运行，这样单个处理器以并行执行线程。这里的单个处理器也可以理解为CPU的一个核心；这样便可以理解为什么开启了超线程技术后，逻辑CPU的数目是核心数的两倍了。 在Linxu下查看物理cpu、核心数、逻辑CPU和是否支持超线程关于CPU的一些信息可在 /proc/cpuinfo 这个文件中查看，这个文件显示的内容类似于下图所示 可以看到里面的内容是以 processor （也就是逻辑CPU）为基本单元进行划分的，processor 下的 core id表示这个逻辑CPU属于哪个核心，而physical id则表示这个核心或者说逻辑CPU属于哪个物理CPU。了解这些信息，便可以方便地查看上面说到的那些参数。 查看物理CPU数量物理CPU就是不同的phycical id的个数，可通过下面命令实现：1cat /proc/cpuinfo | grep 'physical id' | uniq |wc -l uniq是为了去掉多个逻辑CPU属于同一个物理CPU的重复记录。 查看核心数核心数就是不同core id的个数，可通过下面的命令实现1cat /proc/cpuinfo | grep 'core id' | uniq |wc -l 原理同上 查看逻辑CPU数目逻辑CPU就是processor的数目1cat /proc/cpuinfo | grep 'processor' | wc -l 查看逻辑CPU时不需要去重 查看是否支持超线程如果支持超线程就是说同一个core下会有两个processors，这样可以简单地观察/proc/cpuinfo中的内容，如果两个的processor下的core id相同，那么说明支持超线程。 还有另外一种方法是查看siblings和cpu cores的数值是否一致，评判方法如下 如果”siblings”和”cpu cores”一致，则说明不支持超线程，或者超线程未打开。 如果”siblings”是”cpu cores”的两倍，则说明支持超线程，并且超线程已打开。 另外，top命令中看到的CPU数目是逻辑CPU（输入top后再按1）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[什么才算是真正的编程能力？]]></title>
      <url>%2F2016%2F01%2F04%2F%E4%BB%80%E4%B9%88%E6%89%8D%E7%AE%97%E6%98%AF%E7%9C%9F%E6%AD%A3%E7%9A%84%E7%BC%96%E7%A8%8B%E8%83%BD%E5%8A%9B%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[本文综合整理自知乎同名问答帖链接：https://www.zhihu.com/question/31034164/ 题主问题： 还在读书，也在实验室帮忙做了些东西，自己也搭过几个网站。在周围人看来似乎好像我很厉害，做了那么多东西，但是我发现这些东西虽然是我做的，但是实际上我手把手自己写的代码却并没有多少，很多都是用开源的东西，我写的代码无非是把别人的东西整合下，类似于胶水一样的工作。我之前所认为的编程是全手动一行一行敲代码，但是现在我发现哪怕是工程上，也有很多人是复制黏贴来解决问题的，并且提倡不要重复造轮子。 但是靠谷歌和复制别人的轮子，虽然我做出了很多东西，可是我并不觉得自己能力上有提升，倒是利用搜索引擎的能力的确提升了不少。而学校里另外一部分在搞ACM的人，他们每天都在刷题练算法，但单凭我个人的感受感觉他们似乎对工程上有些东西并不了解，或许算法的能力才算是实打实的编程能力？那”胶水”的能力和整合轮子的能力算不算编程能力呢? 所以我现在就很困惑，所谓的编程能力到底是什么，我该如何提升自己的编程能力？ 下面是两个人的回答： 下面是刘贺的回复,专栏：http://zhuanlan.zhihu.com/shanhu链接：https://www.zhihu.com/question/31034164/answer/50423838 非常好的一个问题。这可能是我在知乎见到过的问编程有关的问题中问得最好的一个了。我非常喜欢这个问题。 计算机科学有两类根本问题。一类是理论：算法，数据结构，复杂度，机器学习，模式识别，等等等。一类是系统：操作系统，网络系统，分布式系统，存储系统，游戏引擎，等等等等。 理论走的是深度，是在追问在给定的计算能力约束下如何把一个问题解决得更快更好。而系统走的是广度，是在追问对于一个现实的需求如何在众多的技术中设计出最多快好省的技术组合。 搞ACM的人，只练第一类。像你这样的更偏向于第二类。其实挺难得的，但很可惜的是第二类能力没有简单高效的测量考察方法，不像算法和数据结构有ACM竞赛，所以很多系统的苗子都因为缺少激励和正确引导慢慢就消隐了。 所以比尔盖茨才会说，看到现在学编程的人经常都把编程看作解各种脑筋急转弯的问题，他觉得很遗憾。 做系统，确实不提倡“重复发明轮子”。但注意，是不提倡“重复发明”，不是不提倡“重新制造”。恰恰相反的，我以为，系统的编程能力正体现在“重新制造”的能力。 能把已有的部件接起来，这很好。但当你恰好缺一种关键的胶水的时候，你能写出来吗？当一个已有的部件不完全符合你的需求的时候，你能改进它吗？如果你用的部件中有bug，你能把它修好吗？在网上繁多的类似功能的部件中，谁好谁坏？为什么？差别本质吗？一个开源代码库，你能把它从一个语言翻译到另一个语言吗？从一个平台移植到另一个平台吗？能准确估计自己翻译和移植的过程需要多少时间吗？能准确估计翻译和移植之后性能是会有提升还是会有所下降吗？ 系统编程能力体现在把已有的代码拿来并变成更好的代码，体现在把没用的代码拿来并变成有用的代码，体现在把一个做好的轮子拿来能画出来轮子的设计蓝图，并用道理解释出设计蓝图中哪些地方是关键的，哪些地方是次要的，哪些地方是不容触碰的，哪些地方是还可以改进的。 如果你一点不懂理论，还是应该学点的。对于系统性能的设计上，算法和数据结构就像在自己手头的钱一样，它们不是万能的，但不懂是万万不行的。 怎么提高系统编程能力呢？土办法：多造轮子。就像学画画要画鸡蛋一样，不是这世界上没有人会画鸡蛋，但画鸡蛋能驯服手指，感受阴影线条和笔触。所以，自己多写点东西吧。写个编译器？渲染器？操作系统？web服务器？web浏览器？部件都一个个换成自己手写的，然后和已有的现成部件比一比，看看谁的性能好，谁的易用性好？好在哪儿？差在哪儿？为什么？ 更聪明一点的办法：多拆轮子。多研究别人的代码是怎么写的。然而这个实践起来经常很难。原因：大部分工业上用的轮子可能设计上的思想和技术是好的，都设计和制造过程都很烂，里面乱成一团，让人乍一看毫无头绪，导致其对新手来说非常难拆。这种状况其实非常糟糕。所以，此办法一般只对比较简单的轮子好使，对于复杂的轮子，请量力而行。 轮子不好拆，其实是一个非常严重的问题。重复发明轮子固然是时间的浪费，但当轮子复杂而又不好拆的时候，尤其是原来造轮子的人已经不在场的时候，重新发明和建造轮子往往会成为无奈之下最好的选择。这是为什么工业界在明知道重复发明/制造轮子非常不好的情况下还在不断重复发明/制造轮子的根本原因。 程序本质是逻辑演绎的形式化表达，记载的是人类对这个世界的数字化理解。不能拆的轮子就像那一篇篇丢了曲谱的宋词一样，能读，却不能唱。 鄙人不才，正在自己研究怎么设计建造一种既好用又好拆的轮子。您没那么幸运，恐怕是等不到鄙人的技术做出来并发扬光大了。在那之前，多造轮子，多拆好拆的小轮子，应该是提高编程能力最好的办法了。 以上。嗯。 （文章属个人观点，与本人工作雇主无关。） 下面是疯坦克的回答链接：https://www.zhihu.com/question/31034164/answer/74716106 真正的编程能力其实并不是对语法细节的理解，也不在于手写或者复制粘贴，更不在于对什么操作系统的使用，或者常用库的api的记忆。而是找出解决方法的能力，把现实问题转换为代码逻辑的能力。这个是最重要的。语法很好学，只要看一看，再不行网上搜一搜都有，但是解决问题的能力，在网上搜不到，找不来，谁也帮不了。只能在长期的分析问题解决问题的过程中得到。 在工作中，见过太多面试的时候打高分，把什么const char*, char const*, char*const i+++++i这种奇技淫巧玩的烂熟，解决问题的时候一筹莫展的。只能你清晰明了的告诉他流程他才能实现。这样的人，要是不思进取，沉浸在这种很多公司禁用的语法技巧里沾沾自喜，可能永远只能是个代码流水线工人。也有很多人面试的时候各种语法都模棱两可，提起做过的项目和程序，却能够条理清楚，头头是道。给他一个问题，他几分钟就给出还不错的解决方案。这样的人，随便什么语言，什么语法，什么库，对他来说都是工具。他知道与否，都能最终解决问题。其实不管是复制黏贴也好，自己手写也好，关键的是解决问题。编程最终还是个生产工具，目的是解决问题，不能解决问题的，一切都是空中楼阁，毫无价值。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[手机监控服务器登陆情况]]></title>
      <url>%2F2016%2F01%2F03%2F%E6%89%8B%E6%9C%BA%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%99%BB%E9%99%86%E6%83%85%E5%86%B5%2F</url>
      <content type="text"><![CDATA[通过往手机发短信提醒用户登录的方式也许有很多种，下面讲一种最容易实现的，实现起来也比较简单的。 原理很简单:中国移动提供139.com这样的邮箱,如果有邮件到达的会同时发送邮件标题到管理员对应手机，邮箱名是 你的手机号@139.com。例如:当13036110648@139.com邮箱接收到邮件时，会同时给13036110648这个手机发送邮件到达信息,邮箱注册地址http://mail.139.com/。 其次，用户登录的时候会自动加载其用户主目录下的.bashrc文件,那么我们可以在这个脚本里面加入执行发送邮件的命令，发送的内容为当前登录的用户及来源。 发送邮件的命令为mail,如果提示找不到这个命令需要安装mailx这个软件包，发送邮件的命令如下所示：mail -s &quot;邮件主题&quot; XXX@139.com &lt; 文本形式的邮件 文本形式的邮件里面的内容可以为空，这里的内容是记录该用户所有的登录记录。 只需要在当前用户(这里以test用户为例)主目录下的.bashrc文件添加下面这些内容即可12echo "$(who am i)" &gt;&gt; /home/test/login_history.logmail -s "$(who am i)" 手机号@139.com &lt;/home/test/login_history.log 这样每一次test用户登录都会发邮件到139邮箱，邮件主题是这次登陆的一些信息，正文内容则是这个用户的所有登录记录。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通配符与正则表达式]]></title>
      <url>%2F2016%2F01%2F01%2F%E9%80%9A%E9%85%8D%E7%AC%A6%E4%B8%8E%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[通配符与正则表达式均可以匹配符合某些格式的字符串，但是通配符一般用于Linux的shell下，而正则表达式适用范围则更广，不仅Linux的shell下支持（grep、awk、sed等工具），而且很多程序语言也支持（Java，Python等）。 通配符 通配符 ? 匹配文件名中的单个字符，而通配符 * 匹配零个或多个字符。像 data?.dat 这样的模式将查找下列文件： 1234data1.datdata2.datdatax.datdataN.dat 使用 * 字符代替 ? 字符扩大了找到的文件的数量。data*.dat 匹配下列所有文件： 123456data.datdata1.datdata2.datdata12.datdatax.datdataXYZ.dat 正则表达式(regular expression)特殊字符特殊字符就是有特殊含义的字符，下面的表格将列出一些常用的特殊字符，如果要匹配这些特殊字符可以在其之前加上反斜杠\ 特殊字符 含义 . 匹配除了\n以外的任何单个字符，与通配符中的?作用相同 ？ 匹配前面的子表达式零次或一次，如.?表示没有字符或一个任意字符，he(hee)?表示he或hehee + 匹配前面的子表达式一次或多次，如.+表示一个或一个以上的任意字符 * 匹配前面的子表达式零次或多次，如.*表示一个或一个以上的任意字符 [ ] 匹配中括号内任意一个字符，如[Pp]ython表示Python或python,[a-z]匹配任何一个小写字母，[a-zA-Z0-9]匹配任何一个字母或数字 ( ) 标记一个字表达式的开始和结束 { } 花括号里面放的是限定符表达式，用来匹配前面指定的表达式重复的次数，如？等价于{0,1}，+等价于{1,} ^ 在方括号[]内使用表示不包含方括号内任意元素，其余情况均表示匹配输入字符串的起始位置 $ 匹配输入字符串的末尾 \b 表示字符和空格间的位置（也叫作字边界） \B 任何其他非字边界的位置 \w 匹配包括下划线的任何单词字符。等价于’[A-Za-z0-9_ ]’ \W 匹配任何非单词字符。等价于 ‘[^A-Za-z0-9_ ] \d 匹配一个数字字符。等价于 [0-9] \D 匹配一个非数字字符。等价于 [^0-9] 贪婪匹配和非贪婪匹配对于字符串&lt;H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1&gt; 如果匹配的正则表达式为&lt;(.*)&gt;，则为贪婪匹配，匹配出来的内容为H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1，正则表达式没有括号时(即为&lt;.*&gt;)，匹配的内容为&lt;H1&gt;Chapter 1 – Introduction to Regular Expressions&lt;/H1&gt; 如果匹配的正则表达式为&lt;(.*?)&gt;，则为非贪婪匹配，匹配出来的内容为H1和/H1 这样应该可以看出非贪婪匹配（又称最小匹配）和贪婪匹配的区别了，贪婪匹配就是匹配符合正则表达式的尽可能多的内容，而非贪婪匹配则是匹配符合正则表达式的第一个子串（如果有多个符合）。 一些例子 正则表达式 匹配内容 [^\\/\^] 除了(\)(/)(^)之外的所有字符 [^\”\’] 除了双引号(“)和单引号(‘)之外的所有字符 .{2} 所有的两个字符 \t{2} 两个制表符 ^a{2,}\$ 以两个或两个以上的a开头的字符串 ^a{2,4}\$ aa,aaa或aaaa ^[0-9]{1,}\$ 所有的正数 ^\-{0,1}[0-9]{1,}\$ 所有的整数 ^\-?[0-9]*\.?[0-9]+\$ 所有小数 关于正则表达式的一些基本概念就是上面这些，当然正则表达式的功能比上面所说的还要强大得多。只是无需记住所有的特性，在需要用到的时候google一下即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sae上通过python获取访问网站ip及其来源]]></title>
      <url>%2F2015%2F12%2F22%2Fsae%E4%B8%8A%E9%80%9A%E8%BF%87python%E8%8E%B7%E5%8F%96%E8%AE%BF%E9%97%AE%E7%BD%91%E7%AB%99ip%E5%8F%8A%E5%85%B6%E6%9D%A5%E6%BA%90%2F</url>
      <content type="text"><![CDATA[这篇文章是当时在新浪云上搭建博客时写的，后来因为新浪云开始各种收费了，所以就把博客转到了github上。这里还是把文章贴出来，做个记录 常常看到有些网站会显示访问过该网站的所有人数及其分布地点，所以就琢磨着这个怎么实现，好给自己的网站也添加上去；在google上搜了一下发现大都是通过分析日志得出的，而新浪云上也提供了日志访问的API，所以下面就说说怎么通过这个API获取访问的IP及其来源地。 大致的步骤就是先通过身份校验获取访问日志的权限，然后通过HTTP请求摘取日志中表示访问ip和访问次数的段记录。剔除其中的私网IP，再获取IP所在地，存入数据库。 下面为具体的实施步骤 获取访问的IP及其访问次数身份验证这个是新浪提供的用于校验身份的一个api，校验身份是通过应用的ACESSKEY和SECRETKEY来实现的。 代码下载链接地址：https://raw.githubusercontent.com/sinacloud/sae-python-dev-guide/master/examples/apibus/apibus_handler.py 也可复制下面的代码创建一个python源文件，命名为apibus_handler.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#-*-coding: utf8 -*-"""SAE API auth handler for urllib2 and requestsurllib2:&gt;&gt;&gt; import urllib2&gt;&gt;&gt; apibus_handler = SaeApibusAuthHandler(ACCESSKEY, SECRETKEY)&gt;&gt;&gt; opener = urllib2.build_opener(apibus_handler)&gt;&gt;&gt; print opener.open('http://g.sae.sina.com.cn/log/http/2015-06-18/1-access.log').read()requests:&gt;&gt;&gt; import requests&gt;&gt;&gt; print requests.get('http://g.sae.sina.com.cn/log/http/2015-06-18/1-access.log?head/0/10|fields/ /1/2/3/4', auth=SaeApibusAuth(ACCESSKEY, SECRETKEY)).content"""import hmacimport base64import hashlibimport timeimport urllibfrom urllib2 import BaseHandler, Request_APIBUS_URL_PREFIX = 'http://g.sae.sina.com.cn/'class SaeApibusAuthHandler(BaseHandler): # apibus handler must be in front handler_order = 100 def __init__(self, accesskey, secretkey): self.accesskey = accesskey self.secretkey = secretkey def http_request(self, req): orig_url = req.get_full_url() if not orig_url.startswith(_APIBUS_URL_PREFIX): return req timestamp = str(int(time.time())) headers = [ ('x-sae-timestamp', timestamp), ('x-sae-accesskey', self.accesskey), ] req.headers.update(headers) method = req.get_method() resource = urllib.unquote(req.get_full_url()[len(_APIBUS_URL_PREFIX)-1:]) sae_headers = [(k.lower(), v.lower()) for k, v in req.headers.items() if k.lower().startswith('x-sae-')] req.add_header('Authorization', _signature(self.secretkey, method, resource, sae_headers)) return req https_request = http_requesttry: from requests.auth import AuthBase class SaeApibusAuth(AuthBase): """Attaches HTTP Basic Authentication to the given Request object.""" def __init__(self, accesskey, secretkey): self.accesskey = accesskey self.secretkey = secretkey def __call__(self, r): timestamp = str(int(time.time())) r.headers['x-sae-timestamp'] = timestamp r.headers['x-sae-accesskey'] = self.accesskey resource = urllib.unquote(r.url[len(_APIBUS_URL_PREFIX)-1:]) #resource = r.url[len(_APIBUS_URL_PREFIX)-1:] sae_headers = [(k.lower(), v.lower()) for k, v in r.headers.items() if k.lower().startswith('x-sae-')] r.headers['Authorization'] = _signature(self.secretkey, r.method, resource, sae_headers) return rexcept ImportError: # requests was not present! passdef _signature(secret, method, resource, headers): msgToSign = "\n".join([ method, resource, "\n".join([(k + ":" + v) for k, v in sorted(headers)]), ]) return "SAEV1_HMAC_SHA256 " + base64.b64encode(hmac.new(secret, msgToSign, hashlib.sha256).digest()) 通过http请求获取日志提供了通过requests包和urllib包两种方式，代码来源后面的参考文章,将下面代码保存成sae_log_util.py即可：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#-*-coding: utf8 -*-#sae_log_util.py#sae log utility based on sae apibus_handler#author blog: http://bookshadow.com#src date: 2015-09-17status_code_dict = &#123;200 : 'OK', 206 : 'Partial Content', 400 : 'Bad Request', \ 500 : 'Internal Server Error' , 404 : 'Not Found'&#125;service_ident_dict = &#123;'http': ['access', 'error', 'alert', 'debug', 'warning', 'notice'], \ 'taskqueue' : ['error'], \ 'cron' : ['error'], \ 'mail': ['access', 'error'], \ 'rdc' : ['error', 'warning'], \ 'storage' : ['access'], \ 'push' : ['access'], \ 'fetchurl' : ['access']&#125;_URL_PREFIX = 'http://g.sae.sina.com.cn/log/'class SaeLogFetcher(object): def __init__(self, access_key, secret_key): self.access_key = access_key self.secret_key = secret_key def fetch_log(self, service, date, ident, fop = '', version = 1): assert self.access_key, 'access_key should not be empty' assert self.secret_key, 'secret_key should not be empty' assert service in service_ident_dict, 'invalid service parameter' assert ident in service_ident_dict[service], 'invalid ident parameter' url = _URL_PREFIX + service + '/' + date + '/' + str(version) + '-' + ident + '.log' content = None try: import requests from apibus_handler import SaeApibusAuth r = requests.get(url + ('?' + fop if fop else ''), \ auth=SaeApibusAuth(self.access_key, self.secret_key)) status_code, status = r.status_code, status_code_dict.get(r.status_code, 'Unknown') if status_code == 200: content = r.content except ImportError: # requests was not present! from apibus_handler import SaeApibusAuthHandler import urllib, urllib2 apibus_handler = SaeApibusAuthHandler(self.access_key, self.secret_key) opener = urllib2.build_opener(apibus_handler) if fop: url += '?' + urllib.quote(fop, safe='') content = opener.open(url).read() return content 调用上面的代码下面通过代码获取访问过的ip及次数,代码也是来源于参考链接，可将代码复制后另存为ip_counter.py：1234567891011121314151617181920212223242526#-*-coding: utf8 -*-#ip_counter.py#ip counter based on sae_log_util#author blog: http://bookshadow.com#src date: 2015-09-17from collections import Counterfrom sae_log_util import SaeLogFetcherdate = &apos;2015-09-16&apos;service = &apos;http&apos;ident = &apos;access&apos;fop = &apos;fields/ /2&apos; #fetch ip onlyversion = 1ACCESSKEY = &apos;&lt;&lt;ACCESSKEY&gt;&gt;&apos;SECRETKEY = &apos;&lt;&lt;SECRETKEY&gt;&gt;&apos;log_fetcher = SaeLogFetcher(ACCESSKEY, SECRETKEY)result = log_fetcher.fetch_log(service, date, ident, fop, version)content = result.split(&apos;\n&apos;)[:-1]for e, c in Counter(content).most_common(): print e, c 将代码内的&lt;&lt;ACCESSKEY&gt;&gt;与&lt;&lt;SECRETKEY&gt;&gt;替换为你的sae应用具体的值。 然后将上面的代码放到同一个工作目录，执行ip_counter.py这个文件，即可获取访问的ip， 剔除私网IP上面显示出出来的结果会显示出有私网ip，猜测是sae内部一些服务器间的通信，比如说memcached、mysql等服务与应用不在同一台服务器等，但是无论如何，这些私网ip都是我们不希望看到的，所以下面是剔除私网IP的过程。 私网IP总共有A、B、C三类，而每一类IP的nei-id均是固定的，详见下面所示：123A类：10.0.0.0/8： 10.0.0.0～10.255.255.255B类：172.16.0.0/12： 172.16.0.0～172.31.255.255C类：192.168.0.0/16： 192.168.0.0～192.168.255.255 这样便可将IP移位后与三类私网IP的net-id比较，从而判断该IP是否属于私网IP。实现代码如下： 1234567891011def ip_into_int(ip): #以192.168.1.13为例，先把 192.168.1.13 变成16进制的 c0.a8.01.0d ，再去了“.”后转成10进制的 3232235789 即可。 #(((((192 * 256) + 168) * 256) + 1) * 256) + 13 return reduce(lambda x,y:(x&lt;&lt;8)+y,map(int,ip.split('.')))def is_internal_ip(ip): ip = ip_into_int(ip) net_a = ip_into_int('10.255.255.255') &gt;&gt; 24 net_b = ip_into_int('172.31.255.255') &gt;&gt; 20 net_c = ip_into_int('192.168.255.255') &gt;&gt; 16 return ip &gt;&gt; 24 == net_a or ip &gt;&gt;20 == net_b or ip &gt;&gt; 16 == net_c 查询IP所在地可以通过淘宝提供的API来查询IP所在地，查询代码如下所示： 1234567891011121314151617181920#encoding:utf-8import requests#输入的ip的数据结构为字典:&#123;'ip':具体的ip地址&#125;def find_ip_place(ip): URL = 'http://ip.taobao.com/service/getIpInfo.php' try: r = requests.get(URL, params=ip, timeout=3) except requests.RequestException as e: print(e) else: json_data = r.json() if json_data['code'] == 0: print u'所在国家:',json_data[u'data'][u'country'] print u'所在地区:',json_data[u'data'][u'area'] print u'所在省份:',json_data[u'data'][u'region'] print u'所在城市:',json_data[u'data'][u'city'] print u'所属运营商:',json_data[u'data'][u'isp'] else: print '查询失败,请稍后再试！' 然后就可以将获取到的关于ip的信息存入数据库，同时存入更新时间，这样便在数据库中有了访问网站的记录，便于后续的可视化分析。 为了方便，还是利用了sae的cron服务每天定时将昨天的访问记录存入数据库。 参考：使用SAE实时日志API统计IP来访次数 SAE实时日志API Python使用小记 Python判断内网IP]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java解释XML的四种方法]]></title>
      <url>%2F2015%2F12%2F21%2FJava%E8%A7%A3%E9%87%8AXML%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
      <content type="text"><![CDATA[最近毕设需要用Java解释XML文件，在网上搜到了Java解释XML的一篇比较详细的文章，原文链接,下面为原文众所周知，现在解析XML的方法越来越多，但主流的方法也就四种，即：DOM、SAX、JDOM和DOM4J 下面首先给出这四种方法的jar包下载地址 DOM：在现在的Java JDK里都自带了，在xml-apis.jar包里 SAX：http://sourceforge.net/projects/sax/ JDOM：http://jdom.org/downloads/index.html DOM4J：http://sourceforge.net/projects/dom4j/ 介绍及优缺点分析DOM（Document Object Model)DOM是用与平台和语言无关的方式表示XML文档的官方W3C标准。DOM是以层次结构组织的节点或信息片断的集合。这个层次结构允许开发人员在树中寻找特定信息。分析该结构通常需要加载整个文档和构造层次结构，然后才能做任何工作。由于它是基于信息层次的，因而DOM被认为是基于树或基于对象的。 【优点】 ①允许应用程序对数据和结构做出更改。 ②访问是双向的，可以在任何时候在树中上下导航，获取和操作任意部分的数据。【缺点】 ①通常需要加载整个XML文档来构造层次结构，消耗资源大。 SAX（Simple API for XML)SAX处理的优点非常类似于流媒体的优点。分析能够立即开始，而不是等待所有的数据被处理。而且，由于应用程序只是在读取数据时检查数据，因此不需要将数据存储在内存中。这对于大型文档来说是个巨大的优点。事实上，应用程序甚至不必解析整个文档；它可以在某个条件得到满足时停止解析。一般来说，SAX还比它的替代者DOM快许多。 选择DOM还是选择SAX？ 对于需要自己编写代码来处理XML文档的开发人员来说， 选择DOM还是SAX解析模型是一个非常重要的设计决策。 DOM采用建立树形结构的方式访问XML文档，而SAX采用的是事件模型。 DOM解析器把XML文档转化为一个包含其内容的树，并可以对树进行遍历。用DOM解析模型的优点是编程容易，开发人员只需要调用建树的指令，然后利用navigation APIs访问所需的树节点来完成任务。可以很容易的添加和修改树中的元素。然而由于使用DOM解析器的时候需要处理整个XML文档，所以对性能和内存的要求比较高，尤其是遇到很大的XML文件的时候。由于它的遍历能力，DOM解析器常用于XML文档需要频繁的改变的服务中。 SAX解析器采用了基于事件的模型，它在解析XML文档的时候可以触发一系列的事件，当发现给定的tag的时候，它可以激活一个回调方法，告诉该方法制定的标签已经找到。SAX对内存的要求通常会比较低，因为它让开发人员自己来决定所要处理的tag.特别是当开发人员只需要处理文档中所包含的部分数据时，SAX这种扩展能力得到了更好的体现。但用SAX解析器的时候编码工作会比较困难，而且很难同时访问同一个文档中的多处不同数据。 【优势】 ①不需要等待所有数据都被处理，分析就能立即开始。 ②只在读取数据时检查数据，不需要保存在内存中。 ③可以在某个条件得到满足时停止解析，不必解析整个文档。 ④效率和性能较高，能解析大于系统内存的文档。 【缺点】 ①需要应用程序自己负责TAG的处理逻辑（例如维护父/子关系等），文档越复杂程序就越复杂。 ②单向导航，无法定位文档层次，很难同时访问同一文档的不同部分数据，不支持XPath。 JDOM(Java-based Document Object Model)JDOM的目的是成为Java特定文档模型，它简化与XML的交互并且比使用DOM实现更快。由于是第一个Java特定模型，JDOM一直得到大力推广和促进。正在考虑通过“Java规范请求JSR-102”将它最终用作“Java标准扩展”。从2000年初就已经开始了JDOM开发。 JDOM与DOM主要有两方面不同。首先，JDOM仅使用具体类而不使用接口。这在某些方面简化了API，但是也限制了灵活性。第二，API大量使用了Collections类，简化了那些已经熟悉这些类的Java开发者的使用。 JDOM文档声明其目的是“使用20%（或更少）的精力解决80%（或更多）Java/XML问题”（根据学习曲线假定为20%）。JDOM对于大多数Java/XML应用程序来说当然是有用的，并且大多数开发者发现API比DOM容易理解得多。JDOM还包括对程序行为的相当广泛检查以防止用户做任何在XML中无意义的事。然而，它仍需要您充分理解XML以便做一些超出基本的工作（或者甚至理解某些情况下的错误）。这也许是比学习DOM或JDOM接口都更有意义的工作。 JDOM自身不包含解析器。它通常使用SAX2解析器来解析和验证输入XML文档（尽管它还可以将以前构造的DOM表示作为输入）。它包含一些转换器以将JDOM表示输出成SAX2事件流、DOM模型或XML文本文档。JDOM是在Apache许可证变体下发布的开放源码。 【优点】 ①使用具体类而不是接口，简化了DOM的API。 ②大量使用了Java集合类，方便了Java开发人员。 【缺点】 ①没有较好的灵活性。 ②性能较差。 DOM4J(Document Object Model for Java)虽然DOM4J代表了完全独立的开发结果，但最初，它是JDOM的一种智能分支。它合并了许多超出基本XML文档表示的功能，包括集成的XPath支持、XML Schema支持以及用于大文档或流化文档的基于事件的处理。它还提供了构建文档表示的选项，它通过DOM4J API和标准DOM接口具有并行访问功能。从2000下半年开始，它就一直处于开发之中。 为支持所有这些功能，DOM4J使用接口和抽象基本类方法。DOM4J大量使用了API中的Collections类，但是在许多情况下，它还提供一些替代方法以允许更好的性能或更直接的编码方法。直接好处是，虽然DOM4J付出了更复杂的API的代价，但是它提供了比JDOM大得多的灵活性。 在添加灵活性、XPath集成和对大文档处理的目标时，DOM4J的目标与JDOM是一样的：针对Java开发者的易用性和直观操作。它还致力于成为比JDOM更完整的解决方案，实现在本质上处理所有Java/XML问题的目标。在完成该目标时，它比JDOM更少强调防止不正确的应用程序行为。 DOM4J是一个非常非常优秀的Java XML API，具有性能优异、功能强大和极端易用使用的特点，同时它也是一个开放源代码的软件。如今你可以看到越来越多的Java软件都在使用DOM4J来读写XML，特别值得一提的是连Sun的JAXM也在用DOM4J. 【优点】 ①大量使用了Java集合类，方便Java开发人员，同时提供一些提高性能的替代方法。 ②支持XPath。 ③有很好的性能。 【缺点】 ①大量使用了接口，API较为复杂。 比较 DOM4J性能最好，连Sun的JAXM也在用DOM4J。目前许多开源项目中大量采用DOM4J，例如大名鼎鼎的Hibernate也用DOM4J来读取XML配置文件。如果不考虑可移植性，那就采用DOM4J. JDOM和DOM在性能测试时表现不佳，在测试10M文档时内存溢出，但可移植。在小文档情况下还值得考虑使用DOM和JDOM.虽然JDOM的开发者已经说明他们期望在正式发行版前专注性能问题，但是从性能观点来看，它确实没有值得推荐之处。另外，DOM仍是一个非常好的选择。DOM实现广泛应用于多种编程语言。它还是许多其它与XML相关的标准的基础，因为它正式获得W3C推荐（与基于非标准的Java模型相对），所以在某些类型的项目中可能也需要它（如在JavaScript中使用DOM）。 SAX表现较好，这要依赖于它特定的解析方式－事件驱动。一个SAX检测即将到来的XML流，但并没有载入到内存（当然当XML流被读入时，会有部分文档暂时隐藏在内存中）。 我的看法：如果XML文档较大且不考虑移植性问题建议采用DOM4J；如果XML文档较小则建议采用JDOM；如果需要及时处理而不需要保存数据则考虑SAX。但无论如何，还是那句话：适合自己的才是最好的，如果时间允许，建议大家讲这四种方法都尝试一遍然后选择一种适合自己的即可。 示例为了节约篇幅，这里暂时不给出这四种建立XML文档的方法与差异，仅给出解析XML文档的代码，如果需要完整工程（建立XML文档+解析XML+测试比较），可去我的CSDN下载。 这里以下面的XML内容为例进行解析：1234567891011121314151617181920212223&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;users&gt; &lt;user id="0"&gt; &lt;name&gt;Alexia&lt;/name&gt; &lt;age&gt;23&lt;/age&gt; &lt;sex&gt;Female&lt;/sex&gt; &lt;/user&gt; &lt;user id="1"&gt; &lt;name&gt;Edward&lt;/name&gt; &lt;age&gt;24&lt;/age&gt; &lt;sex&gt;Male&lt;/sex&gt; &lt;/user&gt; &lt;user id="2"&gt; &lt;name&gt;wjm&lt;/name&gt; &lt;age&gt;23&lt;/age&gt; &lt;sex&gt;Female&lt;/sex&gt; &lt;/user&gt; &lt;user id="3"&gt; &lt;name&gt;wh&lt;/name&gt; &lt;age&gt;24&lt;/age&gt; &lt;sex&gt;Male&lt;/sex&gt; &lt;/user&gt;&lt;/users&gt; 首先定义XML文档解析的接口： 123456789101112131415/** * @author Alexia * * 定义XML文档解析的接口 */public interface XmlDocument &#123; /** * 解析XML文档 * * @param fileName * 文件全路径名称 */ public void parserXml(String fileName);&#125; DOM示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.xml;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.PrintWriter;import javax.xml.parsers.DocumentBuilder;import javax.xml.parsers.DocumentBuilderFactory;import javax.xml.parsers.ParserConfigurationException;import javax.xml.transform.OutputKeys;import javax.xml.transform.Transformer;import javax.xml.transform.TransformerConfigurationException;import javax.xml.transform.TransformerException;import javax.xml.transform.TransformerFactory;import javax.xml.transform.dom.DOMSource;import javax.xml.transform.stream.StreamResult;import org.w3c.dom.Document;import org.w3c.dom.Element;import org.w3c.dom.Node;import org.w3c.dom.NodeList;import org.xml.sax.SAXException;/** * @author Alexia * * DOM 解析XML文档 */public class DomDemo implements XmlDocument &#123; private Document document; public void parserXml(String fileName) &#123; try &#123; DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance(); DocumentBuilder db = dbf.newDocumentBuilder(); Document document = db.parse(fileName); NodeList users = document.getChildNodes(); for (int i = 0; i &lt; users.getLength(); i++) &#123; Node user = users.item(i); NodeList userInfo = user.getChildNodes(); for (int j = 0; j &lt; userInfo.getLength(); j++) &#123; Node node = userInfo.item(j); NodeList userMeta = node.getChildNodes(); for (int k = 0; k &lt; userMeta.getLength(); k++) &#123; if(userMeta.item(k).getNodeName() != "#text") System.out.println(userMeta.item(k).getNodeName() + ":" + userMeta.item(k).getTextContent()); &#125; System.out.println(); &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (ParserConfigurationException e) &#123; e.printStackTrace(); &#125; catch (SAXException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; SAX示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.xml;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.io.StringWriter;import javax.xml.parsers.ParserConfigurationException;import javax.xml.parsers.SAXParser;import javax.xml.parsers.SAXParserFactory;import javax.xml.transform.OutputKeys;import javax.xml.transform.Result;import javax.xml.transform.Transformer;import javax.xml.transform.TransformerConfigurationException;import javax.xml.transform.sax.SAXTransformerFactory;import javax.xml.transform.sax.TransformerHandler;import javax.xml.transform.stream.StreamResult;import org.xml.sax.Attributes;import org.xml.sax.SAXException;import org.xml.sax.helpers.AttributesImpl;import org.xml.sax.helpers.DefaultHandler;/** * @author Alexia * * SAX 解析XML文档 */public class SaxDemo implements XmlDocument &#123; public void parserXml(String fileName) &#123; SAXParserFactory saxfac = SAXParserFactory.newInstance(); try &#123; SAXParser saxparser = saxfac.newSAXParser(); InputStream is = new FileInputStream(fileName); saxparser.parse(is, new MySAXHandler()); &#125; catch (ParserConfigurationException e) &#123; e.printStackTrace(); &#125; catch (SAXException e) &#123; e.printStackTrace(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class MySAXHandler extends DefaultHandler &#123; boolean hasAttribute = false; Attributes attributes = null; public void startDocument() throws SAXException &#123; // System.out.println("文档开始打印了"); &#125; public void endDocument() throws SAXException &#123; // System.out.println("文档打印结束了"); &#125; public void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException &#123; if (qName.equals("users")) &#123; return; &#125; if (qName.equals("user")) &#123; return; &#125; if (attributes.getLength() &gt; 0) &#123; this.attributes = attributes; this.hasAttribute = true; &#125; &#125; public void endElement(String uri, String localName, String qName) throws SAXException &#123; if (hasAttribute &amp;&amp; (attributes != null)) &#123; for (int i = 0; i &lt; attributes.getLength(); i++) &#123; System.out.print(attributes.getQName(0) + ":" + attributes.getValue(0)); &#125; &#125; &#125; public void characters(char[] ch, int start, int length) throws SAXException &#123; System.out.print(new String(ch, start, length)); &#125;&#125; JDOM 示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.xml;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.util.List;import org.jdom2.Document;import org.jdom2.Element;import org.jdom2.JDOMException;import org.jdom2.input.SAXBuilder;import org.jdom2.output.XMLOutputter;/** * @author Alexia * * JDOM 解析XML文档 * */public class JDomDemo implements XmlDocument &#123; public void parserXml(String fileName) &#123; SAXBuilder builder = new SAXBuilder(); try &#123; Document document = builder.build(fileName); Element users = document.getRootElement(); List userList = users.getChildren("user"); for (int i = 0; i &lt; userList.size(); i++) &#123; Element user = (Element) userList.get(i); List userInfo = user.getChildren(); for (int j = 0; j &lt; userInfo.size(); j++) &#123; System.out.println(((Element) userInfo.get(j)).getName() + ":" + ((Element) userInfo.get(j)).getValue()); &#125; System.out.println(); &#125; &#125; catch (JDOMException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; DOM4J示例12345678910111213141516171819202122232425262728293031323334353637383940414243package com.xml;import java.io.File;import java.io.FileWriter;import java.io.IOException;import java.io.Writer;import java.util.Iterator;import org.dom4j.Document;import org.dom4j.DocumentException;import org.dom4j.DocumentHelper;import org.dom4j.Element;import org.dom4j.io.SAXReader;import org.dom4j.io.XMLWriter;/** * @author Alexia * * Dom4j 解析XML文档 */public class Dom4jDemo implements XmlDocument &#123; public void parserXml(String fileName) &#123; File inputXml = new File(fileName); SAXReader saxReader = new SAXReader(); try &#123; Document document = saxReader.read(inputXml); Element users = document.getRootElement(); for (Iterator i = users.elementIterator(); i.hasNext();) &#123; Element user = (Element) i.next(); for (Iterator j = user.elementIterator(); j.hasNext();) &#123; Element node = (Element) j.next(); System.out.println(node.getName() + ":" + node.getText()); &#125; System.out.println(); &#125; &#125; catch (DocumentException e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python中的模块与包]]></title>
      <url>%2F2015%2F12%2F19%2Fpython%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%9D%97%E4%B8%8E%E5%8C%85%2F</url>
      <content type="text"><![CDATA[文章为转载，原文见这里，侵删 python中的Module是比较重要的概念。常见的情况是，事先写好一个.py文 件，在另一个文件中需要 import 时，将事先写好的.py文件拷贝 到当前目录，或者是在sys.path中增加事先写好的.py文件所在的目录，然后import。这样的做法，对于少数文件是可行的，但如果程序数目很 多，层级很复杂，就很吃力了。 有没有办法，像Java的Package一样，将多个.py文件组织起来，以便在外部统一调用，和在内部互相调用呢？答案是有的。 主要是用到python的包的概念，python __init__.py在包里起一个比较重要的作用 要弄明白这个问题，首先要知道，python在执行import语句时，到底进行了什么操作，按照python的文档，它执行了如下操作：第1步，创建一个新的，空的module对象（它可能包含多个module）；第2步，把这个module对象插入sys.module中第3步，装载module的代码（如果需要，首先必须编译）第4步，执行新的module中对应的代码。 在执行第3步时，首先要找到module程序所在的位置，搜索的顺序是：当前路径 （以及从当前目录指定的sys.path）-&gt;然后是PYTHONPATH-&gt;然后是python的安装设置相关的默认路径。 正因为存在这样的顺序，如果当前 路径或PYTHONPATH中存在与标准module同样的module，则会覆盖标准module。也就是说，如果当前目录下存在xml.py，那么执 行import xml时，导入的是当前目录下的module，而不是系统标准的xml。 了解了这些，我们就可以先构建一个package，以普通module的方式导入，就可以直接访问此package中的各个module了。 Python中的package定义很简单，其层次结构与程序所在目录的层次结构相同，这一点与Java类似，唯一不同的地方在于，python中的package必须包含一个init.py的文件。例如，我们可以这样组织一个package: 123456789101112package1/ __init__.py subPack1/ __init__.py module_11.py module_12.py module_13.py subPack2/ __init__.py module_21.py module_22.py …… __init__.py可以为空，但是必须要存在，只要它存在，就表明此目录应被作为一个package处理。当然，init.py中也可以设置相应的内容，下文详细介绍。 好了，现在我们在module_11.py中定义一个函数： 123def funA(): print "funcA in module_11" return 在顶层目录（也就是package1所在的目录，当然也参考上面的介绍，将package1放在解释器能够搜索到的地方）运行python: 123&gt;&gt;&gt;from package1.subPack1.module_11 import funcA&gt;&gt;&gt;funcA()funcA in module_11 这样，我们就按照package的层次关系，正确调用了module_11中的函数。 有时在import语句中会出现通配符*，导入某个module中的所有元素，这是怎么实现的呢？ 答案就在init.py中。我们在subPack1的init.py文件中写 __all__ = [&#39;module_13&#39;, &#39;module_12&#39;] 然后进入python12345&gt;&gt;&gt;from package1.subPack1 import *&gt;&gt;&gt;module_11.funcA()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ImportError: No module named module_11 也就是说，以*导入时，package内的module是受__init__.py中的__all__列表限制的。 为了避免import后面跟的层级过长，可以在__init__.py中先导入所需的module。比如上面的例子可以改为下面所示 1234567891011#package1的 __init__.pyfrom subPack1 import *__all__=['module_11','module_13']&gt;&gt;&gt;from package1 import *&gt;&gt;&gt;module_11.funcA()funcA in module_11&gt;&gt;&gt;module_12.funcA()Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;ImportError: No module named module_12 下面看一下package内部互相调用 如果希望调用同一个package中的module，则直接import即可。也就是说，在module_12.py中，可以直接使用 import module_11 如果不在同一个package中，例如我们希望在module_21.py中调用module_11.py中的FuncA，则应该这样： from module_11的包名.module_11 import funcA]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python中的多进程]]></title>
      <url>%2F2015%2F12%2F15%2Fpython%E4%B8%AD%E7%9A%84%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[本文提到了python中实现多进程的几种方法（fork、multiprocessing、pool）以及进程间的简单通信。 fork()函数Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。 子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。 Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程： 123456789# multiprocessing.pyimport osprint 'Parent Process (%s) start...' % os.getpid()pid = os.fork()if pid==0: print 'I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid())else: print 'I (%s) just created a child process (%s).' % (os.getpid(), pid) 运行结果如下： 123Parent Process (876) start...I (876) just created a child process (877).I am child process (877) and my parent is 876. 由于Windows没有fork调用，上面的代码在Windows上无法运行。 有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。 multiprocessing模块如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？ 由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。 multiprocessing模块提供了一个Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束： 1234567891011121314151617181920212223#coding=utf-8#多进程from multiprocessing import Processfrom time import ctime,sleepimport os# 子进程要执行的代码def run_proc(name): print 'Run child process %s (%s),parent pid is (%s),start at %s...' % (name, os.getpid(),os.getppid(),ctime()) sleep(2)processes=[]p1 = Process(target=run_proc, args=('p1',))processes.append(p1)p2=Process(target=run_proc,args=('p2',))processes.append(p2)if __name__ == '__main__': print 'Parent Process pid(%s),start at %s...' %(os.getpid(),ctime()) for p in processes: p.start() p.join() print "all over at %s" %ctime() 上面的代码也是在Linux上执行（如果要在Windows下执行，可以去掉os的getppid()方法）执行结果如下： 1234Parent Process pid(30994),start at Tue Jan 12 10:37:48 2016...Run child process p1 (30995),parent pid is (30994),start at Tue Jan 12 10:37:48 2016...Run child process p2 (31013),parent pid is (30994),start at Tue Jan 12 10:37:50 2016...all over at Tue Jan 12 10:37:52 2016 创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单。 join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。 如何理解上面这句话？将上面的代码中的p.join()去掉，执行结果如下：1234Parent Process pid(31038),start at Tue Jan 12 10:39:27 2016...all over at Tue Jan 12 10:39:27 2016Run child process p1 (31039),parent pid is (31038),start at Tue Jan 12 10:39:27 2016...Run child process p2 (31040),parent pid is (31038),start at Tue Jan 12 10:39:27 2016... 从输出可以看到父进程还没等子进程结束就执行了最后的print &quot;all over at %s&quot; %ctime()语句，而且两个子进程的开始执行时间也相同。 Pool模块如果要启动大量的子进程，可以用进程池的方式批量创建子进程： 123456789101112131415161718192021#coding:utf-8#进程池from multiprocessing import Poolimport os, time, randomdef child_task(name): print 'Child Process %s (%s) starts' % (name, os.getpid()) start = time.time() time.sleep(random.random() * 2) end = time.time() print 'Child Process %s runs %0.2f seconds.' % (name, (end - start))if __name__=='__main__': print 'Parent Process %s starts' % os.getpid() p = Pool() for i in range(5): p.apply_async(child_task, args=(i,)) print 'Waiting for all subprocesses done...' p.close() p.join() print 'All subprocesses done.' 执行结果如下：123456789101112Parent Process 6028 startsWaiting for all subprocesses done...Child Process 0 (6448) startsChild Process 1 (6912) startsChild Process 2 (5176) startsChild Process 3 (7676) startsChild Process 2 runs 0.86 seconds.Child Process 4 (5176) startsChild Process 3 runs 0.98 seconds.Child Process 0 runs 1.80 seconds.Child Process 4 runs 1.97 seconds.All subprocesses done. 代码解读： 对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。 请注意输出的结果，Child Process 0，1，2，3是立刻执行的，而Child Process要等待前面某个Child Process完成后才执行，这是因为Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。我的笔记本是四核的，所以创建五个进程的时候有一个需要等待。 但是如果只有一核，输出会如下所示： 12345678910111213Parent Process 32075 startsWaiting for all subprocesses done...Child Process 0 (32076) startsChild Process 0 runs 1.99 seconds.Child Process 1 (32076) startsChild Process 1 runs 0.39 seconds.Child Process 2 (32076) startsChild Process 2 runs 0.75 seconds.Child Process 3 (32076) startsChild Process 3 runs 0.98 seconds.Child Process 4 (32076) startsChild Process 4 runs 1.97 seconds.All subprocesses done. 这是Pool有意设计的限制，并不是操作系统的限制。如果改成：p = Pool(5)就可以同时跑5个进程。 进程间通信Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的multiprocessing模块包装了底层的机制，提供了Queue、Pipes等多种方式来交换数据。 我们以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据： 12345678910111213141516171819202122232425262728293031# coding:utf-8#进程间的通信sfrom multiprocessing import Process, Queueimport os, time, random# 写数据进程执行的代码:def write(q): for value in ['A', 'B', 'C']: print 'Put %s to queue...' % value q.put(value) time.sleep(random.random()*3)# 读数据进程执行的代码:def read(q): while True: value = q.get(True) print 'Get %s from queue.' % valueif __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 启动子进程pr，读取: pr.start() # 等待pw结束: pw.join() # pr进程里是死循环，无法等待其结束，只能强行终止: pr.terminate() 运行结果如下：123456Put A to queue...Get A from queue.Put B to queue...Get B from queue.Put C to queue...Get C from queue. 在Unix/Linux下，multiprocessing模块封装了fork()调用，使我们不需要关注fork()的细节。由于Windows没有fork调用，因此，multiprocessing需要“模拟”出fork的效果，父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。 小结 在Unix/Linux下，可以使用fork()调用实现多进程。 要实现跨平台的多进程，可以使用multiprocessing模块。 进程间通信是通过Queue、Pipes等实现的。 参考：多进程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop中MapReduce快速入门]]></title>
      <url>%2F2015%2F12%2F14%2FHadoop%E4%B8%ADMapReduce%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
      <content type="text"><![CDATA[因为研究生的方向是数据挖掘，所以免不了要接触到Hadoop,Hadoop是一个用Java语言实现开源软件框架，通过大量计算机组成的集群对海量数据进行分布式计算。 Hadoop中两个重要组成部分为HDFS和MapReduce。其中HDFS用于存储海量的数据，MapRudece则负责处理这些数据，从中获取所需的信息。 HDFS简单介绍HDFS（Hadoop Distributed File System）翻译过来就是”Hadoop 分布式文件系统”，用于存储海量的数据。从“分布式文件系统”的名字可以知道这个文件系统运行在集群上。对于一个文件，Hadoop会将其先分成若干个block（每个block的大小默认为64M,当然也可以自己指定block的大小），然后再将block存储到集群上。为了保证数据的冗余性，HDFS会为每个block创建2个副本，然后将这三个相同的block分别存储在不同的机器上。 例如下图就是将data1分成了1、2、3共三个block，为每个block创建副本后再存储在不同的机器上；同理将data2分成了4、5共两个block MapReduce介绍有了数据就可以对其进行处理，从中提取出我们所需的信息。在Hadoop中是通过MapReduce来实现的。 MapReduce任务过程被分为两个阶段：Map阶段和Reduce阶段。每个阶段都用key/value作为输入和输出；每个阶段都需要定义函数，也就是map函数和reduce函数；可以简单认为map函数是对原始数据提出出有用的部分，而reduce函数则是对提取出来的数据进行处理。 所以实际编写程序时需要编写三个函数：Map函数，Reduce函数和调用他们执行任务的主函数，在编写程序时必须要有这个整体的概念。 下面会以Hadoop官方文档中的WordCount任务为例阐述MapReduce，WordCount的任务很简单，就是计算出一个文本中每个单词出现了多少次。下面分别来分析这几个函数： 需要注意的而是在编写这三个函数时均需要用到Hadoop本身提供的jar包下面的实例是Hadoop 1.2.1 版本提供的jar包 Map函数在本例中map函数的主要作用就是以k-v形式记录所有出现过的词，代码如下 12345678910111213141516171819202122232425262728/**WordCount的map程序*/package com.lc.hadoop;import java.io.IOException;import java.util.StringTokenizer;//引入Hadoop本身提供的jar包import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;/*继承Mapper类，&lt;Object,Text,Text,IntWritable&gt;表示输入输出的key-value 类型*/public class TokenizerMapper extends Mapper&lt;Object,Text,Text,IntWritable&gt; &#123; IntWritable one=new IntWritable(1); Text text=new Text(); public void map(Object key,Text value,Context context)throws IOException,InterruptedException&#123;/*key为输入的key，value为输入的value，因为用不上输入的key的类型，所以直接定义为Object类型，而Context是定义在Mapper类内部的，用于存储key-value键值对*/ StringTokenizer tokenizer=new StringTokenizer(value.toString()); while(tokenizer.hasMoreTokens())&#123; text.set(tokenizer.nextToken()); context.write(text,one); &#125; &#125;&#125; 关于程序的几点解释： StringTokenizer类的作用是根据某一分隔符将String分隔开，默认是采用空格。 IntWritable 类表示的是一个整数，是一个以类表示的可序列化的整数 Text 类代表的是可序列化的String类型 Mapper 类将输入键值对映射到输出键值对，也就是 MapReduce 里的 Map 过程 经过map过程后，文章被分割成大量的k-v对，k为实际的单词，v均为1，下一步就是要将相同的单词合并在一起。 Reduce函数Reduce函数的作用就是将相同的单词出现的次数合并在一起，代码如下： 123456789101112131415161718192021222324/**WordCount的reduce程序*/package com.lc.hadoop;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class CountReducer extends Reducer&lt;Text ,IntWritable,Text,IntWritable&gt;&#123; IntWritable result=new IntWritable(); public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context)throws IOException,InterruptedException&#123; int sum=0; for(IntWritable iw : values)&#123; sum+=iw.get(); &#125; result.set(sum); context.write(key,result); &#125;&#125; Reduce与Map函数有很多地方比较相似，均是继承了hadoop提供的jar包中的类，只是map函数继承了Mapper类，而reduce函数继承了Reducer类，输入输出的类型均是k-v键值对。而且reduce函数的输入就是map函数的输出。 主函数主函数的任务就是要创建一个任务，并且把map和reduce类都引进来，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041/**WordCount的主程序*/package com.lc.hadoop;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.GenericOptionsParser;public class WordCount&#123; public static void main(String[] args) throws Exception&#123; Configuration conf=new Configuration();//从hadoop配置文件中读取参数 //从命令行读取参数 String[] otherArgs=new GenericOptionsParser(conf,args).getRemainingArgs(); if(otherArgs.length!=2)&#123; System.out.println("Usage:wordcount &lt;in&gt; &lt;out&gt;"); System.exit(2); &#125; Job job=new Job(conf,"WordCount"); job.setJarByClass(WordCount.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(TokenizerMapper.class); job.setReducerClass(CountReducer.class); FileInputFormat.addInputPath(job,new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job,new Path(otherArgs[1])); System.exit( (job.waitForCompletion(true)?0:1)); &#125;&#125; 关于程序有几点需要注意的地方： Configuration 类用于读写和保存各种配置资源 Path 类保存文件或者目录的路径字符串 Job 类：在hadoop中每个需要执行的任务是一个 Job，这个 Job 负责很多事情，包括参数配置，设置MapReduce 细节，提交到 Hadoop 集群，执行控制，查询执行状态，等等 FileInputFormat和FileOutputFormat用于处理文件的输入和输入（针对MapReduce而言） GenericOptionsParser 类负责解析 hadoop 的命令行参数 执行任务编写好源程序后，需要在hadoop上执行我们在源程序中写好的代码，大致的过程如下：编译-&gt;打包-&gt;执行，下面分别介绍。为了程序的规范性，首先建立一个wordcount的文件夹，下面再建两个子文件夹src和classes，分别放置源程序文件和编译好后的class文件。且默认是在Linux上执行这些操作的。 编译首先将上面写好的三个源文件放到wordcount的src目录下，同时拷贝安装hadoop后提供的两个jar包hadoop-core-1.2.1.jar和commons-cli-1.2.jar。进入wordcount目录，采用下面命令进行编译 javac -classpath hadoop-core-1.2.1.jar:commons-cli-1.2.jar -d ./classes ./src/* 这条命令的作用是将src目录下的所有文件进行编译，生成的class文件放到classes目录下，编译过程中需要引入的hadoop-core-1.2.1.jar和commons-cli-1.2.jar两个包，里面包含了上面源文件中导入的hadoop的类。编译完成后，可以在classes目录下发现以下子目录的结构classes-&gt;com-&gt;lc-&gt;hadoop,最后在目录hadoop下会有三个class文件，分别对应上面的的三个源文件。 打包打包需要用到 jar 命令，jar 命令是 JDK 的打包命令行工具，跟 tar 非常像。 先切换到WordCount目录，再执行下面的命令： jar -cvf WordCount.jar -C ./classes/* . 在命令里，-C 是指需要打包的class文件的路径，。打包结果是 wordcount.jar 文件，放在当前目录下。 执行执行hadoop任务需要在HDFS上进行，所以文件的输入输出路径也就是在HDFS上的路径 首先需要将待处理的文件放入到HDFS中，可以按顺序输入以下命令：hadoop fs -mkdir in //在HDFS中创建一个名为in的文件夹 hadoop fs -put Readme.txt readme.txt //将Linux当前目录下的Readme.txt文件放置到HDFS中的in目录 hadoop jar WordCount.jar com.lc.hadoop WordCount in/readme.txt out//执行Linux当前目录下的WordCountjar包里面的WordCount类，输入文件是HDFS中in目录下的readme.txt文件，输出文件放到HDFS中的out目录 hadoop fs -cat out/part-r-0000 //查看得到的结果 需要注意的是HDFS中的文件路径不能够在Linux下直接通过cd或ls进行切换或查看，而必须要通过hadoop fs进行操作。 以上就是Hadoop中MapReduce的流程，针对不同的应用会有不同的变化，但是总体上的流程是一致的，就是先编写好三个函数（map函数，reduce函数和主函数），然后要经历编译-&gt;打包-&gt;执行的流程。再查看得到的结果即可。 参考资料：最短路径系列之一从零开始学习Hadoop]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[为什么相爱容易,相守不易？]]></title>
      <url>%2F2015%2F12%2F12%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E7%9B%B8%E7%88%B1%E5%AE%B9%E6%98%93-%E7%9B%B8%E5%AE%88%E4%B8%8D%E6%98%93%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[文章为转载，作者：淡水天 该文从比较科学的角度阐述了一个不怎么科学的现象，值得一看。下面为原文 一桩桩分手或出轨的事件，像恼人的烟雾一样缠绕住心怀，更是让许多人直呼“再也不相信爱情了”。哎，为何世间好物如此不坚牢，开头美好，结局却潦倒？相爱容易，相守却很难？ 哪里有什么爱情？不过是生殖冲动爱是什么？大约谁也道不清。它是一道隐秘的风，没有轮廓，没有重量，吹过人们柔软的身体，也吹皱他们的心。初初相爱的男女为轻飘飘的快乐所托起，在云层间来回轻踱。 爱没有名字。若是非要给它找一个丈量的器具，那么怦然跃动的心，手掌里沁出的汗，面颊上映出的云霞….或可作为一种参照。在爱之开端，激情浇灌着两颗年轻的心。然而，也因了爱情模糊的面目，个体的紧张与唤醒状态也会被贴上“爱情”的标签。心理学家哈特菲尔德甚至认为，爱情就是生理唤醒和心理标签相互作用的结果。 阿瑟·阿伦（ArthurAron，1974）曾经做过一个经典的实验，他找来一位漂亮的女助手，由她去找男性被试完成一个简单的问卷，再让他们根据一张图片编一个小故事。参加实验的大学生被分为三组，分别在安静的公园、坚固而低矮的石桥，以及危险的吊桥上接受调查。最后，这位漂亮的女助手把自己的联系方式告诉了每一位参加实验的大学生，如果他们想进一步了解实验可以给她打电话。 猜一猜，最后的结果如何？没错，与其他两组相比，在危险的吊桥上参加实验的大学生给女调查者打电话的人数最多，而他们所编撰的故事中，也更多含有情爱的色彩。 这就是著名的“吊桥效应”，其理论来源是沙赫特的“情绪三因素理论”，情绪=刺激×生理唤醒×认知标签。个体如何解释情境及生理反应，往往与外部的线索和“诱因”有关。大部分男性把横渡吊桥时因为紧张所致的口渴感，以及心跳加速等生理上的兴奋误认为性冲动，于是在心底埋下了一颗爱情的种子。 事实上，在加拿大温哥华北部的千山万壑之中确实存在着一座“爱情桥”——卡皮兰诺吊桥。它全长137米，悬挂在高达70米的河谷上。无数往来的男女在这座惊心动魄的吊桥上演绎了一段段爱情佳话。 爱情正是这样不靠谱的存在，无怪乎钱钟书先生在《围城》一书中借方鸿渐之口说出，“哪里有什么爱情？不过是生殖冲动。”爱情的产生就是一场索然无趣的化学反应，一段美好的爱情关系，通常是人体内几种不同的化学物质（如多巴胺、催产素等）相互作用的结果。大脑神经会经历很多不同的化学反应过程，当外界的刺激按照正确的顺序，刺激到正确的复合体时，人们就会相爱。 短暂的“投射—认同”游戏相爱是不难的，难的是相依相伴，相守终生。犹记得《笑傲江湖》里，苍茫大雪纷飞，林平之执剑在少室山的雪人身后刻下“海枯石烂，此情不渝”八个大字，那时的情浓意长，你焉能说他没有一点真心？却也还是敌不住日后的风霜刀剑严相逼啊。 爱情也有生命，也会经历生老病死，种种的一切。爱意的萌生或许只是刹那间的电光火石，但维护爱情的生命却需付诸百倍的时间与耐性。作为最强烈的人际吸引形式，爱情也有其循序渐进的过程，在时日缓慢的雕琢中，从定向阶段，到情感探索阶段，再到情感交流阶段，最后才会步入稳定交往阶段。 所谓的“经营爱情”大多指的就是在“情感探索”和“情感交流”这两个阶段的相处与沟通之道。通常在情感探索阶段的初期，双方因为相当程度上的保留与节制，仍然通过“投射”与“认同”来维护关系，恋人小心翼翼地开放着自己的私人领域，也好奇地探索着关于对方的一切，他们修饰着自己也美化着对方，同时因为接触与了解的程度不深，也可以较少地受到对方自身特质的干扰，迅速地把自己的早期客体形象，投射到对方身上。而对方为了讨得恋人的欢心，也会迅速地认同这个投射，做出恋人理想中的样子来。 这是最甜蜜的阶段，他们就像是两个小孩子，围着一个神秘的果酱罐，一点一点地尝它，看看里面有多少甜。 然而，爱情并不只是甜的。当那个果酱罐被一点点地打开之后，它的神秘感与新鲜感都将消失，那时呈现在眼前的或许只是一只平凡无奇的小罐头而已，不同于最初的想象。 甚至，令你失望。 我是流泪的洋葱，你还爱我吗？随着交往的加深，早期的“投射—认同”游戏已经无法再继续下去，恋人被压抑的真实自我在呼唤得到释放，他们无法再去扮演一个理想的恋人，而更想去做一个真实的恋人。 这也是亲密关系的真谛。人们之所以想组建亲密关系，之所以想爱与被爱，就是想获得一种亲密感。亲密需要把自我最深处的部分向他人也向自己呈现，卸除掉层层的伪装与防护，建立起真我与真我之间的连接感。 在这个阶段，双方的信任感、依恋感开始建立，沟通的深度和广度也有所发展，开始牵涉到较深的情感卷入。这是心与心之间相知的契机，但同时也是一场充满风险的赌博。 人们的内心常常由三层组成：最外面一层是保护层，接下来是伤痛，而最深处是真我。每个人的成长多多少少都会经历一些伤痛，或者来自于父母，或者来自重要他人，或者是一些创伤性的体验。因为这些伤痛的存在，所以人们发展出了保护层和种种防御机制，以守护脆弱的内核。当恋人鼓起勇气一层层地剥落掉充满防御机制的外壳时，也是在将自己的伤痛展示给对方看，这是痛苦的，必定会像剥洋葱皮一样，一边剥落一边掉泪。 如果对方的反应是积极的、接纳的，那么恋人即便落泪痛苦，也会因为感受到情感上的陪伴与支持，而坚定地一层层剥落掉那些保护壳，直到露出完整的内核。倘若双方都能如此坦然地接纳对方，那么爱情就会发展到稳定交往阶段，琴瑟和鸣，风雨同舟。 但很多时候，受限于自己的狭隘、缺陷，以及种种未处理完的情结，人们渴望亲密，又害怕亲密，宁愿相信坦露真实的自我只会招致他人的批判、拒绝和抛弃，也很难相信有人肯接纳自己内在的真实。于是，在关系发展到一定程度时，就会退缩、害怕、扮演角色，在操纵与受控之间游走，生出嫉妒与厌恶、逃离与背叛之心，将关系场演变成游乐场，上演出种种的闹剧与幻想。 爱是一门艺术归根结底，相爱容易，而相守不易，也是因为我们错误地理解了“爱”。 人是孤独的，他与外物、与社会、与世界分离，这种孤独创造了一种无法忍受的监禁感，使得人们迫切地寻求与他人的连接。但也正因如此，人们才将爱情的重心放在“体验被爱”，而不是“体验爱”上。于是酿制出种种矛盾，衍生出种种失望。 弗洛姆认为，爱不仅是情感，爱也是一种能力，更是一门艺术。如果说爱情始于生理唤醒，又在“你投射，我认同”的模式下发酵发展，那么能够相依相伴必定是因为习得了爱的能力，掌握了爱的艺术。 爱的四要素包括关心、尊重、责任和认识。只有当我们从整体上理解爱人，为对方投注以主动的关怀，尊重他的真实存在，为他的精神成长负起责任时，我们才能维持长久的关系。 这是一场相互的驯化，是两个个体之间“不完整的融合”，是在距离感与亲密感之间取得一个合理的平衡点。它需要很多的耐心与时间，又在耐心与时间中酿造出新的意义。 乱入图片一张 做到这一切并不容易。爱有起点，也必有终点，以死亡为句读是爱最好的结局，也是最难的结局。但当你用心地去呵护爱情的生命时，你会发现，一切终将黯淡，唯有被爱的目光镀过金的日子依然在岁月的深谷里闪耀着永恒的光芒。 那是赠与你的礼物。即便爱情终将死亡，你也可以做那个延长它生命的人。即便你护卫不了对方的心，你也始终可以护卫自己的心，守住自己对爱的这份忠贞。 别再说什么不相信爱情了，我知道——你只是不愿去相信自己。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Screen管理Linux远程会话]]></title>
      <url>%2F2015%2F12%2F10%2F%E4%BD%BF%E7%94%A8Screen%E7%AE%A1%E7%90%86Linux%E8%BF%9C%E7%A8%8B%E4%BC%9A%E8%AF%9D%2F</url>
      <content type="text"><![CDATA[通过 SSH 或者 telent 远程登录到 Linux服务器执行一些长时间运行的任务，比如系统备份、ftp 传输等等。因为他们执行的时间太长了。必须等待它执行完毕，在此期间可不能关掉窗口或者断开连接，否则这个任务就会被杀掉，一切半途而废了。本文分析了这个问题的原因以及解决方法。 为什么关掉窗口/断开连接会使得正在运行的程序死掉？元凶：SIGHUP 信号在Linux/Unix中，有这样几个概念： 进程组（process group）：一个或多个进程的集合，每一个进程组有唯一一个进程组ID，即组长进程的ID。 会话（session）：一个或多个进程组的集合，开始于用户登录，终止与用户退出，此期间所有进程都属于这个会话。一个会话一般包含一个会话首进程、一个前台进程组和一个后台进程组。 守护进程（daemon）：Linux大多数服务都是通过守护进程实现的，完成许多系统任务如0号进程为调度进程，是内核一部分；1号进程为init进程,负责内核启动后启动Linux系统。守护进程不因为用户、终端或者其他的变化而受到影响。 当终端接口检测到网络连接断开，将挂断信号（SIGHUP）发送给控制进程（会话期首进程）。而挂断信号默认的动作是终止程序。如果会话期首进程终止，则该信号发送到该会话期前台进程组。 也就是说：ssh打开以后，bash等都是他的子程序，一旦ssh关闭，系统将所有前台进程杀掉。（后台进程和守护进程不会被关闭！！！） 测试案例测试例一打开两个SSH终端窗口，在其中一个运行了一个循环打印的python脚本。执行命令如下：[root@localhost ~]# python test.pytest.py内容如下：12while True: print &quot;hehe&quot; 另外一个终端用pstree -p查看当前的进程树。显示如下1234[root@localhost ~]# pstree -p（省略）├─sshd(958)─┬─sshd(1282)───bash(1286)───pstree(1436)│ └─sshd(1410)───bash(1414)───python(1433) 可以看到2个bash进程代表了2个终端，pstree是当前进程正在运行的程序，而python进程则是另外一个终端正在运行的程序。 关掉启动python的终端，在刚刚执行pstree的终端上查找pid为1433的进程（也就是原来的python进程），发现没有这个pid的进程，说明python随着终端的关闭而终止了，此时输入pstree -p变为了下面这样：123[root@localhost ~]# pstree -p（省略）├─sshd(958)──sshd(1282)───bash(1286)───pstree(1436) 测试例二步骤同例一，只是在执行python脚本时将其放到后台执行，执行命令如下：[root@localhost ~]# python test.py &amp;这样在关闭执行python的中断后，python进程并没有被中断，通过pstree -p查看到进程数类似于下面的情况：123(省略)├─python(1493)├─sshd(958)───sshd(1282)───bash(1286)───pstree(1497) 因为python执行的是个后台进程，而SIGHup信号只会发送给前台进程组，当父进程结束后，其原来子进程中的后台进程会成为孤儿进程被init进程收养。详见孤儿进程和僵尸进程 注：网上一些资料显示执行某些复杂程序的时候，只加&amp;也会终止，但是博主还没遇到过这种情况，因为我不会这样去执行一个执行时间较长的程序。 同样,nohup命令可以达到这个目的，值得注意的是nohup命令只是使得程序忽略SIGHUP信号，还需要使用标记&amp;把它放在后台运行。这种情况能够保证程序不会被终止。nohup &lt;command&gt; [argument…] &amp; 使用screen管理远程会话虽然nohup和后台进程很容易使用，但还是比较“简陋”的，对于简单的命令能够应付过来，对于复杂的需要人机交互的任务就麻烦了。 其实我们可以使用一个更为强大的实用程序screen。 简单来说，Screen是一个可以在多个进程之间多路复用一个物理终端的窗口管理器。Screen中有会话的概念，用户可以在一个screen会话中创建多个screen窗口。 创建新的会话在screen中创建一个新的会话有2种方式 1．直接在命令行键入screen命令[root@localhost ~]# screenScreen将创建一个执行shell的全屏窗口。你可以执行任意shell程序，就像在ssh窗口中那样。在该窗口中键入exit退出该窗口，如果这是该screen会话的唯一窗口，该screen会话退出，否则screen自动切换到前一个窗口。也可通过screen -S name 来为启动的session取名字。2．Screen命令后跟你要执行的程序[root@localhost ~]# python test.pyScreen创建一个执行python test.py的单窗口会话，终止进程将退出该窗口/会话。 进入已创建会话即使关闭了启动所有终端，在screen会话中启动的进程也不会终止，再次连接时可通过screen -ls查看已经启动的screen会话(detached状态)，用screen -r name恢复指定会话，也可在会话中通过exit退出screen会话。12345678[root@localhost ~]# screen -lsThere is a screen on: 1518.lc (Detached)1 Socket in /var/run/screen/S-root.重新连接会话[root@localhost ~]# screen -r lc或screen -r 1518退出当前screen会话[root@localhost ~]#exit screen的一些常用参数如下所示 分享操作 screen -x name进入一个还在连接着（attached）的screen，然后所有操作能够被另外所有正在连着的screen看到 分屏 创建一个新的窗口：ctrl+a+S （注意是大写的s）,此时新的窗口还没启动bash 启动新窗口的bash：ctrl+a+c 切换窗口：ctrl+a+tab 关掉当前窗口:ctrl+a+X（注意是大写的x）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[以太网中的MTU与MSS]]></title>
      <url>%2F2015%2F12%2F08%2F%E4%BB%A5%E5%A4%AA%E7%BD%91%E4%B8%AD%E7%9A%84MTU%E4%B8%8EMSS%2F</url>
      <content type="text"><![CDATA[以太网（Ethernet）最大的数据帧是1518字节。以太网帧的帧头的14字节和帧尾CRC校验4字节共占了18字节，剩下的承载上层协议的地方也就是Data域最大就只剩1500字节.这个值我们就把它称之为MTU。MTU的全称是maximum transmission unit（最大传输单元）。MTU可以认为是网络层能够传输的最大IP包。 而MSS（Maximum segment size）可以认为是传输层的概念，也就是TCP数据包每次能够传输的最大量。为了达到最佳的传输效能，TCP协议在建立连接的时候通常要协商双方的MSS值，这个值TCP协议在实现的时候往往用MTU值代替（需要减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes）所以往往MSS为1460。通讯双方会根据双方提供的MSS值得最小值确定为这次连接的最大MSS值。 MSS为1460是由1500-20（IP头）-20（TCP头）计算出的。但是在实际场景下，TCP包头中会带有12字节的选项–时间戳（用户在发送每一个TCP报文的时候都放置一个时间戳，接受方在确认中返回这个时间戳值。发送方就可以根据这个时间戳来计算RTT（往返传输时间–发送端从发送TCP包开始到接收到它的立即响应所耗费的传输时间.）。从而使得RTT更加精确，减少不必要的重传。减低网络的负载。) 这样，单个TCP包实际传输的最大量就缩减为1448字节。1448=1500-20（IP头）-32（20字节TCP头和12字节TCP选项时间戳）。 问题来了：“每个TCP包在理论上应该能打包更多数据才对，但是实际场景下TCP传输为什么会以这个1448作为打包单位呢？” 理论上，单个TCP包能打包的数据量远远多于1448字节，现在为了适应MTU，只要在以太网上跑TCP，系统就默认最大以1448字节打包TCP。 假如我们用更大的数据量来打包会有什么结果呢？答案是降低了传输效率。 超过MTU的大包反而降低效率的原因如下： IP层非常关心MTU，因为IP层会根据MTU来决定是否把上层传下来的数据进行分片。就像一条运输线路的承载能力是有限的，碰到大东西要运输，只能把大东西拆开成为散件，分开运输，到达目的地之后还必须能再次组装起来。 当两台远程PC互联的时候，它们的数据需要穿过很多的路由器和各种各样的网络媒介才能到达对端，网络中不同媒介的MTU各不相同，就好比一长段的水管，由不同粗细的水管组成（MTU不同 :)）通过这段水管最大水量就要由中间最细的水管决定。对于网络层的上层协议而言（我们以TCP/IP协议族为例）它们对水管粗细不在意，它们认为这个是网络层的事情。网络层IP协议会检查每个从上层协议下来的数据包的大小，并根据本机MTU的大小决定是否作“分片”处理。分片最大的坏处就是降低了传输性能，本来一次可以搞定的事情，分成多次搞定，所以在网络层更高一层（就是传输层）的实现中往往会对此加以注意！ 这个就是在以太网上，TCP不发大包，反而发送1448小包的原因。只要这个值TCP才能对链路进行效能最高的利用。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux锁定用户与解锁]]></title>
      <url>%2F2015%2F12%2F06%2FLinux%E9%94%81%E5%AE%9A%E7%94%A8%E6%88%B7%E4%B8%8E%E8%A7%A3%E9%94%81%2F</url>
      <content type="text"><![CDATA[禁止个别用户登录禁止单个用户的登录有两种方法。下面以禁止test用户登录为例 方法一直接命令禁止passwd -l test这条命令的意思是锁定test用户，这样该用户就不能登录了。passwd -u test对锁定的用户test进行解锁，用户可登录了。 方法二修改/etc/passwd文件中用户登录的shell vi /etc/passwdtest:x:500:500::/home/test:/bin/bash更改为：test:x:500:500::/home/lynn:/sbin/nologin该用户就无法登录了。 禁止所有用户登录touch /etc/nologin除root以外的用户不能登录了！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[孤儿进程和僵尸进程]]></title>
      <url>%2F2015%2F12%2F05%2F%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[基本概念正常情况下，子进程是通过父进程创建的，子进程在创建新的进程。子进程的结束和父进程的运行是一个异步过程,即父进程永远无法预测子进程 到底什么时候结束。 孤儿进程和僵尸进程就是由于这个异步过程没有正确执行而引起的问题。下面先看看这两类进程的一些概念。 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 僵尸进程：一个进程使用fork创建子进程。如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。 问题及危害unix提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息,包括 进程号(the process ID) 退出状态(the termination status of the process) 运行时间(the amount of CPU time taken by the process)等 僵尸进程危害：直到父进程通过wait/waitpid来取时才释放。但这样就导致了问题，如果进程不调用wait/waitpid的话，那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害，应当避免。 孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了init进程身上，init进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤 儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init进程就会代表党和政府出面处理它的一切善后工作。因此孤儿进程并不会有什么危害。 寻找并杀掉僵尸进程僵尸进程危害场景：例如有个进程，它定期的产 生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程 退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程，倘若用ps命令查看的话，就会看到很多状态为Z的进程。 严格地来说，僵死进程并不是问题的根源，罪魁祸首是产生出大量僵死进程的那个父进程。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是把产生大 量僵死进程的那个元凶枪毙掉（也就是通过kill发送SIGTERM或者SIGKILL信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进 程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程 就能瞑目而去了。 上面是具体的思路，下面是实际的操作： 查找僵尸进程：可通过top命令查看当前是否有僵尸进程，如下图0 zombie表示没有僵尸进程 假如该值不为0，可以通过ps和grep命令查找僵尸进程,如:ps -A -o stat,ppid,pid,cmd | grep -e &#39;^[Zz]&#39;命令注解：-A 参数列出所有进程-o 自定义输出字段 我们设定显示字段为 stat（状态）, ppid（进程父id）, pid(进程id)，cmd（命令）这四个参数因为状态为 z或者Z的进程为僵尸进程，所以我们使用grep抓取stat状态为zZ进程。 找到这些进程后，可以直接kill掉僵尸进程，如果僵尸进程的数量比较多，也可以kill掉其父进程的pid让init经常回收这些僵尸进程的资源。 参考资料：[1] http://www.cnblogs.com/Anker/p/3271773.html[2] http://be-evil.org/linux-find-and-kill-zombie-process.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度优先搜索和广度优先搜索]]></title>
      <url>%2F2015%2F12%2F03%2F%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%E5%92%8C%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2%2F</url>
      <content type="text"><![CDATA[在图论中图的遍历是非常常见的操作，两种图的遍历的经典方法：深度优先搜索和广度优先搜索。因为经常忘记其实现方法，这里特意写篇文章记录这两种方法的实现的关键点。可能会存在很多实现方法，这里只记录我知道而且觉得最好理解的方法。 深度优先搜索 深度优先搜索实现的一个关键的思想就是递归。 关键代码如下：12345678910void dfs(int i)&#123; cout&lt;&lt;i&lt;&lt;' '; //输出点i，代表访问这个点 visited[i]=1; for(int j=0;j&lt;n;j++) //访问与点i有连接且还没访问的点 if( edge[i][j] == 1 &amp;&amp; visited[j]==0 )&#123; dfs(j); //对访问的点再进行深搜 &#125; return ;&#125; 完整代码如下： 12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;using namespace std;int edge[1000][1000] = &#123; 0 &#125;;int visited[1000] = &#123; 0 &#125;;int nodes;int edges;void dfs(int i)&#123; cout &lt;&lt; i &lt;&lt; ' '; //输出点i，代表访问这个点 visited[i] = 1; for (int j = 0; j&lt;nodes; j++) //访问与点i有连接且还没访问的点 if (edge[i][j] == 1 &amp;&amp; visited[j] == 0)&#123; dfs(j); //对访问的点再进行深搜 &#125; return;&#125;int main()&#123; int s, d; cin &gt;&gt; nodes &gt;&gt; edges; for (int i = 0; i&lt;edges; i++)&#123; cin &gt;&gt; s &gt;&gt; d; edge[s][d] = 1; edge[d][s] = 1; &#125; dfs(0);//从第0个点开始 return 0;&#125; 输入:1234565 50 10 20 41 32 4 输出：10 1 3 2 4 广度优先搜索 广度优先搜索实现的一个关键点是通过队列来存储访问过的点，并以队列里面的点作为始点访问其周围的点。 关键代码见下：123456789101112131415161718vector&lt;int&gt; que; //存储已经访问过的点int head=tail=0; //head表示当前所访问的点，tail表示最后一个点的后一个点visited[0]=1;//从第0个点开始访问，访问过的点加入到队列中que.push_back(0);tail++；while(head&lt;tail)&#123; //当head和tail相等时表示所有点已经访问过了 for(int i=0;i&lt;nodes;i++)&#123; int cur=que[head];//获取当前访问节点 //将与访问点距离为1且还没访问的点加入到队列中 if( edge[cur][i] == 1 &amp;&amp; visited[i]==0 )&#123; que.push_back(i); visited[i]=1; tail++; &#125; &#125; //访问完当前访问点周围的点，再访问下一个点 head++;&#125; 完整代码见下所示：123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;int main()&#123; int edge[100][100]=&#123;0&#125;; int visited[100]=&#123;0&#125;; int nodes,edges, s,d; cin&gt;&gt;nodes&gt;&gt;edges; for(int i=0;i&lt;edges;i++)&#123; cin&gt;&gt;s&gt;&gt;d; edge[s][d]=1; edge[d][s]=1; &#125; vector&lt;int&gt; que; //存储已经访问过的点int head=0,tail=0; //head表示当前所访问的点，tail表示最后一个点的后一个点visited[0]=1;//从第0个点开始访问，访问过的点加入到队列中que.push_back(0);tail++;while(head&lt;tail)&#123; //当head和tail相等时表示所有点已经访问过了 for(int i=0;i&lt;nodes;i++)&#123; int cur=que[head]; //将与访问点距离为1且还没访问的点加入到队列中 if( edge[cur][i] == 1 &amp;&amp; visited[i]==0 )&#123; que.push_back(i); visited[i]=1; tail++; &#125; &#125; //访问完当前访问点周围的点，再访问下一个点 head++;&#125;for(int i=0;i&lt;que.size();i++)&#123; cout&lt;&lt;que[i]&lt;&lt;' ';&#125;return 0;&#125; 输入:1234565 50 10 20 41 32 4 输出：10 1 2 4 3]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python批量下载文件]]></title>
      <url>%2F2015%2F12%2F02%2Fpython%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[最近感觉要练练口语了，所以就上普特逛逛有哪些资源比较好的，发现美国惯用语板块挺好的,但是点进去想下载音频时，发现浏览器已经解释了这个MP3格式的音频文件；要下载就只能右键另存为，总共有900多个，这么点来点去岂不是要点一天才能下完。 正当蛋疼之时，忽然发现了播放MP3文件的url格式比较统一，比如说下面这几个 前面的前缀都是相同的，可以推测在服务器上这些音频文件都是放到同一个文件夹下，而且会依据数字来加上前缀来命名，这就为脚本自动下载提供了一个很好的条件。 首先，从最简单的入手，就是下载给出url所代表的资源，代码如下123456import urllib2url="http://down02.putclub.com/homepage/courses/middle/oftenused/wi_01.mp3"f=urllib2.urlopen(url)data=f.read()with open("1.mp3","wb") as file: file.write(data) 如果资源url的确存在，那执行脚本后会在当前目录生成跟一个名为1.mp3的音频文件，当然名称也可以自己取，这里为了简便就直接用数字来作为文件名了。如果资源url不存在则在urlopen时就会抛出一个URLError(实际上也的确有某几期不提供音频文件)。根据测试，发现音频文件的前缀有wi、wi_、putclub_mgxgy、wi_0这几种。所以总体思路就是先得到合法的url，再将其下载；因为基于数字命名，所以可以通过循环来批量下载。是不是很简单，这里给出完整的代码 123456789101112131415161718192021222324252627282930313233#encoding:utf-8#下载普特英语文音频件的脚本import urllib2#得到合法的资源urldef getLegalUrl(i): base_url="http://down02.putclub.com/homepage/courses/middle/oftenused/" url_preletter_list=['wi_','wi','putclub_mgxgy','wi_0'] for j in url_preletter_list: try: url=base_url+j+str(i)+'.mp3' f=urllib2.urlopen(url) return url #不合法的url会抛出URLError的错误，不抛出则说明url存在 except urllib2.URLError: continue return "" #下载给定的合法的url的资源def download(url,i): f=urllib2.urlopen(url) data=f.read() with open(str(i)+'.mp3','wb') as file: file.write(data) if __name__ == '__main__': for i in range(0,600): url=getLegalUrl(i) if url == "": #记录无法下载的那几期，以便验证是否资源本来就没有 with open("download.log",'a') as log: log.write(str(i)+' not found\n') else: download(url,i) 等到脚本执行完，在文件夹下就能得到下面这些文件,密集恐惧者可忽略日志如下，经检查，这些资源本来就不存在 这个方法只限于那些资源名称有规律的文件下载，但是好像普特上的文件都是这么存储的，所以以后就能愉快的下载了。对于我这种小白而言只能想出这种方法，如果你有更好的方法，欢迎交流。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[vim中SuperTab的安装与使用]]></title>
      <url>%2F2015%2F12%2F01%2Fvim%E4%B8%ADSuperTab%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[vim是Linux下常用的编辑器，但是默认是没有补全功能的，所以插件SuperTab就是实现这个功能的。 下载链接：http://www.vim.org/scripts/script.php?script_id=1643 下载.vmb文件即可,下载后可通过rz命令上传（需要安装lrzsz） 安装步骤也非常简单 先用vim打开下载的文件，vim supertab.vmb 在命令模式下输入:source % 至此就可以使用SuperTab的功能了，在vim编辑模式时，输入文件中已经有的字符串的前几个字母，再按Tab键即可补全这个字符串，只能补全文件中已经出现的字符串。 SuperTab的github地址：https://github.com/ervandew/supertab]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux挂载NTFS文件系统]]></title>
      <url>%2F2015%2F11%2F30%2FLinux%E6%8C%82%E8%BD%BDNTFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
      <content type="text"><![CDATA[最近将服务器内的数据迁移到移动硬盘上做备份时，发现Centos 6.5识别不了NTFS文件系统的移动硬盘，google了一下才发现原因是Linux内核不支持NTFS。重新编译内核是一种方法，但是也可以采用安装一个软件来解决，本文就是讲述如何安装这个软件以及在Linux挂载NTFS文件系统的移动硬盘。 这个软件就是NTFS-3G。NTFS-3g是一个开源软件，它支持在Linux下面读写NTFS格式的分区。更多信息可参考NTFS-3G官网：http://www.ntfs-3g.org 安装安装方式有两种： yum源安装 如果配置的yum源有ntfs-3g这个包，那么可以通过yum install ntfs-3g来直接安装，如果配置的yum源没有这个包，可以参照下一种安装方式 编译安装下载地址为：http://www.tuxera.com/community/open-source-ntfs-3g/,也可通过wget下载，安装过程如下 123456# wget https://tuxera.com/opensource/ntfs-3g_ntfsprogs-2015.3.14.tgz# tar -zxvf ntfs-3g_ntfsprogs-2015.3.14.tgz# cd ntfs-3g_ntfsprogs-2015.3.14# ./configure# make # make install 使用### 获取NFTS设备名称12#fdisk -l | grep -i ntfs /dev/sdb1 1 10443 83883366 7 HPFS/NTFS 可知设备名为/dev/sdb1 建立挂载点并挂载12# mkdir /mnt/ntfs# mount -t ntfs-3g /dev/sda1 /mnt/ntfs 这样访问/mnt/ntfs目录便可往硬盘进行读写了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[vim编辑器使用]]></title>
      <url>%2F2015%2F11%2F30%2Fvim%E7%BC%96%E8%BE%91%E5%99%A8%2F</url>
      <content type="text"><![CDATA[vim是Linux下非常常用的一个编辑工具，所以有必要了解一下vim的一下使用技巧。 这里将vim的状态分成两大类：命令模式和编辑模式。当你输入命令vim filename时就进入了命令模式，而此时按下i即可进入编辑模式,进入编辑模式后可同过按ESC退回命令模式 跳到某一行 跳到第一行：在命令模式下，连续两次按下g 跳到最后一行：在命令模式下，shift+g即可 跳到某一行：命令行模式下，通过冒号加行号即可实现，如:15即可跳到第15行。 跳到当前行往上或往下的n行：命令模式下，先按数字n，然后按往上或往下的按钮（就是键盘上的上下左右的按钮） 跳到一行的开头：可以使用键盘上的编辑键Home,也可以在命令模式中使用快捷键&quot;^&quot;（即Shift+6）或0（数字0) 跳到一行的末尾：以使用编辑键End，也可以在命令模式中使用快捷键&quot;$&quot;（Shift+4）。快捷键”\$”前可以加上数字表示移动的行数。例如使用”1\$”表示当前行的行尾，”2$”表示当前行的下一行的行尾。 查找字符串 往前查找：命令模式下输入?word,按n往前查找下一个，shift+n往后查找下一个 往后查找：命令模式下输入/word,按n往后查找下一个，shift+n往前查找下一个注：这里的往前和往后指的是文本的顺序，其实只需要记住按n是顺序查找，shift+n是逆序查找即可 删除 删除一行：命令模式下，光标定位到要删除的那一行，连续按下两次d即可 删除多行：命令模式下，光标定位到要删除的那一行，先按下数字n，然后连续按下两次d，表示删除n行（包括当前行） 修改 替换字符串：命令模式下输入:n1,n2s/w1/w2/g,表示将n1到n2行的w1转为w2（1代表第一行，$代表最后一行，没数字为整个文本），注意前面有冒号 复制：命令模式下，将光标移到需要复制的那行，然后连续两次按下y，即可复制当前行，如果要复制多行，在按下yy前需要按下数字n，表示复制包括当前光标一下的n行，原理同删除操作 块复制：命令模式下，通过ctrl+v进入块复制模式，选择高亮后按y复制 剪切：实际上上面提到的删除命令除了将所选的内容删除掉，还将其复制到了剪切板上，按下p即可粘贴；所以剪切实际上就是dd+p。 粘贴：命令模式下，按下p即可将已经复制或剪切的内容复制到光标所在行下面的的一行或多行 撤销与恢复前一步的操作 撤销前一步的工作:命令模式下按u 撤消后恢复前一步的工作：命令模式下按Ctrl+u 另存为 命令模式下输入:n1,n2 w new_file_name可将修改文本n1到n2行（不加数字为整个文本）另存为其他文件]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux传输文件的小工具lrzsz]]></title>
      <url>%2F2015%2F11%2F28%2FLinux%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6%E7%9A%84%E5%B0%8F%E5%B7%A5%E5%85%B7lrzsz%2F</url>
      <content type="text"><![CDATA[常常有些小文件需要从本地的Windows传到Linux服务器或者从Linux服务器下载到本地，如果用ftp就显得杀鸡用牛刀了，这时候工具lrzsz就显得比较有用了 首先需要安装这个工具，以CentOS为例，通过yum安装即可，即1yum -y install lrzsz 可用的命令为rz和sz,可通过下面的方式来记忆 sz中的s意为send（发送），告诉客户端，我（服务器）要发送文件 send to cilent，就等同于客户端在下载。 rz中的r意为received（接收），告诉客户端，我（服务器）要接收文件 received by cilent，就等同于客户端在上传。记住一点，不论是send还是received，动作都是在服务器上发起的。 运行命令rz，Xshell(或SecureCrt)就会弹出文件选择对话框，选好文件以及传输方式（文本还是二进制）之后关闭对话框，文件就会上传到linux里的当前目录。 运行命令sz file 就是发文件到Windows上（保存的目录是可以配置,因为sz利用了ZModem协议来传输文件，所以一般可在使用的连接工具（如Xshell等）中设置）；常用的参数如下所示： -a 以文本方式传输（ascii） -b 以二进制方式传输（binary） -e 对控制字符转义（escape），这可以保证文件传输正确 也可将文件先压缩成一个压缩文件再传输。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[文本文件和二进制文件]]></title>
      <url>%2F2015%2F11%2F26%2F%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[文本文件和二进制文件的定义 首先，计算机的存储在物理上是二进制的，也就是在物理存储方面没有区别都是01码。所以文本文件与二进制文件的区别并不是物理上的，而是逻辑上的，也就是编码上。简单来说，文本文件是基于字符编码的文件，常见的编码有ASCII编码，UNICODE编码等等。二进制文件是基于值编码的文件，你可以根据具体应用，指定某个值是什么意思（这样一个过程，可以看作是自定义编码。 文本文件与二进制文件的存取 文本工具打开一个文件的过程是怎样的呢？拿记事本来说，它首先读取文件物理上所对应的二进制比特流，然后按照你所选择的解码方式来解释这个流，然后将解释结果显示出来。一般来说，你选取的解码方式会是ASCII码形式（ASCII码的一个字符是８个比特），接下来，它8个比特8个比特地来解释这个文件流。例如对于这么一个文件流”01000000_01000001_0100001001000011”(下划线’’‘’，为了增强可读性手动添加的)，第一个8比特’’01000000’’按ASCII码来解码的话，所对应的字符是字符’’A’’，同理其它3个8比特可分别解码为’’BCD’’，即这个文件流可解释成“ABCD”，然后记事本就将这个“ABCD”显示在屏幕上。 文本文件格式存储时是将值作为字符然后存入其字符编码的二进制，文本文件用‘字符’作为单位来表示和存储数据，比如对于1这个值，文本文件会将其看做字符‘1’然后保存其ASCII编码值（这里假定是ASCII编码），这样在物理上就是0x31这个二进制值，而若是二进制保存1，则直接保存其二进制值，比如如果程序中是处理1为整数则保存的二进制值就是 0x00000001 (4字节）。 假如文件存储的编码与读取的编码不同，那么就无法呈现文章原来的信息，例如用记事本打开文本文件会乱码，用音乐播放器无法打开视频文件。 总结 综上，可以知道文本文件与二进制文件就是编码方式不一样而已，而这个是用户行为，把一个数据以什么样的编码（字符还是值本身）存入文件是由用户主动选择的，也就是写入的接口选择，如果以二进制接口方式写入文件那么就是一个二进制文件，如果以字符方式写入文件就是一个文本文件了。既然有写入时候的编码也就会有读出的编码，只有两个编码对应才能读出正确的结果，如用记事本打开一个二进制文件会呈现乱码的，这里稍微提一下后缀名，后缀名并不能确定其是否就是文本文件，二进制文件也可以是txt后缀名，后缀名只是用来关联打开程序，给用户做备注用的，与文件的具体编码没有关系。 最后文本文件和二进制文件主要是windows下的概念，UNIX/Linux并没有区分这两种文件，他们对所有文件一视同仁，将所有文件都看成二进制文件。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下安装sun/oracle的jdk]]></title>
      <url>%2F2015%2F11%2F26%2FLinux%E4%B8%8B%E5%AE%89%E8%A3%85sun-oracle%E7%9A%84jdk%2F</url>
      <content type="text"><![CDATA[Linux 自带的jdk是openjdk，但是sun/oracle的jdk更加常用一些，据说bug也更少。所以下面就是卸载openjdk安装sun/oralce jdk的一个教程。 检查OpenJDK是否已经安装rpm -q &lt; rpm package name&gt; 用来查询一个包是否被安装，而rpm -qa则列出了所有被安装的rpm包 1234$ rpm -qa | grep javatzdata-java-2013b-1.el6.noarchjava-1.6.0-openjdk-1.6.0.0-1.61.1.11.11.el6_4.x86_64java-1.7.0-openjdk-1.7.0.19-2.3.9.1.el6_4.x86_64 检查OpenJDK版本1234$ java -versionjava version &quot;1.7.0_19&quot;OpenJDK Runtime Environment (rhel-2.3.9.1.el6_4-x86_64)OpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode) 卸载Openjdk用root用户登录终端,rpm -e --nodeps 表示强制卸载某个rpm包，因为采用rpm -e删除时有时会出现... is needed by ...的依赖提示而不能卸载这个包123$ rpm -e --nodeps java-1.7.0-openjdk-1.7.0.19-2.3.9.1.el6_4.x86_64$ rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.61.1.11.11.el6_4.x86_64$ rpm -e --nodeps tzdata-java-2013b-1.el6.noarch 下载并安装jdk-7u17-linux-x64.rpm下载地址：http://pan.baidu.com/share/link?shareid=397488&amp;uk=638583574，`rpm -ivh 为安装某个rpm包的命令，参数ivh`各自的意义如下所示123-i, --install install package(s)-v, --verbose provide more detailed output-h, --hash print hash marks as package installs (good with -v) 而rpm -Uvh则表示升级一个软件包 12345678910111213141516$ cd /jdk1.7所在目录$ rpm -ivh jdk-7u17-linux-x64.rpmPreparing... ########################################### [100%] 1:jdk ########################################### [100%]Unpacking JAR files... rt.jar...Error: Could not open input file: /usr/java/jdk1.7.0_17/jre/lib/rt.pack jsse.jar...Error: Could not open input file: /usr/java/jdk1.7.0_17/jre/lib/jsse.pack charsets.jar...Error: Could not open input file: /usr/java/jdk1.7.0_17/jre/lib/charsets.pack tools.jar...Error: Could not open input file: /usr/java/jdk1.7.0_17/lib/tools.pack localedata.jar...Error: Could not open input file: /usr/java/jdk1.7.0_17/jre/lib/ext/localedata.pack以上那些错误可以忽略，不影响jdk到安装和使用 配置环境变量这是很关键的一步，jdk使用过程中绝大部分问题都跟环境变量的配置有关，需要配置的变量有JAVA_HOME，PATH和CLASSPATH,其中JAVA_HOME表示Java的安装目录，PATH是为了让系统在任何路径下都可以识别出java的命令，CLASSPATH则指定Java运行时查找class文件的路径，尤其需要注意CLASSPATH需要包含当前目录，也就是.，而且还要包含工具类库tool.jar；如果需要Swing包，还可以添加dt.jar。所以上面这三个变量的最简配置如下所示：12345$vi /etc/profile #在最后加入以下内容：JAVA_HOME=/usr/java/jdk1.7.0_17PATH=$PATH:$JAVA_HOME/binCLASSPATH=.:$JAVA_HOME/lib/tools.jarexport JAVA_HOME PATH CLASSPATH 使环境变量立即生效1$source /etc/profile 测试安装是否成功依次输入java,java -version,javac，看到输出信息即可,例如1234# java -versionjava version &quot;1.7.0_17&quot;Java(TM) SE Runtime Environment (build 1.7.0_17-b02)Java HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[C++ 一些基本语法]]></title>
      <url>%2F2015%2F11%2F24%2FC%2B%2B%E5%81%9AOJ%E6%97%B6%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
      <content type="text"><![CDATA[本文主要涉及到 C++ 一些基本语法，在做 oj 时经常用到，特此记录。 字符和字符串 数字转字符串：std::to_string(int) 字符串转数字：std::stoi(string) 上面两个函数均需要 #include&lt;string&gt; 字符串可以用数组方式来访问 字符串长度可用其length()函数获取 字符串可以通过substr(i，n)方法来提取子字符串,表示从第i个字符开始提取n个字符（包括i） 字符串大小写转换：利用STL中的transform函数，见下面的例子 12345678910111213#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;string&gt;using namespace std;int main()&#123; string s = "abcdEFg"; transform(s.begin(), s.end(), s.begin(), ::toupper); cout &lt;&lt; s&lt;&lt;endl; //输出ABCDEFG transform(s.begin(), s.end(), s.begin(), ::tolower); cout &lt;&lt; s &lt;&lt; endl; //输出abcdefg return 0;&#125; 单个字符大小写转换 需要记住ASCII码中A-65，a-97 将char c从大写转为小写可以通过下面代码 1234567891011121314if(c&gt;='A' &amp;&amp; c&lt;='Z') c=char(c+32);``` - 数字从char类型转为int类型 - 需要记住0对应的ASCII码为48 - 一个简单的例子如下```cppchar c='0';int a=int(c);cout&lt;&lt;a&lt;&lt;endl; //输出48int b=int(c)-48;cout&lt;&lt;b&lt;&lt;endl; //输出0 数组初始化数组（不初始化时为随机的地址值） 方法一：直接用数值初始化，见下面代码123int a[3]=&#123;1,2,3&#125; //元素依次为1,2,3int a[3]=&#123;0&#125; //元素依次为0,0,0int a[3]=&#123;1&#125; //元素依次为1,0,0 采用这种方法初始化时如果{}里面的元素的个数小于数组长度，则不足长度的元素默认值为0 方法二：for循环动态分配数组长度方法(一维) 方法一：通过vector实现，需要#include &lt; vector &gt; 方法二：通过malloc和free实现，如下面例子就初始化了一个长度为n的数组，且数组的值为从0到n-1（这个是继承了C分配内存的特性，C++可通过new和delete来实现，见方法三） 12345cin&gt;&gt;n;int *a=(int *)malloc(sizof(int)*n);for(int i=0;i&lt;n;i++) a[i]=i;free(a); 方法三：通过new和delete来实现,见下面代码 12345cin&gt;&gt;n;int *a=new int[n];for(int i=0;i&lt;n;i++) a[i]=i;delete []a; 容器vector 需要 #include &lt;vector&gt; vector的方法： vector中的元素可以以数组下标访问 push_back( ) 将一个元素放到vector中 vector.size( ) 获取vector的大小 查找元素t是否在vector中 1234T t;vector&lt;T&gt;::iterator it=find(vector.begin( ),vector.end( ),t);if(it == vector.end( ) ) cout&lt;&lt;"not found"; 其他方法（需要 #include &lt;algorithm&gt;）： sort(vector.begin( )，vector.end( ) ) //针对vector数据类型的排序 reverse(vector.begin( )，vector.begin( )+5 ) //针对vector数据类型的 反转 ,注意：reverse(vector.begin(),vector.begin()+5) 仅仅对5个元素进行reverse操作，不包括vector.begin()+5 map 需要 #include &lt;map&gt; 用数组下标的形式往map中添加元素和查找元素 当map的key为结构体类型时，可通过重载 &lt; 来判断该如何排序，见下面的代码 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;map&gt;using namespace std;struct stu&#123; string name; int sco; /*重载运算符 &lt;,达到从大到小或从小到大的排序效果，下面的代码是从小到大，如改成 return a.sco &gt;b.sco 则是从大到小 */ friend bool operator &lt; (const stu &amp;a, const stu &amp;b)&#123; return a.sco &lt; b.sco; &#125;&#125;;int main()&#123; map&lt;stu, int&gt; score; stu tmp; for (int i = 0; i &lt; 10; i++)&#123; tmp.name = to_string(i); tmp.sco = i; score[tmp] = i+1; &#125; //获取顺序排列时的第一个元素，从大到小还是从小到大要看重载的&lt; map&lt;stu, int&gt;::iterator it = score.begin(); cout &lt;&lt; it-&gt;first.name &lt;&lt; ' '&lt;&lt;it-&gt;second &lt;&lt; endl; //获取逆序排序的第一个元素，从大到小还是从小到大要看重载的&lt; map&lt;stu,int&gt;::reverse_iterator rit = score.rbegin(); cout &lt;&lt; rit-&gt;first.name &lt;&lt; ' ' &lt;&lt; rit-&gt;second &lt;&lt; endl; return 0;&#125; 遍历map 1234map&lt;T,T&gt; m;map&lt;T,T&gt;::iterator it;for(it=m.begin();it!=m.end();it++) cout&lt;&lt;it-&gt;first&lt;&lt;':'&lt;&lt;it-&gt;second&lt;&lt;endl; 输入输出 printf函数格式化数字的输出 数字前补0到达指定位数: 如12int a=3,b=4;printf("%04d %05d %d",a,b,b) //输出为 `0003 00004 4` 可通过scanf从输入的一定格式的字符串中提取数字如： 12int year,month,day;scanf("%d/%d/%d",&amp;year,&amp;month,&amp;day); //输入2014/09/06时，year=2014，month=9,day=6 cin或cout的类型为string时需要include 结构体的数据类型可以是别的结构体，也可以是自身结构体的指针 结构体内部也可以放函数，函数可用来初始化一个结构体]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux命令的系统调用和库函数的调用]]></title>
      <url>%2F2015%2F11%2F21%2FLinux%E5%91%BD%E4%BB%A4%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E5%92%8C%E5%BA%93%E5%87%BD%E6%95%B0%E7%9A%84%E8%B0%83%E7%94%A8%2F</url>
      <content type="text"><![CDATA[查看Linux命令的系统调用和库函数的调用可通过下面的命令。 strace -c command：判断command命令的系统调用的类型、次数、消耗时间（-f则连同command命令fork出来的子进程一同统计，-e指定列出某一具体的系统调用的参数） ltrace 用法同strace,但是追踪的是命令调用的库函数，strace追踪的是系统调用]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CPU缓存]]></title>
      <url>%2F2015%2F11%2F21%2FCPU%E7%BC%93%E5%AD%98%2F</url>
      <content type="text"><![CDATA[下面是CPU缓存的一些概念，所用命令均是在Linux平台下 可通过命令getconf -a| grep CACHE | grep size 查看CPU的各级缓存大小 也可以通过命令lscpu | grep ^L 查看 CPU缓存以行（line）单位，主内存以页（page）为单位，磁盘以块（block）为单位 CPU缓存一般分为指令缓存（I-Cache）和数据缓存（D-Cache），且两者一般都是分开的 缓存控制器（cache controller）判断CPU要获取的指令和数据是否在CPU缓存中，从一级缓存往下找，直到主内存和磁盘，且从找到的那一级开始往上面所有级缓存,如下图所示： 评判软件优秀与否的一种标准：对cpu缓存的命中率]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux的CPU调度]]></title>
      <url>%2F2015%2F11%2F21%2FLinux%E7%9A%84CPU%E8%B0%83%E5%BA%A6%2F</url>
      <content type="text"><![CDATA[内核版本与CPU调度算法早期 Linux 版本中的调度算法非常简单易懂：在每次进程切换时，内核扫描可运行进程的链表，计算进程的优先权，然后选择“最佳”进程来运行。这个算法的主要缺点是选择“最佳”进程所要消耗的时间与可运行的进程数量相关，因此，这个算法的开销太大，在运行数千个进程的高端系统中，要消耗太多的时间。 Linux 2.6 的调度算法就复杂多了。通过设计，该算法较好地解决了与可运行进程数量的比例关系，因为它在固定的时间内（时间复杂度O(1)）选中要运行的进程。它也很好地处理了与处理器数量的比例关系，因为每个 CPU 都拥有自己的可运行进程队列。而且，新算法较好地解决了区分交互式进程和批处理进程的问题。因此，在高负载的系统中，用户感到在 Linux2.6 中交互应用的响应速度比早期的 Linux 版本要快。 调度方式内核2.6版本有5种调度方式，分别是 SCHED_FIFO SCHED_RR SCHED_IDLE SCHED_BATCH SCHED_OTHER ##两种类型进程比较 两种进程 动态优先级进程(非实时进程) 静态优先级进程(实时进程) 优先级 低 高 能否调整 用户可调nice值，但最终由系统动态调整 用户可通过chrt改，系统不会动态调整 应用 一般用户进程 内核进程 调度方式 SCHED_IDLE(BATCH/OTHER) SCHED_FIFO(RR) top显示的pr值 大于0 RT或负值 由此可知，top命令显示的pr值越小，优先级越高 chrt工具通过chrt（change real time）工具可以改变实时进程优先级和所有进程的调度方式，其可调整的内容如下所示： 用top显示出的pr值与进程实际优先级的关系]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下的环境变量]]></title>
      <url>%2F2015%2F11%2F21%2FLinux%E4%B8%8B%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%2F</url>
      <content type="text"><![CDATA[linux环境变量种类按照生成周期看，可以分为二类 永久的（需要修改配置文件，变量永久生效） 临时的，使用export命令声明即可，变量在关闭shell时失效. 设置变量三种方法（1）在/etc/profile文件中添加变量(对所有用户生效,永久的)例如添加CLASSPATH变量， # vi /etc/profile JAVA_HOME=/usr/java/jdk1.7.0_17 JRE_HOME=/usr/java/jdk1.7.0_17/jre PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/ export JAVA_HOME JRE_HOME PATH CLASSPATH 要想马上生效，需要 source /etc/profile （2）在用户目录下的.bash_profile文件添加变量(对单一用户生效，永久的)（3）直接运行export , 对当前shell有效 环境变量查看 查看所有环境变量, 命令 env 查看单个 echo $CLASSPATH set查看本地定义环境变量 , unset可以删除指定环境变量 常用环境变量介绍 PATH 指定shell在那个目录下寻找命令或程序 HOME 当前用户登录名 HISTORY 历史记录 LOGNAME 当前用户登录名 HOSTNAME 指定主机名称 SHELL 当前shell类型 LANGUGE 语言相关环境变量 MAIL 当前邮件存放目录 PSI 基本提示符，对root 是# 普通用户$ 设置Linux的环境变量,语法解释 在修改了PATH值或任何环境变量后,都要用export将其输出,新的PATH值才能生效. PATH=\$PATH:路径1:路径2:...:路径n 意思是可执行文件的路径包括原先设定的路径,也包括从 路径1 到 路径n 的所有路径.当用户输入一个一串字符并按回车后,shell会依次在这些路径里找对应的可执行文件并交给系统核心 $PATH 表示原先设定的路径仍然有效,注意不要漏掉。 与DOS/Window不同,UNIX类系统环境变量中路径名用冒号分隔,不是分号.另外,软件越装越多,环境变量越添越多,为了避免造成混乱,建议所有语句都添加在文件结尾,按软件的安装顺序添加,格式如下()：# 软件名-版本号-安装日期]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[/etc/ld.so.conf文件详解]]></title>
      <url>%2F2015%2F11%2F21%2Fetc-ld-so-conf%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[可执行程序的类型Linux系统上有两类不同的Linux可执行程序。 第一类是静态链接的可执行程序。静态可执行程序包含执行所需的所有函数 — 换句话说，它们是“完整的”。因为这一原因，静态可执行程序不依赖任何外部库就可以运行。 第二类是动态链接的可执行程序。动态可执行程序是”不完整”的程序，它依靠外部共享库来提供运行所需的许多函数。 静态可执行程序与动态可执行程序比较我们可以用 ldd 命令来确定某一特定可执行程序是否为静态链接的： #ldd /sbin/sln not a dynamic executable “not a dynamic executable”是 ldd 说明 sln 是静态链接的一种方式。现在，让我们比较 sln 与其非静态同类 ln 的大小： # ls -l /bin/ln /sbin/sln -rwxr-xr-x 1 root root 23000 Jan 14 00:36 /bin/ln -rwxr-xr-x 1 root root 381072 Jan 14 00:31 /sbin/sln sln 的大小超过 ln 十倍。ln比sln小这么多是因为它是动态可执行程序. 动态链接相关性查看 ln 依赖的所有共享库的列表，可以使用 ldd 命令： # ldd /bin/ln libc.so.6 =&gt; /lib/libc.so.6 (0x40021000) /lib/ld-linux.so.2 =&gt; /lib/ld-linux.so.2 (0x40000000) 可见，ln 依赖外部共享库libc.so.6和ld-linux.so.2。通常，动态链接的程序比其静态链接的等价程序小得多。不过，静态链接的程序可以在某些低级维护任务中发挥作用。例如，sln 是修改位于 /lib中的不同库符号链接的极佳工具。但通常您会发现几乎所有 Linux 系统上的可执行程序都是某种动态链接的变体。 动态装入器 那么，如果动态可执行程序不包含运行所需的所有函数，Linux 的哪部分负责将这些程序和所有必需的共享库一起装入，以使它们能正确执行呢？答案是动态装入器（dynamic loader），它实际上是在 ln 的ldd 清单中看到的作为共享库相关性列出的 ld-linux.so.2 库。动态装入器负责装入动态链接的可执行程序运行所需的共享库。那么，动态装入器如何在系统上找到适当的共享库？ 动态装入器找到共享库要依靠两个文件 /etc/ld.so.conf 和 /etc/ld.so.cache /etc/ld.so.conf文件文件进行cat操作，您可能会看到一个与下面类似的清单： $ cat /etc/ld.so.conf /usr/X11R6/lib /usr/lib/gcc-lib/i686-pc-linux-gnu/2.95.3 /usr/lib/mozilla /usr/lib/qt-x11-2.3.1/lib /usr/local/lib ld.so.conf 文件包含一个所有目录（/lib 和 /usr/lib 除外，它们会自动包含在其中）的清单，动态装入器将在其中查找共享库。 /etc/ld.so.cache文件在动态装入器能“看到”/etc/ld.so.conf里面的信息之前，必须将它转换到 ld.so.cache文件中。可以通过运行 ldconfig 命令做到这一点： # ldconfig 当 ldconfig 操作结束时，您会有一个最新的 /etc/ld.so.cache 文件，它反映对 /etc/ld.so.conf 所做的更改。从这一刻起，动态装入器在寻找共享库时会查看在 /etc/ld.so.conf 中指定的所有新目录。 ldconfig 技巧 要查看 ldconfig 可以“看到”的所有共享库，可以输入： # ldconfig -p | less 还有另一个方便的技巧可以用来配置共享库路径。有时候希望告诉动态装入器在尝试任何 /etc/ld.so.conf 路径以前先尝试使用特定目录中的共享库。在运行的较旧的应用程序不能与当前安装的库版本一起工作的情况下，可以通过LD_LIBRARY_PATH这个环境变量来实现，计入需要指示动态装入器首先检查某个目录，需要将 LD_LIBRARY_PATH 变量设置成希望搜索的目录。多个路径之间用冒号分隔；例如： # export LD_LIBRARY_PATH=&quot;/usr/lib/old:/opt/lib&quot; 执行上面命令后，所有从当前shell启动的动态链接可执行程序都将使用 /usr/lib/old 或 /opt/lib 中的库，如果仍不能满足一些共享库相关性要求，则转回到 /etc/ld.so.conf 中指定的库。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux shell下删除文件名乱码文件]]></title>
      <url>%2F2015%2F11%2F21%2FLinux%20shell%E4%B8%8B%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E5%90%8D%E4%B9%B1%E7%A0%81%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[当文件名为乱码的时候，无法通过键盘输入文件名，所以在终端下就不能直接利用rm，mv等命令管理文件了。但是我们知道每个文件都有一个i节点号，我们可以考虑通过i节点号来管理文件。首先，我们要取得文件的i节点号。这个可以通过ls命令的-i选项获得得。 123#ls -i41697812 a 32983551 di 32983554 ethnet.c 32983543 hard_link32983542 de.c 32983544 ethnet 32983541 ethnet.h 32983543 kstat 每个文件名前面的数字就是文件的i节点号。有了文件的i节点号，我们就可以利用find命令的-inum选项配合常用的文件管理命令进行文件管理了。例如，如果要删除di文件，命令如下： 1# find . -inum 32983551 -exec rm &#123;&#125; \; 命令中的“{}”表示find命令找到的文件，要重命名一个文件，命令也很简单，如下： 12345678$ ls -i32983542 de.c 32983554 ethnet.c 32983543 hard_link 32983545 kstat.c32983544 ethnet 32983541 ethnet.h 32983543 kstat 32983681 sys_link$ find . -inum 32983542 -exec mv &#123;&#125; di.c \;$ ls -i32983542 di.c 32983554 ethnet.c 32983543 hard_link 32983545 kstat.c32983544 ethnet 32983541 ethnet.h 32983543 kstat 32983681 sys_link de.c文件被重命名为di.c了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux shell下常用快捷键]]></title>
      <url>%2F2015%2F11%2F21%2FLinux%20shell%E4%B8%8B%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
      <content type="text"><![CDATA[下述所有命令在 Linux/unix 的 shell 下有效，这里以 bash 为主。如有出入，以你自己的服务器为准。本文所指的 Linux 主要指 RHEL/CentOS，unix 指的是 FreeBSD，这也是服务器中用得最多的版本。 Ctrl + a切换到命令行开始这个操作跟 Home 实现的结果一样的，但 Home 在某些 unix 环境下无法使用，便可以使用这个组合；在 Linux 下的 vim，这个也是有效的；另外，在 windows 的许多文件编辑器里，这个也是有效的。 Ctrl + e切换到命令行末尾这个操作跟 END 实现的结果一样的，但 End 键在某些 unix 环境下无法使用，便可以使用这个组合；在 Linux 下的 vim，这个也是有效的；另外，在 windows 的许多文件编辑器里，这个也是有效的。 Ctrl + l清除屏幕内容效果等同于 clear Ctrl + u清除剪切光标之前的内容 Ctrl + k剪切清除光标之后的内容 Ctrl + y粘贴刚才所删除的字符此命令比较强悍，删除的字符有可能是几个字符串，但极有可能是一行命令。 Ctrl + r在历史命令中查找输入关键字就调出以前的命令了，强烈推荐，有时 history 比较多时，想找一个比较复杂的，直接在这里，shell 会自动查找并调用，方便极了。 Ctrl + c终止命令 Ctrl + z转入后台运行不过，由 Ctrl + z 转入后台运行的进程在当前用户退出后就会终止，所以用这个不如用 nohup 命令&amp;，因为 nohup 命令的作用就是用户退出之后进程仍然继续运行，而现在许多脚本和命令都要求在 root 退出时仍然有效。 Ctrl + d退出 shell，logout !!重复执行最后一条命令 history显示你所有执行过的编号+历史命令。这个可以配合!编辑来执行某某命令 !$显示系统最近的一条参数.比如我先用 cat /etc/sysconfig/iptables，然后我想用 vim 编辑。一般的做法是先用↑ 显示最后一条命令，然后用 Home 移动到命令最前，删除 cat，然后再输入 vim 命令。其实完全可以用 vim !$ 来代替。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我们这一代人的困惑]]></title>
      <url>%2F2015%2F11%2F20%2F%E6%88%91%E4%BB%AC%E8%BF%99%E4%B8%80%E4%BB%A3%E4%BA%BA%E7%9A%84%E5%9B%B0%E6%83%91%2F</url>
      <content type="text"><![CDATA[本文是于宙在TEDx大会上的演讲。这篇文章有点长，不过非常值得你花20分钟把它看完。 以下是演讲全文：大家下午好，很荣幸能够参加本次TEDx大会。 自我介绍：我是大连人，高中就读于大连市二十四中。因为当时学习十分不努力，所以高中毕业之后选择了出国留学。这其实是很多本科出国留学的人不能说的秘密，辗转了几个学校，最终毕业于美国印第安纳大学凯利商学院，主修投资和金融衍生品。 上学的时候迷恋炒股，学习依旧散漫，没能成为一个“放弃了华尔街的高薪工作毅然回国”的海归精英，真的颇为遗憾，因为实在没有什么华尔街的公司愿意要我。碰巧的是，毕业前两年股市和外汇的行情比较好，赚到了一点点资本，于是我决定回国做点生意。现在在大连从事餐饮行业，目前拥有4家芝士蛋糕店和3家火烧店。 引言大学毕业之后第一次面对这么多人做演讲，坦率地说，非常的紧张。虽然年轻的时候我曾经畅想过很多次，功成名就之后能像我曾经的那些偶像一样和年轻的朋友们分享一下我是如何从一无所有走上人生巅峰的经验，然后语重心长地告诉大家，人活着不能像一根草而是要像一棵树，能走到金字塔顶端的只有雄鹰和蜗牛两种动物，我的成功你也可以复制等等。 可是过了26岁之后我忽然意识到一个严肃的问题，就是自己的一生未必会取得很大的成就啊，所以当TEDxDUFE团队找到我说没关系即便你只是一个开小吃店的，我们也愿意为你提供这样一个和很多人交流思想的机会时，我的心情是多么地激动。因为公司还没上市，所以小草大树、雄鹰蜗牛、睡地板捡易拉罐这样的故事还不到说的时候。今天，只想和大家分享几个困扰了我和我身边的一些朋友十几年的问题，和在经历了一些变故和挫折后，我对这些问题的看法。 努力奋斗真的能实现梦想吗？大家现在可以想象一下汪峰老师坐在转椅上，深情地望着你，对你说，“你的梦想是什么？”周星驰老师的那句“做人如果没有梦想，和咸鱼有什么区别？”据说也激励了几代人。梦想这个东西是如此的重要，简直就是人生的一盏明灯。成功的人们成功的原因各不相同，但他们都不会忘记告诉你，无论到什么时候，都不曾忘记梦想，是他们成功的首要原因。以至于我们这一代人对于人生意义的最通常的理解，就在于坚持梦想并最终实现它。可很少有人愿意面对的一件事情是，大部分人的梦想永远，没错，永远都实现不了。 你没听错，大部分人的梦想永远都实现不了。 先和大家分享一个我之前的梦想。上大学的时候，我热衷于各式各样的赌博游戏，是学校旁边赌场的常客。我赌徒生涯的起点源于赌场里最基本游戏轮盘赌，轮盘上1到36个数字和两个0，赔率是1赔36。1到36分为红黑两色，押注红黑的赔率是1赔1。 作为一个合格的接受过九年义务教育的人都知道，每一次轮盘开始转动的那一刻，都是一次纯粹的独立随机事件。但是赌博这件事情的魅力就在于，当你真正身处赌场，看到已经连续4次开出红色的时候，几乎所有人都会想把筹码压在黑色的那一面。而我当时的梦想，就是破译这其中的奥秘。 我最初的策略非常简单，当连续三次开出奇数，就押注偶数，连续三次红色，就押注黑色。难以置信的事情发生了，在我严格地执行这个策略的情况下，前几次去赌场不但全身而退，每次都还赚了不少，以至于我产生了一种幻觉，也许游戏是有规律可循的，我已经看到了人生巅峰就在不远处向我招手。当然最终的结尾你们一定想到了，在经历过连续18个偶数，连续开出21次黑色后，我把之前赚到的钱都乖乖地还给了赌场。 后来我知道，我那个愚蠢的梦想叫做赌徒谬论，就不具体展开讲了。但它对我意义深刻，我终于明白了在纯粹的随机事件面前，一切规律都是无谓的。 生活中的事情有极个别和轮盘赌一样，属于纯粹的随机事件，比如双色球。可是几乎每一个中了双色球的人都会告诉你啊，他们花了多少精力去钻研往期号码，研究历史规律，付出了多少辛勤的努力，最终获得了成功。实际上，即使是纯粹由随机性主导的事情，只要参与的人的基数足够大，小概率事件总会发生。有趣的是，几乎所有在随机事件中的受益者，都会把这完全由运气决定的结果归功于自己的努力上。不仅仅是参与者本身，旁观者也会这么认为。再比如，中国好声音的冠军嘛。 我们生活中遇到的所有事情基本可以分为三类，第一类纯粹由随机性决定，比如布朗运动和轮盘赌博；第二类纯粹由能力决定，比如英语六级考试、110米栏之类；第三类，也是我们最常遇到的，由能力和随机性共同决定，比如创业、投资、恋爱或是梦想。 我对励志大师们总告诉年轻人要不惜一切代价追逐梦想感到深深厌倦的原因就在于，大多数人的梦想虽然不是纯粹的双色球，但也绝对是由随机性主导的。在强大的随机性面前，付出再多辛勤的汗水，就好比夜以继日蹲在轮盘赌旁边渴望参透其中规律。前面说到中国好声音的冠军，张碧晨的那一句 you are my destiny，听得我也是醉了。但毕竟那一刻，中国又有多少唱歌唱得和她一样好甚至更好的姑娘，如果真把成为好声音冠军作为一生的梦想，一生中都得在痛苦中度过。 我个人很喜欢黄渤，但绝对不会用黄渤作为例子去激励一个我这种长相差的年轻人不惜一切代价去追逐演员梦，注意是不惜一切代价。因为无论是唱歌还是演戏，再多的努力也只能让你变得很优秀，它们并不存在可以量化的评判标准，想成为万众瞩目的明星，随机性的重要程度都远远大于实力。 我想，一个人在年轻的时候，做的每一件事情，能清楚地区分其中随机性所占的比例并能心平气和地接受它，在我看来就是最宝贵的财富。 那么在你的梦想中，运气又扮演了多重要的角色呢？当你深深地感知到这件事情的随机性也许不会青睐于你，是否还愿意坚持下去呢？对我而言，梦想永远是值得执着追求的，但我可以无比心平气和地接受，它就是永远无法实现。 既然连梦都实现不了，还有什么事情值得努力呢去年这个时候，我发过一条微博: 这些年我一直提醒自己一件事情，千万不要自己感动自己。大部分人看似的努力，不过是愚蠢导致的。什么熬夜看书到天亮，连续几天只睡几小时，多久没放假了，如果这些东西也值得夸耀，那么富士康流水线上任何一个人都比你努力多了。人难免天生有自怜的情绪，唯有时刻保持清醒，才能看清真正的价值在哪里。 这段话在网上的疯传，是我始料不及的。更出乎我意料之外的是，我在评论中看到了相当一部分的骂声，还有人认真地给我写下了相当深刻的话，“你在拥有自己的光亮时不要吹熄别人的蜡烛，你不能因为你自己的不喜欢就否定别人。” 很莫名其妙是吧，即使你刚刚听完我上一段关于随机性的看法，你也会知道，我从来都不觉得努力是一件无所谓的事情。恰恰相反，我一直相信，在能力没达到一定程度之前，你连面对随机性的资格都没有啊。张碧晨能拿好声音冠军自然离不开运气，但换成杨幂，评委不但不会转身，可能直接撒腿就跑了。 可现在问题来了，那究竟什么才算是有价值的努力？这可以从我那条微博说起。去年这个时候，我和朋友在琢磨去大庆做点服装生意，决定去考察几个商场。我当时住在北京，因为之前晚上和朋友在外面玩得比较尽兴，回到家里已经比较晚了，担心睡觉睡过头会错过航班，那晚上就直接在沙发上靠了一晚。那是我第一次去哈尔滨，十一月份已经很冷了，衣服拿得不足，下了飞机冻得头疼。又因为没有提前订票，到了哈尔滨之后才买的火车票，发现就只剩站票了。于是，当我一晚上没睡，冻得头晕眼花，又在绿皮火车上站了两个多小时之后，抵达大庆的那一瞬间我觉得自己实在是太不容易了，将来必须要写进回忆录里面。可是，回头仔细一想，这些所谓的“努力”对我最终把那个服装生意做好，没有半毛钱关系。更何况，如果我前一天晚上能早点上床睡觉，多准备点衣服，提前在网上把火车票订好，完全可以舒舒服服地达到同样的目的。? 我的那次经历像是自己二十多年生活中很多事情的缩影，沉溺在对结果没有直接帮助只是因为自己遭受了一些痛苦的行为中，误以为那就是努力。 当我终于意识到我并不是唯一曾经把无意义的消耗当作努力的时候，忽然发现，原来生活中我觉得很努力的人，也许没那么勤奋，如果在正确的方向上坚持行动，超过他们也并不困难。 因为我们这一代人对于勤奋和努力的理解，几乎清一色地来自于学校，更精确地说，在前二十多年的生活中，我们眼中最努力的人，就是那些最能拼命看书和做题的人。实际上，这种理解是极其片面而幼稚的，因为看书和做题本身，都是为了一个极其鲜明的目的而存在的，就是通过考试。这种勤奋的付出极其纯粹，更多的复习时间，更高的复习强度，一般而言，都可以直接地提高考试的分数，它们之间的联系鲜明而直接，每个人都看得懂。 但生活的美妙之处却在于，很多事情在我们没做到一定程度之前，是完全没法理解的。 这就好比学英语，十几年漫长的岁月里我都在幻想，要通过多么复杂的流程，多么精密的设计，多么全面的涉及和多么不可思议的努力，终于有那么一天，或许我就能因为前期的那些无懈可击的学习，说一口比较流利的英语，像说中文一样，可以边说边想，而不是说每一句话之前设计它的句式时态词汇然后在心里复述几遍再看上去流利地背诵出来。谁不是这么设想的呢？可惜，它不仅从来没有实现，并且让我看不到有任何实现的趋势，对于每一个设立目标的人来说，没有比这更痛苦的感受。 但是在去了美国两年左右的时间之后，我忽然发现自己已经可以毫无障碍地说一口流利的英语了。这并非我采用了什么新的学习方法，而是因为去了印第安纳之后身边中国人很少，在没有选择的情况下，只能被迫用英语去交流和表达，在这个过程中，我并没有认真想过自己每天进步了多少，也没有阶段性的检验学习效果，只是不停地去听和说，因为没有选择嘛。直到两年多后的忽然有一天我才意识到，咦，自己好像真的已经可以了。但是我确实无法总结出来是如何一步一步做到的，只是那两年的时间，我一直都在很不情愿地用英语去生活嘛。 一个人能获得的最可贵的能力，都和掌握一门语言一样，你所付出的努力不是能够获得即时回馈的，甚至在很长的一段时间内没有任何收获，直到积累到了一定的阶段后，忽然爆发出惊人的力量，连你自己都不清楚这一切是如何发生的。比如锻炼身体，读书写作，或者是做生意。当你经历了足够的量变终于引起质变时拥有的技能，大部分人是终身难以企及的，不是因为他们太笨，恰恰相反，因为他们都太聪明了。 触发人类行动的最基本原理被称为反射，我们是需要即时回馈的物种。所以绝大多数人对于世界的理解度是线性的，但更多情况下，事物却是以漫长的潜伏震荡后爆发突破的形式发展的。我现在时常觉得，人在少年时期更容易掌握语言、乐器、美术这些成年后很难学的技艺，并非那小时候就是天资聪颖，而是小孩子很少会一个星期质疑一次自己收获了多少，都是闷头一练就是好几年，直到学会了才知道哦自己已经会了。只有聪明的成年人，才相信一本书读懂易经，10句话揭秘马云的成功之道，30天成为吉他高手的故事。 简而言之，现实生活中，付出和结果之间往往没有那么立竿见影。在离开学校之后，当我们遇到的很多事情不再像做题和考试之间联系得那么紧密的时候，很多人的付出都是浅尝辄止的。而最可贵的努力，是选择一个正确的方向，那些无法立即获得回报的事情，依然能付出十年如一日的专注和热情，最终的结果也许不足以让你独孤求败，但足以出类拔萃。 人这一生中是否有一个节点，过了之后一切都会好起来前面说了这么多，谈论的都与目标和实现目标有关。仔细想想，我们的一生好像都是在实现目标中挣扎着度过的。上初中的时候，老师告诉你，中考的淘汰率是最高的，只要闯过去，上了高中一切就好了。但上了高中的时候发现不是那么回事嘛，高中老师又说了啊，考上大学就进了天堂。于是你考上了大学，依然空虚迷茫各种草样年华，父母老师又告诉你，找到工作就好了。工作之后发现烦恼和忧虑依然都在，女朋友给你看马云的故事，告诉你等你事业有成就好了…… 你发现了吗，其实人这一辈子的每一个阶段都有新的痛苦和顾虑，周而复始，生生不息。绝对不会因为你考上大学，事业有成，迎娶了女神就从此 happily ever after。但每一个阶段也有每一个阶段的快乐，无法替代。生活不是安徒生童话也不是好莱坞电影，从出生的那一刻起直到生命的尽头，都不存在什么节点，过去了之后一切幸福美满无忧无虑。 每一段岁月都有它存在的价值，没有高低贵贱之分，都不应该被辜负。而我能想到的人这一生能做的最愚蠢的事情，就是把全部人生的希望都孤注一掷到未来的某个节点上，而忽略了生活本身应有的乐趣。哪怕你以后真正实现了那个执念中的目标，才会发现它远远没你想的那么美好。 年轻的时候和哥们在操场上打篮球喝可乐的快乐，是以后高尔夫球会所里品红酒替代不了的。尤其男生，千万不要总想着等将来有钱了如何如何，且不说你以后很可能不会太有钱，而且相信我，就是有钱了也真的不能怎么样。生命就在每天的生活里，一切执念都是虚妄。和身边的人愉快相处，认真安排好每一天的活动，用心去感受每一天的心境，就是生活的意义本身。这其实是我今天最想分享给你们的事情。 谢谢大家。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[详解Python程序与服务器连接的WSGI接口]]></title>
      <url>%2F2015%2F11%2F20%2F%E8%AF%A6%E8%A7%A3Python%E7%A8%8B%E5%BA%8F%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9E%E6%8E%A5%E7%9A%84WSGI%E6%8E%A5%E5%8F%A3%2F</url>
      <content type="text"><![CDATA[文章为转载，原文见这里，侵删 这篇文章主要介绍了Python程序与服务器连接的WSGI接口,是Python网络编程学习当中的重要内容,需要的朋友可以参考下 了解了HTTP协议和HTML文档，我们其实就明白了一个Web应用的本质就是： 1.浏览器发送一个HTTP请求;2.服务器收到请求，生成一个HTML文档;3.服务器把HTML文档作为HTTP响应的Body发送给浏览器;4.浏览器收到HTTP响应，从HTTP Body取出HTML文档并显示。 所以，最简单的Web应用就是先把HTML用文件保存好，用一个现成的HTTP服务器软件，接收用户请求，从文件中读取HTML，返回。Apache、Nginx、Lighttpd等这些常见的静态服务器就是干这件事情的。如果要动态生成HTML，就需要把上述步骤自己来实现。不过，接受HTTP请求、解析HTTP请求、发送HTTP响应都是苦力活，如果我们自己来写这些底层代码，还没开始写动态HTML呢，就得花个把月去读HTTP规范。正确的做法是底层代码由专门的服务器软件实现，我们用Python专注于生成HTML文档。因为我们不希望接触到TCP连接、HTTP原始请求和响应格式，所以，需要一个统一的接口，让我们专心用Python编写Web业务。这个接口就是WSGI：Web Server Gateway Interface。WSGI接口定义非常简单，它只要求Web开发者实现一个函数，就可以响应HTTP请求。我们来看一个最简单的Web版本的“Hello, web!”：123def application(environ, start_response): start_response('200 OK', [('Content-Type', 'text/html')]) return '&lt;h1&gt;Hello, web!&lt;/h1&gt;' 上面的application()函数就是符合WSGI标准的一个HTTP处理函数，它接收两个参数： environ：一个包含所有HTTP请求信息的dict对象; start_response：一个发送HTTP响应的函数。 在application()函数中，调用start_response(&#39;200 OK&#39;, [(&#39;Content-Type&#39;, &#39;text/html&#39;)])就发送了HTTP响应的Header，注意Header只能发送一次，也就是只能调用一次start_response()函数。start_response()函数接收两个参数，一个是HTTP响应码，一个是一组list表示的HTTP Header，每个Header用一个包含两个str的tuple表示。 通常情况下，都应该把Content-Type头发送给浏览器。其他很多常用的HTTP Header也应该发送。然后，函数的返回值Hello, web!将作为HTTP响应的Body发送给浏览器。 有了WSGI，我们关心的就是如何从environ这个dict对象拿到HTTP请求信息，然后构造HTML，通过start_response()发送Header，最后返回Body。整个application()函数本身没有涉及到任何解析HTTP的部分，也就是说，底层代码不需要我们自己编写，我们只负责在更高层次上考虑如何响应请求就可以了。 不过，等等，这个application()函数怎么调用?如果我们自己调用，两个参数environ和start_response我们没法提供，返回的str也没法发给浏览器。所以application()函数必须由WSGI服务器来调用。有很多符合WSGI规范的服务器，我们可以挑选一个来用。但是现在，我们只想尽快测试一下我们编写的application()函数真的可以把HTML输出到浏览器，所以，要赶紧找一个最简单的WSGI服务器，把我们的Web应用程序跑起来。 好消息是Python内置了一个WSGI服务器，这个模块叫wsgiref，它是用纯Python编写的WSGI服务器的参考实现。所谓“参考实现”是指该实现完全符合WSGI标准，但是不考虑任何运行效率，仅供开发和测试使用。 我们先编写hello.py，实现Web应用程序的WSGI处理函数： 1234#hello.pydef application(environ, start_response): start_response('200 OK', [('Content-Type', 'text/html')]) return '&lt;h1&gt;Hello, web!&lt;/h1&gt;' 然后，再编写一个server.py，负责启动WSGI服务器，加载application()函数： 1234567891011# server.py# 从wsgiref模块导入:from wsgiref.simple_server import make_server# 导入我们自己编写的application函数:from hello import application # 创建一个服务器，IP地址为空，端口是8000，处理函数是application:httpd = make_server('', 8000, application)print "Serving HTTP on port 8000..." # 开始监听HTTP请求:httpd.serve_forever() 确保以上两个文件在同一个目录下，然后在命令行输入python server.py来启动WSGI服务 启动成功后，打开浏览器，输入http://localhost:8000/，就可以看到结果了 再看看刚刚打开的cmd窗口，会输出请求的如下所示 可以看到总共有两个请求源，其中LC-PC是通过电脑浏览器输入localhost访问,而192.168.1.109是通过手机浏览器访问的（电脑和手机在同一局域网下）；输出的内容包括请求的ip，时间和http请求头。 无论多么复杂的Web应用程序，入口都是一个WSGI处理函数。HTTP请求的所有输入信息都可以通过environ获得，HTTP响应的输出都可以通过start_response()加上函数返回值作为Body。复杂的Web应用程序，光靠一个WSGI函数来处理还是太底层了，我们需要在WSGI之上再抽象出Web框架，进一步简化Web开发。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Python简明教程》学习笔记]]></title>
      <url>%2F2015%2F11%2F19%2F%E3%80%8APython%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[《python简明教程》python入门的一个非常好的文档。如需下载，请点击这里(密码：grjj)。最近又看了一遍，把里面一些容易忽略的知识点记录下来。 运算符 ** 乘方，2**3=8 // 取商的整数部分, 10//3=3 &amp; 按位与, 5&amp;3=1 | 按位或，5|3=7 ^ 按位异或，5^3=6 ~ 按位取反，~5=-6，取反后数的正负反转，且正数要比负数绝对值小1 and、or、not就是Java里面的&amp;&amp;、||、！，但是在python里面&amp;&amp;、||、！是非法字符 数据结构 python里面常用的数据结构有列表（list）、元组（tuple）、字典（dic)和集合(set) list,用方括号[]括起来,里面元素用逗号分隔，元素有序可变 增加元素用append()方法，删除元素用del方法 tuple，用圆括号()括起来,里面元素用逗号分隔，元素有序不可变 仅有一个元素时也要加,，避免与运算符优先级混淆 常用于打印语句，如print &quot;name:%s,age: %d&quot; %(name,age) dic，用花括号{}括起来,里面元素用逗号分隔，元素无序可变，每个元素是用冒号:分隔的键值对 通过dic[k]访问元素，dic[newK]=newV 增加元素,del dic[k] 删除元素 遍历的方法为 for k,v in dic.items():,实际上dic.items()会返回一个元组的列表 判断某一个k是否在dic if k in dic: 序列，序列不是一种具体的数据结构，而是一类数据结构，字符串，列表和元组均属于序列，序列有一些通用的方法 通过len()获取序列的长度 表示序列的范围：[a:b]表示下表a到b-1，[a:]表示从下标a到最后一个,[:b]表示从第一个(也就是0)到下标b-1,[:]表示所有元素,默认步长为1，也可以添加多一个参数变为[a:b:c],这时步长为c，与range()函数相似 对于一个序列，复制这个序列与给这个序列使用一个别名不同，详见下面代码 123456list1=[1,2,3]list2=list1 #别名list3=list1[:] #复制del list1[1]print list2print list3 控制语句 python的控制语句有if，while，for，continue，break，但是没有switch if、while判断条件的判断条件均没有括号 while、for语句均有一个 可选的 else语句，while语句条件不成立时退出while并执行else语句；for语句中的else语句则在for循环后执行一次；但是如果在while和for循环里面遇到break退出程序，则不会执行else语句 for i in k:语句里，k可以是任何序列，包括字符串、列表、元组、字典等 for里面常用的range函数的范围不包括第二个参数，默认步长为1，可通过第三个参数选择步长 12range(1,5) --&gt; 1,2,3,4range(1,5,2)--&gt;1,3 函数 函数没有return语句时默认返回一个None（没有任何东西的特殊类） pass语句表示一个空的语法句 文档字符串：用来描述函数或者类的功能，一般格式：首行以大写字母开始，句号结尾。第二行是空行，从第三行开始是详细的描述 1234def printMax(x, y):&apos;&apos;&apos;Prints the maximum of two numbers. The two values must be integers.&apos;&apos;&apos; 文档字符串也可以通过函数或类的__doc__属性获取 一些常见的函数 range()，一般用于for循环 dir(module),返回module的名称列表，不提供module值时默认是当前module len(arr),用于获取序列arr的长度，arr可以是list、tuple、dic或者是string os.system(command)，执行系统命令command time.strftime()，按指定格式输出当前系统时间,如time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) time.sleep(t),一般用于循环，让系统休眠t秒 string.startswith(str),字符串对象的方法，判断字符串string是否以 str开头 string.find(str),字符串对象的方法，判断 str是否在字符串string里面 string.join(list),字符串对象的方法，用字符串 string 连接list列表 模块 可以导入的模块有两种：（1）标准模块,常见的有os、sys（2）自定义模块，就是.py结尾的python文件 import操作会在sys.path列出的目录列表里面查找需要import的模块 为了使导入模块的操作更快，会在第一次导入模块时创建模块的pyc文件（字节编译文件） 每个模块都有一个__name__,当模块被直接执行时，该值为__main__ 类 语法 class 类名: 类方法与普通函数形式上最大区别在于类方法的第一个形参必须为self,且该形参不需要实参，self类似于Java中的this指针 __init__为构造函数，__del__为析构函数 所有的类成员都是公开的，而以双下划线开头的成员是属于类的，是私有的，作为惯例，一般属于类的成员都以单下划线开头 继承,语法 class son(father):,注意子类不会自动调用父类的构造函数，因此必须显示调用，但是不调用时也没有报错。 异常捕获 语法 123456try： 可能抛出异常的方法except (错误或异常的元组)： #没有指定异常时捕获所有的错误异常 处理finally： 无论是否有异常均要执行 其他一些用法 python 程序第一行的 #！ 目的是确定使用哪个解释器，一般在Linux平台下写的是 #!/usr/bin/env python 而不是直接写python解析器的位置，目的是即使程序在其他机子上跑时也能够找得到python解析器；而且程序迁移到windows下也不会报错问题，但是 #!/usr/bin/env python 并不适用于Windows默认的cmd窗口，假如直接在dos/powershell下输入python的文件名，是会用文件关联的程序打开（关联程序可以通过右键设置打开方式来设定）。也可在Windows下安装一个git shell，在git shell 下输入python的文件名即可执行。 可以通过内置的help( )函数来找到函数和类的帮助信息，实际上help( )是通过抽取文档字符串（DocStrings，也就是函数和类的__doc__属性）并打印输出的 字符串或者程序在一行放不下时通过反斜杠\可以在下一行继续写, 字符串前加上u或者U表示采用Unicode编码,字符串的连接可以使用加号+ print语句的一些技巧：在for循环的print最后加逗号可以避免分行；print str*i可以重复输出i个str 强制类型转换一般形式是被转换的内容用圆括号括起来，类型不用扩。如str(5) 是从整数转为字符串 ，int(raw_input(&#39;input an integer&#39;)) 是从字符串转为整形 列表综合：从一个已有的列表导出一个新的列表。实例如下： 12list1=[1,2,3,4,5]list2=[i*2 for i in list1 if i&gt;2] #list2=[6,8,10] 也可从一个列表导出一个字典(dict只能接受一个参数，这里为一个列表)12l1=[1,2,3]timesten=dict([(v,v*10) for v in l1 if i &gt;1])#timesten=&#123;2:20,3:30&#125; 函数形参为元组和字典：可用*和**加在形参前，代表这是元组和字典，但是不加也能够正常使用。加上的目的是为了能让函数接受不定参数,例子如下： 12345def add(x, *args): total = x for arg in args: total += arg return total 这里的 *args 表示参数数目不定，可以看成一个元组，把第一个参数后面的参数当作元组中的元素。运行 1234&gt;&gt;&gt;print add(1, 2, 3, 4)10&gt;&gt;&gt;print add(1, 2)3 这样定义的函数不能使用关键词传入参数，要使用关键词，可以这样： 123456def add(x, **kwargs): total = x for arg, value in kwargs.items(): print "adding ", arg total += value return total 这里的 **kwargs 表示参数数目不定，相当于一个字典，关键词和值对应于键值对。运行如下 12345&gt;&gt;&gt;print add(10, y=11, z=12, w=13)adding yadding zadding w46]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[写给大四]]></title>
      <url>%2F2015%2F11%2F18%2F%E5%86%99%E7%BB%99%E5%A4%A7%E5%9B%9B%2F</url>
      <content type="text"><![CDATA[文章为转载，作者不详 无论你是找工作了，还是保研了，我们或多或少对未来的不确定性感到彷徨和焦虑，看完希望对你有用。 昨天有个同学来找我，谈毕业选择的事情。谈完以后她突然过来拥抱了我一下。为此我深受鼓舞，决定给你们写一点东西。你们正处在一个艰难的阶段。我想以下面这段文字，争取给你们提供点正能量。虽然写了很长，但不会谈如何申学校、找什么工作、要不要保研这种具体的问题。我想谈的是，如何排解你心中的恐慌。 焦虑的本质大四是一个焦虑的时期。你们的烦恼有时候是具体问题带来的压力，有时候却是无端的、莫名其妙的，有时候还极易受到外界的影响，别人一句话就会激起内心难以遏制的波澜。所有这些焦虑、恐慌、敏感，在我看来，其实归根结底是源于三件事： 人生的有限性 未来的不确定性 过去的不可更改性。 这三件事是人生不可回避的三道阴影。每当想到它们我就想写诗。 “我要如何准备才能胜出？”“我会失败吗？”这是未来的不确定性在你的内心敲打。 “我走这条路是不是多浪费一两年的时间？”“我会不会走弯路？”这是人生的有限性在摆弄你。 “我的选择是不是错了？”“这么大的投入最后回报得来吗？”，你们患得患失，因为没有后悔药可以吃。 怎么样？其实每天让你感到痛苦的，不是那些具体的事，而是上面这些萦绕不去的疑问。我开宗明义地拔高到人生的高度来剖析，不只是因为我一以贯之的文艺与深沉，更是因为我觉得下面这个前提非常重要：如果你感到的恐惧，是人生某些不可改变的特性带来的，那么你拼命地想抓住什么、甚至你终于完成了一个目标、得到了一块暂时的喘息之地，你仍然解决不了你内心的痛苦。 这就是为什么你们没拿到Offer的时候很煎熬，拿到了offer的又患得患失；等到6月份还没出路的痛苦无比，早在9月份就保研的同样不开心。痛苦最多可以缓解，永远得不到解决。这次痛苦比较小的人，下次痛苦没准会很大。 由于人生的规律没法改变，所以要想解决内心的痛苦，外面如何挣扎和抓取都没用，最终只能靠内心的成长。 怎么破？如果现在要打的是人生的终极大boss——所谓人最大的敌人就是自己。那么我这个水平，当然不知道怎么破。不过我已经感觉到了人内心成长的历程，可以跟你们分享。 首先你们要重视自己的理性。人有一个不断强大的头脑，和一颗充满恐惧、贪欲和脆弱的玻璃心。人的大多数行为仿佛更倾向于受到情绪的驱动，而不是头脑的指示。你的头脑作为弱势群体，要学会和内心对话，了解你内心深藏的力量和驱动，并争取引导它，逐渐达到心智的合一。 这是一个漫长的成长过程，一些人最终变得更有智慧，大气、从容、坚定，能够更好地把握自己的人生。但绝大多数人终此一生也无法主宰自己，像浮萍一样地随风浪摇摆。这其中可能包括你的父母。 但我常常感觉到，情绪的力量非常强大，理性的作用一开始往往有限。你可以很容易明白很好的道理，但是你控制不了自己，尤其在受到外界干扰的时候。你内心成长的心智就像一颗小树苗，在它尚还柔弱的时候，恐怕经不起太严酷的风吹雨打。外部的环境太恶劣了，再强大的内心都有可能被压倒。 所以，我的经验是：内外双修。 跟自己的内心对话深感艰难的阶段，我们要多跟自己讲讲道理。听别人讲也行，自己跟自己讲也行。其实被情绪所操纵的心灵很傻的，说的多了，它就会慢慢相信。这就是为什么要多跟女人说我爱你的原因。你们可以经常给自己说说这些道理： 相信命运我这么说不是要你们去算命。而是说作为受过大学教育的人，你们要明白事物的不可操控性。我们高中学习的知识和成长的经历，容易给我们塑造一种观念，就是万事是因果相连的。有了条件，就可以推算出答案。这个世界由根本的、简明的自然规律和公理推动运转，一切都可以预测和解释。好好学习，一般来说就可以考上好大学。能不能达到预设的结果，全在于能不能满足一定的条件，比如说有没有努力。 可惜由于我们的大学教育整体比较失败，所以前面形成的观念基本得不到修正和挑战。如果你大学继续学习数理化，你就知道你高中的那些定理和公理都是17世纪牛顿时代的东西。20世纪出现的相对论、量子理论、混沌理论，说明连自然界都不是那么确定无疑的。微观来看，因果关系固然牢靠，但是一旦到了宏观，变量可能太过复杂，复杂到超越人在有限时空里可以理解或掌握的程度。 自然是这样，人生也是这样。有的事情变量更简单，可把握性更强；有的事情变量更复杂，可把握性更小。高考就算可把握性比较强的事情，虽然也有个别点背的。但是学习这种事情，绝对是社会现象中的特例。你一生里遇到的其他事，其可把握性都远远低于在学校里的生活。 现在你们即将踏入社会，社会给你们上的第一课，就是全然不同于学校学习的“低把握性逻辑”。你想要找一条好的出路，只能尽量为之准备，但结果如何，非常地“混沌”。我给你们讲过很多我身边的故事。我大学同班，学习最好没有留在北京，最像共产党员的去了外企，最不上课的当了公务员，最北京味儿的出了国，最学术的之一当了导演，最不像搞学术的一个如今在大学当老师，就是我。 我不是想告诉你们世事无常，躺下来等着天上掉铁饼就好了。我有一个同学进了外交部，她一直想进外交部；还有一个同学也在当大学老师，他当年确实是一个学霸。但这两个同学当时并不比别的同学更努力，他们现在也并不比别的同学更成功。 你们必须学会理解和接受人生的偶然性。未知当然让人恐惧，但这是你心灵的自然反应。你的头脑可以相信：每个人都有一条路。这条路有很多错综复杂的因素促成，但一定由一个人在当时当地能够做的最好的选择组成。所以你提前恐慌或者不恐慌，并不能改变什么。走上去以后，就好好地走，才能精彩。 没有弯路踏上社会之前的恐慌，很大程度上来自于“输在起跑线”上的幻想。真正的人生不是一场竞赛；至少也不是一场跑向同一终点的竞赛。这也是高考养成的逻辑。从进大学开始就已经不管用。 我也给你们讲过很多故事。习近平青年时候被下放到农村种了七年地，22岁才回城读大学。胡锦涛大学毕业以后到西北修水坝去了。奥巴马和克林顿都是穷小子，好不容易考上了个东部的常青藤，结果一个回老家干社区工作，一个回老家在名不见经传的大学教书。按照前面的逻辑，他们都输在了起跑线上。 如果你们真要有志向当个人生的赢家，可以多看看那些有成就的人。大多数伟人的人生都充满了波折，有的还几起几落。可是从后往前看，哪里又有什么弯路？你当过村支书，别人没有当过村支书，几十年后，竞争国家主席你就有优势。 你可能不想当伟人，只不过不想吃苦。但前面已经说过，你们的人生总有一条路，有时候会是弯的，有时候是直的。不要只想走直路。直路开的太快，便无心欣赏风景，难有宽广的眼界和丰富的心灵，于是你缺少快乐的潜质。总之所有的努力都不会白费。在哪儿努力都行。 我自己定大四这种时候，你们每天最好给自己打打气：凡事我做主。 我不知道你们会有一个什么样的爸妈，总之他们很有可能会很烦。有的平时很开明，到了这种时候，也偶尔冒出一两句不懂事的话，让你平添无意义的难受。七大姑八大姨都会打电话。放个假回家就会不停地问。哥们闺蜜死党什么的纷纷提建议。同班同学这呀那的，你努力从每一个传闻中获得给自己的启示。 最后，你接收了很多信息，询问了很多建议，却迷失了自己。其实你的事情只有你最懂，别人都不行。从今天开始，你要立足于自己。你希望别人帮你，其实是害怕对可能的失败负全责。可是，你的前途，谁能替你负责？真正的自信，就从自负其责开始；真正的自由，也是从自负其责开始。 改变外围环境除了常常与自己的内心对话，你们还要争取为这种对话营造一个好的外在环境，靠近阳光雨露，远离风吹雨打。 不要来烦我我好像试图挑拨你们与父母的关系。其实我的意思是：到了这个年龄，你们必须开始重塑你们与父母之间的关系。在这个特殊的阶段，最关心你们前途的是你们的父母；最影响你们心情往往也是他们。你们需要父母的关心。但如果他们跟你一样被情绪所驱动，缺乏坚定与理性，时常而来的关心不过是一种情绪发作后的排解。那你就要明确告诉他们：请克制自己的恐慌，多给我传递正能量。 父母的建议可能有用。但多半用处不大。如果生活在农业时代，你基本上只需要按照父母说的做就行了，因为你们两代人的人生没有什么大的区别。可是现如今，你在外地的父母怎么知道北京的事？又怎么能知道美国的事？如果要沟通，让父母多给你分享一些人生感悟，而不是具体建议。 同样的的道理，远离负能量的种种议论，除非你们能相互鼓励。远离打扰你内心平静的环境，为内心的对话保留空间。如果可能的话，尽量地给周围的人一些正能量。如果你确定你知道什么是正能量的话。 不要拖沓这个时候你们都在为了各种事做准备。不管在追求什么，制定好计划，有规律地生活，该做的事不要拖沓。当你感到愈发恐慌的时候，往往是有该做的事却没做的时候。那件事在你的心底大声地发出嘲讽，让你越发地焦虑和没有自信。如果你所努力的事情按部就班、取得进展，你的情绪化的心灵会驯服很多，讲的道理它才会耐心去听。 完成小心愿一个年轻的心灵，最需要灌溉的泉水，就是自信。做一些能够让你提升自信的事情。这些事情可能都很小，但是短期能看到效果，取得进步，或者得到结果。你们如果排除无谓的焦虑，其实有相当的时间可以做这些事情。 出去短途旅游。独自计划、独自出行、独自完成，回来后你会觉得自己成长了。 读一些书，讲给没读过的人听。那人最好不是大四。 练习一项体育运动，看到自己的进步。 做任何你想做、靠相对简单的努力能够做成的事情。 希望有用我花了两天的时间来构思，又用一整天的时间把这些话写出来，中间鼓起了很多次的勇气，要自己坚持下去。一方面我很不愿意说教，虽然我经常说教。我对你们说教，实际上我自己和内心对话的一个形式，所以无比真诚。你们叫我人生导师，可我的人生也亟需引导。目前只能进行 人生有的阶段不可逾越，有的痛苦必须体验。身处其中的人很难解脱。评论的人常常是站着说话不腰疼。比如现在如果有个老同志对我说：“你年轻人不要为买不起房、排不上车号、每个月存不下钱而焦虑。这些都是虚妄的。”我一方面觉得他说的有道理，另一方面心底有一个强大的声音在呼喊：“把你的给我嘛”。 许多道理，只有走过了，回转来才能真正体会。上学期我在毕业典礼上做的发言，在社会上有不少媒体刊登和转载，光各种稿费单子就收了一堆。可是在学校里的同学们，感受就隔了一层。他们找到了一个黑我的新办法。一度我只要问“为什么”，他们就会说“因为我们来自山顶”。 所以，接下来我再问为什么，你们可以说：“请先跟你的内心对话。” 我现在内心里的想法，就是希望你们接下来这一年不被虚度。这是你们人生中最美好时代中的一年，它不是拿来过渡、等待或者牺牲的。现在这一年才刚刚开始。 大四加油！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[听说你觉得生活很无聊]]></title>
      <url>%2F2015%2F11%2F18%2F%E5%90%AC%E8%AF%B4%E4%BD%A0%E8%A7%89%E5%BE%97%E7%94%9F%E6%B4%BB%E5%BE%88%E6%97%A0%E8%81%8A%2F</url>
      <content type="text"><![CDATA[文章为转载，作者：马德，作家，已出版《当我放过自己的时候》、《允许自己虚度时光》等。 （一） 一辈子活下来，常常是，在最有意思的时候，没有有意思地过，在最没意思的时候，想要有意思地过结果却再也过不出意思。 或者，换一种表述就是，在看不透的时候，好看的人生过得不好看；看透了，想过得好看，可是人生已经没法看了。 这句话说得并不绕。其实，人生比这个绕多了。 人生就是这样的一场游戏：在欲望浮沉中，把生命扔到很远很远，最后，只为了找到很近很近的那个简单的自己。 （二） 有一年，到大连旅游，参观旅顺日俄监狱。印象中，地牢般的监狱，只有很窄的一方窗户开在地上，可以看到人世的阳光。 在一孔窗户周围，看到一茎绿草，小小的，嫩嫩的，在风中摇曳。我想，这应是在那里苦难度日的囚犯们，所能见到的全部蓬勃和生机了吧。但是，那么多的监牢，每一孔窗户前，会恰好有一粒草的种子落在那里吗？会有生命的绿意，落在绝望的人生里吗？ 那得多么幸运啊！ 而我们的窗外，就有蓝天白云，我们的身边，就有鲜花绿草，没有谁囚禁我们，但我们却囚禁了自己。 常常是，在追不上的时候，才去追；在味道尽去的时候，才想品；在不得已时候，才珍惜得已；在人生的大片美好过到支离破碎后，才去捡拾一些碎片，拼凑美好。 （三）生活就是一个七天接着一个七天。 不是日子重复导致了枯燥和无聊，而是你枯燥无聊，把气撒在了日子的重复上。 其实，都在重复。位高权重的，富可敌国的，没有谁的日子不是一个七天接着另一个七天。只不过，当你仰慕谁，就会美化对方的重复，认为人家重复得有趣味有意义。其实，这一切，都是仰慕的光环散发出的五彩。 重复，赋予每个人的本质和意义都是一样的。 多重复才算重复呢？你看那些一天到晚打麻将的人，每天面对的就是那一百多张牌，然后，洗牌，码牌，打牌，和牌。论理说，该盯得头晕眼花，坐得腰酸腿疼，琢磨得心力交瘁了吧，但嗜打的人从来乐此不疲，没有一个喊累的，也没有一个喊重复的。 为什么呢？上瘾。 其实，有瘾，才是快乐生活的关键。瘾，就是情趣，它会让每一个日子，像绽开的花朵，一寸一寸阳光踩过的花瓣，无论多重复，都会美得各不相同。 （四） 活得没滋味的时候，去坐坐北京地铁，从1号线到15号线，在上班的早高峰。 你一下子就释然了。当然了，一下子也更崩溃了。 密密麻麻的人，如雨前的蚁，簇拥着，没有喧闹，没有声响，是令人压抑的寂静。几乎不用走，“哗”被推上车，“哗”又被挤下车。就这样，每天，还未曾上班呢，两三个小时，先折耗在了路上。无论你蓄了多少激情和活力，也会被日复一日地磨蚀殆尽。关键是，还有下班呢，还有一个晚高峰等着呢。 谁比谁活得更容易？ 但，即便这样，一定也有活得幸福的“北漂”。幸福的人生活里不是没有不堪和琐碎，不是没有疲惫和失望，而是不管生活给了多大的泥淖，也要让生命拔腿出来，临清流，吹惠风，也要在心中修篱种菊，怡养内在的优雅和高贵。 幸福是一种自我剥离的能力，以及自我生成的能力。生活中，没有多少幸福是现成的，有幸福的人，只是会幸福罢了。 （五） 一个整宿睡得很好的人，会嫉妒一个睡眠质量不怎么好、甚至半宿还会醒一会儿的人。乍听，简直不可思议。再解释，你就明白了。原来，那个睡得很“好”的人，是靠安定这种镇静药片睡过一个晚上又一个晚上的。 如果不说透，从表面上看，应该是后者羡慕甚至嫉妒前者才是。因为，前者太好了，好得简直无与伦比。 生活，有多少是我们看透了本质的。你羡慕的权贵，前呼后拥，看起来那么风光，可是风光背后有多少痛苦，对方不说，你不会知道；你羡慕的富有，宝马香车，锦衣玉食，看起来，是那么荣华，这荣华背后有多少痛苦，对方不说，你不会知道。 也就是说，即便失点眠，你依然是那个睡得很好的人。即便过得平凡而宁静，你也会赢得别人羡慕。甚至，这里边，那些你羡慕着的人也在羡慕你。 只是，你要知道，这个世界没有一个人愿把这种羡慕轻易告诉你。 转载作者：马德，作家，已出版《当我放过自己的时候》、《允许自己虚度时光》等。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[修改Linux主机名]]></title>
      <url>%2F2015%2F10%2F23%2F%E4%BF%AE%E6%94%B9Linux%E4%B8%BB%E6%9C%BA%E5%90%8D%2F</url>
      <content type="text"><![CDATA[修改Linux主机可分为临时修改和永久修改 临时修改：通过hostname NewHostName命令临时修改，修改后通过exit注销后重新登录就可以在命令提示符看到新的主机名称，但是重启后会失效 永久修改：修改配置文件/etc/sysconfig/network中的HOSTNAME项，修改后需要重启才能生效。 另外一个与主机名相关的配置文件是/etc/hosts,但是这个文件与本机的主机名无关，一般是用来记录网络上其他主机的ip和主机名的对应关系，其作用有点像本地的DNS服务器。 比如说本机没有配置DNS服务器，与本机在同一个局域网的一台名为memcached的主机的ip是192.168.0.23，那么此时本机输入命令ping 192.168.0.23能够ping通这个ip，但是如果ping memcached则ping不通了，因为本机不知道memcached这台主机的ip是多少，同时也无法向DNS查询。这时可以在本机的/etc/hosts文件中添加这样一行1192.168.0.23 memcached 表示这个ip和这个主机名的对应关系。然后再ping主机memcached就可以ping通了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CentOS下安装python的sklearn模块]]></title>
      <url>%2F2015%2F10%2F23%2FCentOS%E4%B8%8B%E5%AE%89%E8%A3%85python%E7%9A%84sklearn%E6%A8%A1%E5%9D%97%2F</url>
      <content type="text"><![CDATA[sklearn是python的一个机器学习库，封装了很多常用的机器学习算法，在数据挖掘中经常会用到这个库。在MacOS上安装只需要一条命令，可以说是最简单的。在windows下也可以直接安装封装好这些模块的程序如winpython等。但是我在CentOS下的安装可谓痛苦，用pip安装会有各种依赖，用yum解决依赖又会因为支持sklearn的库（如numpy，scipy）版本不够新而导致sklearn无法安装。 最后还是在官方的安装指南中找到了解决方法，所以官方文档真的是非常有参考价值，而且权威性也是很好的。下面说说具体的解决方法。 解决方法实际上是安装一个类似于Windows下winpython的程序，在Linux中就是Anaconda,可以认为Anaconda是封装了python和sklearn等第三方库的一个程序。下面讲一下安装步骤以及注意事项。 安装非常简单： 1.下载安装文件安装可以选择32位或者64位以及python的版本（提供2.7和3.5），下面下载的是64位的python 2.7 版本的andconda，若要下载其他版本的请移步到https://www.continuum.io/downloads 1wget https://3230d63b5fc54e62148e-c95ac804525aac4b6dba79b00b39d1d3.ssl.cf1.rackcdn.com/Anaconda2-2.5.0-Linux-x86_64.sh 2.运行安装文件1bash Anaconda2-2.5.0-Linux-x86_64.sh 在安装过程中会询问安装的目录，默认是当前目录下创建anaconda2目录，也可以自己在询问时输入指定目录。安装到最后还会问是否要在~/.bashrc中添加环境变量，添加环境变量的作用是为了方便某些命令的输入如python、conda等。连配置环境变量的功夫都省去了。 3.安装第三方模块因为Anaconda采用了conda作为包管理程序，所以更新或卸载已安装的模块、安装其他模块都可以用conda命令。如conda update scikit-learn可以更新已经安装了的sklearn，conda remove scikit-learn可以卸载已安装的sklearn。conda install可以安装新的第三方模块。简直就是一个加强版的python shell。 到这里安装就结束了，比起之前折腾的一个一个包来安装的要方便得多。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[修改Linux下bash的命令提示符]]></title>
      <url>%2F2015%2F10%2F11%2F%E4%BF%AE%E6%94%B9Linux%E4%B8%8Bbash%E7%9A%84%E5%91%BD%E4%BB%A4%E6%8F%90%E7%A4%BA%E7%AC%A6%2F</url>
      <content type="text"><![CDATA[Linux终端的命令提示符能够显示诸如当前用户，主机名，时间等信息，根据自己实际需要配置自己的命令提示符能够使得工作更为便利。下面介绍一下修改命令提示符的方法。 根据用户的不同，可以为系统所有用户修改命令提示符，也可以为单一用户修改命令提示符。两者的区别仅仅是修改的配置文件不同，前者需要修改/etc/profile文件，后者则只需要修改用户主目录下的 .bashrc文件。 修改的内容就是PS1这个环境变量，一种修改方法如下所示： 1export PS1=&apos;[\u@\H: \d \t \w \$]&apos; 将上面的内容添加到当前用户的~/.bashrc文件中，各个参数的具体含义后面会讲。 但是此时配置还没生效，需要运行下面的命令让该文件配置立即生效。1source ~/.bashrc 生效后可以看到命令提示符变成了下面的形式： 1[root@memcached1: Sat Jan 23 03:03:13 /etc/sysconfig #] 此时便可以明白了上面修改的PS1的各个参数的含义了，\u表示当前用户，\H表示主机名，\d表示当前日期，而且格式是“weekday month date”\t表示当前时间，24小时制，格式是”HH:MM:SS“,\w表示用完整路径表示当前的目录，\$表示提示字符，root用户用#,一般用户用$。 除了上面提到的几个比较常用的参数外，还有下面一些参数及其含义： 参数 含义 \W 显示当前路径的相对路径，小写的w是完整路径 \T 表示当前时间，12小时制，格式是”HH:MM:SS“ \# 执行的第几个命令 \v BASH的版本信息]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Centos下安装python2.7]]></title>
      <url>%2F2015%2F03%2F23%2FCentos%E4%B8%8B%E5%AE%89%E8%A3%85python2-7%2F</url>
      <content type="text"><![CDATA[Centos是一个Linux 发行版，因为稳定性得到了比较广泛的应用，但是存在着软件版本不够新的问题。比如说python版本为2.6，但是python 2.7对第三方模块的支持往往更好，下面就说一下怎么在Centos下安装python2.7。 简介需要注意的是，系统内部的一些命令依赖python环境运行（比如说yum），所以假如卸载系统自带的python环境会导致这些程序不能运行，所以建议不要动原来系统自带的python 而在另外一个路径安装python 2.7，调用python命令时调用这个安装路径的python路径即可。 安装利用了miniconda,里面集成了python 和 conda，conda 可以简单认为是一个包管理系统，允许在同一台机器上安装同一软件的多个版本。 安装步骤下载安装脚本1wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh 运行安装脚本1sh Miniconda-latest-Linux-x86_64.sh -b -p /usr/local/miniconda -p 参数会指定安装的目录,可以根据自己的修改 修改环境变量因为系统自带的 python 2.6 命令路径为 /usr/bin/python,而/usr/bin本来就存在系统的环境变量PATH中，所以假如输入python的时候希望进入python 2.7，那么python2.7 的环境变量就必须添加在PATH前，因为系统是从前往后读PATH变量的。 修改~/.bashrc 或 /etc/profile（前者针对的是当前用户，后者针对的是全部用户，可参考这篇文章） , 在文件末尾添加环境变量如下: 1export PATH=/usr/local/miniconda:$PATH /usr/local/miniconda是你安装python的目录，一定要添加在$PATH前，否则输入python还是会跑回原来2.6版本的python。 然后输入source ~/.bashrc或source /etc/profile让配置生效。 这时输入python应该就能看到 python 2.7 了。 安装第三方模块因为上面安装的miniconda中除了python还安装了conda，而conda提供了包管理机制，所以可以通过conda安装第三方模块。 如conda install numpy就安装了numpy模块。更详细内容参考conda官方文档。]]></content>
    </entry>

    
  
  
</search>
